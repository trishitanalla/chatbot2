<html>
    <head>
        <meta charset="utf-8">
        
            <script src="lib/bindings/utils.js"></script>
            <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/vis-network/9.1.2/dist/dist/vis-network.min.css" integrity="sha512-WgxfT5LWjfszlPHXRmBWHkV2eceiWTOBvrKCNbdgDYTHrT2AeLCGbF4sZlZw3UMN3WtL0tGUoIAKsu8mllg/XA==" crossorigin="anonymous" referrerpolicy="no-referrer" />
            <script src="https://cdnjs.cloudflare.com/ajax/libs/vis-network/9.1.2/dist/vis-network.min.js" integrity="sha512-LnvoEWDFrqGHlHmDD2101OrLcbsfkrzoSpvtSQtxK3RMnRV0eOkhhBN2dXHKRrUU8p2DGRTk35n4O8nWSVe1mQ==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
            
        
<center>
<h1>Machine Learning Knowledge Graph</h1>
</center>

<!-- <link rel="stylesheet" href="../node_modules/vis/dist/vis.min.css" type="text/css" />
<script type="text/javascript" src="../node_modules/vis/dist/vis.js"> </script>-->
        <link
          href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/css/bootstrap.min.css"
          rel="stylesheet"
          integrity="sha384-eOJMYsd53ii+scO/bJGFsiCZc+5NDVN2yr8+0RDqr0Ql0h+rP48ckxlpbzKgwra6"
          crossorigin="anonymous"
        />
        <script
          src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/js/bootstrap.bundle.min.js"
          integrity="sha384-JEW9xMcG8R+pH31jmWH6WWP0WintQrMb4s7ZOdauHnUtxwoG2vI5DkLtS3qm9Ekf"
          crossorigin="anonymous"
        ></script>


        <center>
          <h1>Machine Learning Knowledge Graph</h1>
        </center>
        <style type="text/css">

             #mynetwork {
                 width: 100%;
                 height: 95vh;
                 background-color: #ffffff;
                 border: 1px solid lightgray;
                 position: relative;
                 float: left;
             }

             
             #loadingBar {
                 position:absolute;
                 top:0px;
                 left:0px;
                 width: 100%;
                 height: 95vh;
                 background-color:rgba(200,200,200,0.8);
                 -webkit-transition: all 0.5s ease;
                 -moz-transition: all 0.5s ease;
                 -ms-transition: all 0.5s ease;
                 -o-transition: all 0.5s ease;
                 transition: all 0.5s ease;
                 opacity:1;
             }

             #bar {
                 position:absolute;
                 top:0px;
                 left:0px;
                 width:20px;
                 height:20px;
                 margin:auto auto auto auto;
                 border-radius:11px;
                 border:2px solid rgba(30,30,30,0.05);
                 background: rgb(0, 173, 246); /* Old browsers */
                 box-shadow: 2px 0px 4px rgba(0,0,0,0.4);
             }

             #border {
                 position:absolute;
                 top:10px;
                 left:10px;
                 width:500px;
                 height:23px;
                 margin:auto auto auto auto;
                 box-shadow: 0px 0px 4px rgba(0,0,0,0.2);
                 border-radius:10px;
             }

             #text {
                 position:absolute;
                 top:8px;
                 left:530px;
                 width:30px;
                 height:50px;
                 margin:auto auto auto auto;
                 font-size:22px;
                 color: #000000;
             }

             div.outerBorder {
                 position:relative;
                 top:400px;
                 width:600px;
                 height:44px;
                 margin:auto auto auto auto;
                 border:8px solid rgba(0,0,0,0.1);
                 background: rgb(252,252,252); /* Old browsers */
                 background: -moz-linear-gradient(top,  rgba(252,252,252,1) 0%, rgba(237,237,237,1) 100%); /* FF3.6+ */
                 background: -webkit-gradient(linear, left top, left bottom, color-stop(0%,rgba(252,252,252,1)), color-stop(100%,rgba(237,237,237,1))); /* Chrome,Safari4+ */
                 background: -webkit-linear-gradient(top,  rgba(252,252,252,1) 0%,rgba(237,237,237,1) 100%); /* Chrome10+,Safari5.1+ */
                 background: -o-linear-gradient(top,  rgba(252,252,252,1) 0%,rgba(237,237,237,1) 100%); /* Opera 11.10+ */
                 background: -ms-linear-gradient(top,  rgba(252,252,252,1) 0%,rgba(237,237,237,1) 100%); /* IE10+ */
                 background: linear-gradient(to bottom,  rgba(252,252,252,1) 0%,rgba(237,237,237,1) 100%); /* W3C */
                 filter: progid:DXImageTransform.Microsoft.gradient( startColorstr='#fcfcfc', endColorstr='#ededed',GradientType=0 ); /* IE6-9 */
                 border-radius:72px;
                 box-shadow: 0px 0px 10px rgba(0,0,0,0.2);
             }
             

             

             
        </style>
    </head>


    <body>
        <div class="card" style="width: 100%">
            
            
            <div id="mynetwork" class="card-body"></div>
        </div>

        
            <div id="loadingBar">
              <div class="outerBorder">
                <div id="text">0%</div>
                <div id="border">
                  <div id="bar"></div>
                </div>
              </div>
            </div>
        
        

        <script type="text/javascript">

              // initialize global variables.
              var edges;
              var nodes;
              var allNodes;
              var allEdges;
              var nodeColors;
              var originalNodes;
              var network;
              var container;
              var options, data;
              var filter = {
                  item : '',
                  property : '',
                  value : []
              };

              

              

              // This method is responsible for drawing the graph, returns the drawn network
              function drawGraph() {
                  var container = document.getElementById('mynetwork');

                  

                  // parsing and collecting nodes and edges from the python
                  nodes = new vis.DataSet([{"color": "#8afcee", "id": "Supervised Learning", "label": "Supervised Learning", "shape": "star", "size": 25, "title": "Learning process where a model is trained on labeled data to predict outcomes."}, {"color": "#2e3b9d", "id": "Regression Problem", "label": "Regression Problem", "shape": "dot", "size": 10, "title": "Statistical approach to predicting a continuous outcome variable based on one or more predictor variables."}, {"color": "#8bf479", "id": "Classification Problem", "label": "Classification Problem", "shape": "dot", "size": 10, "title": "Type of supervised learning where the target variable takes on discrete values."}, {"color": "#52fb6d", "id": "Hypothesis", "label": "Hypothesis", "shape": "dot", "size": 10, "title": "Function learned by a model to predict outcomes based on input data."}, {"color": "#0a23fd", "id": "Linear Regression", "label": "Linear Regression", "shape": "star", "size": 25, "title": "Technique for modeling the relationship between a scalar dependent variable y and one or more explanatory variables X."}, {"color": "#6c142a", "id": "Feature Selection", "label": "Feature Selection", "shape": "dot", "size": 10, "title": "Process of selecting which features to include in a model to improve performance."}, {"color": "#34cee5", "id": "Machine Learning Basics", "label": "Machine Learning Basics", "shape": "star", "size": 25, "title": "Fundamental concepts in machine learning including training and test datasets."}, {"color": "#a8152b", "id": "Gradient Descent", "label": "Gradient Descent", "shape": "dot", "size": 10, "title": "Optimization algorithm used to minimize a function by iteratively moving towards the minimum value of that function."}, {"color": "#a3e8d0", "id": "LMS Update Rule", "label": "LMS Update Rule", "shape": "dot", "size": 10, "title": "Specific update rule derived from cost function for a single training example."}, {"color": "#3c82e8", "id": "Widrow-Hoff Learning Rule", "label": "Widrow-Hoff Learning Rule", "shape": "dot", "size": 10, "title": "Alternative name for the LMS update rule, used in adaptive filters and neural networks."}, {"color": "#64490b", "id": "Error Term", "label": "Error Term", "shape": "dot", "size": 10, "title": "Difference between actual output and predicted value that guides parameter updates."}, {"color": "#a4f921", "id": "Batch Gradient Descent", "label": "Batch Gradient Descent", "shape": "dot", "size": 10, "title": "Version of gradient descent that uses the entire dataset to make a single update."}, {"color": "#50f931", "id": "Matrix Derivatives", "label": "Matrix Derivatives", "shape": "star", "size": 25, "title": "Derivation of function f with respect to matrix A."}, {"color": "#60f2ba", "id": "Gradient Calculation", "label": "Gradient Calculation", "shape": "dot", "size": 10, "title": "Process of calculating gradients for parameters to update model weights."}, {"color": "#cd050b", "id": "Least Squares Revisited", "label": "Least Squares Revisited", "shape": "star", "size": 25, "title": "Revisiting least squares using matrix derivatives."}, {"color": "#f5c8fb", "id": "Design Matrix", "label": "Design Matrix", "shape": "dot", "size": 10, "title": "Matrix containing training examples\u0027 input values in its rows."}, {"color": "#a82096", "id": "Vector y", "label": "Vector y", "shape": "dot", "size": 10, "title": "n-dimensional vector with target values from the training set."}, {"color": "#dbe63d", "id": "Machine Learning Algorithms", "label": "Machine Learning Algorithms", "shape": "star", "size": 25, "title": "Overview of machine learning algorithms including gradient descent and stochastic methods."}, {"color": "#667225", "id": "Locally Weighted Linear Regression (LWLR)", "label": "Locally Weighted Linear Regression (LWLR)", "shape": "dot", "size": 10, "title": "A variant of linear regression where weights are assigned to data points based on their proximity to the point being predicted."}, {"color": "#2cde2b", "id": "Weight Calculation", "label": "Weight Calculation", "shape": "dot", "size": 10, "title": "Determines how much influence each training example has on the prediction at a given query point."}, {"color": "#deb6a2", "id": "Bandwidth Parameter (\u03c4)", "label": "Bandwidth Parameter (\u03c4)", "shape": "dot", "size": 10, "title": "Controls the rate of decay in weight as distance from the query point increases."}, {"color": "#d5697f", "id": "Learning a model for an MDP", "label": "Learning a model for an MDP", "shape": "star", "size": 25, "title": "Techniques to estimate the transition dynamics of an environment from data."}, {"color": "#6f62a9", "id": "Continuous state MDPs", "label": "Continuous state MDPs", "shape": "dot", "size": 10, "title": "Handling environments with a continuous state space in reinforcement learning."}, {"color": "#cc69a4", "id": "Discretization", "label": "Discretization", "shape": "dot", "size": 10, "title": "Approaches to converting continuous states into discrete representations for RL algorithms."}, {"color": "#f0d50d", "id": "Value function approximation", "label": "Value function approximation", "shape": "dot", "size": 10, "title": "Methods for estimating value functions in environments with a large or infinite number of states."}, {"color": "#7001a8", "id": "Connections between Policy and Value Iteration (Optional)", "label": "Connections between Policy and Value Iteration (Optional)", "shape": "dot", "size": 10, "title": "Theoretical connections and relationships between policy iteration and value iteration methods."}, {"color": "#cd9929", "id": "LQR, DDP and LQG", "label": "LQR, DDP and LQG", "shape": "star", "size": 25, "title": "Control theory concepts applied to reinforcement learning problems."}, {"color": "#37af8d", "id": "Finite-horizon MDPs", "label": "Finite-horizon MDPs", "shape": "dot", "size": 10, "title": "Analysis of Markov decision processes with a fixed time horizon."}, {"color": "#2b1aef", "id": "Linear Quadratic Regulation (LQR)", "label": "Linear Quadratic Regulation (LQR)", "shape": "dot", "size": 10, "title": "Special case of finite-horizon setting in reinforcement learning with linear transitions and quadratic rewards."}, {"color": "#9f94a0", "id": "From non-linear dynamics to LQR", "label": "From non-linear dynamics to LQR", "shape": "dot", "size": 10, "title": "Approaches to apply LQR in nonlinear dynamic environments"}, {"color": "#027785", "id": "Linearization of dynamics", "label": "Linearization of dynamics", "shape": "dot", "size": 10, "title": "Techniques for linearizing nonlinear system dynamics"}, {"color": "#cdc450", "id": "Differential Dynamic Programming (DDP)", "label": "Differential Dynamic Programming (DDP)", "shape": "dot", "size": 10, "title": "Optimization technique used for trajectory optimization in robotics and control systems."}, {"color": "#9394c9", "id": "Linear Quadratic Gaussian (LQG)", "label": "Linear Quadratic Gaussian (LQG)", "shape": "dot", "size": 10, "title": "Combination of LQR with stochastic dynamics and noisy measurements"}, {"color": "#09f044", "id": "Policy Gradient (REINFORCE)", "label": "Policy Gradient (REINFORCE)", "shape": "star", "size": 25, "title": "Model-free algorithm for learning randomized policies without value functions."}, {"color": "#282955", "id": "Supervised Learning Introduction", "label": "Supervised Learning Introduction", "shape": "star", "size": 25, "title": "Introduction to supervised learning concepts and examples"}, {"color": "#65a576", "id": "Machine Learning Concepts", "label": "Machine Learning Concepts", "shape": "star", "size": 25, "title": "Overview of key concepts in machine learning including EM algorithm and variational inference."}, {"color": "#408243", "id": "Underfitting", "label": "Underfitting", "shape": "dot", "size": 10, "title": "Scenario where a model is too simple to capture the underlying pattern of the data."}, {"color": "#6cf0a2", "id": "Overfitting", "label": "Overfitting", "shape": "dot", "size": 10, "title": "Situation where a model fits noise in training data rather than underlying pattern."}, {"color": "#b4fa6f", "id": "Locally Weighted Linear Regression (LWR)", "label": "Locally Weighted Linear Regression (LWR)", "shape": "dot", "size": 10, "title": "Regression algorithm that assigns more weight to nearby points when making predictions."}, {"color": "#7f8c94", "id": "Maximum Likelihood Estimation (MLE)", "label": "Maximum Likelihood Estimation (MLE)", "shape": "dot", "size": 10, "title": "Process for estimating the parameters that maximize the probability of observing the training data."}, {"color": "#bd30b1", "id": "Least Squares Regression", "label": "Least Squares Regression", "shape": "dot", "size": 10, "title": "Method for fitting a line to data points by minimizing squared errors."}, {"color": "#94bfed", "id": "Probabilistic Assumptions", "label": "Probabilistic Assumptions", "shape": "dot", "size": 10, "title": "Assumptions about the probability distribution of class labels given input features."}, {"color": "#4017f7", "id": "Cost Function J(\u03b8)", "label": "Cost Function J(\u03b8)", "shape": "dot", "size": 10, "title": "Function to be minimized, representing error between predictions and actual values."}, {"color": "#d7a2ba", "id": "Function Representation", "label": "Function Representation", "shape": "dot", "size": 10, "title": "How functions are represented and approximated in machine learning models."}, {"color": "#5d55dd", "id": "Linear Function Approximation", "label": "Linear Function Approximation", "shape": "dot", "size": 10, "title": "Approximating the target function using linear equations with parameters."}, {"color": "#a789cc", "id": "Parameters (Weights)", "label": "Parameters (Weights)", "shape": "dot", "size": 10, "title": "Coefficients that define the form of the linear approximation."}, {"color": "#202d66", "id": "Cost Function", "label": "Cost Function", "shape": "dot", "size": 10, "title": "Measures the performance of a machine learning model by quantifying the error between predicted and actual values."}, {"color": "#9dbf0a", "id": "Ordinary Least Squares", "label": "Ordinary Least Squares", "shape": "dot", "size": 10, "title": "A special case of GLM where target variable is continuous and modeled as Gaussian distribution."}, {"color": "#f1e4b2", "id": "LMS Algorithm", "label": "LMS Algorithm", "shape": "dot", "size": 10, "title": "Iterative method to minimize the cost function by adjusting parameters."}, {"color": "#723c06", "id": "Convex Function", "label": "Convex Function", "shape": "dot", "size": 10, "title": "Function with a single global minimum, ensuring gradient descent convergence."}, {"color": "#e1078c", "id": "Stochastic Gradient Descent", "label": "Stochastic Gradient Descent", "shape": "dot", "size": 10, "title": "Algorithm that uses random samples to update parameters, reducing computational load."}, {"color": "#1eea7c", "id": "Optimization Problem", "label": "Optimization Problem", "shape": "star", "size": 25, "title": "Mathematical problem of finding the best solution from all feasible solutions."}, {"color": "#eb31e1", "id": "Gradient Descent Methods", "label": "Gradient Descent Methods", "shape": "star", "size": 25, "title": "Techniques for minimizing cost function in machine learning."}, {"color": "#86a025", "id": "Learning Rate Decay", "label": "Learning Rate Decay", "shape": "dot", "size": 10, "title": "Gradually decreases learning rate to ensure convergence."}, {"color": "#ba0870", "id": "Normal Equations Method", "label": "Normal Equations Method", "shape": "star", "size": 25, "title": "Direct method for minimizing cost function without iteration."}, {"color": "#4b1a20", "id": "Least-Squares Cost Function", "label": "Least-Squares Cost Function", "shape": "dot", "size": 10, "title": "Method to find the best fit line by minimizing the sum of squared residuals."}, {"color": "#fe0e74", "id": "Probabilistic Interpretation", "label": "Probabilistic Interpretation", "shape": "dot", "size": 10, "title": "Interpreting linear regression from a probabilistic perspective, assuming Gaussian noise."}, {"color": "#112eaa", "id": "Invertibility of X^TX Matrix", "label": "Invertibility of X^TX Matrix", "shape": "dot", "size": 10, "title": "Condition for the matrix to be invertible in least-squares solution."}, {"color": "#b88cbe", "id": "Gaussian Distribution Assumption", "label": "Gaussian Distribution Assumption", "shape": "dot", "size": 10, "title": "Assumes Qi is a Gaussian distribution with independent coordinates, mean governed by q(x;phi), variance by v(x;psi)."}, {"color": "#f484c3", "id": "Normal Equations", "label": "Normal Equations", "shape": "dot", "size": 10, "title": "Direct method for finding the optimal parameters in linear regression without iteration."}, {"color": "#c3892a", "id": "Probability Distribution", "label": "Probability Distribution", "shape": "dot", "size": 10, "title": "Distribution of y given x and parameters \u03b8."}, {"color": "#b1c8e6", "id": "Design Matrix X", "label": "Design Matrix X", "shape": "dot", "size": 10, "title": "Matrix containing all input variables x(i)."}, {"color": "#4bd8b8", "id": "Likelihood Function", "label": "Likelihood Function", "shape": "dot", "size": 10, "title": "Function that measures how likely a set of parameters is, given observed data."}, {"color": "#c03933", "id": "Independence Assumption", "label": "Independence Assumption", "shape": "dot", "size": 10, "title": "Assumption on \u03b5(i)\u0027s leading to product form likelihood."}, {"color": "#7f5cd5", "id": "Maximum Likelihood Estimation", "label": "Maximum Likelihood Estimation", "shape": "dot", "size": 10, "title": "Estimating parameters to maximize probability of observed data under model."}, {"color": "#941004", "id": "Log Likelihood", "label": "Log Likelihood", "shape": "dot", "size": 10, "title": "Expression for log likelihood in terms of joint probability distribution."}, {"color": "#3bebff", "id": "Learning Rate (\u03b1)", "label": "Learning Rate (\u03b1)", "shape": "dot", "size": 10, "title": "Hyperparameter controlling the size of each step in gradient descent."}, {"color": "#3d5ca0", "id": "Update Rule", "label": "Update Rule", "shape": "dot", "size": 10, "title": "Rule for updating parameters in each iteration of gradient descent."}, {"color": "#a6939a", "id": "Locally Weighted Linear Regression", "label": "Locally Weighted Linear Regression", "shape": "dot", "size": 10, "title": "A method for learning to estimate future states from current state-action pairs."}, {"color": "#4d9214", "id": "Non-Parametric Algorithms", "label": "Non-Parametric Algorithms", "shape": "dot", "size": 10, "title": "Algorithms where the amount of information needed grows with the dataset size."}, {"color": "#a8e069", "id": "Parametric Algorithms", "label": "Parametric Algorithms", "shape": "dot", "size": 10, "title": "Algorithms that have a fixed number of parameters regardless of data size."}, {"color": "#389efb", "id": "Binary Classification", "label": "Binary Classification", "shape": "dot", "size": 10, "title": "Classification task where output is binary, using logistic function to predict probabilities."}, {"color": "#121f36", "id": "Logistic Regression", "label": "Logistic Regression", "shape": "dot", "size": 10, "title": "Statistical method for modeling binary dependent variables by estimating probabilities using a logistic function."}, {"color": "#481394", "id": "Linear Regression Approach", "label": "Linear Regression Approach", "shape": "dot", "size": 10, "title": "Initial attempt to solve classification with linear regression."}, {"color": "#fa9e7c", "id": "Logistic Function", "label": "Logistic Function", "shape": "dot", "size": 10, "title": "Function used to convert logit values into probability estimates for classification tasks."}, {"color": "#5c860d", "id": "Derivative of Sigmoid", "label": "Derivative of Sigmoid", "shape": "dot", "size": 10, "title": "Calculation showing the derivative of the sigmoid function."}, {"color": "#8a1827", "id": "Machine Learning Models", "label": "Machine Learning Models", "shape": "star", "size": 25, "title": "Overview of models used in machine learning including classification and regression."}, {"color": "#d6c5bd", "id": "Classification Model", "label": "Classification Model", "shape": "dot", "size": 10, "title": "A model that predicts categorical class labels based on input features."}, {"color": "#c74660", "id": "Log-Likelihood", "label": "Log-Likelihood", "shape": "dot", "size": 10, "title": "Natural logarithm of the likelihood function for easier computation and optimization."}, {"color": "#d56916", "id": "Gradient Ascent", "label": "Gradient Ascent", "shape": "dot", "size": 10, "title": "Optimization technique used to maximize a function by iteratively moving in the direction of steepest ascent."}, {"color": "#5780b8", "id": "Logistic Regression Derivation", "label": "Logistic Regression Derivation", "shape": "dot", "size": 10, "title": "Derivation of logistic regression formulae."}, {"color": "#da6c10", "id": "Perceptron Algorithm", "label": "Perceptron Algorithm", "shape": "dot", "size": 10, "title": "Attempts to find a linear decision boundary between classes."}, {"color": "#f8befc", "id": "Multi-class Classification", "label": "Multi-class Classification", "shape": "dot", "size": 10, "title": "Classification problem where response variable can take on multiple values."}, {"color": "#21cf91", "id": "Classification and Logistic Regression", "label": "Classification and Logistic Regression", "shape": "dot", "size": 10, "title": "Classifying data into discrete categories using logistic regression."}, {"color": "#4a7113", "id": "Perceptron Learning Algorithm", "label": "Perceptron Learning Algorithm", "shape": "dot", "size": 10, "title": "Algorithm for learning linear classifiers."}, {"color": "#2ef97c", "id": "Maximizing l(theta)", "label": "Maximizing l(theta)", "shape": "dot", "size": 10, "title": "Alternative algorithm for maximizing the likelihood function."}, {"color": "#eff6c7", "id": "Generalized Linear Models", "label": "Generalized Linear Models", "shape": "dot", "size": 10, "title": "Extension of linear models to non-normal distributions."}, {"color": "#364142", "id": "Exponential Family", "label": "Exponential Family", "shape": "dot", "size": 10, "title": "Family of probability distributions with common properties."}, {"color": "#41c390", "id": "Constructing GLMs", "label": "Constructing GLMs", "shape": "dot", "size": 10, "title": "Methods for constructing generalized linear models."}, {"color": "#078768", "id": "Logistic Regression (GLM)", "label": "Logistic Regression (GLM)", "shape": "dot", "size": 10, "title": "Application of logistic regression within generalized linear models framework."}, {"color": "#cec961", "id": "Generative Learning Algorithms", "label": "Generative Learning Algorithms", "shape": "dot", "size": 10, "title": "Models that generate data based on underlying probability distributions."}, {"color": "#870c21", "id": "Gaussian Discriminant Analysis", "label": "Gaussian Discriminant Analysis", "shape": "dot", "size": 10, "title": "Method for classification using Gaussian distributions."}, {"color": "#382d89", "id": "Multivariate Normal Distribution", "label": "Multivariate Normal Distribution", "shape": "dot", "size": 10, "title": "A generalization of the one-dimensional normal distribution to higher dimensions."}, {"color": "#4c51fd", "id": "GDA Model", "label": "GDA Model", "shape": "dot", "size": 10, "title": "Model for classification using multivariate normal distributions."}, {"color": "#b32894", "id": "Discussion: GDA and Logistic Regression", "label": "Discussion: GDA and Logistic Regression", "shape": "dot", "size": 10, "title": "Comparison between Gaussian discriminant analysis and logistic regression."}, {"color": "#56c7b4", "id": "Naive Bayes", "label": "Naive Bayes", "shape": "dot", "size": 10, "title": "Simplified probabilistic classifier based on applying Bayes\u0027 theorem with strong independence assumptions."}, {"color": "#2b110b", "id": "Laplace Smoothing", "label": "Laplace Smoothing", "shape": "dot", "size": 10, "title": "Technique to improve Naive Bayes by handling zero probabilities for unseen events."}, {"color": "#4a0c9f", "id": "Event Models for Text Classification", "label": "Event Models for Text Classification", "shape": "dot", "size": 10, "title": "Models used for classifying text documents based on word occurrences."}, {"color": "#d95305", "id": "Kernel Methods", "label": "Kernel Methods", "shape": "star", "size": 25, "title": "Techniques that allow algorithms to work in high-dimensional feature spaces without explicit computation of vectors."}, {"color": "#a585b0", "id": "Feature Maps", "label": "Feature Maps", "shape": "dot", "size": 10, "title": "Transformation of input data into higher-dimensional space to fit more complex models."}, {"color": "#1e4861", "id": "LMS with Features", "label": "LMS with Features", "shape": "dot", "size": 10, "title": "Gradient descent algorithm for fitting models using features."}, {"color": "#14cb4b", "id": "LMS with Kernel Trick", "label": "LMS with Kernel Trick", "shape": "dot", "size": 10, "title": "Efficient computation of LMS using kernel functions."}, {"color": "#530bf6", "id": "Properties of Kernels", "label": "Properties of Kernels", "shape": "dot", "size": 10, "title": "Exploration of properties associated with kernel functions in machine learning."}, {"color": "#de4d19", "id": "Support Vector Machines", "label": "Support Vector Machines", "shape": "star", "size": 25, "title": "Algorithm for classification that finds the hyperplane maximizing the margin between classes."}, {"color": "#b435af", "id": "Margins: Intuition", "label": "Margins: Intuition", "shape": "dot", "size": 10, "title": "Understanding the concept of margins in SVMs."}, {"color": "#7ee346", "id": "Notation (Optional)", "label": "Notation (Optional)", "shape": "dot", "size": 10, "title": "Mathematical notation used in support vector machines."}, {"color": "#bfd243", "id": "Functional and Geometric Margins", "label": "Functional and Geometric Margins", "shape": "dot", "size": 10, "title": "Definitions of functional and geometric margins in SVMs."}, {"color": "#983b40", "id": "Optimal Margin Classifier (Optional)", "label": "Optimal Margin Classifier (Optional)", "shape": "dot", "size": 10, "title": "Finding the optimal margin classifier for linearly separable data."}, {"color": "#687e62", "id": "Lagrange Duality (Optional)", "label": "Lagrange Duality (Optional)", "shape": "dot", "size": 10, "title": "Use of Lagrangian duality in solving optimization problems in SVMs."}, {"color": "#2a14e2", "id": "Dual Formulation (Optional)", "label": "Dual Formulation (Optional)", "shape": "dot", "size": 10, "title": "Optimal margin classifier expressed in the dual form."}, {"color": "#2594a0", "id": "Regularization and Non-separable Case (Optional)", "label": "Regularization and Non-separable Case (Optional)", "shape": "dot", "size": 10, "title": "Handling non-separable data with regularization techniques."}, {"color": "#55b36c", "id": "SMO Algorithm (Optional)", "label": "SMO Algorithm (Optional)", "shape": "dot", "size": 10, "title": "Sequential minimal optimization algorithm for solving SVM problems."}, {"color": "#209cab", "id": "Coordinate Ascent", "label": "Coordinate Ascent", "shape": "dot", "size": 10, "title": "An iterative optimization algorithm that optimizes one variable at a time."}, {"color": "#1e6a8c", "id": "SMO Details", "label": "SMO Details", "shape": "dot", "size": 10, "title": "Detailed explanation of the sequential minimal optimization process."}, {"color": "#96f302", "id": "Deep Learning", "label": "Deep Learning", "shape": "star", "size": 25, "title": "Subfield of machine learning using neural networks to learn representations from data."}, {"color": "#77e8b4", "id": "Supervised Learning with Non-linear Models", "label": "Supervised Learning with Non-linear Models", "shape": "dot", "size": 10, "title": "Using non-linear models in supervised learning tasks."}, {"color": "#3416a2", "id": "Neural Networks", "label": "Neural Networks", "shape": "dot", "size": 10, "title": "Inspired by biological neural networks, used for complex function approximation."}, {"color": "#16bcba", "id": "Modules in Modern Neural Networks", "label": "Modules in Modern Neural Networks", "shape": "dot", "size": 10, "title": "Introduces various building blocks and ways to combine them in modern neural networks."}, {"color": "#17e450", "id": "Backpropagation", "label": "Backpropagation", "shape": "dot", "size": 10, "title": "Algorithm for training multi-layered neural networks by adjusting weights based on error gradients."}, {"color": "#40908d", "id": "Preliminaries on Partial Derivatives", "label": "Preliminaries on Partial Derivatives", "shape": "dot", "size": 10, "title": "Introduction to partial derivatives necessary for backpropagation."}, {"color": "#301db3", "id": "General Strategy of Backpropagation", "label": "General Strategy of Backpropagation", "shape": "dot", "size": 10, "title": "Overview of the general strategy used in backpropagation algorithm."}, {"color": "#bcfa4f", "id": "Backward Functions for Basic Modules", "label": "Backward Functions for Basic Modules", "shape": "dot", "size": 10, "title": "Derivation and implementation of backward functions for basic neural network modules."}, {"color": "#5664ff", "id": "Back-propagation for MLPs", "label": "Back-propagation for MLPs", "shape": "dot", "size": 10, "title": "Process of computing gradients through a multi-layer perceptron network to update weights during training."}, {"color": "#f78ae8", "id": "Stochastic Gradient Ascent Rule", "label": "Stochastic Gradient Ascent Rule", "shape": "dot", "size": 10, "title": "Update rule for parameters using stochastic gradient ascent."}, {"color": "#a029f9", "id": "Logistic Loss Function", "label": "Logistic Loss Function", "shape": "dot", "size": 10, "title": "Function that measures the performance of a classification model where labels are binary values."}, {"color": "#27e52c", "id": "Negative Log-Likelihood", "label": "Negative Log-Likelihood", "shape": "dot", "size": 10, "title": "Loss function measuring the dissimilarity between predicted and actual distributions."}, {"color": "#7d8c16", "id": "Logit", "label": "Logit", "shape": "dot", "size": 10, "title": "Linear combination of input features and parameters before applying the logistic function."}, {"color": "#fe255c", "id": "Loss Functions", "label": "Loss Functions", "shape": "dot", "size": 10, "title": "Functions used to measure the performance of a model during training and testing."}, {"color": "#844a54", "id": "Cross-Entropy Loss", "label": "Cross-Entropy Loss", "shape": "dot", "size": 10, "title": "Summation of negative log-likelihoods over training data for optimization."}, {"color": "#68c374", "id": "Softmax Function", "label": "Softmax Function", "shape": "dot", "size": 10, "title": "Function used to convert logits into a probability vector for multi-class classification."}, {"color": "#d6aa4a", "id": "Machine Learning", "label": "Machine Learning", "shape": "star", "size": 25, "title": "Field of study that uses algorithms to make predictions or decisions based on data."}, {"color": "#a5852d", "id": "Classification", "label": "Classification", "shape": "dot", "size": 10, "title": "Technique for categorizing data into predefined classes."}, {"color": "#4cc05b", "id": "Multinomial Distribution", "label": "Multinomial Distribution", "shape": "dot", "size": 10, "title": "Probability distribution over multiple outcomes."}, {"color": "#9d8c21", "id": "Logits", "label": "Logits", "shape": "dot", "size": 10, "title": "Inputs to the softmax function before transformation."}, {"color": "#05b7c5", "id": "Probability Vector", "label": "Probability Vector", "shape": "dot", "size": 10, "title": "Output of softmax, a vector with nonnegative entries summing to 1."}, {"color": "#10b4dc", "id": "Probabilistic Model", "label": "Probabilistic Model", "shape": "star", "size": 25, "title": "Model using softmax outputs as probabilities for classification tasks."}, {"color": "#ac7709", "id": "Newton\u0027s Method", "label": "Newton\u0027s Method", "shape": "star", "size": 25, "title": "Optimization technique using second-order derivatives to find local minima or maxima."}, {"color": "#7d0a56", "id": "Finding Roots", "label": "Finding Roots", "shape": "dot", "size": 10, "title": "The process of determining the values of x where f(x) = 0."}, {"color": "#8f7501", "id": "Maximizing Functions", "label": "Maximizing Functions", "shape": "dot", "size": 10, "title": "Using Newton\u0027s method to find maxima by setting first derivative to zero."}, {"color": "#0190e3", "id": "Multidimensional Generalization", "label": "Multidimensional Generalization", "shape": "dot", "size": 10, "title": "Extending Newton\u0027s method to handle vector-valued functions in logistic regression."}, {"color": "#076c1d", "id": "Hessian Matrix", "label": "Hessian Matrix", "shape": "dot", "size": 10, "title": "A square matrix of second-order partial derivatives used for multidimensional optimization."}, {"color": "#b6b74b", "id": "Optimization Methods", "label": "Optimization Methods", "shape": "dot", "size": 10, "title": "Papers discussing optimization methods such as Adam and variational auto-encoding."}, {"color": "#8b64b5", "id": "Fisher Scoring", "label": "Fisher Scoring", "shape": "dot", "size": 10, "title": "A variant of Newton\u0027s method used in logistic regression."}, {"color": "#08aff2", "id": "Generalized Linear Models (GLMs)", "label": "Generalized Linear Models (GLMs)", "shape": "star", "size": 25, "title": "Models that extend linear models to accommodate non-normal distributions and non-linear relationships."}, {"color": "#1519a7", "id": "Exponential Family Distributions", "label": "Exponential Family Distributions", "shape": "dot", "size": 10, "title": "A class of probability distributions that can be expressed in a specific form involving natural parameters and sufficient statistics."}, {"color": "#309fd6", "id": "Loss Function", "label": "Loss Function", "shape": "dot", "size": 10, "title": "Mathematical function used to measure the performance of a model and guide its training."}, {"color": "#99cfda", "id": "Cross Entropy Loss", "label": "Cross Entropy Loss", "shape": "dot", "size": 10, "title": "A loss function commonly used in logistic regression and neural networks."}, {"color": "#66a130", "id": "Algorithm Application", "label": "Algorithm Application", "shape": "dot", "size": 10, "title": "Application of Newton\u0027s method in logistic regression for maximizing likelihood."}, {"color": "#068867", "id": "Modern Neural Networks", "label": "Modern Neural Networks", "shape": "star", "size": 25, "title": "Overview of modern neural network architectures and training techniques."}, {"color": "#920561", "id": "Preliminaries on partial derivatives", "label": "Preliminaries on partial derivatives", "shape": "dot", "size": 10, "title": "Introduction to the mathematical concepts needed for backpropagation."}, {"color": "#b0b730", "id": "General strategy of backpropagation", "label": "General strategy of backpropagation", "shape": "dot", "size": 10, "title": "Overview of how backpropagation works in neural networks."}, {"color": "#71877e", "id": "Backward functions for basic modules", "label": "Backward functions for basic modules", "shape": "dot", "size": 10, "title": "Detailed explanation of backward propagation through simple network components."}, {"color": "#e5882d", "id": "Vectorization over training examples", "label": "Vectorization over training examples", "shape": "dot", "size": 10, "title": "Techniques for optimizing the computation of gradients across multiple data points."}, {"color": "#9e05ac", "id": "Generalization and regularization", "label": "Generalization and regularization", "shape": "star", "size": 25, "title": "Strategies to improve model performance on unseen data through generalization and regularization techniques."}, {"color": "#9d8eb4", "id": "Generalization", "label": "Generalization", "shape": "dot", "size": 10, "title": "Concepts related to improving a model\u0027s ability to generalize from training data to new, unseen data."}, {"color": "#076bb1", "id": "Bias-variance tradeoff", "label": "Bias-variance tradeoff", "shape": "dot", "size": 10, "title": "Discussion on the balance between model complexity and error due to variance or bias."}, {"color": "#d969a4", "id": "A mathematical decomposition (for regression)", "label": "A mathematical decomposition (for regression)", "shape": "dot", "size": 10, "title": "Mathematical breakdown of the bias-variance tradeoff in a regression context."}, {"color": "#90b069", "id": "The double descent phenomenon", "label": "The double descent phenomenon", "shape": "dot", "size": 10, "title": "Phenomenon where model performance improves after reaching a peak due to overfitting."}, {"color": "#da5529", "id": "Sample complexity bounds (optional readings)", "label": "Sample complexity bounds (optional readings)", "shape": "dot", "size": 10, "title": "Theoretical analysis of the number of samples needed for learning tasks."}, {"color": "#cb135b", "id": "Regularization and model selection", "label": "Regularization and model selection", "shape": "star", "size": 25, "title": "Techniques to prevent overfitting by penalizing complex models or selecting optimal hyperparameters."}, {"color": "#284bcf", "id": "Regularization", "label": "Regularization", "shape": "dot", "size": 10, "title": "Technique to prevent overfitting by adding a regularizer term to the loss function."}, {"color": "#21dff0", "id": "Implicit regularization effect (optional reading)", "label": "Implicit regularization effect (optional reading)", "shape": "dot", "size": 10, "title": "Exploration of how certain algorithms inherently regularize models without explicit penalties."}, {"color": "#610c74", "id": "Model selection via cross validation", "label": "Model selection via cross validation", "shape": "dot", "size": 10, "title": "Procedure for choosing the best model based on performance across different data splits."}, {"color": "#91f173", "id": "Bayesian statistics and regularization", "label": "Bayesian statistics and regularization", "shape": "dot", "size": 10, "title": "Application of Bayesian methods to regularize models and improve generalization."}, {"color": "#2b6afb", "id": "Unsupervised learning", "label": "Unsupervised learning", "shape": "star", "size": 25, "title": "Techniques for learning from data without labeled responses."}, {"color": "#64d8fd", "id": "Clustering and the k-means algorithm", "label": "Clustering and the k-means algorithm", "shape": "dot", "size": 10, "title": "Introduction to clustering methods with a focus on the k-means algorithm."}, {"color": "#5cbf64", "id": "EM algorithms", "label": "EM algorithms", "shape": "dot", "size": 10, "title": "Expectation-Maximization techniques for parameter estimation in probabilistic models."}, {"color": "#f347c2", "id": "EM for mixture of Gaussians", "label": "EM for mixture of Gaussians", "shape": "dot", "size": 10, "title": "Application of EM to Gaussian Mixture Models (GMMs)."}, {"color": "#35a02e", "id": "Jensen\u0027s inequality", "label": "Jensen\u0027s inequality", "shape": "dot", "size": 10, "title": "Mathematical principle used in the derivation and understanding of EM algorithms."}, {"color": "#2ad77c", "id": "General EM algorithms", "label": "General EM algorithms", "shape": "dot", "size": 10, "title": "Overview of the general framework and applications of EM beyond GMMs."}, {"color": "#33b2ba", "id": "Other interpretation of ELBO", "label": "Other interpretation of ELBO", "shape": "dot", "size": 10, "title": "Alternative perspectives on the Evidence Lower Bound (ELBO) in variational inference."}, {"color": "#4b9a51", "id": "Mixture of Gaussians revisited", "label": "Mixture of Gaussians revisited", "shape": "dot", "size": 10, "title": "Re-examination and advanced topics related to Gaussian Mixture Models using EM."}, {"color": "#d2b8df", "id": "Variational inference and variational auto-encoder (optional reading)", "label": "Variational inference and variational auto-encoder (optional reading)", "shape": "dot", "size": 10, "title": "Advanced topic on probabilistic modeling with VAEs and VI."}, {"color": "#ff315b", "id": "Principal components analysis", "label": "Principal components analysis", "shape": "dot", "size": 10, "title": "Dimensionality reduction technique that projects data onto a lower-dimensional space."}, {"color": "#902c87", "id": "Independent components analysis", "label": "Independent components analysis", "shape": "dot", "size": 10, "title": "Technique for separating mixed signals into their independent sources."}, {"color": "#c6bd0d", "id": "ICA ambiguities", "label": "ICA ambiguities", "shape": "dot", "size": 10, "title": "Discussion on the inherent limitations and challenges in ICA."}, {"color": "#35d233", "id": "Densities and linear transformations", "label": "Densities and linear transformations", "shape": "dot", "size": 10, "title": "Exploration of how densities change under linear transformations relevant to ICA."}, {"color": "#e3bf2c", "id": "ICA algorithm", "label": "ICA algorithm", "shape": "dot", "size": 10, "title": "Detailed explanation of the Independent Components Analysis procedure."}, {"color": "#e2b3e7", "id": "Self-supervised learning and foundation models", "label": "Self-supervised learning and foundation models", "shape": "star", "size": 25, "title": "Approaches to training large-scale models using self-supervision techniques."}, {"color": "#ae6c92", "id": "Pretraining and adaptation", "label": "Pretraining and adaptation", "shape": "dot", "size": 10, "title": "Overview of pre-training methods and subsequent fine-tuning for specific tasks."}, {"color": "#05a7a9", "id": "Pretraining methods in computer vision", "label": "Pretraining methods in computer vision", "shape": "dot", "size": 10, "title": "Techniques used to train visual recognition systems without labeled data."}, {"color": "#3213b0", "id": "Pretrained large language models", "label": "Pretrained large language models", "shape": "dot", "size": 10, "title": "Discussion on the development and applications of pre-trained language models."}, {"color": "#a46bbe", "id": "Open up the blackbox of Transformers", "label": "Open up the blackbox of Transformers", "shape": "dot", "size": 10, "title": "Exploration of the architecture and workings of Transformer-based models."}, {"color": "#e41663", "id": "Zero-shot learning and in-context learning", "label": "Zero-shot learning and in-context learning", "shape": "dot", "size": 10, "title": "Capabilities of models to perform tasks without explicit training data."}, {"color": "#ffae3a", "id": "Reinforcement Learning and Control", "label": "Reinforcement Learning and Control", "shape": "star", "size": 25, "title": "Techniques for agents to learn optimal behavior through interaction with an environment."}, {"color": "#e968e1", "id": "Reinforcement learning", "label": "Reinforcement learning", "shape": "dot", "size": 10, "title": "Introduction to the principles of reinforcement learning and its applications."}, {"color": "#ebeebe", "id": "Markov decision processes", "label": "Markov decision processes", "shape": "dot", "size": 10, "title": "Mathematical framework for modeling sequential decision-making problems."}, {"color": "#51b837", "id": "Value iteration and policy iteration", "label": "Value iteration and policy iteration", "shape": "dot", "size": 10, "title": "Algorithms for finding optimal policies in MDPs through value or policy updates."}, {"color": "#1ec133", "id": "Generalized Linear Model (GLM)", "label": "Generalized Linear Model (GLM)", "shape": "dot", "size": 10, "title": "A method to model problems using exponential family distributions."}, {"color": "#387fe3", "id": "Poisson Distribution", "label": "Poisson Distribution", "shape": "dot", "size": 10, "title": "Distribution used to model count data such as customer arrivals or page views."}, {"color": "#e42f71", "id": "Conditional Distribution Assumption", "label": "Conditional Distribution Assumption", "shape": "dot", "size": 10, "title": "Assumes y|x follows an exponential family distribution."}, {"color": "#a06124", "id": "Prediction Goal", "label": "Prediction Goal", "shape": "dot", "size": 10, "title": "Aims to predict the expected value of T(y) given x."}, {"color": "#6fc224", "id": "Linear Relationship Assumption", "label": "Linear Relationship Assumption", "shape": "dot", "size": 10, "title": "Assumes a linear relationship between natural parameter and inputs."}, {"color": "#eb462f", "id": "Generalized Linear Models (GLM)", "label": "Generalized Linear Models (GLM)", "shape": "dot", "size": 10, "title": "Models that extend linear regression to accommodate non-normal distributions."}, {"color": "#caa161", "id": "Bernoulli Distribution", "label": "Bernoulli Distribution", "shape": "dot", "size": 10, "title": "Binary outcome probability distribution, an example of exponential family."}, {"color": "#5f6e1f", "id": "Gaussian Distribution", "label": "Gaussian Distribution", "shape": "dot", "size": 10, "title": "Describes the distribution of state at time t+1 given observations up to time t."}, {"color": "#792f88", "id": "Natural Parameter", "label": "Natural Parameter", "shape": "dot", "size": 10, "title": "Parameter \u03b7 in the exponential family distribution formula."}, {"color": "#19e973", "id": "Sufficient Statistic", "label": "Sufficient Statistic", "shape": "dot", "size": 10, "title": "Statistic T(y) that summarizes data relevant to parameter estimation."}, {"color": "#cdf30a", "id": "Log Partition Function", "label": "Log Partition Function", "shape": "dot", "size": 10, "title": "Function a(\u03b7) ensuring the distribution sums/integrates to 1."}, {"color": "#b059b9", "id": "Natural Parameter for Bernoulli", "label": "Natural Parameter for Bernoulli", "shape": "dot", "size": 10, "title": "\u03b7 = log(\u03c6/(1-\u03c6)), where \u03c6 is the mean of the distribution."}, {"color": "#28812b", "id": "Sufficient Statistic for Bernoulli", "label": "Sufficient Statistic for Bernoulli", "shape": "dot", "size": 10, "title": "T(y) = y, indicating the outcome itself as sufficient statistic."}, {"color": "#e01f5f", "id": "Log Partition Function for Bernoulli", "label": "Log Partition Function for Bernoulli", "shape": "dot", "size": 10, "title": "a(\u03b7) = log(1 + e^\u03b7), ensuring normalization of distribution."}, {"color": "#5f2d10", "id": "Assumptions/Design Choices", "label": "Assumptions/Design Choices", "shape": "dot", "size": 10, "title": "Three foundational principles that lead to the derivation of GLMs."}, {"color": "#c14100", "id": "Response Variable", "label": "Response Variable", "shape": "dot", "size": 10, "title": "Continuous target variable in the context of GLM formulation."}, {"color": "#fed917", "id": "Conditional Distribution", "label": "Conditional Distribution", "shape": "dot", "size": 10, "title": "Modeling the conditional distribution of y given x and theta."}, {"color": "#71faf6", "id": "Bayesian Classification", "label": "Bayesian Classification", "shape": "star", "size": 25, "title": "Classification method using Bayes\u0027 theorem to predict class membership probabilities."}, {"color": "#313c74", "id": "Class Priors", "label": "Class Priors", "shape": "dot", "size": 10, "title": "Prior probability of each class before observing data."}, {"color": "#624d81", "id": "Conditional Probability", "label": "Conditional Probability", "shape": "dot", "size": 10, "title": "Probability of an event given that another event has occurred."}, {"color": "#6f4a18", "id": "Posterior Distribution", "label": "Posterior Distribution", "shape": "dot", "size": 10, "title": "The posterior distribution of latent variables given observed data under current parameters."}, {"color": "#e3818b", "id": "Gaussian Discriminant Analysis (GDA)", "label": "Gaussian Discriminant Analysis (GDA)", "shape": "star", "size": 25, "title": "A probabilistic model that assumes the data is generated from a mixture of several Gaussian distributions with unknown parameters."}, {"color": "#7e7406", "id": "Conditional Distribution Modeling", "label": "Conditional Distribution Modeling", "shape": "dot", "size": 10, "title": "Modeling the distribution of y given x."}, {"color": "#855cd1", "id": "Canonical Response Function", "label": "Canonical Response Function", "shape": "dot", "size": 10, "title": "Function giving the mean of a distribution as a function of its natural parameter."}, {"color": "#75db60", "id": "Canonical Link Function", "label": "Canonical Link Function", "shape": "dot", "size": 10, "title": "Inverse of the canonical response function, mapping from expected value to natural parameter."}, {"color": "#fa0f9e", "id": "Discriminative Learning Algorithms", "label": "Discriminative Learning Algorithms", "shape": "dot", "size": 10, "title": "Algorithms that directly model the decision boundary between classes."}, {"color": "#ada052", "id": "Bayes Rule", "label": "Bayes Rule", "shape": "dot", "size": 10, "title": "Used to derive posterior distribution p(y|x) from p(x|y) and p(y)."}, {"color": "#0a1c17", "id": "Probability Distributions", "label": "Probability Distributions", "shape": "dot", "size": 10, "title": "Discussion on probability distributions used in ML models."}, {"color": "#c2dffd", "id": "Mean", "label": "Mean", "shape": "dot", "size": 10, "title": "Definition and calculation of mean in Gaussian distribution."}, {"color": "#7bc31c", "id": "Covariance", "label": "Covariance", "shape": "dot", "size": 10, "title": "Explanation of covariance matrix and its significance."}, {"color": "#43ff23", "id": "Standard Normal Distribution", "label": "Standard Normal Distribution", "shape": "dot", "size": 10, "title": "Definition and properties of standard normal distribution."}, {"color": "#cee29c", "id": "Density Visualization", "label": "Density Visualization", "shape": "dot", "size": 10, "title": "Examples showing how density changes with covariance matrix values."}, {"color": "#4adf3b", "id": "Covariance Matrix", "label": "Covariance Matrix", "shape": "dot", "size": 10, "title": "Matrix that describes the variance and covariance between random variables."}, {"color": "#1f30c3", "id": "Contour Plots", "label": "Contour Plots", "shape": "dot", "size": 10, "title": "Graphical representation of density contours for different values of parameters."}, {"color": "#1fc4df", "id": "Multivariate Normal Distributions for Classes", "label": "Multivariate Normal Distributions for Classes", "shape": "dot", "size": 10, "title": "Represents different classes with separate mean vectors and a common covariance matrix."}, {"color": "#e09075", "id": "Decision Boundaries", "label": "Decision Boundaries", "shape": "dot", "size": 10, "title": "Boundaries that separate different classes in feature space."}, {"color": "#3f4d06", "id": "Model Assumptions", "label": "Model Assumptions", "shape": "dot", "size": 10, "title": "Theoretical assumptions made by models about the data distribution."}, {"color": "#4bb7bd", "id": "Model Parameters", "label": "Model Parameters", "shape": "dot", "size": 10, "title": "Parameters include prior probability of spam, conditional probabilities of words given spam or non-spam"}, {"color": "#304965", "id": "Log-Likelihood Function", "label": "Log-Likelihood Function", "shape": "dot", "size": 10, "title": "Function used to estimate parameters by maximizing likelihood of observed data given the model."}, {"color": "#b2fdab", "id": "Decision Boundary", "label": "Decision Boundary", "shape": "dot", "size": 10, "title": "Boundary in feature space where model predicts equal probabilities for both classes."}, {"color": "#36753a", "id": "GDA (Generative Discriminative Approach)", "label": "GDA (Generative Discriminative Approach)", "shape": "dot", "size": 10, "title": "Model that makes strong assumptions about data distribution."}, {"color": "#0c0550", "id": "Robustness of Logistic Regression", "label": "Robustness of Logistic Regression", "shape": "dot", "size": 10, "title": "Discusses the robust nature and performance in large datasets."}, {"color": "#898560", "id": "Naive Bayes (Discrete Features)", "label": "Naive Bayes (Discrete Features)", "shape": "dot", "size": 10, "title": "Algorithm for classification with discrete-valued features."}, {"color": "#671c1c", "id": "Feature Vector Selection", "label": "Feature Vector Selection", "shape": "dot", "size": 10, "title": "Choosing relevant features for classification tasks."}, {"color": "#4d3261", "id": "Generative Models", "label": "Generative Models", "shape": "dot", "size": 10, "title": "Models that generate data based on learned parameters."}, {"color": "#273e4c", "id": "Naive Bayes Classifier", "label": "Naive Bayes Classifier", "shape": "dot", "size": 10, "title": "A probabilistic classifier based on applying Bayes\u0027 theorem with strong (naive) independence assumptions between the features."}, {"color": "#4f68cc", "id": "Conditional Independence Assumption", "label": "Conditional Independence Assumption", "shape": "dot", "size": 10, "title": "Assumption that features are independent given the class label."}, {"color": "#576122", "id": "Text Classification", "label": "Text Classification", "shape": "dot", "size": 10, "title": "Process of categorizing text into predefined categories using machine learning techniques."}, {"color": "#a6a3fe", "id": "Spam Filter", "label": "Spam Filter", "shape": "dot", "size": 10, "title": "System that uses machine learning to classify emails as spam or non-spam."}, {"color": "#71ad9a", "id": "Training Set", "label": "Training Set", "shape": "dot", "size": 10, "title": "Collection of examples used for training a model, each with features and labels."}, {"color": "#11416f", "id": "Feature Vector", "label": "Feature Vector", "shape": "dot", "size": 10, "title": "Representation of an email as a vector where each dimension corresponds to the presence or absence of words in the vocabulary."}, {"color": "#37ae92", "id": "Vocabulary", "label": "Vocabulary", "shape": "dot", "size": 10, "title": "Set of unique words used to represent emails in feature vectors."}, {"color": "#9d8833", "id": "Stop Words", "label": "Stop Words", "shape": "dot", "size": 10, "title": "Commonly occurring words that are often excluded from the vocabulary due to their lack of discriminatory power."}, {"color": "#b57c7a", "id": "Naive Bayes Algorithm", "label": "Naive Bayes Algorithm", "shape": "star", "size": 25, "title": "A probabilistic classifier based on applying Bayes\u0027 theorem with strong independence assumptions between the features."}, {"color": "#1aa066", "id": "Bayesian Inference", "label": "Bayesian Inference", "shape": "dot", "size": 10, "title": "Statistical method for updating the probability estimate for a hypothesis as more evidence or information becomes available."}, {"color": "#3b2614", "id": "Parameter Estimation", "label": "Parameter Estimation", "shape": "dot", "size": 10, "title": "Process of estimating parameters such as \u03c6\u208a|y=1 and \u03c6\u208a|y=0 from a training set."}, {"color": "#6163db", "id": "Binary Features", "label": "Binary Features", "shape": "dot", "size": 10, "title": "Features are binary-valued in the basic formulation of Naive Bayes."}, {"color": "#1babe5", "id": "Multinomial Features", "label": "Multinomial Features", "shape": "dot", "size": 10, "title": "Generalization to features with multiple discrete values."}, {"color": "#1a4860", "id": "Spam Classification Example", "label": "Spam Classification Example", "shape": "dot", "size": 10, "title": "Illustrative example of applying Laplace smoothing in email spam detection."}, {"color": "#a90de4", "id": "Machine Learning Conferences", "label": "Machine Learning Conferences", "shape": "star", "size": 25, "title": "Top machine learning conferences including NeurIPS."}, {"color": "#42370f", "id": "NeurIPS Conference", "label": "NeurIPS Conference", "shape": "dot", "size": 10, "title": "One of the top machine learning conferences with a submission deadline in May-June."}, {"color": "#0b1cc3", "id": "Naive Bayes Spam Filter", "label": "Naive Bayes Spam Filter", "shape": "star", "size": 25, "title": "A probabilistic classifier used for spam detection based on Bayesian probability theory."}, {"color": "#6a1547", "id": "Zero Frequency Problem", "label": "Zero Frequency Problem", "shape": "dot", "size": 10, "title": "Issue where the probability of an unseen event is estimated as zero based on finite training data."}, {"color": "#b9da51", "id": "Probability Estimation", "label": "Probability Estimation", "shape": "star", "size": 25, "title": "Discusses the statistical issues with estimating probabilities as zero."}, {"color": "#64f531", "id": "Multinomial Random Variable", "label": "Multinomial Random Variable", "shape": "dot", "size": 10, "title": "A random variable taking values in a finite set of outcomes."}, {"color": "#64fcde", "id": "Maximum Likelihood Estimates", "label": "Maximum Likelihood Estimates", "shape": "dot", "size": 10, "title": "Estimates for parameters based on observed data, which can result in zero probabilities."}, {"color": "#0047bc", "id": "Bernoulli Event Model", "label": "Bernoulli Event Model", "shape": "dot", "size": 10, "title": "Model for text classification assuming binary presence or absence of words."}, {"color": "#f7ecd1", "id": "Multinomial Event Model", "label": "Multinomial Event Model", "shape": "dot", "size": 10, "title": "Model for generating emails where each word is chosen independently from a multinomial distribution based on spam/non-spam classification"}, {"color": "#1d3bb3", "id": "Spam/Non-Spam Classification", "label": "Spam/Non-Spam Classification", "shape": "dot", "size": 10, "title": "Determining whether an email is spam or not before generating its content"}, {"color": "#825ee9", "id": "Word Generation Process", "label": "Word Generation Process", "shape": "dot", "size": 10, "title": "Process of independently selecting each word from a multinomial distribution over words given the classification"}, {"color": "#5811ac", "id": "Probability Calculation", "label": "Probability Calculation", "shape": "dot", "size": 10, "title": "Calculating overall probability of an email as product of probabilities for each word and spam/non-spam classification"}, {"color": "#1a5cbb", "id": "Feature Map", "label": "Feature Map", "shape": "dot", "size": 10, "title": "Transformation from attributes to features."}, {"color": "#551555", "id": "Linear Function Over Features", "label": "Linear Function Over Features", "shape": "dot", "size": 10, "title": "Rewriting a cubic function as a linear combination of feature variables."}, {"color": "#bbc3cf", "id": "Attributes", "label": "Attributes", "shape": "dot", "size": 10, "title": "Original input values in machine learning problems."}, {"color": "#43b868", "id": "Features Variables", "label": "Features Variables", "shape": "dot", "size": 10, "title": "New set of quantities derived from attributes using a feature map."}, {"color": "#4645d1", "id": "Cubic Function Representation", "label": "Cubic Function Representation", "shape": "dot", "size": 10, "title": "Expressing cubic functions as linear combinations in higher dimensions."}, {"color": "#66fb83", "id": "Gradient Descent Update Rule", "label": "Gradient Descent Update Rule", "shape": "dot", "size": 10, "title": "Update rule for gradient descent in the context of high-dimensional features."}, {"color": "#d9c0c0", "id": "Feature Mapping", "label": "Feature Mapping", "shape": "dot", "size": 10, "title": "Transformation of input data into a higher-dimensional feature space for better model fitting."}, {"color": "#b762d3", "id": "Kernel Trick", "label": "Kernel Trick", "shape": "dot", "size": 10, "title": "Technique used in SVMs to handle non-linearly separable data by transforming it into a higher-dimensional space."}, {"color": "#79739c", "id": "Computational Complexity", "label": "Computational Complexity", "shape": "dot", "size": 10, "title": "Discussion on computational challenges with high-dimensional feature mappings."}, {"color": "#9e3724", "id": "Iterative Update Process", "label": "Iterative Update Process", "shape": "dot", "size": 10, "title": "Process for updating coefficients in feature mapping iteratively."}, {"color": "#876ea7", "id": "Linear Combination Representation", "label": "Linear Combination Representation", "shape": "dot", "size": 10, "title": "Representation of the vector theta as a linear combination of transformed input vectors."}, {"color": "#b22ba5", "id": "Feature Maps and Kernels", "label": "Feature Maps and Kernels", "shape": "dot", "size": 10, "title": "Discussion on feature maps and their corresponding kernel functions."}, {"color": "#c77f84", "id": "Kernel Function Definition", "label": "Kernel Function Definition", "shape": "dot", "size": 10, "title": "Definition of the kernel function based on inner products."}, {"color": "#6aeff6", "id": "Inner Product Computation", "label": "Inner Product Computation", "shape": "dot", "size": 10, "title": "Efficient computation of inner products between feature vectors in high-dimensional space."}, {"color": "#95c400", "id": "Algorithm Implementation", "label": "Algorithm Implementation", "shape": "dot", "size": 10, "title": "Steps to implement the algorithm using kernel functions."}, {"color": "#16f396", "id": "Beta Update Equation", "label": "Beta Update Equation", "shape": "dot", "size": 10, "title": "Equation describing how \beta_i is updated in each iteration."}, {"color": "#8359cb", "id": "Feature Map Phi", "label": "Feature Map Phi", "shape": "dot", "size": 10, "title": "Mapping function that transforms input data into a higher-dimensional space."}, {"color": "#bf3f3a", "id": "Pre-computation Strategy", "label": "Pre-computation Strategy", "shape": "dot", "size": 10, "title": "Technique to pre-calculate all pairwise inner products before the main loop starts."}, {"color": "#c33741", "id": "Efficient Inner Product Calculation", "label": "Efficient Inner Product Calculation", "shape": "dot", "size": 10, "title": "Method for computing inner products without explicitly calculating feature maps."}, {"color": "#026e8a", "id": "Kernels in Machine Learning", "label": "Kernels in Machine Learning", "shape": "dot", "size": 10, "title": "Introduction to kernel functions and their properties."}, {"color": "#db9dd2", "id": "Kernel Functions", "label": "Kernel Functions", "shape": "dot", "size": 10, "title": "Description and properties of kernel functions, their relation to feature maps."}, {"color": "#fc9ac6", "id": "Explicit Definition of Kernels", "label": "Explicit Definition of Kernels", "shape": "dot", "size": 10, "title": "How kernels can be defined explicitly or implicitly through feature maps."}, {"color": "#05e4d0", "id": "Characterization of Valid Kernels", "label": "Characterization of Valid Kernels", "shape": "dot", "size": 10, "title": "Criteria for determining if a function is a valid kernel corresponding to some feature map."}, {"color": "#317016", "id": "Computational Efficiency", "label": "Computational Efficiency", "shape": "dot", "size": 10, "title": "Efficiency of calculating kernel functions compared to direct computation in high-dimensional space."}, {"color": "#7bdcdb", "id": "Polynomial Kernels", "label": "Polynomial Kernels", "shape": "dot", "size": 10, "title": "Kernels that map data into polynomial feature spaces."}, {"color": "#ad77a8", "id": "Kernels as Similarity Metrics", "label": "Kernels as Similarity Metrics", "shape": "dot", "size": 10, "title": "Using kernels to measure similarity between inputs."}, {"color": "#d5a153", "id": "Gaussian Kernel", "label": "Gaussian Kernel", "shape": "dot", "size": 10, "title": "A specific kernel function that measures similarity in an infinite dimensional space."}, {"color": "#c86b42", "id": "Valid Kernels Conditions", "label": "Valid Kernels Conditions", "shape": "dot", "size": 10, "title": "Criteria for a function to be considered a valid kernel."}, {"color": "#d82d40", "id": "Feature Extraction for Strings", "label": "Feature Extraction for Strings", "shape": "dot", "size": 10, "title": "Techniques to extract features from variable-length string data."}, {"color": "#1a8eff", "id": "Support Vector Machines (SVM)", "label": "Support Vector Machines (SVM)", "shape": "dot", "size": 10, "title": "A supervised learning model for classification and regression analysis."}, {"color": "#c5ba0f", "id": "Kernel Matrix Properties", "label": "Kernel Matrix Properties", "shape": "star", "size": 25, "title": "Properties of the kernel matrix in machine learning."}, {"color": "#ffcfe0", "id": "Sufficient Conditions for Valid Kernels", "label": "Sufficient Conditions for Valid Kernels", "shape": "dot", "size": 10, "title": "Conditions that are both necessary and sufficient for a function to be a valid kernel."}, {"color": "#af3414", "id": "Mercer\u0027s Theorem", "label": "Mercer\u0027s Theorem", "shape": "dot", "size": 10, "title": "Theorem stating necessary and sufficient conditions for a kernel to be valid."}, {"color": "#f97610", "id": "Testing Kernel Validity", "label": "Testing Kernel Validity", "shape": "dot", "size": 10, "title": "Methods to test if a given function is a valid kernel."}, {"color": "#889207", "id": "Examples of Kernels in Practice", "label": "Examples of Kernels in Practice", "shape": "star", "size": 25, "title": "Practical applications and examples of kernels in machine learning problems."}, {"color": "#8517e7", "id": "Necessary Conditions for Valid Kernels", "label": "Necessary Conditions for Valid Kernels", "shape": "dot", "size": 10, "title": "Conditions a kernel must meet to correspond to some feature mapping."}, {"color": "#195ef2", "id": "Symmetry Property", "label": "Symmetry Property", "shape": "dot", "size": 10, "title": "A valid kernel matrix is symmetric."}, {"color": "#d951ed", "id": "Positive Semi-Definiteness", "label": "Positive Semi-Definiteness", "shape": "dot", "size": 10, "title": "A valid kernel matrix must be positive semi-definite."}, {"color": "#0fd657", "id": "Kernel Matrix", "label": "Kernel Matrix", "shape": "dot", "size": 10, "title": "Matrix representation of a kernel function over a set of points."}, {"color": "#8d5ba4", "id": "Functional Margins", "label": "Functional Margins", "shape": "dot", "size": 10, "title": "Measure of confidence for predictions based on distance from decision boundary."}, {"color": "#866375", "id": "Geometric Margins", "label": "Geometric Margins", "shape": "dot", "size": 10, "title": "Concept explaining the geometric interpretation of margins in machine learning models."}, {"color": "#922460", "id": "Support Vector Machines (SVMs)", "label": "Support Vector Machines (SVMs)", "shape": "dot", "size": 10, "title": "A type of machine learning model used for classification and regression analysis."}, {"color": "#17a55a", "id": "Notation for SVMs", "label": "Notation for SVMs", "shape": "dot", "size": 10, "title": "Introduction of notation using w and b parameters for linear classifiers."}, {"color": "#10f720", "id": "Functional Margin", "label": "Functional Margin", "shape": "dot", "size": 10, "title": "Measure indicating the confidence and correctness of predictions."}, {"color": "#816fae", "id": "Geometric Margin", "label": "Geometric Margin", "shape": "dot", "size": 10, "title": "Smallest margin between the decision boundary and closest data points in training set."}, {"color": "#4c101b", "id": "Confidence Measure Limitation", "label": "Confidence Measure Limitation", "shape": "dot", "size": 10, "title": "Explains why functional margin is not a reliable measure of confidence."}, {"color": "#48abf7", "id": "Normalization Condition", "label": "Normalization Condition", "shape": "dot", "size": 10, "title": "Proposed normalization to improve the reliability of the functional margin."}, {"color": "#a1e4ed", "id": "Function Margin with Training Set", "label": "Function Margin with Training Set", "shape": "dot", "size": 10, "title": "Definition and calculation of function margin for a set of training examples."}, {"color": "#9adcdd", "id": "Margins", "label": "Margins", "shape": "dot", "size": 10, "title": "Concept of margins in SVMs to maximize the distance between classes."}, {"color": "#e93f8f", "id": "Optimal Margin Classifier", "label": "Optimal Margin Classifier", "shape": "dot", "size": 10, "title": "Classifier that maximizes geometric margin to achieve confident predictions on linearly separable data sets."}, {"color": "#5526fe", "id": "Lagrange Duality", "label": "Lagrange Duality", "shape": "dot", "size": 10, "title": "Theory for transforming constrained optimization problems into dual form."}, {"color": "#f4f5f7", "id": "Kernels", "label": "Kernels", "shape": "dot", "size": 10, "title": "Technique for handling non-linearly separable data in high-dimensional spaces."}, {"color": "#de8101", "id": "SMO Algorithm", "label": "SMO Algorithm", "shape": "dot", "size": 10, "title": "Algorithm that updates two Lagrange multipliers simultaneously to solve the dual optimization problem."}, {"color": "#bc608d", "id": "Vector w", "label": "Vector w", "shape": "dot", "size": 10, "title": "Orthogonal to the decision boundary and points towards positive class."}, {"color": "#2ec3cd", "id": "Distance to Decision Boundary", "label": "Distance to Decision Boundary", "shape": "dot", "size": 10, "title": "Calculated using vector projection and hyperplane equation."}, {"color": "#0389bd", "id": "Unit Vector w/||w||", "label": "Unit Vector w/||w||", "shape": "dot", "size": 10, "title": "Normalized version of vector w pointing in the same direction."}, {"color": "#560bda", "id": "Model Parameters (w, b)", "label": "Model Parameters (w, b)", "shape": "dot", "size": 10, "title": "Parameters used to define the decision boundary in a linear classifier."}, {"color": "#2c4da3", "id": "Scaling Constraint on w", "label": "Scaling Constraint on w", "shape": "dot", "size": 10, "title": "Arbitrary scaling constraints that can be imposed on parameter w without changing model behavior."}, {"color": "#fc2ad7", "id": "Maximizing Geometric Margin", "label": "Maximizing Geometric Margin", "shape": "dot", "size": 10, "title": "Process of finding decision boundary with maximum margin between positive and negative examples."}, {"color": "#51903c", "id": "Support Vector Machine (SVM)", "label": "Support Vector Machine (SVM)", "shape": "star", "size": 25, "title": "Binary classification model maximizing margin between classes."}, {"color": "#e69ae4", "id": "Non-Convex Constraint", "label": "Non-Convex Constraint", "shape": "dot", "size": 10, "title": "Constraint that complicates optimization problem."}, {"color": "#511766", "id": "Scaling Constraint", "label": "Scaling Constraint", "shape": "dot", "size": 10, "title": "Constraint to simplify the objective function."}, {"color": "#b31eb5", "id": "Objective Function Transformation", "label": "Objective Function Transformation", "shape": "dot", "size": 10, "title": "Transformation of the original problem into a convex one."}, {"color": "#91381a", "id": "Constrained Optimization", "label": "Constrained Optimization", "shape": "star", "size": 25, "title": "Optimization problems with equality and inequality constraints."}, {"color": "#8f1d45", "id": "Lagrange Multipliers", "label": "Lagrange Multipliers", "shape": "dot", "size": 10, "title": "Variables used in the SMO algorithm to optimize the SVM\u0027s objective function."}, {"color": "#2f0186", "id": "Generalized Lagrangian", "label": "Generalized Lagrangian", "shape": "dot", "size": 10, "title": "Function combining the objective and constraint functions with multipliers."}, {"color": "#d0cb52", "id": "Primal Problem", "label": "Primal Problem", "shape": "dot", "size": 10, "title": "Optimization problem defined by maximizing over alpha and beta while minimizing over w."}, {"color": "#2f8b6c", "id": "\\(\\theta_{\\cal P}(w)\\)", "label": "\\(\\theta_{\\cal P}(w)\\)", "shape": "dot", "size": 10, "title": "Maximum value of the generalized Lagrangian for given \\(w\\)."}, {"color": "#126a70", "id": "Convex Quadratic Objective", "label": "Convex Quadratic Objective", "shape": "dot", "size": 10, "title": "Objective function in the form of a convex quadratic equation."}, {"color": "#1e7390", "id": "Linear Constraints", "label": "Linear Constraints", "shape": "dot", "size": 10, "title": "Constraints that are linear equations or inequalities."}, {"color": "#9c2620", "id": "Dual Formulation", "label": "Dual Formulation", "shape": "dot", "size": 10, "title": "Alternative formulation of the original problem that simplifies computation."}, {"color": "#f2b4c1", "id": "Optimization Problems", "label": "Optimization Problems", "shape": "dot", "size": 10, "title": "Formulation and solution of optimization problems in ML."}, {"color": "#8d2d83", "id": "KKT Conditions", "label": "KKT Conditions", "shape": "dot", "size": 10, "title": "Conditions ensuring optimality in constrained optimization problems"}, {"color": "#325fe6", "id": "Dual Complementarity Condition", "label": "Dual Complementarity Condition", "shape": "dot", "size": 10, "title": "Condition indicating active constraints in optimization problems."}, {"color": "#1037af", "id": "Support Vectors", "label": "Support Vectors", "shape": "dot", "size": 10, "title": "Points that lie on the decision boundary and influence the optimal solution."}, {"color": "#6fa846", "id": "SVM Optimization Problem", "label": "SVM Optimization Problem", "shape": "dot", "size": 10, "title": "Primal and dual forms of SVM optimization problem."}, {"color": "#1ffd0a", "id": "Dual Problem", "label": "Dual Problem", "shape": "star", "size": 25, "title": "Optimization problem where the order of max and min operations is reversed compared to the primal problem."}, {"color": "#f2f126", "id": "Objective Function Primal", "label": "Objective Function Primal", "shape": "dot", "size": 10, "title": "Function \u03b8\u211aP(w) that represents the objective in the primal problem."}, {"color": "#f03e78", "id": "Objective Function Dual", "label": "Objective Function Dual", "shape": "dot", "size": 10, "title": "Function \u03b8\u211aD(\u03b1,\u03b2) representing the dual optimization\u0027s objective."}, {"color": "#8ab4bc", "id": "Primal Constraints", "label": "Primal Constraints", "shape": "dot", "size": 10, "title": "Constraints that w must satisfy in the primal problem."}, {"color": "#25632e", "id": "Dual Constraints", "label": "Dual Constraints", "shape": "dot", "size": 10, "title": "Non-negativity constraints on \u03b1 for the dual problem."}, {"color": "#71453d", "id": "Lagrangian Function", "label": "Lagrangian Function", "shape": "star", "size": 25, "title": "Function used to derive dual problem in SVMs with L1 regularization."}, {"color": "#c8189e", "id": "Value Primal Problem", "label": "Value Primal Problem", "shape": "dot", "size": 10, "title": "Optimal value p* of the objective function for the primal problem."}, {"color": "#1cd5da", "id": "Value Dual Problem", "label": "Value Dual Problem", "shape": "dot", "size": 10, "title": "Optimal value d* of the dual problem\u0027s objective function."}, {"color": "#e9c914", "id": "Machine Learning Theory", "label": "Machine Learning Theory", "shape": "star", "size": 25, "title": "Theoretical foundations of machine learning including generalization and hypothesis classes."}, {"color": "#132db7", "id": "Duality Gap", "label": "Duality Gap", "shape": "dot", "size": 10, "title": "Difference between primal and dual problem solutions."}, {"color": "#396b1b", "id": "Convex Functions", "label": "Convex Functions", "shape": "dot", "size": 10, "title": "Functions where the line segment between any two points on the graph lies above or on the function."}, {"color": "#56946f", "id": "Affine Constraints", "label": "Affine Constraints", "shape": "dot", "size": 10, "title": "Linear constraints that can be shifted by a constant."}, {"color": "#263263", "id": "Feasibility Conditions", "label": "Feasibility Conditions", "shape": "dot", "size": 10, "title": "Conditions ensuring the existence of solutions satisfying all constraints."}, {"color": "#07af23", "id": "Karush-Kuhn-Tucker (KKT) Conditions", "label": "Karush-Kuhn-Tucker (KKT) Conditions", "shape": "dot", "size": 10, "title": "Necessary conditions for a solution in nonlinear programming to be optimal."}, {"color": "#b13628", "id": "Dual Form of Problem", "label": "Dual Form of Problem", "shape": "star", "size": 25, "title": "Formulation focusing on Lagrange multipliers to solve optimization problem."}, {"color": "#941808", "id": "Inner Products", "label": "Inner Products", "shape": "dot", "size": 10, "title": "Key to the kernel trick, expressed as (x^(i))^T x^(j)."}, {"color": "#03defe", "id": "Optimization Constraints", "label": "Optimization Constraints", "shape": "dot", "size": 10, "title": "Constraints on optimization variables in machine learning models."}, {"color": "#f353cc", "id": "Support Vectors Definition", "label": "Support Vectors Definition", "shape": "dot", "size": 10, "title": "Points with non-zero alpha values that define the decision boundary."}, {"color": "#dc20a0", "id": "Lagrangian Optimization", "label": "Lagrangian Optimization", "shape": "star", "size": 25, "title": "Optimization involving Lagrangian function for SVMs"}, {"color": "#756f50", "id": "Dual Problem Formulation", "label": "Dual Problem Formulation", "shape": "dot", "size": 10, "title": "Formulating the dual problem from primal constraints and objective"}, {"color": "#f3221c", "id": "Optimal Parameters Alpha", "label": "Optimal Parameters Alpha", "shape": "dot", "size": 10, "title": "Finding optimal alpha values to maximize the dual objective function"}, {"color": "#6f11db", "id": "Recovering w from Alpha", "label": "Recovering w from Alpha", "shape": "dot", "size": 10, "title": "Using optimal alphas to find the optimal weight vector w"}, {"color": "#d33123", "id": "Optimal Intercept b", "label": "Optimal Intercept b", "shape": "dot", "size": 10, "title": "Calculating the intercept term using the primal problem constraints"}, {"color": "#3c9d43", "id": "Optimal Parameters Calculation", "label": "Optimal Parameters Calculation", "shape": "dot", "size": 10, "title": "Calculation of optimal parameters w and b from training data."}, {"color": "#899f68", "id": "Intercept Term Calculation", "label": "Intercept Term Calculation", "shape": "dot", "size": 10, "title": "Finding the intercept term b using the primal problem solution."}, {"color": "#94dce6", "id": "Prediction Equation", "label": "Prediction Equation", "shape": "dot", "size": 10, "title": "Equation for prediction based on inner product and support vectors."}, {"color": "#2d0dae", "id": "Dual Form Insight", "label": "Dual Form Insight", "shape": "dot", "size": 10, "title": "Insight gained from the dual form of optimization problem."}, {"color": "#9f364b", "id": "Kernel Application", "label": "Kernel Application", "shape": "dot", "size": 10, "title": "Application of kernels to classification problems in SVMs."}, {"color": "#87d584", "id": "Non-separable Case", "label": "Non-separable Case", "shape": "dot", "size": 10, "title": "Handling datasets that are not linearly separable with SVMs."}, {"color": "#f79f27", "id": "L1 Regularization", "label": "L1 Regularization", "shape": "dot", "size": 10, "title": "Penalizes the sum of absolute values of coefficients to handle outliers."}, {"color": "#d7179a", "id": "Dual Formulation of SVM", "label": "Dual Formulation of SVM", "shape": "dot", "size": 10, "title": "Optimization problem reformulated to solve for Lagrange multipliers."}, {"color": "#e50a94", "id": "Sequential Minimal Optimization (SMO) Algorithm", "label": "Sequential Minimal Optimization (SMO) Algorithm", "shape": "dot", "size": 10, "title": "Efficient algorithm to solve the dual problem of SVM optimization."}, {"color": "#2af3ea", "id": "Coordinate Ascent Algorithm", "label": "Coordinate Ascent Algorithm", "shape": "dot", "size": 10, "title": "Optimization technique for solving unconstrained problems by iteratively optimizing one variable at a time."}, {"color": "#0918e0", "id": "Machine Learning Optimization Techniques", "label": "Machine Learning Optimization Techniques", "shape": "star", "size": 25, "title": "Techniques for optimizing functions in machine learning problems."}, {"color": "#23a463", "id": "Quadratic Function Contours", "label": "Quadratic Function Contours", "shape": "dot", "size": 10, "title": "Visual representation of the contours of a quadratic function being optimized."}, {"color": "#d5f1d5", "id": "Dual Optimization Problem", "label": "Dual Optimization Problem", "shape": "dot", "size": 10, "title": "The optimization problem in the dual form for SVMs."}, {"color": "#196b35", "id": "Sequential Minimal Optimization (SMO)", "label": "Sequential Minimal Optimization (SMO)", "shape": "dot", "size": 10, "title": "Algorithm for solving the optimization problem in SVM efficiently."}, {"color": "#1739f2", "id": "Convergence Criteria", "label": "Convergence Criteria", "shape": "dot", "size": 10, "title": "Conditions to determine if SMO has reached a solution."}, {"color": "#4c4415", "id": "Efficient Update Mechanism", "label": "Efficient Update Mechanism", "shape": "dot", "size": 10, "title": "Method to update alpha values efficiently in SMO."}, {"color": "#eda7b5", "id": "Constraints Handling", "label": "Constraints Handling", "shape": "dot", "size": 10, "title": "Management of constraints during the optimization process."}, {"color": "#d326c2", "id": "Alpha Variables", "label": "Alpha Variables", "shape": "dot", "size": 10, "title": "Variables \u03b11 and \u03b22 used to define constraints."}, {"color": "#47c820", "id": "Quadratic Function", "label": "Quadratic Function", "shape": "dot", "size": 10, "title": "Function representing the objective in terms of quadratic form."}, {"color": "#9a6706", "id": "Box Constraint", "label": "Box Constraint", "shape": "dot", "size": 10, "title": "Constraints defining permissible values for \u03b12 within a specified range [L, H]."}, {"color": "#6fa137", "id": "Derivation Example", "label": "Derivation Example", "shape": "dot", "size": 10, "title": "Example of deriving \u03b11 as a function of \u03b12 and y^(2)."}, {"color": "#d953d6", "id": "Machine Learning Overview", "label": "Machine Learning Overview", "shape": "star", "size": 25, "title": "General introduction to machine learning concepts and algorithms."}, {"color": "#756db4", "id": "Alpha Value Update", "label": "Alpha Value Update", "shape": "dot", "size": 10, "title": "Process of updating alpha values within SMO constraints."}, {"color": "#967627", "id": "Deep Learning Introduction", "label": "Deep Learning Introduction", "shape": "star", "size": 25, "title": "Introduction to deep learning concepts and neural networks."}, {"color": "#436f92", "id": "Supervised Learning with Non-Linear Models", "label": "Supervised Learning with Non-Linear Models", "shape": "dot", "size": 10, "title": "Exploration of non-linear models in supervised learning context."}, {"color": "#2178d3", "id": "Non-linear Model h_\u03b8(x)", "label": "Non-linear Model h_\u03b8(x)", "shape": "dot", "size": 10, "title": "An abstract non-linear model for regression and classification tasks."}, {"color": "#0c92f3", "id": "Training Examples", "label": "Training Examples", "shape": "dot", "size": 10, "title": "Set of training data used to learn the model parameters."}, {"color": "#f5ae6d", "id": "Regression Problems", "label": "Regression Problems", "shape": "dot", "size": 10, "title": "Problems where output is a real number, using least square cost function."}, {"color": "#73d1a6", "id": "Mean-Square Cost Function", "label": "Mean-Square Cost Function", "shape": "dot", "size": 10, "title": "Cost function for regression tasks, averaged over all training examples."}, {"color": "#972af8", "id": "Average Loss", "label": "Average Loss", "shape": "dot", "size": 10, "title": "Total loss divided by number of examples."}, {"color": "#4122a7", "id": "Conditional Probabilistic Models", "label": "Conditional Probabilistic Models", "shape": "dot", "size": 10, "title": "Models where output distribution depends on input features."}, {"color": "#5f13e4", "id": "Optimizers", "label": "Optimizers", "shape": "star", "size": 25, "title": "Algorithms for minimizing loss functions."}, {"color": "#de21ac", "id": "Gradient Descent (GD)", "label": "Gradient Descent (GD)", "shape": "dot", "size": 10, "title": "Iterative optimization algorithm using gradients of the function."}, {"color": "#1ff106", "id": "Stochastic Gradient Descent (SGD)", "label": "Stochastic Gradient Descent (SGD)", "shape": "dot", "size": 10, "title": "Variant of GD that uses a single example for each update."}, {"color": "#28c837", "id": "Learning Rate", "label": "Learning Rate", "shape": "dot", "size": 10, "title": "Hyperparameter controlling the size of steps in gradient descent."}, {"color": "#fac13a", "id": "Negative Likelihood Loss Function", "label": "Negative Likelihood Loss Function", "shape": "dot", "size": 10, "title": "Loss function derived from negative log-likelihood for binary classification."}, {"color": "#077f19", "id": "Total Loss Function", "label": "Total Loss Function", "shape": "dot", "size": 10, "title": "Average of loss functions over individual training examples."}, {"color": "#dccf0c", "id": "Logits in Multi-class", "label": "Logits in Multi-class", "shape": "dot", "size": 10, "title": "Output of the model before applying softmax function, representing predictions for each class."}, {"color": "#f94e78", "id": "Negative Log-likelihood Loss Function (Multi-class)", "label": "Negative Log-likelihood Loss Function (Multi-class)", "shape": "dot", "size": 10, "title": "Loss function derived from negative log-likelihood in multi-class classification."}, {"color": "#344777", "id": "Single Neuron Network", "label": "Single Neuron Network", "shape": "dot", "size": 10, "title": "A basic network with a single neuron for simple predictions."}, {"color": "#4eb810", "id": "Housing Price Prediction", "label": "Housing Price Prediction", "shape": "dot", "size": 10, "title": "Predicting housing prices based on derived features such as family size, walkability, and school quality."}, {"color": "#4ac08f", "id": "ReLU Function", "label": "ReLU Function", "shape": "dot", "size": 10, "title": "Element-wise non-linear transformation using the Rectified Linear Unit (ReLU) function."}, {"color": "#51ab20", "id": "Activation Functions", "label": "Activation Functions", "shape": "dot", "size": 10, "title": "Functions that introduce non-linearity in the network, such as ReLU."}, {"color": "#624312", "id": "Single Neuron Model", "label": "Single Neuron Model", "shape": "dot", "size": 10, "title": "Model with a single neuron including weight vector, bias term, and activation function."}, {"color": "#c8526d", "id": "Stacking Neurons", "label": "Stacking Neurons", "shape": "dot", "size": 10, "title": "Process of combining multiple neurons to form complex neural networks."}, {"color": "#ee4ee8", "id": "Housing Prediction Example", "label": "Housing Prediction Example", "shape": "dot", "size": 10, "title": "Illustration using housing price prediction with multiple features and layers."}, {"color": "#092d09", "id": "Mini-batch SGD", "label": "Mini-batch SGD", "shape": "dot", "size": 10, "title": "Variant of SGD that uses small batches of data for updates."}, {"color": "#1e8030", "id": "Hyperparameters", "label": "Hyperparameters", "shape": "dot", "size": 10, "title": "Learning rate and number of iterations in the algorithm."}, {"color": "#eb2ef1", "id": "Mini-batch Hyperparameters", "label": "Mini-batch Hyperparameters", "shape": "dot", "size": 10, "title": "Includes learning rate, batch size, and number of iterations."}, {"color": "#65f732", "id": "Parameter Initialization", "label": "Parameter Initialization", "shape": "dot", "size": 10, "title": "Random initialization of parameters before training starts."}, {"color": "#d91970", "id": "Mini-batch Gradient Calculation", "label": "Mini-batch Gradient Calculation", "shape": "dot", "size": 10, "title": "Gradient calculation using multiple examples simultaneously."}, {"color": "#d0c8e0", "id": "Deep Learning Model Training Steps", "label": "Deep Learning Model Training Steps", "shape": "dot", "size": 10, "title": "Steps to train a deep learning model including parametrization and backpropagation."}, {"color": "#2c9c21", "id": "Neural Networks Overview", "label": "Neural Networks Overview", "shape": "star", "size": 25, "title": "Introduction to neural networks as non-linear models for regression and classification problems."}, {"color": "#0572bf", "id": "Parameters (\u03b8)", "label": "Parameters (\u03b8)", "shape": "dot", "size": 10, "title": "Set of parameters that define the model\u0027s behavior and are learned during training."}, {"color": "#d684a7", "id": "Biological Inspiration", "label": "Biological Inspiration", "shape": "dot", "size": 10, "title": "Comparison between artificial neural networks and biological neural systems."}, {"color": "#ae3a7a", "id": "Two-Layer Neural Network", "label": "Two-Layer Neural Network", "shape": "dot", "size": 10, "title": "A simple model with two layers for predicting housing prices using ReLU activation function."}, {"color": "#e57053", "id": "Derived Features", "label": "Derived Features", "shape": "dot", "size": 10, "title": "Features like family size, walkable neighborhood, and school quality that influence house price."}, {"color": "#ac4de5", "id": "Family Size", "label": "Family Size", "shape": "dot", "size": 10, "title": "Size of the household based on house dimensions and number of bedrooms."}, {"color": "#65e98e", "id": "Walkability", "label": "Walkability", "shape": "dot", "size": 10, "title": "Measure of how easily one can walk to amenities like grocery stores in a neighborhood."}, {"color": "#316ea3", "id": "School Quality", "label": "School Quality", "shape": "dot", "size": 10, "title": "Quality of local elementary schools based on neighborhood wealth and zip code."}, {"color": "#5373ca", "id": "Neural Network Inputs", "label": "Neural Network Inputs", "shape": "dot", "size": 10, "title": "Input features for a neural network, such as house dimensions and number of bedrooms."}, {"color": "#71cfda", "id": "Hidden Units", "label": "Hidden Units", "shape": "dot", "size": 10, "title": "Intermediate variables (hidden units) in the neural network that process input features."}, {"color": "#095b8a", "id": "ReLU Activation Function", "label": "ReLU Activation Function", "shape": "dot", "size": 10, "title": "Rectified Linear Unit activation function used for hidden layers to introduce non-linearity."}, {"color": "#1de4b0", "id": "Output Layer", "label": "Output Layer", "shape": "dot", "size": 10, "title": "Final layer of the neural network that produces the output based on processed input features."}, {"color": "#63cf89", "id": "Vectorization in Neural Networks", "label": "Vectorization in Neural Networks", "shape": "star", "size": 25, "title": "Process of converting loops into matrix operations for efficiency."}, {"color": "#855466", "id": "Matrix Algebra", "label": "Matrix Algebra", "shape": "dot", "size": 10, "title": "Use of matrices and vectors to represent neural network computations."}, {"color": "#4af53e", "id": "BLAS Packages", "label": "BLAS Packages", "shape": "dot", "size": 10, "title": "Highly optimized numerical linear algebra libraries for fast computation."}, {"color": "#544f09", "id": "Two-Layer Fully-Connected Network", "label": "Two-Layer Fully-Connected Network", "shape": "dot", "size": 10, "title": "Example of a neural network structure used to illustrate vectorization concepts."}, {"color": "#bd65f7", "id": "Weight Matrix W^[1]", "label": "Weight Matrix W^[1]", "shape": "dot", "size": 10, "title": "Matrix representation of weights connecting input layer to hidden layer."}, {"color": "#b6d4b5", "id": "Fully-Connected Neural Networks", "label": "Fully-Connected Neural Networks", "shape": "dot", "size": 10, "title": "A type of neural network where each neuron is connected to every neuron in the previous layer."}, {"color": "#f57b24", "id": "Intermediate Variables (a_i)", "label": "Intermediate Variables (a_i)", "shape": "dot", "size": 10, "title": "Variables that depend on all inputs and are used for computation within layers."}, {"color": "#84435c", "id": "Vectorization", "label": "Vectorization", "shape": "dot", "size": 10, "title": "Process of using matrix and vector notations to simplify expressions and improve computational efficiency."}, {"color": "#bf7525", "id": "Weight Matrices", "label": "Weight Matrices", "shape": "dot", "size": 10, "title": "Explanation of weight matrices in the context of layers W^[1] and W^[2]."}, {"color": "#e8036a", "id": "Bias Vectors", "label": "Bias Vectors", "shape": "dot", "size": 10, "title": "Description of bias vectors b^[1] and b^[2] used in each layer."}, {"color": "#17da69", "id": "Hidden Layer", "label": "Hidden Layer", "shape": "dot", "size": 10, "title": "The activation a as the hidden layer output from ReLU function."}, {"color": "#c9a6ea", "id": "Multi-layer Networks", "label": "Multi-layer Networks", "shape": "dot", "size": 10, "title": "Introduction to multi-layer fully-connected neural networks with more than two layers."}, {"color": "#367346", "id": "Multi-layer Fully-Connected Neural Networks", "label": "Multi-layer Fully-Connected Neural Networks", "shape": "star", "size": 25, "title": "Stacking layers to form deeper neural networks with ReLU activation functions."}, {"color": "#3a335c", "id": "Weight Matrices and Biases", "label": "Weight Matrices and Biases", "shape": "dot", "size": 10, "title": "Dimensions of weight matrices and biases for compatibility in multi-layer networks."}, {"color": "#35e46c", "id": "Total Number of Neurons", "label": "Total Number of Neurons", "shape": "dot", "size": 10, "title": "Summation of neurons across all layers in the network."}, {"color": "#a8e077", "id": "Total Number of Parameters", "label": "Total Number of Parameters", "shape": "dot", "size": 10, "title": "Calculation of total parameters including weights and biases."}, {"color": "#c4d8a1", "id": "Notational Consistency", "label": "Notational Consistency", "shape": "dot", "size": 10, "title": "Using consistent notation for input and output layers."}, {"color": "#382283", "id": "Other Activation Functions", "label": "Other Activation Functions", "shape": "star", "size": 25, "title": "Alternative non-linear functions to ReLU in neural networks."}, {"color": "#4245fa", "id": "Feature Engineering", "label": "Feature Engineering", "shape": "dot", "size": 10, "title": "Process of selecting and transforming raw data into features that improve model performance."}, {"color": "#b7ee61", "id": "Neural Networks Parameters", "label": "Neural Networks Parameters", "shape": "dot", "size": 10, "title": "Parameters in neural networks, including weights and biases."}, {"color": "#118f33", "id": "Learned Features", "label": "Learned Features", "shape": "dot", "size": 10, "title": "Features automatically discovered by deep learning models without explicit feature engineering."}, {"color": "#08cdb7", "id": "Sigmoid Function", "label": "Sigmoid Function", "shape": "dot", "size": 10, "title": "Maps real numbers to (0, 1) range; less commonly used due to vanishing gradient problem."}, {"color": "#ff6afe", "id": "Tanh Function", "label": "Tanh Function", "shape": "dot", "size": 10, "title": "Similar to sigmoid but maps to (-1, 1); also suffers from vanishing gradients."}, {"color": "#21ba7f", "id": "Leaky ReLU", "label": "Leaky ReLU", "shape": "dot", "size": 10, "title": "Variant of ReLU with a small gradient for negative inputs."}, {"color": "#d17d60", "id": "GELU Function", "label": "GELU Function", "shape": "dot", "size": 10, "title": "Gaussian Error Linear Unit; used in advanced NLP models like BERT and GPT."}, {"color": "#a2ed2d", "id": "Softplus Function", "label": "Softplus Function", "shape": "dot", "size": 10, "title": "Smoothed ReLU with a proper second-order derivative but less practical use."}, {"color": "#b5d406", "id": "Identity Function", "label": "Identity Function", "shape": "dot", "size": 10, "title": "Function that outputs the input directly; not used due to lack of non-linearity."}, {"color": "#027568", "id": "Deep Learning Representations", "label": "Deep Learning Representations", "shape": "star", "size": 25, "title": "Discusses how neural networks automatically discover useful features for prediction."}, {"color": "#e6714c", "id": "House Price Prediction Example", "label": "House Price Prediction Example", "shape": "dot", "size": 10, "title": "Illustrates the use of fully-connected neural networks in predicting house prices without specifying intermediate quantities."}, {"color": "#34ebdb", "id": "Feature Maps and Representation Transferability", "label": "Feature Maps and Representation Transferability", "shape": "dot", "size": 10, "title": "Explains how feature maps from one dataset can be useful for other datasets, indicating essential data information."}, {"color": "#e5f750", "id": "Complex Features in Neural Networks", "label": "Complex Features in Neural Networks", "shape": "dot", "size": 10, "title": "Discusses the difficulty of human interpretation of complex features discovered by neural networks."}, {"color": "#4ba4b0", "id": "Matrix Multiplication as a Building Block", "label": "Matrix Multiplication as a Building Block", "shape": "dot", "size": 10, "title": "Describes matrix multiplication operation with parameters W and b, operating on an input z."}, {"color": "#b7b89d", "id": "MLP Composition of Modules", "label": "MLP Composition of Modules", "shape": "dot", "size": 10, "title": "Explains how MLP can be written as a composition of multiple matrix multiplication modules and nonlinear activation modules."}, {"color": "#10815b", "id": "Layer Normalization", "label": "Layer Normalization", "shape": "star", "size": 25, "title": "Normalization technique that normalizes the inputs in each training example independently."}, {"color": "#6b8990", "id": "LN-S(z)", "label": "LN-S(z)", "shape": "dot", "size": 10, "title": "Standardized output of layer normalization before affine transformation."}, {"color": "#86a613", "id": "Affine Transformation", "label": "Affine Transformation", "shape": "dot", "size": 10, "title": "Transformation that scales and shifts the standardized output to desired mean and standard deviation."}, {"color": "#da7c27", "id": "Scaling-Invariant Property", "label": "Scaling-Invariant Property", "shape": "dot", "size": 10, "title": "Property ensuring model invariance under scaling of parameters in subsequent layers."}, {"color": "#61cd71", "id": "MLP (Multi-Layer Perceptron)", "label": "MLP (Multi-Layer Perceptron)", "shape": "dot", "size": 10, "title": "A neural network model composed of multiple layers with matrix multiplication and activation functions."}, {"color": "#307455", "id": "Matrix Multiplication Module", "label": "Matrix Multiplication Module", "shape": "dot", "size": 10, "title": "Basic building block in MLP, involves linear transformation using weights and biases."}, {"color": "#b384de", "id": "Nonlinear Activation Module", "label": "Nonlinear Activation Module", "shape": "dot", "size": 10, "title": "Applies a nonlinear function to the output of matrix multiplication modules."}, {"color": "#332eba", "id": "ResNet (Residual Network)", "label": "ResNet (Residual Network)", "shape": "dot", "size": 10, "title": "Neural network architecture that uses residual blocks for improved training of deep networks."}, {"color": "#5b94bb", "id": "Residual Block", "label": "Residual Block", "shape": "dot", "size": 10, "title": "Building block in ResNet, adds input directly to output after nonlinear transformations."}, {"color": "#c31ce6", "id": "Simplified ResNet Architecture", "label": "Simplified ResNet Architecture", "shape": "dot", "size": 10, "title": "A composition of residual blocks followed by a matrix multiplication layer."}, {"color": "#5cd83b", "id": "Machine Learning Architectures", "label": "Machine Learning Architectures", "shape": "star", "size": 25, "title": "Overview of different architectures in machine learning."}, {"color": "#6ebab4", "id": "ResNet Architecture", "label": "ResNet Architecture", "shape": "dot", "size": 10, "title": "Deep residual network architecture using convolution layers and batch normalization."}, {"color": "#41b0cc", "id": "Convolutional Layers", "label": "Convolutional Layers", "shape": "dot", "size": 10, "title": "Layer type commonly used in deep learning for image and signal processing."}, {"color": "#e986fe", "id": "Batch Normalization Variants", "label": "Batch Normalization Variants", "shape": "dot", "size": 10, "title": "Different types of batch normalization techniques used in neural networks."}, {"color": "#637d59", "id": "Transformer Architecture", "label": "Transformer Architecture", "shape": "dot", "size": 10, "title": "Architecture widely used in modern large language models."}, {"color": "#452361", "id": "Layer Normalization (LN)", "label": "Layer Normalization (LN)", "shape": "dot", "size": 10, "title": "Normalization technique applied to layers in neural networks."}, {"color": "#bd976c", "id": "LN-S Module", "label": "LN-S Module", "shape": "dot", "size": 10, "title": "Sub-module of layer normalization that normalizes vector to mean zero and standard deviation one."}, {"color": "#28975b", "id": "Affine Transformation in LN", "label": "Affine Transformation in LN", "shape": "dot", "size": 10, "title": "Transformation using learnable parameters beta and gamma for desired mean and standard deviation."}, {"color": "#94a388", "id": "Parameter Sharing", "label": "Parameter Sharing", "shape": "dot", "size": 10, "title": "Mechanism where the same filter is applied across different positions of input data."}, {"color": "#16e4bd", "id": "Efficiency Comparison", "label": "Efficiency Comparison", "shape": "dot", "size": 10, "title": "Comparison between convolution and generic matrix multiplication in terms of computational efficiency."}, {"color": "#cab256", "id": "Channel Concept", "label": "Channel Concept", "shape": "dot", "size": 10, "title": "Concept describing multiple input/output dimensions within a layer."}, {"color": "#68b2ff", "id": "Convolutional Neural Networks (CNN)", "label": "Convolutional Neural Networks (CNN)", "shape": "dot", "size": 10, "title": "Type of neural network used for image and signal processing."}, {"color": "#c09137", "id": "1-D Convolution Layer", "label": "1-D Convolution Layer", "shape": "dot", "size": 10, "title": "Simplified version of 1-dimensional convolution layer used in CNNs."}, {"color": "#8e3c6c", "id": "Filter Vector", "label": "Filter Vector", "shape": "dot", "size": 10, "title": "Vector of weights used to extract features from input data."}, {"color": "#da6335", "id": "Bias Scalar", "label": "Bias Scalar", "shape": "dot", "size": 10, "title": "Scalar value added to the output of each neuron in the layer."}, {"color": "#6a6334", "id": "Matrix Multiplication with Shared Parameters", "label": "Matrix Multiplication with Shared Parameters", "shape": "dot", "size": 10, "title": "Operation where convolution is represented as a matrix multiplication with shared parameters."}, {"color": "#15c139", "id": "Conv1D-S Module", "label": "Conv1D-S Module", "shape": "dot", "size": 10, "title": "A 1-dimensional convolutional module with specific parameters."}, {"color": "#f537ec", "id": "Total Parameters Conv1D", "label": "Total Parameters Conv1D", "shape": "dot", "size": 10, "title": "Calculation of total number of parameters in a Conv1D layer."}, {"color": "#5bfbff", "id": "2-D Convolution (Conv2D-S)", "label": "2-D Convolution (Conv2D-S)", "shape": "dot", "size": 10, "title": "A 2-dimensional convolutional module with input and output matrices."}, {"color": "#6b82da", "id": "Total Parameters Conv2D", "label": "Total Parameters Conv2D", "shape": "dot", "size": 10, "title": "Calculation of total number of parameters in a Conv2D layer."}, {"color": "#c2d559", "id": "Scale-Invariant Property", "label": "Scale-Invariant Property", "shape": "dot", "size": 10, "title": "Property of modern DL architectures regarding weight scaling."}, {"color": "#8cbcfa", "id": "Other Normalization Layers", "label": "Other Normalization Layers", "shape": "dot", "size": 10, "title": "Alternative normalization techniques used in neural networks."}, {"color": "#fd1c61", "id": "Batch Normalization", "label": "Batch Normalization", "shape": "dot", "size": 10, "title": "Normalization technique commonly used in computer vision applications."}, {"color": "#57edc0", "id": "Group Normalization", "label": "Group Normalization", "shape": "dot", "size": 10, "title": "Normalization method for groups of channels in neural networks."}, {"color": "#8c278b", "id": "Convolutional Neural Networks (CNNs)", "label": "Convolutional Neural Networks (CNNs)", "shape": "dot", "size": 10, "title": "Neural network architecture designed for image and sequence data processing."}, {"color": "#1de449", "id": "Differentiable Circuit", "label": "Differentiable Circuit", "shape": "dot", "size": 10, "title": "Composition of arithmetic operations and elementary functions in a network."}, {"color": "#a639ff", "id": "Gradient Computation", "label": "Gradient Computation", "shape": "dot", "size": 10, "title": "Process of calculating gradients with respect to the loss function and network parameters during backpropagation."}, {"color": "#1c4f6a", "id": "Machine Learning Fundamentals", "label": "Machine Learning Fundamentals", "shape": "star", "size": 25, "title": "Basic concepts and principles of machine learning."}, {"color": "#6e6922", "id": "Backward Function in Machine Learning", "label": "Backward Function in Machine Learning", "shape": "dot", "size": 10, "title": "Explanation of the backward function used to compute gradients."}, {"color": "#aac8eb", "id": "Jacobian Matrix", "label": "Jacobian Matrix", "shape": "dot", "size": 10, "title": "Matrix representation of partial derivatives, not fully detailed here."}, {"color": "#3815a8", "id": "Chain Rule Application", "label": "Chain Rule Application", "shape": "dot", "size": 10, "title": "Use of the chain rule to compute gradients through intermediate variables."}, {"color": "#eec9ba", "id": "Partial Derivatives in ML", "label": "Partial Derivatives in ML", "shape": "dot", "size": 10, "title": "Understanding partial derivatives in the context of machine learning functions."}, {"color": "#65da76", "id": "Chain Rule for Auto-Differentiation", "label": "Chain Rule for Auto-Differentiation", "shape": "dot", "size": 10, "title": "Application of chain rule to compute gradients efficiently in neural networks."}, {"color": "#6bb28b", "id": "Scalar Functions and Vectors", "label": "Scalar Functions and Vectors", "shape": "dot", "size": 10, "title": "Focus on derivatives involving scalar functions with respect to vectors or matrices."}, {"color": "#26e477", "id": "Multi-Variate Function Challenges", "label": "Multi-Variate Function Challenges", "shape": "dot", "size": 10, "title": "Challenges and computational difficulties of dealing with multi-variate function derivatives."}, {"color": "#f8c563", "id": "Chain Rule", "label": "Chain Rule", "shape": "dot", "size": 10, "title": "Mathematical rule for computing derivatives of composite functions."}, {"color": "#058f6e", "id": "Loss Function Composition", "label": "Loss Function Composition", "shape": "dot", "size": 10, "title": "Abstract representation of loss functions as compositions of modules."}, {"color": "#efaad4", "id": "Auto-Differentiation", "label": "Auto-Differentiation", "shape": "dot", "size": 10, "title": "Automatic computation of gradients in deep learning frameworks."}, {"color": "#facd59", "id": "Deep Learning Packages", "label": "Deep Learning Packages", "shape": "dot", "size": 10, "title": "Software libraries for implementing neural networks and machine learning models."}, {"color": "#ebcc35", "id": "Backpropagation Algorithm", "label": "Backpropagation Algorithm", "shape": "dot", "size": 10, "title": "Algorithm for computing gradients in neural networks."}, {"color": "#03d3b6", "id": "Efficiency of Backward Functions", "label": "Efficiency of Backward Functions", "shape": "dot", "size": 10, "title": "Efficient computation of backward functions for atomic modules."}, {"color": "#d80141", "id": "Backward Functions", "label": "Backward Functions", "shape": "dot", "size": 10, "title": "Computation of backward functions for modules used in networks."}, {"color": "#489795", "id": "Matrix Multiplication Module (MM)", "label": "Matrix Multiplication Module (MM)", "shape": "dot", "size": 10, "title": "Calculation of backward function using equation 7.62 and 7.63."}, {"color": "#8e8ec0", "id": "Activations", "label": "Activations", "shape": "dot", "size": 10, "title": "Computation of backward functions for activation modules."}, {"color": "#c50b21", "id": "Backward Function Overview", "label": "Backward Function Overview", "shape": "star", "size": 25, "title": "Overview of backward functions in machine learning."}, {"color": "#1449e6", "id": "Matrix Multiplication Backward Function", "label": "Matrix Multiplication Backward Function", "shape": "dot", "size": 10, "title": "Details on the backward function for matrix multiplication operations."}, {"color": "#d61655", "id": "Vectorized Notation", "label": "Vectorized Notation", "shape": "dot", "size": 10, "title": "Explanation of vectorized notation used in backward functions."}, {"color": "#5c36e7", "id": "Efficiency Considerations", "label": "Efficiency Considerations", "shape": "dot", "size": 10, "title": "Discussion on computational efficiency for matrix multiplication operations."}, {"color": "#dc049c", "id": "Activation Functions Backward Function", "label": "Activation Functions Backward Function", "shape": "dot", "size": 10, "title": "Details on the backward function for activation functions in neural networks."}, {"color": "#3ee0f1", "id": "Binary Classification Problem", "label": "Binary Classification Problem", "shape": "dot", "size": 10, "title": "A specific type of classification problem with two classes"}, {"color": "#9004ef", "id": "MLP Model", "label": "MLP Model", "shape": "dot", "size": 10, "title": "Multi-layer perceptron model used in binary classification"}, {"color": "#64140b", "id": "Modules and Parameters", "label": "Modules and Parameters", "shape": "dot", "size": 10, "title": "Description of modules involved in MLP with parameters or fixed operations"}, {"color": "#1d8def", "id": "Intermediate Variables", "label": "Intermediate Variables", "shape": "dot", "size": 10, "title": "Variables used during the computation process in a loss function"}, {"color": "#c99380", "id": "Forward Pass", "label": "Forward Pass", "shape": "dot", "size": 10, "title": "Process of computing intermediate variables sequentially"}, {"color": "#f0be45", "id": "Backward Pass", "label": "Backward Pass", "shape": "dot", "size": 10, "title": "Calculation of derivatives in reverse order to compute gradients"}, {"color": "#7e46a9", "id": "Intermediate Values Storage", "label": "Intermediate Values Storage", "shape": "dot", "size": 10, "title": "Storing values like $a^{[i]}$ and $z^{[i]}$ after forward pass."}, {"color": "#453bd7", "id": "Parallelism in Training Examples", "label": "Parallelism in Training Examples", "shape": "star", "size": 25, "title": "Using matrix notation to handle multiple training examples simultaneously."}, {"color": "#d7e117", "id": "Basic Idea", "label": "Basic Idea", "shape": "dot", "size": 10, "title": "Concept of evaluating forward and backward passes for multiple examples using matrices."}, {"color": "#60bbff", "id": "Backward Function for Loss Functions", "label": "Backward Function for Loss Functions", "shape": "dot", "size": 10, "title": "Explains the backward function computation for different loss functions."}, {"color": "#d64b16", "id": "Squared Loss (MSE)", "label": "Squared Loss (MSE)", "shape": "dot", "size": 10, "title": "Details the backward function for squared loss or mean squared error."}, {"color": "#fb84bd", "id": "Logistic Loss", "label": "Logistic Loss", "shape": "dot", "size": 10, "title": "Explains the backward pass computation for logistic loss."}, {"color": "#bb0ebc", "id": "Machine Learning Loss Functions", "label": "Machine Learning Loss Functions", "shape": "star", "size": 25, "title": "Overview of loss functions used in machine learning models."}, {"color": "#bcd708", "id": "Cross-Entropy Loss Function", "label": "Cross-Entropy Loss Function", "shape": "dot", "size": 10, "title": "Loss function used in classification problems to measure the dissimilarity between predicted and actual probability distributions."}, {"color": "#88fd83", "id": "Forward Pass in MLP", "label": "Forward Pass in MLP", "shape": "dot", "size": 10, "title": "Sequence of operations that transforms input data into predictions using an MLP model."}, {"color": "#2d0a55", "id": "Matrix Notation in Machine Learning", "label": "Matrix Notation in Machine Learning", "shape": "star", "size": 25, "title": "Overview of using matrix notation for machine learning operations."}, {"color": "#0e187a", "id": "Training Examples Representation", "label": "Training Examples Representation", "shape": "dot", "size": 10, "title": "Representation of multiple training examples in matrix form."}, {"color": "#65b12d", "id": "First-Layer Activations", "label": "First-Layer Activations", "shape": "dot", "size": 10, "title": "Calculation of first-layer activations for each example using matrix notation."}, {"color": "#92e007", "id": "Broadcasting", "label": "Broadcasting", "shape": "dot", "size": 10, "title": "Technique used for adding a scalar or column vector to each column of a matrix."}, {"color": "#230309", "id": "Generalization to Multiple Layers", "label": "Generalization to Multiple Layers", "shape": "dot", "size": 10, "title": "Extension of the matricization approach to multiple layers with implementation subtleties."}, {"color": "#974428", "id": "Matricization Approach", "label": "Matricization Approach", "shape": "dot", "size": 10, "title": "Generalizing the approach to multiple layers with implementation subtleties."}, {"color": "#3d7ada", "id": "Implementation Subtlety", "label": "Implementation Subtlety", "shape": "dot", "size": 10, "title": "Handling data points as rows in deep learning packages vs columns in papers."}, {"color": "#3e5c6f", "id": "Data Matrix Representation", "label": "Data Matrix Representation", "shape": "dot", "size": 10, "title": "Conversion between row and column vector representations for consistency."}, {"color": "#bad0b1", "id": "Generalization and Regularization", "label": "Generalization and Regularization", "shape": "star", "size": 25, "title": "Analyzing model performance on unseen test data."}, {"color": "#d502ca", "id": "Training Loss Function", "label": "Training Loss Function", "shape": "dot", "size": 10, "title": "Function used to fit the training dataset for supervised learning problems."}, {"color": "#d66749", "id": "Training Loss", "label": "Training Loss", "shape": "dot", "size": 10, "title": "The loss calculated on the training dataset, also known as empirical loss or risk."}, {"color": "#012a03", "id": "Test Error", "label": "Test Error", "shape": "dot", "size": 10, "title": "Evaluation metric for a model\u0027s performance on unseen data, representing true generalization ability."}, {"color": "#0a7182", "id": "Mean Squared Error (MSE)", "label": "Mean Squared Error (MSE)", "shape": "dot", "size": 10, "title": "A specific loss function that measures the average squared difference between predicted and actual values."}, {"color": "#766978", "id": "Empirical Distribution", "label": "Empirical Distribution", "shape": "dot", "size": 10, "title": "The distribution of training data, used to approximate population distribution for empirical risk minimization."}, {"color": "#a115f6", "id": "Population Distribution", "label": "Population Distribution", "shape": "dot", "size": 10, "title": "True underlying distribution from which test examples are drawn, representing the target generalization goal."}, {"color": "#651ae3", "id": "Training vs Test Distributions", "label": "Training vs Test Distributions", "shape": "dot", "size": 10, "title": "Difference between training and test distributions and its impact on model performance."}, {"color": "#3740df", "id": "Generalization Gap", "label": "Generalization Gap", "shape": "dot", "size": 10, "title": "Difference between training error and test error indicating model\u0027s ability to generalize."}, {"color": "#c07355", "id": "Bias-Variance Tradeoff", "label": "Bias-Variance Tradeoff", "shape": "star", "size": 25, "title": "Decomposition of test error into bias and variance components for understanding overfitting and underfitting."}, {"color": "#91ddd1", "id": "Double Descent Phenomenon", "label": "Double Descent Phenomenon", "shape": "dot", "size": 10, "title": "Phenomenon where test errors decrease, increase, then decrease again with increasing model parameters or data samples."}, {"color": "#118a2c", "id": "Training and Test Datasets", "label": "Training and Test Datasets", "shape": "dot", "size": 10, "title": "Illustration of datasets used to evaluate model performance."}, {"color": "#5dd623", "id": "Linear Regression Models", "label": "Linear Regression Models", "shape": "dot", "size": 10, "title": "Examples of linear models and their limitations in capturing non-linear relationships."}, {"color": "#0c4923", "id": "Quadratic Function Example", "label": "Quadratic Function Example", "shape": "dot", "size": 10, "title": "Example using a quadratic function to demonstrate bias-variance tradeoff concepts."}, {"color": "#5bf171", "id": "Polynomial Fitting", "label": "Polynomial Fitting", "shape": "dot", "size": 10, "title": "Fitting polynomials to data sets and the issues that arise."}, {"color": "#10ad2f", "id": "Variance", "label": "Variance", "shape": "dot", "size": 10, "title": "Measure of variability in models trained on different datasets from the same distribution."}, {"color": "#0e786b", "id": "Linear Model Limitations", "label": "Linear Model Limitations", "shape": "dot", "size": 10, "title": "Explains the limitations of linear models in capturing data structure."}, {"color": "#09dd7d", "id": "Bias Definition", "label": "Bias Definition", "shape": "dot", "size": 10, "title": "Defines bias as test error with infinite training data."}, {"color": "#cf8852", "id": "5th Degree Polynomial Models", "label": "5th Degree Polynomial Models", "shape": "dot", "size": 10, "title": "Discusses the behavior of 5th degree polynomial models with training data."}, {"color": "#48594a", "id": "Generalization Failure", "label": "Generalization Failure", "shape": "dot", "size": 10, "title": "Describes the failure of 5th degree polynomials to generalize well on test data."}, {"color": "#86247b", "id": "Model Complexity", "label": "Model Complexity", "shape": "dot", "size": 10, "title": "Measure of the simplicity or complexity of a model, often related to parameters."}, {"color": "#20e897", "id": "Test Error Decomposition", "label": "Test Error Decomposition", "shape": "dot", "size": 10, "title": "Breakdown of test error into bias and variance components."}, {"color": "#81e1e9", "id": "Bias Term", "label": "Bias Term", "shape": "dot", "size": 10, "title": "Error due to overly simplistic models unable to capture true relationships."}, {"color": "#13d5de", "id": "Variance Term", "label": "Variance Term", "shape": "dot", "size": 10, "title": "Error due to model\u0027s sensitivity to training data variations."}, {"color": "#e0bb2d", "id": "Model-wise Double Descent", "label": "Model-wise Double Descent", "shape": "dot", "size": 10, "title": "Peak in test error related to model capacity and sample size relationship."}, {"color": "#87fcbf", "id": "Model Evaluation", "label": "Model Evaluation", "shape": "dot", "size": 10, "title": "Assessment of model performance using metrics like MSE."}, {"color": "#2a8f86", "id": "Average Model (h_avg)", "label": "Average Model (h_avg)", "shape": "dot", "size": 10, "title": "Hypothetical model representing the average of predictions from an infinite number of datasets."}, {"color": "#279b2d", "id": "Unavoidable Noise (\u03c3^2)", "label": "Unavoidable Noise (\u03c3^2)", "shape": "dot", "size": 10, "title": "Inherent noise in data that cannot be reduced by any model improvement."}, {"color": "#bcddc8", "id": "Training Dataset", "label": "Training Dataset", "shape": "dot", "size": 10, "title": "Set of data used to train a machine learning model."}, {"color": "#79e122", "id": "Test Example", "label": "Test Example", "shape": "dot", "size": 10, "title": "Single instance used for evaluating the performance of a trained model."}, {"color": "#2c2892", "id": "Expected Test Error", "label": "Expected Test Error", "shape": "dot", "size": 10, "title": "Average error expected on unseen data after training with random datasets."}, {"color": "#e28e16", "id": "Claim 8.1.1", "label": "Claim 8.1.1", "shape": "dot", "size": 10, "title": "Mathematical tool for decomposing MSE into bias and variance terms."}, {"color": "#e19907", "id": "Model Complexity and Test Errors", "label": "Model Complexity and Test Errors", "shape": "dot", "size": 10, "title": "Relationship between model complexity and test error performance."}, {"color": "#216ff0", "id": "Overparameterized Models", "label": "Overparameterized Models", "shape": "dot", "size": 10, "title": "Models with more parameters than necessary for the training dataset."}, {"color": "#c74752", "id": "Sample-wise Double Descent", "label": "Sample-wise Double Descent", "shape": "dot", "size": 10, "title": "Test error pattern observed as the number of training samples increases."}, {"color": "#094299", "id": "Historical Context and Recent Discoveries", "label": "Historical Context and Recent Discoveries", "shape": "dot", "size": 10, "title": "Overview of historical context and recent advancements in understanding double descent phenomena."}, {"color": "#240c8a", "id": "Optimal Algorithms", "label": "Optimal Algorithms", "shape": "dot", "size": 10, "title": "Algorithms that can achieve lower test errors when samples are limited."}, {"color": "#c10324", "id": "Regularization Tuning", "label": "Regularization Tuning", "shape": "dot", "size": 10, "title": "Optimal regularization helps mitigate double descent issues."}, {"color": "#c58e6d", "id": "Implicit Regularization", "label": "Implicit Regularization", "shape": "dot", "size": 10, "title": "Optimizer effects like gradient descent provide implicit regularization in overparameterized models."}, {"color": "#53da2b", "id": "Learning Guarantees", "label": "Learning Guarantees", "shape": "dot", "size": 10, "title": "Conditions under which learning algorithms work well."}, {"color": "#9f6470", "id": "Union Bound Lemma", "label": "Union Bound Lemma", "shape": "dot", "size": 10, "title": "Probability bound for union of events."}, {"color": "#7f4782", "id": "Hoeffding Inequality (Chernoff Bound)", "label": "Hoeffding Inequality (Chernoff Bound)", "shape": "dot", "size": 10, "title": "Bound on deviation from true value in Bernoulli distribution."}, {"color": "#1f693d", "id": "Sample Complexity Bounds", "label": "Sample Complexity Bounds", "shape": "dot", "size": 10, "title": "Theoretical bounds on the number of samples needed for learning algorithms to generalize well."}, {"color": "#5f0488", "id": "Model Selection Methods", "label": "Model Selection Methods", "shape": "dot", "size": 10, "title": "Techniques for choosing the right level of model complexity based on training data."}, {"color": "#3f25d4", "id": "Generalization Error", "label": "Generalization Error", "shape": "dot", "size": 10, "title": "Error made by a predictive model when making predictions on data not seen during training."}, {"color": "#53ab14", "id": "Learning Theory", "label": "Learning Theory", "shape": "dot", "size": 10, "title": "Theoretical foundations of machine learning, including generalization bounds and sample complexity."}, {"color": "#cf2019", "id": "Gradient Descent Optimizer", "label": "Gradient Descent Optimizer", "shape": "dot", "size": 10, "title": "Optimizer used to find the minimum norm solution for linear models."}, {"color": "#ad783e", "id": "Minimum Norm Solution", "label": "Minimum Norm Solution", "shape": "dot", "size": 10, "title": "Solution found by gradient descent with zero initialization in overparameterized regime."}, {"color": "#5246d2", "id": "Model Complexity Measures", "label": "Model Complexity Measures", "shape": "dot", "size": 10, "title": "Different measures of model complexity such as number of parameters or norm."}, {"color": "#c3e263", "id": "Number of Parameters", "label": "Number of Parameters", "shape": "dot", "size": 10, "title": "Common measure of model complexity, often leading to double descent phenomenon."}, {"color": "#c73664", "id": "Norm of Learned Model", "label": "Norm of Learned Model", "shape": "dot", "size": 10, "title": "Alternative measure that can mitigate the occurrence of double descent."}, {"color": "#cca043", "id": "Regularization Strength", "label": "Regularization Strength", "shape": "dot", "size": 10, "title": "Impact of regularization on mitigating double descent phenomenon."}, {"color": "#b123e3", "id": "Hypothesis Function", "label": "Hypothesis Function", "shape": "dot", "size": 10, "title": "Function that maps input data to predictions."}, {"color": "#6f320b", "id": "Training Error", "label": "Training Error", "shape": "dot", "size": 10, "title": "Error measured on the dataset used to train a model."}, {"color": "#2eb15e", "id": "Empirical Risk Minimization (ERM)", "label": "Empirical Risk Minimization (ERM)", "shape": "dot", "size": 10, "title": "Learning algorithm that minimizes the empirical risk over the training set."}, {"color": "#a934e4", "id": "PAC Assumptions", "label": "PAC Assumptions", "shape": "dot", "size": 10, "title": "Framework for learning theory assumptions, including same-distribution assumption and i.i.d. samples."}, {"color": "#2bbb82", "id": "Hypothesis Class", "label": "Hypothesis Class", "shape": "dot", "size": 10, "title": "Set of all possible hypotheses a learning algorithm can choose from."}, {"color": "#92cd9f", "id": "Finite Hypothesis Classes", "label": "Finite Hypothesis Classes", "shape": "dot", "size": 10, "title": "Learning problems with a finite set of hypotheses."}, {"color": "#b84ef0", "id": "Uniform Convergence", "label": "Uniform Convergence", "shape": "star", "size": 25, "title": "Property ensuring that the difference between empirical and true errors is small with high probability."}, {"color": "#221591", "id": "Training Error vs Generalization Error", "label": "Training Error vs Generalization Error", "shape": "dot", "size": 10, "title": "Discusses how training error relates to generalization error across different hypotheses."}, {"color": "#d915f4", "id": "Union Bound Application", "label": "Union Bound Application", "shape": "dot", "size": 10, "title": "Uses the union bound to extend a probability result from one hypothesis to all in a class."}, {"color": "#e44dce", "id": "Probability of Error", "label": "Probability of Error", "shape": "dot", "size": 10, "title": "Analyzes how error probabilities relate to training and generalization errors."}, {"color": "#70a7ce", "id": "Sample Size Determination", "label": "Sample Size Determination", "shape": "dot", "size": 10, "title": "Determines the sample size needed for a given probability of error bound."}, {"color": "#d4ee94", "id": "Empirical Risk Minimization", "label": "Empirical Risk Minimization", "shape": "dot", "size": 10, "title": "Process of selecting a hypothesis with the smallest training error."}, {"color": "#890d50", "id": "Generalization Error Guarantees", "label": "Generalization Error Guarantees", "shape": "dot", "size": 10, "title": "Strategies for ensuring that training error closely approximates generalization error."}, {"color": "#33d121", "id": "Bernoulli Random Variable Z", "label": "Bernoulli Random Variable Z", "shape": "dot", "size": 10, "title": "Random variable indicating whether a hypothesis misclassifies an example drawn from the distribution."}, {"color": "#5b2d26", "id": "Training Set Sampling", "label": "Training Set Sampling", "shape": "dot", "size": 10, "title": "Process of drawing examples independently and identically distributed (iid) from \u0394."}, {"color": "#85f859", "id": "Hoeffding Inequality", "label": "Hoeffding Inequality", "shape": "dot", "size": 10, "title": "Inequality used to bound the probability that training error deviates significantly from generalization error."}, {"color": "#084cb4", "id": "Sample Complexity", "label": "Sample Complexity", "shape": "dot", "size": 10, "title": "Number of training examples needed for a certain level of performance guarantee."}, {"color": "#dee9b4", "id": "Hypotheses Space (H)", "label": "Hypotheses Space (H)", "shape": "dot", "size": 10, "title": "Set of all possible hypotheses or models considered in a learning problem."}, {"color": "#d9b03e", "id": "Optimal Hypothesis (h*)", "label": "Optimal Hypothesis (h*)", "shape": "dot", "size": 10, "title": "Hypothesis with the lowest true error over hypothesis space H."}, {"color": "#803183", "id": "Hypothesis Class Switching", "label": "Hypothesis Class Switching", "shape": "star", "size": 25, "title": "Discussion on switching to a larger hypothesis class and its effects."}, {"color": "#ed9d41", "id": "Bias Decrease", "label": "Bias Decrease", "shape": "dot", "size": 10, "title": "Explains how bias decreases when moving to a larger hypothesis class."}, {"color": "#0df2ab", "id": "Variance Increase", "label": "Variance Increase", "shape": "dot", "size": 10, "title": "Describes the increase in variance with a larger hypothesis class."}, {"color": "#46fb50", "id": "Sample Complexity Bound", "label": "Sample Complexity Bound", "shape": "star", "size": 25, "title": "Derivation of sample complexity bound for finite hypothesis classes."}, {"color": "#bafe0b", "id": "Corollary Proof", "label": "Corollary Proof", "shape": "dot", "size": 10, "title": "Proof involving fixed \u03b4 and \u03b3 to derive n\u0027s value."}, {"color": "#200a40", "id": "Infinite Hypothesis Classes", "label": "Infinite Hypothesis Classes", "shape": "star", "size": 25, "title": "Introduction to dealing with hypothesis classes parameterized by real numbers."}, {"color": "#e44872", "id": "Bit Representation", "label": "Bit Representation", "shape": "dot", "size": 10, "title": "Discussion on the finite representation of real numbers in computers using bits."}, {"color": "#89176f", "id": "Floating Point Representation", "label": "Floating Point Representation", "shape": "dot", "size": 10, "title": "Use of 64-bit floating point numbers in parameter representation."}, {"color": "#bd5ac0", "id": "Hypothesis Class Size", "label": "Hypothesis Class Size", "shape": "dot", "size": 10, "title": "Size of the hypothesis class based on model parameters."}, {"color": "#12d042", "id": "Non-ERM Algorithms", "label": "Non-ERM Algorithms", "shape": "dot", "size": 10, "title": "Learning algorithms that do not rely solely on empirical risk minimization."}, {"color": "#4ddd5c", "id": "Hypothesis Space", "label": "Hypothesis Space", "shape": "dot", "size": 10, "title": "The set of all possible hypotheses that a learner can choose from."}, {"color": "#9211ef", "id": "Parameterization of Hypotheses", "label": "Parameterization of Hypotheses", "shape": "dot", "size": 10, "title": "Different ways to parameterize the same hypothesis space."}, {"color": "#199d1b", "id": "VC Dimension", "label": "VC Dimension", "shape": "dot", "size": 10, "title": "Measure of the capacity of a statistical model, defined as the cardinality of the largest set of points that the model can shatter."}, {"color": "#09697c", "id": "Shattering Sets", "label": "Shattering Sets", "shape": "dot", "size": 10, "title": "A hypothesis class can shatter a set if it can realize any labeling on the set."}, {"color": "#25e63e", "id": "Vapnik\u0027s Theorem", "label": "Vapnik\u0027s Theorem", "shape": "star", "size": 25, "title": "Theorem linking VC dimension to generalization error bounds"}, {"color": "#6bffea", "id": "Corollary on Training Examples", "label": "Corollary on Training Examples", "shape": "star", "size": 25, "title": "Number of examples needed for learning is linear in VC dimension"}, {"color": "#32ad40", "id": "Regularization in Deep Learning", "label": "Regularization in Deep Learning", "shape": "star", "size": 25, "title": "Overview of regularization techniques and their impact on deep learning models."}, {"color": "#b2307a", "id": "Explicit Regularization Techniques", "label": "Explicit Regularization Techniques", "shape": "dot", "size": 10, "title": "Techniques such as weight decay, dropout, data augmentation, spectral norm regularization, and Lipschitzness regularization."}, {"color": "#b81bc2", "id": "Implicit Regularization Effect", "label": "Implicit Regularization Effect", "shape": "dot", "size": 10, "title": "The impact of optimizers on model parameters beyond explicit regularization."}, {"color": "#ee10b7", "id": "Regularization in Machine Learning", "label": "Regularization in Machine Learning", "shape": "star", "size": 25, "title": "Techniques to prevent overfitting by adding a penalty for complexity."}, {"color": "#cc031a", "id": "Sparsity Regularization", "label": "Sparsity Regularization", "shape": "dot", "size": 10, "title": "Imposing sparsity on model parameters to reduce complexity and improve generalization."}, {"color": "#fb539c", "id": "L1 Norm (LASSO)", "label": "L1 Norm (LASSO)", "shape": "dot", "size": 10, "title": "A common relaxation for \u03b8\u20960, promoting sparsity in linear models."}, {"color": "#148e88", "id": "L2 Norm Regularization", "label": "L2 Norm Regularization", "shape": "dot", "size": 10, "title": "Penalizes the squared magnitude of coefficients to prevent overfitting."}, {"color": "#6c8c8a", "id": "Deep Learning Regularization Techniques", "label": "Deep Learning Regularization Techniques", "shape": "dot", "size": 10, "title": "Various methods to regularize neural networks and improve generalization."}, {"color": "#31ad91", "id": "Optimizers and Generalization", "label": "Optimizers and Generalization", "shape": "star", "size": 25, "title": "Discussion on how optimizers affect model generalization."}, {"color": "#71cb84", "id": "Global Minima Variability", "label": "Global Minima Variability", "shape": "dot", "size": 10, "title": "Different global minima can lead to different generalization performance."}, {"color": "#e417ff", "id": "Model Selection via Cross Validation", "label": "Model Selection via Cross Validation", "shape": "star", "size": 25, "title": "Process of selecting models using cross validation techniques."}, {"color": "#76b98f", "id": "Cross Validation Techniques", "label": "Cross Validation Techniques", "shape": "dot", "size": 10, "title": "Techniques used for model selection through cross validation."}, {"color": "#ab6483", "id": "Regularized Loss", "label": "Regularized Loss", "shape": "dot", "size": 10, "title": "Combination of training loss and regularizer term, used in model evaluation."}, {"color": "#bc604e", "id": "Regularizer", "label": "Regularizer", "shape": "dot", "size": 10, "title": "Function that measures model complexity, often \u03b8 norm."}, {"color": "#5adbf1", "id": "\u03bb (Lambda)", "label": "\u03bb (Lambda)", "shape": "dot", "size": 10, "title": "Parameter controlling the trade-off between loss and regularizer."}, {"color": "#ffcac9", "id": "\u039b_2 Regularization", "label": "\u039b_2 Regularization", "shape": "dot", "size": 10, "title": "Encourages small \u03b8 norm, also known as weight decay."}, {"color": "#77b3db", "id": "Weight Decay", "label": "Weight Decay", "shape": "dot", "size": 10, "title": "Effect of gradient descent on regularized loss in deep learning."}, {"color": "#2a2650", "id": "Inductive Bias", "label": "Inductive Bias", "shape": "dot", "size": 10, "title": "Structures or biases imposed by regularization to guide model learning."}, {"color": "#58ef47", "id": "Chapter 9 Regularization and Model Selection", "label": "Chapter 9 Regularization and Model Selection", "shape": "star", "size": 25, "title": "Focuses on techniques for controlling model complexity."}, {"color": "#147699", "id": "Training Loss/Cost Function", "label": "Training Loss/Cost Function", "shape": "dot", "size": 10, "title": "Function used to evaluate the performance of a model during training."}, {"color": "#33db45", "id": "Regularizer Term", "label": "Regularizer Term", "shape": "dot", "size": 10, "title": "Additional term added to the loss function to control complexity and prevent overfitting."}, {"color": "#523d53", "id": "Regularization Parameter (\u03bb)", "label": "Regularization Parameter (\u03bb)", "shape": "dot", "size": 10, "title": "Hyperparameter controlling the influence of the regularizer on the overall loss function."}, {"color": "#ad7344", "id": "Model Selection", "label": "Model Selection", "shape": "star", "size": 25, "title": "Process of choosing the best model based on validation techniques."}, {"color": "#58add2", "id": "Cross Validation", "label": "Cross Validation", "shape": "dot", "size": 10, "title": "Technique to evaluate models and select the one with the best performance."}, {"color": "#27bd54", "id": "Polynomial Regression Models", "label": "Polynomial Regression Models", "shape": "dot", "size": 10, "title": "Models with varying degrees of polynomial terms for regression analysis."}, {"color": "#f520ab", "id": "Regularization Parameters", "label": "Regularization Parameters", "shape": "dot", "size": 10, "title": "Parameters like C in SVM that control model complexity and prevent overfitting."}, {"color": "#65d160", "id": "Machine Learning Techniques", "label": "Machine Learning Techniques", "shape": "star", "size": 25, "title": "Various techniques used in machine learning for data analysis and pattern recognition."}, {"color": "#eb950d", "id": "Validation Set Size", "label": "Validation Set Size", "shape": "dot", "size": 10, "title": "Determining an appropriate size for the validation set in machine learning."}, {"color": "#a57388", "id": "Hold Out Cross Validation", "label": "Hold Out Cross Validation", "shape": "dot", "size": 10, "title": "A method where a portion of data is held out as a validation set."}, {"color": "#30e4d6", "id": "k-fold Cross Validation", "label": "k-fold Cross Validation", "shape": "dot", "size": 10, "title": "Divides the dataset into k parts, trains on k-1 and tests on 1 part repeatedly."}, {"color": "#4bb333", "id": "Retraining on Full Dataset", "label": "Retraining on Full Dataset", "shape": "dot", "size": 10, "title": "Optionally retrain selected model on entire training set after validation."}, {"color": "#94c046", "id": "Leave-One-Out Cross Validation", "label": "Leave-One-Out Cross Validation", "shape": "dot", "size": 10, "title": "Uses each data point as a test set once while training on all others."}, {"color": "#190609", "id": "Data Scarcity", "label": "Data Scarcity", "shape": "dot", "size": 10, "title": "Situation where the amount of available data is limited, affecting model evaluation methods."}, {"color": "#dd7c45", "id": "Leave-One-Out CV", "label": "Leave-One-Out CV", "shape": "dot", "size": 10, "title": "Method where one training example is held out at a time."}, {"color": "#3bad7f", "id": "Bayesian Statistics", "label": "Bayesian Statistics", "shape": "star", "size": 25, "title": "Approach to parameter estimation that treats parameters as random variables."}, {"color": "#4e6ecf", "id": "MLE", "label": "MLE", "shape": "dot", "size": 10, "title": "Estimation method where parameters are viewed as constant but unknown values."}, {"color": "#ccb535", "id": "Prior Distribution", "label": "Prior Distribution", "shape": "dot", "size": 10, "title": "Distribution expressing prior beliefs about the parameters before seeing data."}, {"color": "#cd3b02", "id": "Hold-out Cross Validation", "label": "Hold-out Cross Validation", "shape": "dot", "size": 10, "title": "Technique splitting data into training and validation sets for better error estimation."}, {"color": "#28828b", "id": "Training Set S", "label": "Training Set S", "shape": "dot", "size": 10, "title": "Dataset used to train models in machine learning tasks."}, {"color": "#09218b", "id": "Hypotheses Training", "label": "Hypotheses Training", "shape": "dot", "size": 10, "title": "Process of training each model on the full dataset S."}, {"color": "#4c978f", "id": "Training Error Selection", "label": "Training Error Selection", "shape": "dot", "size": 10, "title": "Selection based on minimum training error, often leading to overfitting."}, {"color": "#9fa200", "id": "Validation Set S_cv", "label": "Validation Set S_cv", "shape": "dot", "size": 10, "title": "Subset of data used for validating model performance after training."}, {"color": "#31d854", "id": "Model Selection Based on Validation Error", "label": "Model Selection Based on Validation Error", "shape": "dot", "size": 10, "title": "Choosing the best hypothesis based on its error on the validation set."}, {"color": "#ad04ea", "id": "Bayesian Machine Learning", "label": "Bayesian Machine Learning", "shape": "star", "size": 25, "title": "Predictions made using posterior distribution on parameters."}, {"color": "#15803c", "id": "Predictive Distribution", "label": "Predictive Distribution", "shape": "dot", "size": 10, "title": "Probability distribution of predictions for new examples based on posterior distribution."}, {"color": "#8ee3ea", "id": "Fully Bayesian Prediction", "label": "Fully Bayesian Prediction", "shape": "dot", "size": 10, "title": "Prediction method that averages over the posterior distribution of parameters."}, {"color": "#9a19fd", "id": "Computational Challenges", "label": "Computational Challenges", "shape": "dot", "size": 10, "title": "Difficulties in computing high-dimensional integrals for posterior distributions."}, {"color": "#477cde", "id": "k-means Algorithm", "label": "k-means Algorithm", "shape": "star", "size": 25, "title": "Clustering algorithm that partitions data into k clusters."}, {"color": "#a698b9", "id": "Initialization", "label": "Initialization", "shape": "dot", "size": 10, "title": "Randomly selecting initial cluster centroids."}, {"color": "#8bdece", "id": "Convergence", "label": "Convergence", "shape": "dot", "size": 10, "title": "Guaranteed to converge in a certain sense."}, {"color": "#66f535", "id": "Distortion Function", "label": "Distortion Function", "shape": "dot", "size": 10, "title": "Measures sum of squared distances between examples and cluster centroids."}, {"color": "#c30cbc", "id": "Coordinate Descent on J", "label": "Coordinate Descent on J", "shape": "dot", "size": 10, "title": "Minimizing distortion function iteratively with respect to c and mu."}, {"color": "#17cce4", "id": "Distortion Function J", "label": "Distortion Function J", "shape": "dot", "size": 10, "title": "Function measuring the quality of clustering in k-means."}, {"color": "#89fef7", "id": "Convergence Properties", "label": "Convergence Properties", "shape": "dot", "size": 10, "title": "Properties related to convergence and local optima in k-means."}, {"color": "#b08dc8", "id": "EM Algorithms", "label": "EM Algorithms", "shape": "star", "size": 25, "title": "Expectation-Maximization algorithm used in probabilistic modeling."}, {"color": "#80fcbb", "id": "EM for Mixture of Gaussians", "label": "EM for Mixture of Gaussians", "shape": "dot", "size": 10, "title": "Application of EM to model data with Gaussian distributions."}, {"color": "#1eab7f", "id": "Posterior Approximation", "label": "Posterior Approximation", "shape": "dot", "size": 10, "title": "Techniques for approximating the posterior distribution when exact computation is infeasible."}, {"color": "#1bc386", "id": "MAP Estimation", "label": "MAP Estimation", "shape": "dot", "size": 10, "title": "Estimate parameters by maximizing the posterior probability, incorporating prior knowledge."}, {"color": "#1aaa70", "id": "MLE vs MAP", "label": "MLE vs MAP", "shape": "dot", "size": 10, "title": "Comparison between maximum likelihood and maximum a posteriori estimation methods."}, {"color": "#7a9b94", "id": "Prior Selection", "label": "Prior Selection", "shape": "dot", "size": 10, "title": "Choosing appropriate prior distributions for Bayesian models, e.g., Gaussian distribution."}, {"color": "#0a1c15", "id": "Unsupervised Learning", "label": "Unsupervised Learning", "shape": "star", "size": 25, "title": "Learning from data without labeled responses; finding hidden structure in unlabeled data sets."}, {"color": "#435b30", "id": "Clustering", "label": "Clustering", "shape": "dot", "size": 10, "title": "Techniques for grouping a set of objects into clusters based on their similarity."}, {"color": "#7353ff", "id": "K-Means Algorithm", "label": "K-Means Algorithm", "shape": "dot", "size": 10, "title": "A popular unsupervised learning algorithm that partitions data points into k clusters."}, {"color": "#10136f", "id": "Mixture of Gaussians Model", "label": "Mixture of Gaussians Model", "shape": "dot", "size": 10, "title": "Model using multiple Gaussian distributions with latent variables"}, {"color": "#126977", "id": "Latent Variables", "label": "Latent Variables", "shape": "dot", "size": 10, "title": "Hidden random variables that influence the observed data"}, {"color": "#20d665", "id": "Joint Distribution", "label": "Joint Distribution", "shape": "dot", "size": 10, "title": "Distribution modeling both latent and observable variables"}, {"color": "#d545d0", "id": "Likelihood Estimation", "label": "Likelihood Estimation", "shape": "dot", "size": 10, "title": "Estimating parameters by maximizing likelihood of observed data"}, {"color": "#9a1ca6", "id": "EM Algorithm", "label": "EM Algorithm", "shape": "dot", "size": 10, "title": "Iterative method for finding maximum likelihood or maximum a posteriori estimates in statistical models with latent variables."}, {"color": "#a5ed47", "id": "E-step", "label": "E-step", "shape": "dot", "size": 10, "title": "Estimation step where posterior distribution of hidden variables is computed given observed data and current parameters."}, {"color": "#974d7b", "id": "M-step", "label": "M-step", "shape": "dot", "size": 10, "title": "Maximization step where model parameters are updated to maximize the expected log-likelihood found in E-step."}, {"color": "#7abf3d", "id": "Gaussian Mixture Model", "label": "Gaussian Mixture Model", "shape": "dot", "size": 10, "title": "Model used for clustering data into multiple Gaussian distributions with different means and covariances."}, {"color": "#606bbb", "id": "Soft Assignments", "label": "Soft Assignments", "shape": "dot", "size": 10, "title": "Assigns probabilities to each cluster instead of hard assignments, allowing for probabilistic membership in clusters."}, {"color": "#327485", "id": "K-means Clustering", "label": "K-means Clustering", "shape": "dot", "size": 10, "title": "Clustering algorithm that assigns data points to the nearest centroid; contrasted with EM\u0027s soft assignments."}, {"color": "#77a192", "id": "Expectation-Maximization Algorithm", "label": "Expectation-Maximization Algorithm", "shape": "dot", "size": 10, "title": "Algorithm used for finding maximum likelihood or maximum a posteriori estimates of parameters in statistical models where the model depends on unobserved latent variables."}, {"color": "#f9a2d4", "id": "Convergence Guarantees", "label": "Convergence Guarantees", "shape": "dot", "size": 10, "title": "Conditions and proofs ensuring the algorithm\u0027s convergence to optimal solutions."}, {"color": "#6de4c6", "id": "Jensen\u0027s Inequality", "label": "Jensen\u0027s Inequality", "shape": "star", "size": 25, "title": "Mathematical result used to prove the monotonic increase of log-likelihood in each iteration of EM."}, {"color": "#d0a950", "id": "Strict Convexity", "label": "Strict Convexity", "shape": "dot", "size": 10, "title": "Condition for a convex function to be strictly convex, ensuring unique minimum points."}, {"color": "#b81651", "id": "Theorem Statement", "label": "Theorem Statement", "shape": "dot", "size": 10, "title": "Formal statement of Jensen\u0027s inequality involving expectations and convex functions."}, {"color": "#cb32a1", "id": "Concave Functions", "label": "Concave Functions", "shape": "dot", "size": 10, "title": "Functions where E[f(X)] \u003c= f(E[X]) if f is concave"}, {"color": "#847af8", "id": "Latent Variable Models", "label": "Latent Variable Models", "shape": "dot", "size": 10, "title": "Models with unobserved variables that influence observed data"}, {"color": "#bb8533", "id": "Log-Likelihood Maximization", "label": "Log-Likelihood Maximization", "shape": "dot", "size": 10, "title": "Process of finding parameter values that maximize the probability of observed data under a statistical model."}, {"color": "#520564", "id": "Machine_Learning_Concepts", "label": "Machine_Learning_Concepts", "shape": "star", "size": 25, "title": "Overview of machine learning concepts and algorithms."}, {"color": "#15111d", "id": "EM_Algorithm", "label": "EM_Algorithm", "shape": "dot", "size": 10, "title": "Efficient method for maximum likelihood estimation in probabilistic models."}, {"color": "#e0e674", "id": "Likelihood_Estimation", "label": "Likelihood_Estimation", "shape": "dot", "size": 10, "title": "Process of estimating parameters to maximize the likelihood function."}, {"color": "#dc1b06", "id": "Non_Convex_Optimization", "label": "Non_Convex_Optimization", "shape": "dot", "size": 10, "title": "Challenges in optimizing non-convex functions for parameter estimation."}, {"color": "#db12dd", "id": "E_Step", "label": "E_Step", "shape": "dot", "size": 10, "title": "Expectation step where a lower bound on the likelihood is constructed."}, {"color": "#f52cef", "id": "M_Step", "label": "M_Step", "shape": "dot", "size": 10, "title": "Maximization step where the lower bound is optimized to update parameters."}, {"color": "#f1dc60", "id": "Latent_Variables", "label": "Latent_Variables", "shape": "dot", "size": 10, "title": "Random variables that are not directly observed but influence the model."}, {"color": "#b983a8", "id": "Single_Example_Optimization", "label": "Single_Example_Optimization", "shape": "dot", "size": 10, "title": "Simplification of EM algorithm for optimizing likelihood of a single example."}, {"color": "#214a9f", "id": "Evidence Lower Bound (ELBO)", "label": "Evidence Lower Bound (ELBO)", "shape": "dot", "size": 10, "title": "Objective function used in variational inference to approximate complex probability distributions."}, {"color": "#722bff", "id": "Lower Bound Derivation", "label": "Lower Bound Derivation", "shape": "dot", "size": 10, "title": "Deriving a lower bound on the log-likelihood using Jensen\u0027s inequality."}, {"color": "#e1b554", "id": "Optimizing Q Distribution", "label": "Optimizing Q Distribution", "shape": "dot", "size": 10, "title": "Choosing an optimal distribution Q to make the lower bound tight for given parameters."}, {"color": "#8fce0e", "id": "Log-Likelihood Optimization", "label": "Log-Likelihood Optimization", "shape": "dot", "size": 10, "title": "Optimizing the log-likelihood function under the EM framework."}, {"color": "#9fee58", "id": "Single Example Case", "label": "Single Example Case", "shape": "dot", "size": 10, "title": "Discussion of optimizing for a single training example."}, {"color": "#7ba413", "id": "Multiple Examples Case", "label": "Multiple Examples Case", "shape": "dot", "size": 10, "title": "Extending the optimization to multiple training examples."}, {"color": "#cd11fd", "id": "E-step Calculation", "label": "E-step Calculation", "shape": "dot", "size": 10, "title": "Calculates the probability of latent variables given observed data and current parameter estimates."}, {"color": "#c5e0d4", "id": "M-step Maximization", "label": "M-step Maximization", "shape": "dot", "size": 10, "title": "Maximizes the expected log-likelihood found in the E step as a function of the parameters."}, {"color": "#89053a", "id": "Parameter Updates", "label": "Parameter Updates", "shape": "dot", "size": 10, "title": "Updates for \u03c6, \u03bc, and \u03a3 based on maximizing expected log-likelihood."}, {"color": "#ba8007", "id": "\u03b8 Update Rule", "label": "\u03b8 Update Rule", "shape": "dot", "size": 10, "title": "Rule for updating parameters in the M-step to maximize likelihood function."}, {"color": "#64547a", "id": "ELBO Interpretation", "label": "ELBO Interpretation", "shape": "dot", "size": 10, "title": "Various interpretations of Evidence Lower Bound (ELBO)."}, {"color": "#a1f17a", "id": "Alternative ELBO Formulations", "label": "Alternative ELBO Formulations", "shape": "dot", "size": 10, "title": "Different mathematical formulations of the ELBO equation."}, {"color": "#dc90af", "id": "KL Divergence in ELBO", "label": "KL Divergence in ELBO", "shape": "dot", "size": 10, "title": "Explanation of KL divergence within ELBO context."}, {"color": "#7900a7", "id": "Mixture of Gaussians", "label": "Mixture of Gaussians", "shape": "dot", "size": 10, "title": "Application of EM algorithm to Gaussian mixture models for parameter estimation."}, {"color": "#c939d8", "id": "EM Algorithm Steps", "label": "EM Algorithm Steps", "shape": "dot", "size": 10, "title": "E-step and M-step processes in the context of Gaussian mixtures."}, {"color": "#25ee1b", "id": "Convergence Proof", "label": "Convergence Proof", "shape": "dot", "size": 10, "title": "Proof showing EM algorithm monotonically increases log-likelihood until convergence."}, {"color": "#e2060f", "id": "ELBO (Evidence Lower Bound)", "label": "ELBO (Evidence Lower Bound)", "shape": "star", "size": 25, "title": "Objective function used in variational inference and EM algorithm as a lower bound on the log-likelihood."}, {"color": "#7c178c", "id": "Expectation-Maximization (EM) Algorithm", "label": "Expectation-Maximization (EM) Algorithm", "shape": "dot", "size": 10, "title": "Iterative method for finding maximum likelihood or maximum a posteriori estimates of parameters in statistical models, where the model depends on unobserved latent variables."}, {"color": "#b6d054", "id": "M-step Update Rule", "label": "M-step Update Rule", "shape": "dot", "size": 10, "title": "Rule used to update parameters during the maximization step of the EM algorithm."}, {"color": "#d4217e", "id": "Lagrangian Method", "label": "Lagrangian Method", "shape": "dot", "size": 10, "title": "Mathematical technique for finding local maxima and minima of a function subject to equality constraints."}, {"color": "#d5a182", "id": "Variational Inference", "label": "Variational Inference", "shape": "dot", "size": 10, "title": "Technique used in machine learning to approximate posterior distributions over unobserved variables, especially useful when exact inference is computationally infeasible."}, {"color": "#03a986", "id": "Variational Auto-Encoder (VAE)", "label": "Variational Auto-Encoder (VAE)", "shape": "dot", "size": 10, "title": "Type of generative model that uses variational inference to learn a latent variable model for a set of observed data."}, {"color": "#ef0ab7", "id": "ELBO", "label": "ELBO", "shape": "dot", "size": 10, "title": "Evidence Lower Bound used to optimize variational inference."}, {"color": "#11d142", "id": "Mean Field Assumption", "label": "Mean Field Assumption", "shape": "dot", "size": 10, "title": "Assumption that latent variables are independent, simplifying the optimization problem."}, {"color": "#3830d3", "id": "Discrete Latent Variables", "label": "Discrete Latent Variables", "shape": "dot", "size": 10, "title": "Application of mean field assumption to discrete latent variable models."}, {"color": "#e74b2b", "id": "Continuous Latent Variables", "label": "Continuous Latent Variables", "shape": "dot", "size": 10, "title": "Handling continuous variables requires additional techniques beyond mean field assumptions."}, {"color": "#910e34", "id": "Re-parametrization Trick", "label": "Re-parametrization Trick", "shape": "dot", "size": 10, "title": "Method to enable backpropagation through stochastic nodes."}, {"color": "#9512df", "id": "Gaussian Mixture Models", "label": "Gaussian Mixture Models", "shape": "dot", "size": 10, "title": "Model for clustering data into multiple Gaussian distributions."}, {"color": "#565b23", "id": "Optimizing Continuous Latent Variables", "label": "Optimizing Continuous Latent Variables", "shape": "star", "size": 25, "title": "Process of optimizing parameters for continuous latent variables in machine learning models."}, {"color": "#ac7c80", "id": "Succinct Representation of Distribution Qi", "label": "Succinct Representation of Distribution Qi", "shape": "dot", "size": 10, "title": "Using a Gaussian distribution to represent Qi succinctly over an infinite number of points."}, {"color": "#5c0bc0", "id": "Mean and Variance Functions", "label": "Mean and Variance Functions", "shape": "dot", "size": 10, "title": "Functions q(x;phi) and v(x;psi) map from dimension d to k, parameterized by phi and psi."}, {"color": "#8a4262", "id": "Encoder-Decoder Framework", "label": "Encoder-Decoder Framework", "shape": "dot", "size": 10, "title": "In variational auto-encoders, q and v are often neural networks acting as encoders, g(z;theta) as decoder."}, {"color": "#5cc39d", "id": "Efficient Evaluation of ELBO", "label": "Efficient Evaluation of ELBO", "shape": "dot", "size": 10, "title": "Verification process to ensure efficient evaluation of Evidence Lower Bound for fixed Q."}, {"color": "#7c18e2", "id": "ELBO Optimization", "label": "ELBO Optimization", "shape": "dot", "size": 10, "title": "Techniques for optimizing the Evidence Lower Bound (ELBO)."}, {"color": "#d7a9df", "id": "Gradient Ascent in ELBO", "label": "Gradient Ascent in ELBO", "shape": "dot", "size": 10, "title": "Using gradient ascent for optimizing parameters in ELBO."}, {"color": "#61ab02", "id": "Gaussian Distributions", "label": "Gaussian Distributions", "shape": "dot", "size": 10, "title": "Utilizing Gaussian distributions to efficiently evaluate ELBO values."}, {"color": "#52a372", "id": "Expectation Maximization (EM)", "label": "Expectation Maximization (EM)", "shape": "dot", "size": 10, "title": "Technique for finding maximum likelihood or maximum a posteriori estimates of parameters in statistical models where the model depends on unobserved latent variables."}, {"color": "#5ab604", "id": "Reparameterization Trick", "label": "Reparameterization Trick", "shape": "dot", "size": 10, "title": "Technique to compute gradients through stochastic variables by re-expressing them in a differentiable form."}, {"color": "#f22e3a", "id": "Data Normalization", "label": "Data Normalization", "shape": "star", "size": 25, "title": "Process of standardizing data attributes to ensure comparability and zero mean."}, {"color": "#58c626", "id": "Mean Removal", "label": "Mean Removal", "shape": "dot", "size": 10, "title": "Subtracting the mean from each feature to center the data around zero."}, {"color": "#d26e4f", "id": "Variance Scaling", "label": "Variance Scaling", "shape": "dot", "size": 10, "title": "Dividing by standard deviation to ensure unit variance for comparability."}, {"color": "#6a86d7", "id": "Major Axis of Variation", "label": "Major Axis of Variation", "shape": "star", "size": 25, "title": "Direction in which the data shows maximum variation after normalization."}, {"color": "#37572e", "id": "Projection and Variance Maximization", "label": "Projection and Variance Maximization", "shape": "dot", "size": 10, "title": "Finding unit vector u to maximize variance when data is projected onto it."}, {"color": "#e13463", "id": "Gradient Estimation", "label": "Gradient Estimation", "shape": "dot", "size": 10, "title": "Process of estimating gradients for optimization in probabilistic models."}, {"color": "#72997b", "id": "Principal Components Analysis (PCA)", "label": "Principal Components Analysis (PCA)", "shape": "star", "size": 25, "title": "Dimensionality reduction technique that identifies the subspace where data approximately lies."}, {"color": "#4af47d", "id": "Data Subspace Identification", "label": "Data Subspace Identification", "shape": "dot", "size": 10, "title": "Process of identifying a lower-dimensional space in which the data can be represented accurately."}, {"color": "#18c47d", "id": "Data Redundancy Detection", "label": "Data Redundancy Detection", "shape": "star", "size": 25, "title": "Identifying and removing redundant data attributes."}, {"color": "#fd456a", "id": "PCA Algorithm Introduction", "label": "PCA Algorithm Introduction", "shape": "dot", "size": 10, "title": "Introduction to Principal Component Analysis for detecting redundancy."}, {"color": "#f875d5", "id": "Normalization Process", "label": "Normalization Process", "shape": "dot", "size": 10, "title": "Preprocessing step before PCA involving mean and variance adjustment."}, {"color": "#3a5d07", "id": "Car Example", "label": "Car Example", "shape": "dot", "size": 10, "title": "Example illustrating linear dependency in car attributes."}, {"color": "#360551", "id": "Pilot Survey Example", "label": "Pilot Survey Example", "shape": "dot", "size": 10, "title": "Survey data example showing correlation between piloting skill and enjoyment."}, {"color": "#32e4bc", "id": "Normalization Formula", "label": "Normalization Formula", "shape": "dot", "size": 10, "title": "Formula for normalizing features to have mean 0 and variance 1."}, {"color": "#73428d", "id": "Principal Component Analysis (PCA)", "label": "Principal Component Analysis (PCA)", "shape": "star", "size": 25, "title": "Dimensionality reduction technique that transforms data into principal components."}, {"color": "#894802", "id": "Projection of Data Points", "label": "Projection of Data Points", "shape": "dot", "size": 10, "title": "Projecting data points onto a unit vector to reduce dimensionality."}, {"color": "#9bf5c0", "id": "Variance Maximization", "label": "Variance Maximization", "shape": "dot", "size": 10, "title": "Maximizing the variance of projections for optimal direction selection."}, {"color": "#9605c3", "id": "Empirical Covariance Matrix", "label": "Empirical Covariance Matrix", "shape": "dot", "size": 10, "title": "Matrix representing the covariance between data points."}, {"color": "#bd4f67", "id": "Principal Eigenvector", "label": "Principal Eigenvector", "shape": "dot", "size": 10, "title": "Eigenvector corresponding to the largest eigenvalue of the covariance matrix."}, {"color": "#2ef2a9", "id": "k-Dimensional Subspace", "label": "k-Dimensional Subspace", "shape": "dot", "size": 10, "title": "Projection of data into a lower-dimensional space using top k eigenvectors."}, {"color": "#9301ca", "id": "PCA", "label": "PCA", "shape": "star", "size": 25, "title": "Principal Component Analysis for dimensionality reduction"}, {"color": "#adb6a9", "id": "Eigenvectors of Sigma", "label": "Eigenvectors of Sigma", "shape": "dot", "size": 10, "title": "Top k eigenvectors used to form a new orthogonal basis"}, {"color": "#436e55", "id": "Dimensionality Reduction", "label": "Dimensionality Reduction", "shape": "dot", "size": 10, "title": "Reduces data from d dimensions to k dimensions"}, {"color": "#112df2", "id": "Principal Components", "label": "Principal Components", "shape": "dot", "size": 10, "title": "First k eigenvectors of Sigma, representing the new basis"}, {"color": "#f5efa8", "id": "Approximation Error Minimization", "label": "Approximation Error Minimization", "shape": "dot", "size": 10, "title": "Derivation based on minimizing error from projection onto k-dimensional subspace"}, {"color": "#1120d6", "id": "Applications", "label": "Applications", "shape": "dot", "size": 10, "title": "Various uses including data compression and visualization"}, {"color": "#5d9214", "id": "Independent Component Analysis (ICA)", "label": "Independent Component Analysis (ICA)", "shape": "star", "size": 25, "title": "Technique for separating mixed signals into independent components."}, {"color": "#df4973", "id": "Cocktail Party Problem", "label": "Cocktail Party Problem", "shape": "dot", "size": 10, "title": "Motivating example involving separating mixed audio signals into individual sources."}, {"color": "#8da686", "id": "Mixing Matrix (A)", "label": "Mixing Matrix (A)", "shape": "dot", "size": 10, "title": "Matrix representing the mixing process of original sources into observed data."}, {"color": "#4b2119", "id": "Unmixing Matrix (W)", "label": "Unmixing Matrix (W)", "shape": "dot", "size": 10, "title": "Inverse matrix used to recover original sources from mixed signals."}, {"color": "#1e1bbb", "id": "ICA Ambiguities", "label": "ICA Ambiguities", "shape": "star", "size": 25, "title": "Discussion on the limitations and uncertainties in recovering the unmixing matrix W."}, {"color": "#5da21b", "id": "Data Visualization", "label": "Data Visualization", "shape": "dot", "size": 10, "title": "Plotting transformed data to identify clusters and similarities."}, {"color": "#2e156d", "id": "Dimension Reduction", "label": "Dimension Reduction", "shape": "dot", "size": 10, "title": "Reducing dataset dimensions for computational efficiency and overfitting prevention."}, {"color": "#6b8688", "id": "Noise Reduction", "label": "Noise Reduction", "shape": "dot", "size": 10, "title": "Estimating intrinsic features from noisy data to improve signal clarity."}, {"color": "#54498b", "id": "Eigenfaces Method", "label": "Eigenfaces Method", "shape": "dot", "size": 10, "title": "Applying PCA to face images for dimension reduction and noise removal."}, {"color": "#88c764", "id": "Independent Components Analysis (ICA)", "label": "Independent Components Analysis (ICA)", "shape": "star", "size": 25, "title": "Technique for finding independent components in data, differing from PCA in its objectives."}, {"color": "#18258c", "id": "Permutation Matrix", "label": "Permutation Matrix", "shape": "dot", "size": 10, "title": "A matrix used to permute vectors"}, {"color": "#13bbe1", "id": "Scaling Ambiguity", "label": "Scaling Ambiguity", "shape": "dot", "size": 10, "title": "Ambiguity in scaling factors of sources and mixing matrix"}, {"color": "#21d299", "id": "Sign Change Ambiguity", "label": "Sign Change Ambiguity", "shape": "dot", "size": 10, "title": "Ambiguity due to sign changes in source signals"}, {"color": "#cead56", "id": "Density Transformation", "label": "Density Transformation", "shape": "star", "size": 25, "title": "Transformation of density functions under linear transformations."}, {"color": "#90e541", "id": "1D Example", "label": "1D Example", "shape": "dot", "size": 10, "title": "Illustration with a 1-dimensional example."}, {"color": "#a1a5c0", "id": "General Case", "label": "General Case", "shape": "dot", "size": 10, "title": "Extension to vector-valued distributions and general matrices."}, {"color": "#b4dae9", "id": "ICA Algorithm", "label": "ICA Algorithm", "shape": "star", "size": 25, "title": "Derivation of an Independent Component Analysis algorithm based on maximum likelihood estimation."}, {"color": "#40fab9", "id": "Scaling Factor", "label": "Scaling Factor", "shape": "dot", "size": 10, "title": "Impact of scaling a speaker\u0027s speech signal by a positive factor."}, {"color": "#43bc87", "id": "Sign Changes Irrelevance", "label": "Sign Changes Irrelevance", "shape": "dot", "size": 10, "title": "Explanation that sign changes in the signal do not affect the outcome."}, {"color": "#26a50a", "id": "Non-Gaussian Sources", "label": "Non-Gaussian Sources", "shape": "dot", "size": 10, "title": "Sources are non-Gaussian, which resolves ambiguities."}, {"color": "#71fa5d", "id": "Gaussian Data Example", "label": "Gaussian Data Example", "shape": "dot", "size": 10, "title": "Example with Gaussian data showing rotational symmetry and ambiguity."}, {"color": "#3d332d", "id": "Mixing Matrix A", "label": "Mixing Matrix A", "shape": "dot", "size": 10, "title": "Introduction of mixing matrix A in the context of Gaussian data."}, {"color": "#f69257", "id": "Rotation Matrix R", "label": "Rotation Matrix R", "shape": "dot", "size": 10, "title": "Explanation of rotation matrix R and its properties."}, {"color": "#b3b421", "id": "Mixed Data x\u0027", "label": "Mixed Data x\u0027", "shape": "dot", "size": 10, "title": "Observation of mixed data under different mixing matrices A\u0027."}, {"color": "#c2b19e", "id": "Mixing Matrix", "label": "Mixing Matrix", "shape": "dot", "size": 10, "title": "Matrix representing the mixing process in ICA."}, {"color": "#0cd72a", "id": "Gaussian Data Limitation", "label": "Gaussian Data Limitation", "shape": "dot", "size": 10, "title": "Limitations of ICA when data is Gaussian distributed."}, {"color": "#350c10", "id": "Rotationally Symmetric Distributions", "label": "Rotationally Symmetric Distributions", "shape": "dot", "size": 10, "title": "Property of multivariate standard normal distribution affecting ICA."}, {"color": "#013f0e", "id": "Densities and Linear Transformations", "label": "Densities and Linear Transformations", "shape": "star", "size": 25, "title": "Effect of linear transformations on probability densities in machine learning."}, {"color": "#f1ad42", "id": "Linear Transformation Impact", "label": "Linear Transformation Impact", "shape": "dot", "size": 10, "title": "Impact of linear transformations on the density function of random variables."}, {"color": "#547bdf", "id": "ICA Overview", "label": "ICA Overview", "shape": "star", "size": 25, "title": "Introduction to Independent Component Analysis concepts and principles."}, {"color": "#c216e5", "id": "Joint Distribution of Sources", "label": "Joint Distribution of Sources", "shape": "dot", "size": 10, "title": "Modeling the joint distribution as a product of marginals for independent sources."}, {"color": "#4bc3c4", "id": "Density on x=As=W^-1s", "label": "Density on x=As=W^-1s", "shape": "dot", "size": 10, "title": "Deriving density function based on transformation from source to mixed signals."}, {"color": "#cf0dc1", "id": "Cumulative Distribution Function (CDF)", "label": "Cumulative Distribution Function (CDF)", "shape": "dot", "size": 10, "title": "Definition and properties of CDF for real-valued random variables."}, {"color": "#880526", "id": "Sigmoid Function as Default Density", "label": "Sigmoid Function as Default Density", "shape": "dot", "size": 10, "title": "Using sigmoid function to define density for sources in ICA due to its desirable properties."}, {"color": "#d51372", "id": "Data Preprocessing Assumptions", "label": "Data Preprocessing Assumptions", "shape": "dot", "size": 10, "title": "Assumption that data has zero mean or can be expected to have zero mean."}, {"color": "#a233a5", "id": "Logistic Function Properties", "label": "Logistic Function Properties", "shape": "dot", "size": 10, "title": "Properties of the logistic function and its derivative."}, {"color": "#016e90", "id": "Log Likelihood Function", "label": "Log Likelihood Function", "shape": "dot", "size": 10, "title": "Function used to evaluate the performance of a model based on training data."}, {"color": "#bf4b12", "id": "Gradient Ascent Rule", "label": "Gradient Ascent Rule", "shape": "dot", "size": 10, "title": "Rule for updating parameters during training using gradient ascent method."}, {"color": "#85c433", "id": "Training Example Independence Assumption", "label": "Training Example Independence Assumption", "shape": "dot", "size": 10, "title": "Assumption that training examples are independent of each other, and its implications on model performance."}, {"color": "#3fd6e3", "id": "Stochastic Gradient Ascent", "label": "Stochastic Gradient Ascent", "shape": "dot", "size": 10, "title": "Optimization technique used for minimizing loss functions in machine learning models."}, {"color": "#06cc4d", "id": "Self-supervised Learning", "label": "Self-supervised Learning", "shape": "star", "size": 25, "title": "Learning paradigm where the model learns from unlabeled data using self-generated supervisory signals."}, {"color": "#1461e5", "id": "Foundation Models", "label": "Foundation Models", "shape": "dot", "size": 10, "title": "Large-scale models pre-trained on broad datasets and adaptable to various downstream tasks."}, {"color": "#f1f210", "id": "Pretraining Phase", "label": "Pretraining Phase", "shape": "dot", "size": 10, "title": "Training a model on an unlabeled dataset to learn general representations."}, {"color": "#a697c0", "id": "Adaptation Phase", "label": "Adaptation Phase", "shape": "dot", "size": 10, "title": "Phase where the pre-trained model is fine-tuned for specific tasks with limited labeled data."}, {"color": "#f65503", "id": "Transfer Learning", "label": "Transfer Learning", "shape": "dot", "size": 10, "title": "Using a pre-trained model as the starting point for fine-tuning on a new task."}, {"color": "#24c1e2", "id": "Unlabeled Dataset", "label": "Unlabeled Dataset", "shape": "dot", "size": 10, "title": "Dataset used during pretraining phase, typically large and unlabeled."}, {"color": "#f60cf4", "id": "Labeled Task Dataset", "label": "Labeled Task Dataset", "shape": "dot", "size": 10, "title": "Dataset with labeled data for fine-tuning the model on specific tasks."}, {"color": "#f564e2", "id": "Pretrained Model", "label": "Pretrained Model", "shape": "dot", "size": 10, "title": "Model trained in pretraining phase, used as a starting point for adaptation."}, {"color": "#3ebafd", "id": "Self-Supervised Loss", "label": "Self-Supervised Loss", "shape": "dot", "size": 10, "title": "Loss function that uses the data itself to provide supervision during pretraining."}, {"color": "#4cdb4e", "id": "Machine Learning Adaptation Methods", "label": "Machine Learning Adaptation Methods", "shape": "star", "size": 25, "title": "Overview of methods for adapting machine learning models to new tasks."}, {"color": "#99c000", "id": "Labeled Dataset", "label": "Labeled Dataset", "shape": "dot", "size": 10, "title": "Dataset used in downstream tasks with labeled examples."}, {"color": "#f281a4", "id": "Zero-Shot Learning", "label": "Zero-Shot Learning", "shape": "dot", "size": 10, "title": "Scenario where no labeled data is available for the task."}, {"color": "#70a2c0", "id": "Few-Shhot Learning", "label": "Few-Shhot Learning", "shape": "dot", "size": 10, "title": "Situation with a small number of labeled examples (1-50)."}, {"color": "#2b5a2f", "id": "Adaptation Algorithm", "label": "Adaptation Algorithm", "shape": "dot", "size": 10, "title": "Algorithm that takes in a downstream dataset and pretrained model to output an adapted model."}, {"color": "#ef88ad", "id": "Linear Probe Approach", "label": "Linear Probe Approach", "shape": "dot", "size": 10, "title": "Uses a linear head on top of the representation for prediction without modifying the pretrained model."}, {"color": "#771384", "id": "Finetuning Algorithm", "label": "Finetuning Algorithm", "shape": "dot", "size": 10, "title": "Further finetunes the pretrained model along with the downstream prediction model."}, {"color": "#f5717a", "id": "Language Problem Methods", "label": "Language Problem Methods", "shape": "dot", "size": 10, "title": "Specific methods for language problems introduced in 14.3.2."}, {"color": "#6a7db2", "id": "Self-Supervised Learning", "label": "Self-Supervised Learning", "shape": "star", "size": 25, "title": "Training models using only unlabeled data."}, {"color": "#1c8b67", "id": "Representation Function", "label": "Representation Function", "shape": "dot", "size": 10, "title": "Maps semantically similar images to similar representations."}, {"color": "#e48de9", "id": "Supervised Contrastive Algorithms", "label": "Supervised Contrastive Algorithms", "shape": "dot", "size": 10, "title": "Works well with labeled pretraining datasets."}, {"color": "#69f1a0", "id": "Data Augmentation", "label": "Data Augmentation", "shape": "dot", "size": 10, "title": "Generates pairs of augmented images from the same original image."}, {"color": "#2f28d1", "id": "Positive Pair", "label": "Positive Pair", "shape": "dot", "size": 10, "title": "Pair of samples that are semantically similar in the dataset."}, {"color": "#9d308c", "id": "Negative Pair", "label": "Negative Pair", "shape": "dot", "size": 10, "title": "Randomly selected augmented images from different original images, not necessarily semantically related."}, {"color": "#54464b", "id": "Finetuning Pretrained Models", "label": "Finetuning Pretrained Models", "shape": "dot", "size": 10, "title": "Process of adjusting pretrained model parameters for specific tasks."}, {"color": "#20e8b4", "id": "Prediction Model Structure", "label": "Prediction Model Structure", "shape": "dot", "size": 10, "title": "Structure involving both fixed and trainable parts of the model."}, {"color": "#d46810", "id": "Optimization Goal", "label": "Optimization Goal", "shape": "dot", "size": 10, "title": "Objective function to minimize for fitting downstream data."}, {"color": "#400ec5", "id": "Pretraining Methods in CV", "label": "Pretraining Methods in CV", "shape": "star", "size": 25, "title": "Techniques used to pretrain models specifically for computer vision tasks."}, {"color": "#0f3291", "id": "Supervised Pretraining", "label": "Supervised Pretraining", "shape": "dot", "size": 10, "title": "Training with labeled data to initialize model parameters."}, {"color": "#5a529d", "id": "Contrastive Learning", "label": "Contrastive Learning", "shape": "dot", "size": 10, "title": "Self-supervised learning using unlabeled data to find similar image representations."}, {"color": "#3b76eb", "id": "Loss Function Analysis", "label": "Loss Function Analysis", "shape": "dot", "size": 10, "title": "Analysis of loss functions and their impact on model behavior."}, {"color": "#a38385", "id": "Pretrained Large Language Models", "label": "Pretrained Large Language Models", "shape": "star", "size": 25, "title": "Overview of pretraining models in natural language processing."}, {"color": "#db21cd", "id": "Language Model Probability Distribution", "label": "Language Model Probability Distribution", "shape": "dot", "size": 10, "title": "Explanation of the probability distribution used in language modeling."}, {"color": "#c0c45c", "id": "SIMCLR Algorithm", "label": "SIMCLR Algorithm", "shape": "dot", "size": 10, "title": "Specific algorithm based on contrastive learning principle introduced in 2020."}, {"color": "#6951f6", "id": "Augmentation Techniques", "label": "Augmentation Techniques", "shape": "dot", "size": 10, "title": "Methods for creating variations of input data to improve model robustness."}, {"color": "#823781", "id": "Conditional Probability Modeling", "label": "Conditional Probability Modeling", "shape": "dot", "size": 10, "title": "Modeling the probability of an event given that another event has occurred."}, {"color": "#0a1556", "id": "Parameterized Model", "label": "Parameterized Model", "shape": "dot", "size": 10, "title": "A model where parameters are used to adjust predictions based on input data."}, {"color": "#8a5f83", "id": "Embeddings and Representations", "label": "Embeddings and Representations", "shape": "dot", "size": 10, "title": "Numerical representations of categorical variables, such as words."}, {"color": "#589c91", "id": "Transformer Model", "label": "Transformer Model", "shape": "dot", "size": 10, "title": "A model architecture for handling sequence data with self-attention mechanisms."}, {"color": "#0e3206", "id": "Input-Output Interface", "label": "Input-Output Interface", "shape": "dot", "size": 10, "title": "The way input sequences are transformed into output sequences using embeddings and a blackbox function."}, {"color": "#db2411", "id": "Training Process", "label": "Training Process", "shape": "dot", "size": 10, "title": "Involves minimizing the negative log-likelihood of data under a probabilistic model."}, {"color": "#b90c9e", "id": "Autoregressive Text Decoding", "label": "Autoregressive Text Decoding", "shape": "star", "size": 25, "title": "Process of generating text sequentially using a trained Transformer model."}, {"color": "#8043b4", "id": "Machine Learning Adaptation Techniques", "label": "Machine Learning Adaptation Techniques", "shape": "star", "size": 25, "title": "Techniques for adapting machine learning models to new tasks or domains."}, {"color": "#a1de4b", "id": "Zero-shot Learning", "label": "Zero-shot Learning", "shape": "dot", "size": 10, "title": "Adapting a model to perform tasks without any input-output pairs from the task."}, {"color": "#7ba5ea", "id": "In-context Learning", "label": "In-context Learning", "shape": "dot", "size": 10, "title": "Learning from a small set of examples provided in the context."}, {"color": "#5ea3c6", "id": "Language Model Utilization", "label": "Language Model Utilization", "shape": "dot", "size": 10, "title": "Methods to decode answers from language models in zero-shot settings."}, {"color": "#558749", "id": "Prompt Construction", "label": "Prompt Construction", "shape": "dot", "size": 10, "title": "Creating prompts by concatenating labeled examples and test data for model generation."}, {"color": "#c0105d", "id": "Reinforcement Learning", "label": "Reinforcement Learning", "shape": "star", "size": 25, "title": "Field of machine learning concerned with how software agents ought to take actions in an environment to maximize some notion of cumulative reward."}, {"color": "#cd5405", "id": "Sequential Decision Making", "label": "Sequential Decision Making", "shape": "dot", "size": 10, "title": "Decision making in sequences without explicit supervision."}, {"color": "#5da666", "id": "Reward Function", "label": "Reward Function", "shape": "dot", "size": 10, "title": "Function that assigns a scalar value to each possible input or state-action pair."}, {"color": "#640c50", "id": "Language Models", "label": "Language Models", "shape": "dot", "size": 10, "title": "Models that generate text based on learned patterns from large datasets."}, {"color": "#5b7a59", "id": "Conditional Probability in Language Models", "label": "Conditional Probability in Language Models", "shape": "dot", "size": 10, "title": "Probability of generating the next token given previous tokens."}, {"color": "#6dd497", "id": "Temperature Parameter", "label": "Temperature Parameter", "shape": "dot", "size": 10, "title": "Parameter to adjust the randomness or determinism of generated text."}, {"color": "#4b866b", "id": "Adaptation Methods", "label": "Adaptation Methods", "shape": "dot", "size": 10, "title": "Techniques for adapting pretrained models to new tasks without additional training data."}, {"color": "#01dfb9", "id": "Finetuning", "label": "Finetuning", "shape": "dot", "size": 10, "title": "Adjusting model parameters based on specific task data."}, {"color": "#9cd74f", "id": "Policy Execution", "label": "Policy Execution", "shape": "star", "size": 25, "title": "Process of selecting actions based on a policy in given states."}, {"color": "#13087c", "id": "Value Function", "label": "Value Function", "shape": "dot", "size": 10, "title": "Function that calculates the expected sum of discounted rewards for a given state under a policy."}, {"color": "#13e8fc", "id": "Bellman Equations", "label": "Bellman Equations", "shape": "dot", "size": 10, "title": "Set of equations used to solve for the value function in an MDP."}, {"color": "#020a20", "id": "Immediate Reward", "label": "Immediate Reward", "shape": "dot", "size": 10, "title": "Reward received immediately upon entering a state."}, {"color": "#4c4e9f", "id": "Optimal Value Function", "label": "Optimal Value Function", "shape": "dot", "size": 10, "title": "Finding the optimal value function in a finite-horizon setting."}, {"color": "#ca28ad", "id": "Bellman\u0027s Equation", "label": "Bellman\u0027s Equation", "shape": "dot", "size": 10, "title": "Use of Bellman\u0027s equation for solving dynamic programming problems."}, {"color": "#a1be75", "id": "Policy", "label": "Policy", "shape": "star", "size": 25, "title": "Strategy that defines an action for each state in a given environment."}, {"color": "#cea047", "id": "Optimal Policy", "label": "Optimal Policy", "shape": "dot", "size": 10, "title": "Strategy that maximizes the expected cumulative reward over time, which can be stationary or non-stationary depending on the MDP type."}, {"color": "#b42819", "id": "Markov Decision Processes (MDP)", "label": "Markov Decision Processes (MDP)", "shape": "dot", "size": 10, "title": "Models decision-making scenarios where outcomes are partly random and partly under the control of a decision maker."}, {"color": "#961104", "id": "States", "label": "States", "shape": "dot", "size": 10, "title": "Set of all possible conditions or configurations in an environment."}, {"color": "#f9f6f7", "id": "Actions", "label": "Actions", "shape": "dot", "size": 10, "title": "Set of all possible actions that can be taken from a given state."}, {"color": "#5f1ec5", "id": "State Transition Probabilities", "label": "State Transition Probabilities", "shape": "dot", "size": 10, "title": "Knowledge of state transition probabilities in the context of solving MDPs."}, {"color": "#cbeba3", "id": "Discount Factor", "label": "Discount Factor", "shape": "dot", "size": 10, "title": "Parameter that determines the present value of future rewards in reinforcement learning."}, {"color": "#1820dc", "id": "Reinforcement Learning Overview", "label": "Reinforcement Learning Overview", "shape": "star", "size": 25, "title": "Introduction to reinforcement learning concepts and processes."}, {"color": "#646b9a", "id": "Markov Decision Process (MDP)", "label": "Markov Decision Process (MDP)", "shape": "dot", "size": 10, "title": "A framework for modeling decision making in situations where outcomes are partly random and partly under the control of a decision maker."}, {"color": "#975099", "id": "State Transition", "label": "State Transition", "shape": "dot", "size": 10, "title": "The process by which states change based on actions taken."}, {"color": "#d388db", "id": "Total Payoff Calculation", "label": "Total Payoff Calculation", "shape": "dot", "size": 10, "title": "Calculation of total rewards over time, considering discount factors."}, {"color": "#8c93c2", "id": "Discount Factor (\u03b3)", "label": "Discount Factor (\u03b3)", "shape": "dot", "size": 10, "title": "A factor used to discount future rewards based on their temporal distance from the present."}, {"color": "#6b0280", "id": "Policy Definition", "label": "Policy Definition", "shape": "dot", "size": 10, "title": "Definition of a policy as a function mapping states to actions."}, {"color": "#582626", "id": "Markov Decision Processes (MDPs)", "label": "Markov Decision Processes (MDPs)", "shape": "dot", "size": 10, "title": "Models for decision-making problems under uncertainty."}, {"color": "#26d2fd", "id": "Value Iteration", "label": "Value Iteration", "shape": "dot", "size": 10, "title": "Algorithm for finding optimal policies in reinforcement learning by iteratively updating value functions."}, {"color": "#1e41bc", "id": "Policy Iteration", "label": "Policy Iteration", "shape": "dot", "size": 10, "title": "Alternative algorithm for finding optimal policies in MDPs through successive policy evaluations and improvements."}, {"color": "#7c90ee", "id": "Learning Model for MDPs", "label": "Learning Model for MDPs", "shape": "dot", "size": 10, "title": "Estimating transition probabilities and rewards from data in MDP problems."}, {"color": "#541ae2", "id": "Inverted Pendulum Problem", "label": "Inverted Pendulum Problem", "shape": "dot", "size": 10, "title": "Example problem used to illustrate learning models in MDPs."}, {"color": "#900ea2", "id": "State Transition Probabilities Estimation", "label": "State Transition Probabilities Estimation", "shape": "dot", "size": 10, "title": "Method of estimating transition probabilities from observed state-action pairs and outcomes."}, {"color": "#ce3836", "id": "Optimal Policy in MDPs", "label": "Optimal Policy in MDPs", "shape": "dot", "size": 10, "title": "Discussion on the existence of a single optimal policy for all states in an MDP."}, {"color": "#82d9d2", "id": "Value Iteration and Policy Iteration", "label": "Value Iteration and Policy Iteration", "shape": "dot", "size": 10, "title": "Efficient algorithms for solving finite-state Markov Decision Processes (MDPs)."}, {"color": "#6674cb", "id": "Finite-State MDPs", "label": "Finite-State MDPs", "shape": "dot", "size": 10, "title": "Consideration of MDPs with a finite number of states and actions."}, {"color": "#dbe9f6", "id": "Value Iteration Algorithm", "label": "Value Iteration Algorithm", "shape": "dot", "size": 10, "title": "Algorithm for computing the optimal value function using iterative methods."}, {"color": "#6435de", "id": "Synchronous Updates", "label": "Synchronous Updates", "shape": "dot", "size": 10, "title": "Method of updating all state values simultaneously before applying them."}, {"color": "#7a5fdd", "id": "Asynchronous Updates", "label": "Asynchronous Updates", "shape": "dot", "size": 10, "title": "Technique for updating state values one at a time in sequence."}, {"color": "#a1e8c4", "id": "Convergence of Value Functions", "label": "Convergence of Value Functions", "shape": "dot", "size": 10, "title": "Process by which value functions approach the optimal values in iterative algorithms."}, {"color": "#b816b2", "id": "Optimal Policy Determination", "label": "Optimal Policy Determination", "shape": "dot", "size": 10, "title": "Finding the best policy given an MDP, often through value or policy iteration."}, {"color": "#c63c5f", "id": "Bellman\u0027s Equations", "label": "Bellman\u0027s Equations", "shape": "dot", "size": 10, "title": "Set of equations used to determine optimal policies in decision-making processes."}, {"color": "#9d261c", "id": "Greedy Policy with Respect to V", "label": "Greedy Policy with Respect to V", "shape": "dot", "size": 10, "title": "Policy derived from the current value function that maximizes expected rewards."}, {"color": "#7281b2", "id": "Comparison Between Algorithms", "label": "Comparison Between Algorithms", "shape": "dot", "size": 10, "title": "Discussion on pros and cons of different algorithms for solving MDPs."}, {"color": "#722d9c", "id": "Estimating State Transitions", "label": "Estimating State Transitions", "shape": "dot", "size": 10, "title": "Methods for estimating transition probabilities based on observed data."}, {"color": "#fff61e", "id": "Expected Immediate Reward", "label": "Expected Immediate Reward", "shape": "dot", "size": 10, "title": "The expected reward received when an agent is in a particular state."}, {"color": "#b1e556", "id": "Learning in MDPs with Unknown Transitions", "label": "Learning in MDPs with Unknown Transitions", "shape": "dot", "size": 10, "title": "Approach for learning policies when state transition probabilities are not known."}, {"color": "#b21d43", "id": "Continuous State Space", "label": "Continuous State Space", "shape": "star", "size": 25, "title": "Extension of MDPs to handle continuous states rather than discrete ones."}, {"color": "#e819c9", "id": "Continuous State MDPs", "label": "Continuous State MDPs", "shape": "dot", "size": 10, "title": "MDPs with infinite state spaces, such as car or helicopter states."}, {"color": "#80f591", "id": "Finite State MDPs", "label": "Finite State MDPs", "shape": "dot", "size": 10, "title": "MDPs with a finite number of states, contrasting with continuous ones."}, {"color": "#69f4b5", "id": "Model Creation Methods", "label": "Model Creation Methods", "shape": "star", "size": 25, "title": "Different methods to obtain a model for state transitions in machine learning."}, {"color": "#67003e", "id": "Physics Simulation", "label": "Physics Simulation", "shape": "dot", "size": 10, "title": "Using physical laws or software packages to simulate system behavior."}, {"color": "#f5fccc", "id": "Open Dynamics Engine", "label": "Open Dynamics Engine", "shape": "dot", "size": 10, "title": "Free/open-source physics simulator for simulating mechanical systems."}, {"color": "#d99612", "id": "Learning from Data", "label": "Learning from Data", "shape": "dot", "size": 10, "title": "Inferring state transition probabilities from collected data in an MDP."}, {"color": "#557c22", "id": "Discretization in MDPs", "label": "Discretization in MDPs", "shape": "star", "size": 25, "title": "Process of converting continuous state space into discrete states for easier computation."}, {"color": "#93c072", "id": "Supervised Learning Problem", "label": "Supervised Learning Problem", "shape": "dot", "size": 10, "title": "Example of fitting a function using linear regression and piecewise constant representation."}, {"color": "#8faa05", "id": "Piecewise Constant Representation", "label": "Piecewise Constant Representation", "shape": "dot", "size": 10, "title": "Representation that assumes value is constant within each discretized interval."}, {"color": "#3d6233", "id": "Curse of Dimensionality", "label": "Curse of Dimensionality", "shape": "dot", "size": 10, "title": "Problem where the volume of the state space increases exponentially with dimensionality, making it hard to represent accurately."}, {"color": "#f6054c", "id": "State Representation", "label": "State Representation", "shape": "dot", "size": 10, "title": "Methods for representing states in machine learning problems."}, {"color": "#16f71b", "id": "Value Function Approximation", "label": "Value Function Approximation", "shape": "star", "size": 25, "title": "Approximating the value function using supervised learning methods such as linear regression."}, {"color": "#ada4c4", "id": "Model or Simulator", "label": "Model or Simulator", "shape": "dot", "size": 10, "title": "Black-box model providing next-state transitions based on current state and action."}, {"color": "#e50d1e", "id": "Linear Model Prediction", "label": "Linear Model Prediction", "shape": "dot", "size": 10, "title": "Predicting the next state using a linear model based on current state and action."}, {"color": "#a5df55", "id": "Learning Algorithm", "label": "Learning Algorithm", "shape": "dot", "size": 10, "title": "Algorithm used to estimate parameters A and B in the linear prediction model."}, {"color": "#411893", "id": "Deterministic vs Stochastic Models", "label": "Deterministic vs Stochastic Models", "shape": "dot", "size": 10, "title": "Comparison between deterministic and stochastic models for predicting next states."}, {"color": "#61464c", "id": "Non-linear Functions", "label": "Non-linear Functions", "shape": "dot", "size": 10, "title": "Use of non-linear functions in state transition prediction models."}, {"color": "#e0f277", "id": "Fitted Value Iteration", "label": "Fitted Value Iteration", "shape": "dot", "size": 10, "title": "Algorithm for approximating the value function through iterative sampling and regression."}, {"color": "#0ee0a6", "id": "Regression Algorithms", "label": "Regression Algorithms", "shape": "dot", "size": 10, "title": "Techniques to predict continuous outcomes from input data."}, {"color": "#dd4569", "id": "Non-linear Feature Mappings", "label": "Non-linear Feature Mappings", "shape": "dot", "size": 10, "title": "Feature mappings that transform states and actions into non-linear features."}, {"color": "#f42970", "id": "Discrete Action Space", "label": "Discrete Action Space", "shape": "dot", "size": 10, "title": "Assumption of a small, discrete set of actions available at each state."}, {"color": "#2a8fe7", "id": "Supervised Learning Algorithm", "label": "Supervised Learning Algorithm", "shape": "dot", "size": 10, "title": "Use of linear or non-linear regression to approximate the value function based on state features."}, {"color": "#5ddc22", "id": "State Sampling", "label": "State Sampling", "shape": "dot", "size": 10, "title": "Random sampling of states for value function approximation."}, {"color": "#67b095", "id": "Action Evaluation", "label": "Action Evaluation", "shape": "dot", "size": 10, "title": "Evaluation of actions based on expected future rewards and state transitions."}, {"color": "#af8a48", "id": "Expectation Approximation", "label": "Expectation Approximation", "shape": "dot", "size": 10, "title": "Techniques for estimating expectations in reinforcement learning algorithms, such as sampling and deterministic noise removal."}, {"color": "#6cf722", "id": "Deterministic Simulators", "label": "Deterministic Simulators", "shape": "dot", "size": 10, "title": "Simulations where the next state is determined solely by current state and action without random noise."}, {"color": "#c2ca6d", "id": "Gaussian Noise Model", "label": "Gaussian Noise Model", "shape": "dot", "size": 10, "title": "Modeling simulator transitions with a deterministic function plus Gaussian noise for approximation purposes."}, {"color": "#42c017", "id": "Bellman Updates", "label": "Bellman Updates", "shape": "dot", "size": 10, "title": "Iterative process of updating the value function based on Bellman\u0027s equation to converge towards an optimal policy."}, {"color": "#415f13", "id": "VE Procedure", "label": "VE Procedure", "shape": "dot", "size": 10, "title": "Procedure used to evaluate value function under a given policy."}, {"color": "#1e1af3", "id": "k Parameter", "label": "k Parameter", "shape": "dot", "size": 10, "title": "Hyperparameter controlling the number of iterations in VE procedure."}, {"color": "#0761bb", "id": "Initialization Option 1", "label": "Initialization Option 1", "shape": "dot", "size": 10, "title": "Initialize value function to zero for all states."}, {"color": "#efd663", "id": "Initialization Option 2", "label": "Initialization Option 2", "shape": "dot", "size": 10, "title": "Initialize value function based on previous iterations\u0027 values."}, {"color": "#92403b", "id": "Update Rule (15.12)", "label": "Update Rule (15.12)", "shape": "dot", "size": 10, "title": "Rule for updating state values using Bellman equation."}, {"color": "#9840b4", "id": "Policy Update Rule (15.13)", "label": "Policy Update Rule (15.13)", "shape": "dot", "size": 10, "title": "Rule for updating policy based on value function estimates."}, {"color": "#cb91f3", "id": "Chapter 15 Overview", "label": "Chapter 15 Overview", "shape": "star", "size": 25, "title": "Overview of MDPs and value/policy iteration methods."}, {"color": "#ace204", "id": "Optimal Bellman Equation", "label": "Optimal Bellman Equation", "shape": "dot", "size": 10, "title": "Equation defining the optimal value function for an optimal policy."}, {"color": "#cbe78e", "id": "Policy Iteration Speedup", "label": "Policy Iteration Speedup", "shape": "dot", "size": 10, "title": "Discussion on how policy iteration can be faster than repeated single-step updates."}, {"color": "#fc0e3b", "id": "Value Iteration Preference", "label": "Value Iteration Preference", "shape": "dot", "size": 10, "title": "When value iteration is preferred over policy iteration due to computational constraints."}, {"color": "#eefb53", "id": "Chapter 16 Introduction", "label": "Chapter 16 Introduction", "shape": "star", "size": 25, "title": "Introduction to LQR, DDP and LQG concepts in MDPs."}, {"color": "#31f198", "id": "Optimal Value Function Recovery", "label": "Optimal Value Function Recovery", "shape": "dot", "size": 10, "title": "Recovering the optimal policy from the optimal value function."}, {"color": "#531df0", "id": "Finite Horizon MDPs", "label": "Finite Horizon MDPs", "shape": "dot", "size": 10, "title": "Models considering a finite sequence of decisions without the need for a discount factor."}, {"color": "#a16d4b", "id": "Time-Dependent Policies", "label": "Time-Dependent Policies", "shape": "dot", "size": 10, "title": "Policies that change over time in response to the environment."}, {"color": "#1ff388", "id": "Non-Stationary Optimal Policy", "label": "Non-Stationary Optimal Policy", "shape": "dot", "size": 10, "title": "Optimal policies vary based on remaining steps and current state."}, {"color": "#e7f4e4", "id": "Dynamic Environment Models", "label": "Dynamic Environment Models", "shape": "dot", "size": 10, "title": "Models that account for changing dynamics over time."}, {"color": "#aff594", "id": "Expectation Calculation", "label": "Expectation Calculation", "shape": "dot", "size": 10, "title": "Calculating expectations under certain conditions that simplify the expression for policy gradients."}, {"color": "#8cf6ec", "id": "Rewards Dependency on States and Actions", "label": "Rewards Dependency on States and Actions", "shape": "dot", "size": 10, "title": "Description of how rewards depend on both states and actions in an MDP."}, {"color": "#308e81", "id": "Infinite Horizon MDPs", "label": "Infinite Horizon MDPs", "shape": "dot", "size": 10, "title": "Models considering an infinite sequence of decisions with a discount factor for future rewards."}, {"color": "#7bd387", "id": "Discount Factor \u03b3", "label": "Discount Factor \u03b3", "shape": "dot", "size": 10, "title": "Parameter used to ensure convergence in infinite horizon models by discounting future rewards."}, {"color": "#5db2d5", "id": "Value Function in RL", "label": "Value Function in RL", "shape": "dot", "size": 10, "title": "Definition and computation of value functions in reinforcement learning."}, {"color": "#ee06ae", "id": "Dynamic Programming", "label": "Dynamic Programming", "shape": "dot", "size": 10, "title": "Application of dynamic programming to reinforcement learning problems."}, {"color": "#b310a7", "id": "Bellman Update", "label": "Bellman Update", "shape": "dot", "size": 10, "title": "Operator used to update value functions in reinforcement learning."}, {"color": "#f9e6c1", "id": "Geometric Convergence", "label": "Geometric Convergence", "shape": "dot", "size": 10, "title": "Rate at which the approximation error decreases with each iteration."}, {"color": "#e3c49c", "id": "Continuous Setting", "label": "Continuous Setting", "shape": "dot", "size": 10, "title": "Model assumptions for LQR including state and action spaces as real vectors."}, {"color": "#5cde9a", "id": "Linear Transitions", "label": "Linear Transitions", "shape": "dot", "size": 10, "title": "Assumption of linear dynamics in the system with Gaussian noise."}, {"color": "#a5770b", "id": "Quadratic Rewards", "label": "Quadratic Rewards", "shape": "dot", "size": 10, "title": "Rewards defined as quadratic functions of state and action vectors."}, {"color": "#cfd16e", "id": "Quadratic Assumption", "label": "Quadratic Assumption", "shape": "dot", "size": 10, "title": "Assumes that the value functions are quadratic for simplification."}, {"color": "#69332b", "id": "Dynamics of Model", "label": "Dynamics of Model", "shape": "dot", "size": 10, "title": "Incorporates model dynamics into the optimal value function calculation."}, {"color": "#efb65f", "id": "Linear Optimal Action", "label": "Linear Optimal Action", "shape": "dot", "size": 10, "title": "Derives the formula for the optimal action which is a linear function of the state."}, {"color": "#1378a5", "id": "LQR Model Assumptions", "label": "LQR Model Assumptions", "shape": "dot", "size": 10, "title": "Assumptions made in the Linear Quadratic Regulator model."}, {"color": "#0a9151", "id": "LQR Algorithm Steps", "label": "LQR Algorithm Steps", "shape": "dot", "size": 10, "title": "Steps involved in implementing the LQR algorithm."}, {"color": "#81ec75", "id": "Step 1: Estimate Matrices", "label": "Step 1: Estimate Matrices", "shape": "dot", "size": 10, "title": "Estimating matrices A, B, and Sigma using linear regression."}, {"color": "#bd7e5c", "id": "Step 2: Derive Optimal Policy", "label": "Step 2: Derive Optimal Policy", "shape": "dot", "size": 10, "title": "Deriving the optimal policy given known model parameters."}, {"color": "#69353a", "id": "Dynamic Programming Application", "label": "Dynamic Programming Application", "shape": "dot", "size": 10, "title": "Application of dynamic programming to compute V_t* in LQR context."}, {"color": "#749867", "id": "Linear Quadratic Regulator (LQR)", "label": "Linear Quadratic Regulator (LQR)", "shape": "dot", "size": 10, "title": "A method to find optimal control policies in linear systems with quadratic cost functions."}, {"color": "#ccb808", "id": "Discrete Ricatti Equations", "label": "Discrete Ricatti Equations", "shape": "dot", "size": 10, "title": "Set of equations used to solve the LQR problem iteratively."}, {"color": "#9f2fe3", "id": "Inverted Pendulum Example", "label": "Inverted Pendulum Example", "shape": "dot", "size": 10, "title": "Example system demonstrating the application of linearization techniques."}, {"color": "#2f0607", "id": "Linearization of Dynamics", "label": "Linearization of Dynamics", "shape": "dot", "size": 10, "title": "Process of approximating nonlinear dynamics with a linear model for easier analysis."}, {"color": "#4cc40e", "id": "Taylor Expansion", "label": "Taylor Expansion", "shape": "dot", "size": 10, "title": "Mathematical technique used to approximate functions using polynomials, crucial in linearizing systems."}, {"color": "#d85e73", "id": "Optimization in RL", "label": "Optimization in RL", "shape": "dot", "size": 10, "title": "Techniques for optimizing policies in reinforcement learning."}, {"color": "#00977c", "id": "LQG Framework", "label": "LQG Framework", "shape": "star", "size": 25, "title": "Extension of LQR to handle stochastic systems and partial observability."}, {"color": "#190b37", "id": "Partial Observability", "label": "Partial Observability", "shape": "dot", "size": 10, "title": "Situation where the full state is not observable, requiring models like LQG."}, {"color": "#d9ba23", "id": "Nominal Trajectory Generation", "label": "Nominal Trajectory Generation", "shape": "dot", "size": 10, "title": "Creating an initial approximate path using a naive controller."}, {"color": "#026f9c", "id": "Rewriting Dynamics", "label": "Rewriting Dynamics", "shape": "dot", "size": 10, "title": "Expressing the state transition using matrices A and B for non-stationary settings."}, {"color": "#165366", "id": "Reward Function Approximation", "label": "Reward Function Approximation", "shape": "dot", "size": 10, "title": "Using Taylor expansion to approximate rewards around nominal trajectory points."}, {"color": "#65c99a", "id": "Partially Observable MDPs (POMDP)", "label": "Partially Observable MDPs (POMDP)", "shape": "dot", "size": 10, "title": "MDPs with an additional observation layer to handle partial observability."}, {"color": "#01ebbf", "id": "Observation Layer", "label": "Observation Layer", "shape": "dot", "size": 10, "title": "Introduces new variable o_t representing observations given the current state s_t."}, {"color": "#d2363a", "id": "Belief State", "label": "Belief State", "shape": "dot", "size": 10, "title": "Maintains a distribution over states based on past observations to inform policy decisions."}, {"color": "#7759d2", "id": "LQR Extension", "label": "LQR Extension", "shape": "dot", "size": 10, "title": "Extension of Linear Quadratic Regulator to handle partial observability scenarios."}, {"color": "#8ec0e8", "id": "Kalman Filter", "label": "Kalman Filter", "shape": "dot", "size": 10, "title": "Algorithm used for efficient computation and updating of belief states over time."}, {"color": "#05e4dc", "id": "LQR Updates", "label": "LQR Updates", "shape": "dot", "size": 10, "title": "Backward pass computations for optimal control policies."}, {"color": "#0afe77", "id": "Randomized Policy", "label": "Randomized Policy", "shape": "dot", "size": 10, "title": "Learning policy that outputs actions probabilistically based on state input."}, {"color": "#c7ccab", "id": "Expected Total Payoff", "label": "Expected Total Payoff", "shape": "dot", "size": 10, "title": "Objective function for optimizing policy parameters over trajectories."}, {"color": "#aaec78", "id": "Predict Step", "label": "Predict Step", "shape": "dot", "size": 10, "title": "Estimates the next state based on current state distribution."}, {"color": "#0741f7", "id": "Update Step", "label": "Update Step", "shape": "dot", "size": 10, "title": "Refine state estimate with new observation at each time step."}, {"color": "#3fad6f", "id": "Belief States Update", "label": "Belief States Update", "shape": "dot", "size": 10, "title": "Updates belief states through predict and update steps iteratively."}, {"color": "#f512f7", "id": "Kalman Gain", "label": "Kalman Gain", "shape": "dot", "size": 10, "title": "Matrix used in update step to refine state estimate based on new observation."}, {"color": "#3617bf", "id": "Step 1", "label": "Step 1", "shape": "dot", "size": 10, "title": "Initial step to set up the system dynamics and noise model."}, {"color": "#1647d1", "id": "Step 2", "label": "Step 2", "shape": "dot", "size": 10, "title": "Use mean of distribution as state approximation."}, {"color": "#e1e240", "id": "Step 3", "label": "Step 3", "shape": "dot", "size": 10, "title": "Set action based on approximated state and LQR algorithm."}, {"color": "#8f75a3", "id": "System Dynamics", "label": "System Dynamics", "shape": "star", "size": 25, "title": "Model describing how the system evolves over time and is affected by noise."}, {"color": "#a844cf", "id": "LQR Algorithm", "label": "LQR Algorithm", "shape": "star", "size": 25, "title": "Linear Quadratic Regulator algorithm used to determine optimal control actions."}, {"color": "#92d250", "id": "Policy Gradients", "label": "Policy Gradients", "shape": "dot", "size": 10, "title": "Techniques for optimizing policies in reinforcement learning environments."}, {"color": "#9cef17", "id": "Expectation Estimation", "label": "Expectation Estimation", "shape": "dot", "size": 10, "title": "Estimating the expected value of a function under a policy distribution."}, {"color": "#c2c382", "id": "Sample-Based Estimation", "label": "Sample-Based Estimation", "shape": "dot", "size": 10, "title": "Using samples to estimate the gradient of expected values."}, {"color": "#1dfabd", "id": "Log Probability Calculation", "label": "Log Probability Calculation", "shape": "dot", "size": 10, "title": "Calculating log probabilities for policy gradients."}, {"color": "#b00ffd", "id": "Reward Function Estimation", "label": "Reward Function Estimation", "shape": "dot", "size": 10, "title": "Estimating gradients without knowing the exact form of the reward function."}, {"color": "#b74932", "id": "Expectation Maximization", "label": "Expectation Maximization", "shape": "dot", "size": 10, "title": "Using expectations of rewards over policy distributions to estimate gradients."}, {"color": "#f19a9d", "id": "Reparametrization Technique", "label": "Reparametrization Technique", "shape": "dot", "size": 10, "title": "Technique used in VAEs for gradient estimation, not applicable here."}, {"color": "#5d3ce3", "id": "REINFORCE Algorithm", "label": "REINFORCE Algorithm", "shape": "dot", "size": 10, "title": "Algorithm for estimating gradients of policy performance without knowing the reward function."}, {"color": "#29480d", "id": "Policy Gradient Theorem", "label": "Policy Gradient Theorem", "shape": "star", "size": 25, "title": "Theorem that connects policy gradients to expected payoff."}, {"color": "#ffeea5", "id": "Log Probability Derivative", "label": "Log Probability Derivative", "shape": "dot", "size": 10, "title": "Derivative of log probability with respect to parameters \u03b8."}, {"color": "#ccf62b", "id": "Vanilla REINFORCE Algorithm", "label": "Vanilla REINFORCE Algorithm", "shape": "dot", "size": 10, "title": "Algorithm that updates policy parameters using estimated gradients."}, {"color": "#d73340", "id": "Trajectory Probability Change", "label": "Trajectory Probability Change", "shape": "dot", "size": 10, "title": "Change in trajectory probability due to parameter changes."}, {"color": "#780996", "id": "Empirical Trajectories Estimation", "label": "Empirical Trajectories Estimation", "shape": "dot", "size": 10, "title": "Estimating gradients using sample trajectories."}, {"color": "#ce1739", "id": "Trajectory Probability", "label": "Trajectory Probability", "shape": "dot", "size": 10, "title": "Probability of an agent following a specific sequence of actions and states."}, {"color": "#c4e879", "id": "Expectation Equations", "label": "Expectation Equations", "shape": "dot", "size": 10, "title": "Mathematical expressions for expected values in reinforcement learning scenarios."}, {"color": "#3976c0", "id": "Simplification of Formula (17.8)", "label": "Simplification of Formula (17.8)", "shape": "dot", "size": 10, "title": "Derivation and simplification process based on constant reward assumption."}, {"color": "#9201bd", "id": "Policy Gradient Methods", "label": "Policy Gradient Methods", "shape": "dot", "size": 10, "title": "Techniques for optimizing the parameters of a policy directly based on the performance measure, such as expected return or discounted sum of rewards."}, {"color": "#254a41", "id": "Law of Total Expectation", "label": "Law of Total Expectation", "shape": "dot", "size": 10, "title": "A theorem in probability theory that allows one to compute the expectation of any function of a random variable by conditioning on another random variable."}, {"color": "#802e14", "id": "Estimator Simplification", "label": "Estimator Simplification", "shape": "dot", "size": 10, "title": "Simplifying an estimator using the law of total expectation, making it easier to understand and compute."}, {"color": "#f53179", "id": "Baseline Estimation", "label": "Baseline Estimation", "shape": "dot", "size": 10, "title": "Techniques to reduce variance in policy gradient estimators using baselines."}, {"color": "#ee4dc2", "id": "Trajectory Collection", "label": "Trajectory Collection", "shape": "dot", "size": 10, "title": "Process of collecting data through interaction with an environment to train policies."}, {"color": "#8b6232", "id": "Gradient Estimator Update", "label": "Gradient Estimator Update", "shape": "dot", "size": 10, "title": "Updating policy parameters based on the gradient estimator using baselines."}, {"color": "#42869e", "id": "Machine_Learning_Theory", "label": "Machine_Learning_Theory", "shape": "star", "size": 25, "title": "Theoretical foundations of machine learning including generalization and double descent phenomena."}, {"color": "#78692d", "id": "Double_Descent_Phenomenon", "label": "Double_Descent_Phenomenon", "shape": "dot", "size": 10, "title": "Phenomenon where model performance initially improves then worsens before improving again with increased complexity."}, {"color": "#c7b166", "id": "Statistical_Mechanics_of_Learning", "label": "Statistical_Mechanics_of_Learning", "shape": "dot", "size": 10, "title": "Application of statistical mechanics principles to understand learning processes and generalization in neural networks."}, {"color": "#229126", "id": "Machine Learning Literature", "label": "Machine Learning Literature", "shape": "star", "size": 25, "title": "Collection of key papers and reviews in machine learning."}, {"color": "#cd85f9", "id": "Bias-Variance Trade-off Reconciliation", "label": "Bias-Variance Trade-off Reconciliation", "shape": "dot", "size": 10, "title": "Paper discussing the reconciliation between modern ML practice and classical bias-variance trade-off."}, {"color": "#9942eb", "id": "Double Descent for Weak Features", "label": "Double Descent for Weak Features", "shape": "dot", "size": 10, "title": "Study on double descent phenomenon in machine learning models with weak features."}, {"color": "#250680", "id": "Variational Inference Review", "label": "Variational Inference Review", "shape": "dot", "size": 10, "title": "Review paper discussing variational inference methods for statisticians."}, {"color": "#67a173", "id": "Foundation Models Opportunities and Risks", "label": "Foundation Models Opportunities and Risks", "shape": "dot", "size": 10, "title": "Discussion on the opportunities and risks associated with foundation models in ML."}, {"color": "#1f6f7c", "id": "Few-Shot Learning Capabilities", "label": "Few-Shot Learning Capabilities", "shape": "dot", "size": 10, "title": "Research highlighting the few-shot learning capabilities of language models."}, {"color": "#37cb2a", "id": "Contrastive Learning Framework", "label": "Contrastive Learning Framework", "shape": "dot", "size": 10, "title": "Framework for contrastive learning to improve visual representation in machine learning."}, {"color": "#e9de0e", "id": "BERT Pre-training Methodology", "label": "BERT Pre-training Methodology", "shape": "dot", "size": 10, "title": "Introduction of BERT, a deep bidirectional transformer model for language understanding."}, {"color": "#b3914d", "id": "Implicit Bias Study", "label": "Implicit Bias Study", "shape": "dot", "size": 10, "title": "Research on the implicit bias in machine learning models due to noise covariance shape."}, {"color": "#df5c2e", "id": "High-Dimensional Statistical Analysis", "label": "High-Dimensional Statistical Analysis", "shape": "dot", "size": 10, "title": "Discussion on surprising phenomena in high-dimensional statistical analysis and machine learning."}, {"color": "#4d031d", "id": "Machine Learning Papers", "label": "Machine Learning Papers", "shape": "star", "size": 25, "title": "Collection of research papers related to machine learning and statistical learning theory."}, {"color": "#fff3d3", "id": "Implicit Bias in Machine Learning", "label": "Implicit Bias in Machine Learning", "shape": "dot", "size": 10, "title": "Research on the implicit bias introduced by different noise covariances and ridgeless least squares interpolation."}, {"color": "#c6ebb1", "id": "Deep Residual Learning", "label": "Deep Residual Learning", "shape": "dot", "size": 10, "title": "Introduction to deep residual learning for image recognition, a key technique in deep neural networks."}, {"color": "#899e96", "id": "Theoretical Guarantees for Deep Reinforcement Learning", "label": "Theoretical Guarantees for Deep Reinforcement Learning", "shape": "dot", "size": 10, "title": "Research on theoretical frameworks for model-based deep reinforcement learning with guarantees."}, {"color": "#c83d7f", "id": "Generalization Error Analysis", "label": "Generalization Error Analysis", "shape": "dot", "size": 10, "title": "Analysis of generalization error in random features regression and linear regression models."}]);
                  edges = new vis.DataSet([{"arrows": "to", "from": "Independent components analysis", "title": "has_subtopic", "to": "ICA algorithm"}, {"arrows": "to", "from": "Machine Learning Algorithms", "title": "depends_on", "to": "Kernel Trick"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "subtopic", "to": "Linearization of Dynamics"}, {"arrows": "to", "from": "Machine Learning Theory", "title": "depends_on", "to": "Learning Guarantees"}, {"arrows": "to", "from": "Exponential Family Distributions", "title": "subtopic", "to": "Gaussian Distribution"}, {"arrows": "to", "from": "Reward Function Estimation", "title": "subtopic", "to": "REINFORCE Algorithm"}, {"arrows": "to", "from": "Auto-Differentiation", "title": "depends_on", "to": "Deep Learning Packages"}, {"arrows": "to", "from": "Fitted Value Iteration", "title": "depends_on", "to": "Continuous State Space"}, {"arrows": "to", "from": "Mean Field Assumption", "title": "subtopic", "to": "Variational Inference"}, {"arrows": "to", "from": "Cumulative Distribution Function (CDF)", "title": "related_to", "to": "Sigmoid Function as Default Density"}, {"arrows": "to", "from": "Backward Function for Loss Functions", "title": "subtopic", "to": "Squared Loss (MSE)"}, {"arrows": "to", "from": "Dual Formulation of SVM", "title": "related_to", "to": "KKT Conditions"}, {"arrows": "to", "from": "Regularization in Machine Learning", "title": "subtopic", "to": "Deep Learning Regularization Techniques"}, {"arrows": "to", "from": "Policy Gradients", "title": "has_subtopic", "to": "Expectation Equations"}, {"arrows": "to", "from": "Independent Components Analysis (ICA)", "title": "subtopic", "to": "Cocktail Party Problem"}, {"arrows": "to", "from": "Softplus Function", "title": "subtopic", "to": "Activation Functions"}, {"arrows": "to", "from": "Regularization", "title": "depends_on", "to": "Regularized Loss"}, {"arrows": "to", "from": "Decision Boundary", "title": "related_to", "to": "Functional Margins"}, {"arrows": "to", "from": "Loss Function", "title": "subtopic", "to": "Cross Entropy Loss"}, {"arrows": "to", "from": "Transformer Model", "title": "related_to", "to": "Autoregressive Text Decoding"}, {"arrows": "to", "from": "Linear Quadratic Regulation (LQR)", "title": "subtopic", "to": "Continuous Setting"}, {"arrows": "to", "from": "Backpropagation", "title": "subtopic_of", "to": "Preliminaries on partial derivatives"}, {"arrows": "to", "from": "Double Descent for Weak Features", "title": "subtopic", "to": "Machine Learning Literature"}, {"arrows": "to", "from": "\u039b_2 Regularization", "title": "depends_on", "to": "Weight Decay"}, {"arrows": "to", "from": "Mean and Variance Functions", "title": "subtopic", "to": "Succinct Representation of Distribution Qi"}, {"arrows": "to", "from": "Independent Component Analysis (ICA)", "title": "related_to", "to": "Gaussian Data Limitation"}, {"arrows": "to", "from": "Bernoulli Distribution", "title": "related_to", "to": "Exponential Family Distributions"}, {"arrows": "to", "from": "Kalman Filter", "title": "has_subtopic", "to": "Predict Step"}, {"arrows": "to", "from": "Regularization", "title": "related_to", "to": "Bias-Variance Tradeoff"}, {"arrows": "to", "from": "Ordinary Least Squares", "title": "related_to", "to": "Response Variable"}, {"arrows": "to", "from": "Variational Inference", "title": "contains", "to": "ELBO"}, {"arrows": "to", "from": "Initialization", "title": "subtopic", "to": "k-means Algorithm"}, {"arrows": "to", "from": "Two-Layer Neural Network", "title": "depends_on", "to": "ReLU Function"}, {"arrows": "to", "from": "Optimization Problem", "title": "subtopic", "to": "Objective Function Transformation"}, {"arrows": "to", "from": "Machine Learning Overview", "title": "contains", "to": "Unsupervised Learning"}, {"arrows": "to", "from": "Machine Learning Algorithms", "title": "subtopic", "to": "Generative Learning Algorithms"}, {"arrows": "to", "from": "Joint Distribution of Sources", "title": "subtopic", "to": "Density on x=As=W^-1s"}, {"arrows": "to", "from": "Deep Learning Representations", "title": "subtopic", "to": "Feature Maps and Representation Transferability"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "related_to", "to": "VC Dimension"}, {"arrows": "to", "from": "Foundation Models", "title": "subtopic", "to": "Adaptation Phase"}, {"arrows": "to", "from": "Principal Component Analysis (PCA)", "title": "subtopic", "to": "Dimension Reduction"}, {"arrows": "to", "from": "Tanh Function", "title": "subtopic", "to": "Activation Functions"}, {"arrows": "to", "from": "EM Algorithm", "title": "related_to", "to": "Machine Learning Models"}, {"arrows": "to", "from": "Kernel Functions", "title": "subtopic", "to": "Feature Mapping"}, {"arrows": "to", "from": "Mini-batch SGD", "title": "depends_on", "to": "Mini-batch Hyperparameters"}, {"arrows": "to", "from": "Fitted Value Iteration", "title": "subtopic", "to": "State Sampling"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "subtopic", "to": "Feature Mapping"}, {"arrows": "to", "from": "Convolutional Layers", "title": "subtopic", "to": "Efficiency Comparison"}, {"arrows": "to", "from": "Value Function", "title": "defines", "to": "Optimal Value Function"}, {"arrows": "to", "from": "Modules in Modern Neural Networks", "title": "subtopic", "to": "MLP Composition of Modules"}, {"arrows": "to", "from": "Support Vector Machines", "title": "related_to", "to": "Non-separable Case"}, {"arrows": "to", "from": "Fully-Connected Neural Networks", "title": "example_of", "to": "Two-Layer Neural Network"}, {"arrows": "to", "from": "Stochastic Gradient Descent", "title": "related_to", "to": "Learning Rate Decay"}, {"arrows": "to", "from": "Backward Functions", "title": "subtopic", "to": "Activations"}, {"arrows": "to", "from": "Polynomial Fitting", "title": "depends_on", "to": "Overfitting"}, {"arrows": "to", "from": "EM Algorithms", "title": "subtopic", "to": "EM for Mixture of Gaussians"}, {"arrows": "to", "from": "Machine Learning Techniques", "title": "related_to", "to": "EM Algorithms"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "depends_on", "to": "Optimization Constraints"}, {"arrows": "to", "from": "Model Evaluation", "title": "subtopic", "to": "Mean Squared Error (MSE)"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "related_to", "to": "Generalization Error"}, {"arrows": "to", "from": "Model Parameters", "title": "depends_on", "to": "Likelihood Estimation"}, {"arrows": "to", "from": "Mean Squared Error (MSE)", "title": "depends_on", "to": "Average Model (h_avg)"}, {"arrows": "to", "from": "Bias-Variance Tradeoff", "title": "subtopic", "to": "Variance Term"}, {"arrows": "to", "from": "Machine Learning Basics", "title": "related_to", "to": "Gradient Descent"}, {"arrows": "to", "from": "Support Vector Machines (SVM)", "title": "subtopic", "to": "Optimal Margin Classifier"}, {"arrows": "to", "from": "Machine_Learning_Concepts", "title": "has_subtopic", "to": "EM_Algorithm"}, {"arrows": "to", "from": "Machine Learning Basics", "title": "contains", "to": "Loss Functions"}, {"arrows": "to", "from": "Unsupervised learning", "title": "has_subtopic", "to": "Independent components analysis"}, {"arrows": "to", "from": "Language Model Probability Distribution", "title": "subtopic", "to": "Pretrained Large Language Models"}, {"arrows": "to", "from": "MLP (Multi-Layer Perceptron)", "title": "depends_on", "to": "Matrix Multiplication Module"}, {"arrows": "to", "from": "Mixture of Gaussians Model", "title": "subtopic", "to": "Latent Variables"}, {"arrows": "to", "from": "SMO Algorithm", "title": "uses", "to": "Lagrange Multipliers"}, {"arrows": "to", "from": "Reinforcement learning", "title": "subtopic_of", "to": "Value iteration and policy iteration"}, {"arrows": "to", "from": "SMO Algorithm (Optional)", "title": "subtopic", "to": "SMO Details"}, {"arrows": "to", "from": "Geometric Margin", "title": "depends_on", "to": "Decision Boundary"}, {"arrows": "to", "from": "Reinforcement Learning and Control", "title": "has_subtopic", "to": "Reinforcement learning"}, {"arrows": "to", "from": "Bias-Variance Tradeoff", "title": "subtopic", "to": "Bias Term"}, {"arrows": "to", "from": "Naive Bayes Algorithm", "title": "related_to", "to": "Bayesian Inference"}, {"arrows": "to", "from": "Bernoulli Random Variable Z", "title": "subtopic", "to": "Training Set Sampling"}, {"arrows": "to", "from": "Machine Learning Algorithms", "title": "contains", "to": "Deep Learning Model Training Steps"}, {"arrows": "to", "from": "Machine Learning Algorithms", "title": "subtopic", "to": "Policy Gradient (REINFORCE)"}, {"arrows": "to", "from": "Finetuning Algorithm", "title": "subtopic", "to": "Adaptation Algorithm"}, {"arrows": "to", "from": "Double Descent Phenomenon", "title": "has_subtopic", "to": "Model-wise Double Descent"}, {"arrows": "to", "from": "Double Descent Phenomenon", "title": "subtopic", "to": "Model Complexity Measures"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "related_to", "to": "Support Vector Machines (SVM)"}, {"arrows": "to", "from": "Model Parameters", "title": "related_to", "to": "Log Likelihood Function"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "depends_on", "to": "Valid Kernels Conditions"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "depends_on", "to": "Backpropagation"}, {"arrows": "to", "from": "Optimization Problem", "title": "subtopic", "to": "Scaling Constraint"}, {"arrows": "to", "from": "Log Likelihood", "title": "subtopic", "to": "Maximum Likelihood Estimation"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "related_to", "to": "Gradient Descent Optimizer"}, {"arrows": "to", "from": "Model Complexity and Test Errors", "title": "depends_on", "to": "Double Descent Phenomenon"}, {"arrows": "to", "from": "Machine Learning Algorithms", "title": "contains", "to": "Stochastic Gradient Descent (SGD)"}, {"arrows": "to", "from": "Logistic Regression", "title": "related_to", "to": "Decision Boundaries"}, {"arrows": "to", "from": "Belief State", "title": "depends_on", "to": "Kalman Filter"}, {"arrows": "to", "from": "Machine Learning Models", "title": "has_subtopic", "to": "Locally Weighted Linear Regression"}, {"arrows": "to", "from": "Parameter Estimation", "title": "subtopic", "to": "Zero Frequency Problem"}, {"arrows": "to", "from": "Generalized Lagrangian", "title": "depends_on", "to": "\\(\\theta_{\\cal P}(w)\\)"}, {"arrows": "to", "from": "Expectation Equations", "title": "subtopic", "to": "Simplification of Formula (17.8)"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "has_subtopic", "to": "Policy Gradients"}, {"arrows": "to", "from": "Derived Features", "title": "subtopic", "to": "School Quality"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "contains", "to": "Markov Decision Processes (MDP)"}, {"arrows": "to", "from": "Covariance Matrix", "title": "depends_on", "to": "Multivariate Normal Distribution"}, {"arrows": "to", "from": "Labeled Dataset", "title": "related_to", "to": "Machine Learning Adaptation Methods"}, {"arrows": "to", "from": "Self-supervised learning and foundation models", "title": "has_subtopic", "to": "Pretrained large language models"}, {"arrows": "to", "from": "Zero-Shot Learning", "title": "subtopic", "to": "Machine Learning Adaptation Methods"}, {"arrows": "to", "from": "Model Parameters", "title": "related_to", "to": "Likelihood Function"}, {"arrows": "to", "from": "Baseline Estimation", "title": "depends_on", "to": "Value Function Approximation"}, {"arrows": "to", "from": "Back-propagation for MLPs", "title": "depends_on", "to": "Forward Pass in MLP"}, {"arrows": "to", "from": "Maximum Likelihood Estimation (MLE)", "title": "uses", "to": "Likelihood Function"}, {"arrows": "to", "from": "Training Error vs Generalization Error", "title": "subtopic", "to": "Probability of Error"}, {"arrows": "to", "from": "Machine Learning Models", "title": "has_subtopic", "to": "Logistic Regression"}, {"arrows": "to", "from": "Machine Learning Models", "title": "contains", "to": "Generalized Linear Model (GLM)"}, {"arrows": "to", "from": "Finite Horizon MDPs", "title": "related_to", "to": "Time-Dependent Policies"}, {"arrows": "to", "from": "Feature Vector", "title": "subtopic", "to": "Vocabulary"}, {"arrows": "to", "from": "Gradient Calculation", "title": "subtopic", "to": "Log Probability Calculation"}, {"arrows": "to", "from": "Adaptation Methods", "title": "subtopic", "to": "In-context Learning"}, {"arrows": "to", "from": "Eigenvectors of Sigma", "title": "depends_on", "to": "Dimensionality Reduction"}, {"arrows": "to", "from": "Few-Shhot Learning", "title": "subtopic", "to": "Machine Learning Adaptation Methods"}, {"arrows": "to", "from": "Optimal Value Function", "title": "related_to", "to": "Bellman\u0027s Equation"}, {"arrows": "to", "from": "Backpropagation", "title": "related_to", "to": "Forward Pass"}, {"arrows": "to", "from": "Backpropagation", "title": "subtopic", "to": "Gradient Computation"}, {"arrows": "to", "from": "Fitted Value Iteration", "title": "related_to", "to": "Feature Mapping"}, {"arrows": "to", "from": "Sample Complexity Bounds", "title": "subtopic", "to": "Machine Learning Concepts"}, {"arrows": "to", "from": "Markov Decision Process (MDP)", "title": "depends_on", "to": "State Transition Probabilities"}, {"arrows": "to", "from": "Maximum Likelihood Estimation", "title": "subtopic", "to": "Machine Learning Basics"}, {"arrows": "to", "from": "Neural Networks", "title": "subtopic", "to": "Two-Layer Neural Network"}, {"arrows": "to", "from": "Feature Mapping", "title": "contains", "to": "Polynomial Kernels"}, {"arrows": "to", "from": "Sigmoid Function", "title": "subtopic", "to": "Activation Functions"}, {"arrows": "to", "from": "Convex Functions", "title": "related_to", "to": "Strict Convexity"}, {"arrows": "to", "from": "Optimization Problems", "title": "related_to", "to": "Affine Constraints"}, {"arrows": "to", "from": "Normal Equations", "title": "subtopic", "to": "Least Squares Revisited"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "related_to", "to": "Binary Classification Problem"}, {"arrows": "to", "from": "Generalization Error", "title": "subtopic", "to": "Sample Complexity Bounds"}, {"arrows": "to", "from": "Geometric Margins", "title": "subtopic", "to": "Machine Learning Concepts"}, {"arrows": "to", "from": "Model Parameters", "title": "subtopic", "to": "Log-Likelihood Function"}, {"arrows": "to", "from": "Necessary Conditions for Valid Kernels", "title": "depends_on", "to": "Symmetry Property"}, {"arrows": "to", "from": "Gradient Calculation", "title": "subtopic", "to": "Matrix Derivatives"}, {"arrows": "to", "from": "Transformer Model", "title": "depends_on", "to": "Conditional Probability"}, {"arrows": "to", "from": "Data Normalization", "title": "has_subtopic", "to": "Variance Scaling"}, {"arrows": "to", "from": "Overfitting", "title": "subtopic", "to": "Machine Learning Basics"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "related_to", "to": "Housing Price Prediction"}, {"arrows": "to", "from": "Reparameterization Trick", "title": "subtopic", "to": "Gradient Estimation"}, {"arrows": "to", "from": "Value Iteration and Policy Iteration", "title": "subtopic", "to": "Discretization in MDPs"}, {"arrows": "to", "from": "Kernels in Machine Learning", "title": "contains", "to": "Kernel Functions"}, {"arrows": "to", "from": "Densities and Linear Transformations", "title": "contains", "to": "Linear Transformation Impact"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "subtopic", "to": "Double Descent Phenomenon"}, {"arrows": "to", "from": "Naive Bayes Algorithm", "title": "depends_on", "to": "Binary Features"}, {"arrows": "to", "from": "Discriminative Learning Algorithms", "title": "related_to", "to": "Logistic Regression"}, {"arrows": "to", "from": "BERT Pre-training Methodology", "title": "subtopic", "to": "Machine Learning Literature"}, {"arrows": "to", "from": "Hypothesis Class", "title": "subtopic", "to": "Finite Hypothesis Classes"}, {"arrows": "to", "from": "Function Representation", "title": "subtopic", "to": "Linear Function Approximation"}, {"arrows": "to", "from": "Machine Learning Adaptation Techniques", "title": "has_subtopic", "to": "Zero-shot Learning"}, {"arrows": "to", "from": "Regularization", "title": "subtopic", "to": "Regularizer Term"}, {"arrows": "to", "from": "Convergence Criteria", "title": "related_to", "to": "KKT Conditions"}, {"arrows": "to", "from": "Generalization Gap", "title": "related_to", "to": "Training vs Test Distributions"}, {"arrows": "to", "from": "Support Vector Machines", "title": "subtopic", "to": "Regularization and Non-separable Case (Optional)"}, {"arrows": "to", "from": "Naive Bayes Algorithm", "title": "subtopic", "to": "Discretization"}, {"arrows": "to", "from": "ELBO Optimization", "title": "subtopic", "to": "Efficient Evaluation of ELBO"}, {"arrows": "to", "from": "Generalization Error Guarantees", "title": "depends_on", "to": "Hoeffding Inequality"}, {"arrows": "to", "from": "Machine Learning Models", "title": "contains", "to": "Conditional Probability Modeling"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "subtopic", "to": "2-D Convolution (Conv2D-S)"}, {"arrows": "to", "from": "Regularization in Machine Learning", "title": "related_to", "to": "Kernel Methods"}, {"arrows": "to", "from": "Adaptation Phase", "title": "subtopic", "to": "Transfer Learning"}, {"arrows": "to", "from": "Optimal Parameters Calculation", "title": "depends_on", "to": "Intercept Term Calculation"}, {"arrows": "to", "from": "Naive Bayes Algorithm", "title": "related_to", "to": "Multinomial Features"}, {"arrows": "to", "from": "Mixture of Gaussians", "title": "subtopic", "to": "EM Algorithm Steps"}, {"arrows": "to", "from": "Step 2: Derive Optimal Policy", "title": "subtopic", "to": "LQR Algorithm Steps"}, {"arrows": "to", "from": "Classification Problem", "title": "related_to", "to": "Linear Regression Approach"}, {"arrows": "to", "from": "Log-Likelihood Optimization", "title": "related_to", "to": "Multiple Examples Case"}, {"arrows": "to", "from": "Expectation Approximation", "title": "related_to", "to": "Deterministic Simulators"}, {"arrows": "to", "from": "Variational Inference", "title": "subtopic", "to": "Variational Auto-Encoder (VAE)"}, {"arrows": "to", "from": "EM Algorithm", "title": "related_to", "to": "Soft Assignments"}, {"arrows": "to", "from": "Machine Learning Models", "title": "has_subtopic", "to": "Non-linear Feature Mappings"}, {"arrows": "to", "from": "Bayesian Inference", "title": "subtopic", "to": "Posterior Approximation"}, {"arrows": "to", "from": "Generalized Linear Models", "title": "subtopic", "to": "Exponential Family"}, {"arrows": "to", "from": "Principal Component Analysis (PCA)", "title": "depends_on", "to": "Projection of Data Points"}, {"arrows": "to", "from": "Backpropagation Algorithm", "title": "related_to", "to": "Efficiency of Backward Functions"}, {"arrows": "to", "from": "Probability Distributions", "title": "subtopic", "to": "Gaussian Distribution"}, {"arrows": "to", "from": "Optimization Problems", "title": "depends_on", "to": "Support Vectors"}, {"arrows": "to", "from": "LMS Update Rule", "title": "same_as", "to": "Widrow-Hoff Learning Rule"}, {"arrows": "to", "from": "Markov Decision Processes (MDP)", "title": "subtopic", "to": "Rewards Dependency on States and Actions"}, {"arrows": "to", "from": "ICA Ambiguities", "title": "has_subtopic", "to": "Scaling Ambiguity"}, {"arrows": "to", "from": "Probability Distributions", "title": "depends_on", "to": "Log Likelihood"}, {"arrows": "to", "from": "Negative Log-Likelihood", "title": "related_to", "to": "Conditional Probabilistic Models"}, {"arrows": "to", "from": "Machine Learning Models", "title": "contains", "to": "Neural Networks"}, {"arrows": "to", "from": "Mean Squared Error (MSE)", "title": "subtopic", "to": "Unavoidable Noise (\u03c3^2)"}, {"arrows": "to", "from": "Conditional Distribution Modeling", "title": "subtopic", "to": "Bernoulli Distribution"}, {"arrows": "to", "from": "Naive Bayes", "title": "subtopic", "to": "Laplace Smoothing"}, {"arrows": "to", "from": "MLP (Multi-Layer Perceptron)", "title": "depends_on", "to": "Nonlinear Activation Module"}, {"arrows": "to", "from": "SIMCLR Algorithm", "title": "depends_on", "to": "Augmentation Techniques"}, {"arrows": "to", "from": "Multi-class Classification", "title": "depends_on", "to": "Multinomial Distribution"}, {"arrows": "to", "from": "Double Descent Phenomenon", "title": "subtopic", "to": "Regularization Strength"}, {"arrows": "to", "from": "Generalization", "title": "subtopic_of", "to": "Bias-variance tradeoff"}, {"arrows": "to", "from": "Dual Problem", "title": "related_to", "to": "Dual Constraints"}, {"arrows": "to", "from": "Finite Horizon MDPs", "title": "subtopic", "to": "Dynamic Environment Models"}, {"arrows": "to", "from": "Generalized Linear Model (GLM)", "title": "related_to", "to": "Exponential Family Distributions"}, {"arrows": "to", "from": "Policy Iteration", "title": "subtopic", "to": "Bellman Updates"}, {"arrows": "to", "from": "Finite-horizon MDPs", "title": "related_to", "to": "Optimal Value Function Recovery"}, {"arrows": "to", "from": "Support Vector Machines", "title": "subtopic", "to": "Margins: Intuition"}, {"arrows": "to", "from": "Linear Model Limitations", "title": "depends_on", "to": "Bias Definition"}, {"arrows": "to", "from": "Neural Networks", "title": "depends_on", "to": "Classification Problem"}, {"arrows": "to", "from": "Batch Gradient Descent", "title": "related_to", "to": "Feature Map Phi"}, {"arrows": "to", "from": "Generalization Error", "title": "depends_on", "to": "Uniform Convergence"}, {"arrows": "to", "from": "Posterior Approximation", "title": "subtopic", "to": "MAP Estimation"}, {"arrows": "to", "from": "Discretization", "title": "subtopic", "to": "Continuous State MDPs"}, {"arrows": "to", "from": "ICA Ambiguities", "title": "has_subtopic", "to": "Non-Gaussian Sources"}, {"arrows": "to", "from": "Cross Validation", "title": "subtopic", "to": "k-fold Cross Validation"}, {"arrows": "to", "from": "Layer Normalization", "title": "depends_on", "to": "LN-S(z)"}, {"arrows": "to", "from": "Training Error", "title": "related_to", "to": "Empirical Risk Minimization (ERM)"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "subtopic", "to": "Feature Selection"}, {"arrows": "to", "from": "LMS with Features", "title": "depends_on", "to": "Gradient Descent Update Rule"}, {"arrows": "to", "from": "Deep Learning Representations", "title": "subtopic", "to": "Complex Features in Neural Networks"}, {"arrows": "to", "from": "Matrix Multiplication Backward Function", "title": "has_subtopic", "to": "Efficiency Considerations"}, {"arrows": "to", "from": "Data Augmentation", "title": "subtopic", "to": "Positive Pair"}, {"arrows": "to", "from": "Regression Problems", "title": "related_to", "to": "Test Example"}, {"arrows": "to", "from": "Value Function in RL", "title": "subtopic", "to": "Optimal Value Function"}, {"arrows": "to", "from": "1-D Convolution Layer", "title": "subtopic", "to": "Matrix Multiplication with Shared Parameters"}, {"arrows": "to", "from": "Training and Test Datasets", "title": "subtopic", "to": "Quadratic Function Example"}, {"arrows": "to", "from": "Loss Functions", "title": "subtopic", "to": "Training Loss"}, {"arrows": "to", "from": "Machine Learning Techniques", "title": "related_to", "to": "Adaptation Methods"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "related_to", "to": "Naive Bayes Classifier"}, {"arrows": "to", "from": "Mean Squared Error (MSE)", "title": "depends_on", "to": "Claim 8.1.1"}, {"arrows": "to", "from": "Gaussian Data Limitation", "title": "subtopic", "to": "Rotationally Symmetric Distributions"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "depends_on", "to": "Feature Maps and Kernels"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "subtopic", "to": "Jensen\u0027s Inequality"}, {"arrows": "to", "from": "Neural Networks", "title": "depends_on", "to": "Activation Functions"}, {"arrows": "to", "from": "SMO Algorithm (Optional)", "title": "subtopic", "to": "Coordinate Ascent"}, {"arrows": "to", "from": "Posterior Distribution", "title": "related_to", "to": "Predictive Distribution"}, {"arrows": "to", "from": "LQG Framework", "title": "depends_on", "to": "Machine Learning Concepts"}, {"arrows": "to", "from": "Modern Neural Networks", "title": "has_subtopic", "to": "Vectorization over training examples"}, {"arrows": "to", "from": "Multinomial Event Model", "title": "subtopic", "to": "Probability Calculation"}, {"arrows": "to", "from": "Exponential Family Distributions", "title": "subtopic", "to": "Canonical Link Function"}, {"arrows": "to", "from": "Bayesian Classification", "title": "subtopic", "to": "Posterior Distribution"}, {"arrows": "to", "from": "Probability Estimation", "title": "subtopic", "to": "Multinomial Random Variable"}, {"arrows": "to", "from": "Fitted Value Iteration", "title": "depends_on", "to": "Action Evaluation"}, {"arrows": "to", "from": "Model Complexity", "title": "related_to", "to": "Bias-Variance Tradeoff"}, {"arrows": "to", "from": "Fully-Connected Neural Networks", "title": "uses", "to": "ReLU Activation Function"}, {"arrows": "to", "from": "Machine Learning Algorithms", "title": "subtopic", "to": "Coordinate Ascent Algorithm"}, {"arrows": "to", "from": "Piecewise Constant Representation", "title": "subtopic", "to": "Discretization in MDPs"}, {"arrows": "to", "from": "Convolutional Neural Networks (CNN)", "title": "subtopic", "to": "1-D Convolution Layer"}, {"arrows": "to", "from": "Supervised Learning", "title": "subtopic", "to": "Deep Learning"}, {"arrows": "to", "from": "From non-linear dynamics to LQR", "title": "has_subtopic", "to": "Differential Dynamic Programming (DDP)"}, {"arrows": "to", "from": "Training Loss", "title": "depends_on", "to": "Mean Squared Error (MSE)"}, {"arrows": "to", "from": "Sequential Minimal Optimization (SMO)", "title": "subtopic", "to": "Efficient Update Mechanism"}, {"arrows": "to", "from": "Linear Regression", "title": "subtopic", "to": "Cost Function"}, {"arrows": "to", "from": "Linear Regression", "title": "subtopic", "to": "Locally Weighted Linear Regression (LWLR)"}, {"arrows": "to", "from": "Hessian Matrix", "title": "related_to", "to": "Optimization in RL"}, {"arrows": "to", "from": "Kalman Filter", "title": "subtopic", "to": "Update Step"}, {"arrows": "to", "from": "Optimization Problem", "title": "subtopic", "to": "Non-Convex Constraint"}, {"arrows": "to", "from": "Classification and Logistic Regression", "title": "subtopic", "to": "Maximizing l(theta)"}, {"arrows": "to", "from": "Markov Decision Processes (MDP)", "title": "subtopic", "to": "Policy Iteration"}, {"arrows": "to", "from": "Unit Vector w/||w||", "title": "subtopic", "to": "Vector w"}, {"arrows": "to", "from": "Backward Function for Loss Functions", "title": "subtopic", "to": "Cross-Entropy Loss"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "subtopic", "to": "Evidence Lower Bound (ELBO)"}, {"arrows": "to", "from": "Bayesian Machine Learning", "title": "subtopic", "to": "Fully Bayesian Prediction"}, {"arrows": "to", "from": "Policy Iteration", "title": "depends_on", "to": "VE Procedure"}, {"arrows": "to", "from": "Neural Networks", "title": "depends_on", "to": "Backward Functions"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "subtopic", "to": "Locally Weighted Linear Regression (LWLR)"}, {"arrows": "to", "from": "Efficient Evaluation of ELBO", "title": "depends_on", "to": "Gaussian Distributions"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "contains", "to": "ELBO Interpretation"}, {"arrows": "to", "from": "Logistic Regression Derivation", "title": "related_to", "to": "Perceptron Algorithm"}, {"arrows": "to", "from": "Generative Learning Algorithms", "title": "subtopic", "to": "Multinomial Event Model"}, {"arrows": "to", "from": "Generalized Linear Models (GLMs)", "title": "depends_on", "to": "Exponential Family Distributions"}, {"arrows": "to", "from": "Unlabeled Dataset", "title": "related_to", "to": "Pretraining Phase"}, {"arrows": "to", "from": "Deep Learning Representations", "title": "subtopic", "to": "House Price Prediction Example"}, {"arrows": "to", "from": "Supervised Learning", "title": "subtopic", "to": "Linear Regression"}, {"arrows": "to", "from": "Physics Simulation", "title": "related_to", "to": "Open Dynamics Engine"}, {"arrows": "to", "from": "M-step Update Rule", "title": "related_to", "to": "Lagrangian Method"}, {"arrows": "to", "from": "Contrastive Learning", "title": "related_to", "to": "Negative Pair"}, {"arrows": "to", "from": "Policy Gradient Methods", "title": "related_to", "to": "Law of Total Expectation"}, {"arrows": "to", "from": "Feature Maps and Kernels", "title": "subtopic", "to": "Kernel Function Definition"}, {"arrows": "to", "from": "Gaussian Data Example", "title": "has_subtopic", "to": "Mixed Data x\u0027"}, {"arrows": "to", "from": "Multi-layer Fully-Connected Neural Networks", "title": "subtopic", "to": "Total Number of Neurons"}, {"arrows": "to", "from": "Efficient Update Mechanism", "title": "depends_on", "to": "Constraints Handling"}, {"arrows": "to", "from": "Reinforcement learning", "title": "has_subtopic", "to": "Continuous state MDPs"}, {"arrows": "to", "from": "Training Set", "title": "related_to", "to": "Stop Words"}, {"arrows": "to", "from": "Pilot Survey Example", "title": "related_to", "to": "Data Redundancy Detection"}, {"arrows": "to", "from": "Kalman Filter", "title": "subtopic", "to": "Step 2"}, {"arrows": "to", "from": "EM algorithms", "title": "subtopic_of", "to": "Mixture of Gaussians revisited"}, {"arrows": "to", "from": "Batch Gradient Descent", "title": "subtopic", "to": "Beta Update Equation"}, {"arrows": "to", "from": "Double Descent Phenomenon", "title": "related_to", "to": "Overparameterized Models"}, {"arrows": "to", "from": "Sigmoid Function", "title": "related_to", "to": "Tanh Function"}, {"arrows": "to", "from": "Natural Parameter for Bernoulli", "title": "subtopic", "to": "Bernoulli Distribution"}, {"arrows": "to", "from": "Linear Regression", "title": "related_to", "to": "Underfitting"}, {"arrows": "to", "from": "Classification", "title": "subtopic", "to": "Binary Classification"}, {"arrows": "to", "from": "Multivariate Normal Distributions for Classes", "title": "subtopic", "to": "Gaussian Discriminant Analysis (GDA)"}, {"arrows": "to", "from": "Machine Learning Models", "title": "contains", "to": "Transformer Model"}, {"arrows": "to", "from": "Spam Filter", "title": "depends_on", "to": "Training Set"}, {"arrows": "to", "from": "EM Algorithm", "title": "subtopic", "to": "M-step"}, {"arrows": "to", "from": "Supervised Learning", "title": "subtopic", "to": "Classification and Logistic Regression"}, {"arrows": "to", "from": "Cross-Entropy Loss", "title": "related_to", "to": "Probabilistic Model"}, {"arrows": "to", "from": "Regularization and model selection", "title": "has_subtopic", "to": "Bayesian statistics and regularization"}, {"arrows": "to", "from": "Maximum Likelihood Estimation (MLE)", "title": "depends_on", "to": "Probabilistic Assumptions"}, {"arrows": "to", "from": "Generative Learning Algorithms", "title": "subtopic", "to": "Gaussian Discriminant Analysis"}, {"arrows": "to", "from": "Gradient Descent", "title": "subtopic", "to": "Batch Gradient Descent"}, {"arrows": "to", "from": "Negative Log-Likelihood", "title": "depends_on", "to": "Probabilistic Model"}, {"arrows": "to", "from": "Cross Validation", "title": "subtopic", "to": "Hold-out Cross Validation"}, {"arrows": "to", "from": "Expectation-Maximization (EM) Algorithm", "title": "related_to", "to": "Reparameterization Trick"}, {"arrows": "to", "from": "Gaussian Discriminant Analysis (GDA)", "title": "related_to", "to": "Decision Boundary"}, {"arrows": "to", "from": "Support Vector Machines (SVM)", "title": "subtopic", "to": "SMO Algorithm"}, {"arrows": "to", "from": "Expectation-Maximization Algorithm", "title": "has_subtopic", "to": "E-step Calculation"}, {"arrows": "to", "from": "Update Step", "title": "has_subtopic", "to": "Kalman Gain"}, {"arrows": "to", "from": "Gaussian Discriminant Analysis (GDA)", "title": "depends_on", "to": "Model Parameters"}, {"arrows": "to", "from": "Learning a model for an MDP", "title": "has_subtopic", "to": "Connections between Policy and Value Iteration (Optional)"}, {"arrows": "to", "from": "Kernel Trick", "title": "subtopic", "to": "Linear Combination Representation"}, {"arrows": "to", "from": "Logistic Regression", "title": "depends_on", "to": "Logistic Function"}, {"arrows": "to", "from": "Kernel Functions", "title": "subtopic", "to": "Sufficient Conditions for Valid Kernels"}, {"arrows": "to", "from": "Likelihood Function", "title": "related_to", "to": "Log-Likelihood"}, {"arrows": "to", "from": "Logistic Regression", "title": "related_to", "to": "Margins"}, {"arrows": "to", "from": "Value Iteration", "title": "depends_on", "to": "Continuous State MDPs"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "contains", "to": "Value Iteration"}, {"arrows": "to", "from": "Single Neuron Network", "title": "uses", "to": "ReLU Function"}, {"arrows": "to", "from": "Naive Bayes Classifier", "title": "related_to", "to": "Bernoulli Event Model"}, {"arrows": "to", "from": "Gaussian Discriminant Analysis", "title": "subtopic", "to": "Multivariate Normal Distribution"}, {"arrows": "to", "from": "Fully-Connected Neural Networks", "title": "depends_on", "to": "Intermediate Variables (a_i)"}, {"arrows": "to", "from": "Vector w", "title": "related_to", "to": "Decision Boundary"}, {"arrows": "to", "from": "Principal Component Analysis (PCA)", "title": "subtopic", "to": "Data Visualization"}, {"arrows": "to", "from": "Generalized Linear Model (GLM)", "title": "subtopic", "to": "Prediction Goal"}, {"arrows": "to", "from": "Classification", "title": "subtopic", "to": "Multi-class Classification"}, {"arrows": "to", "from": "Hypothesis Space", "title": "subtopic", "to": "Parameterization of Hypotheses"}, {"arrows": "to", "from": "Chapter 15 Overview", "title": "related_to", "to": "Policy Iteration Speedup"}, {"arrows": "to", "from": "Machine Learning Models", "title": "has_subtopic", "to": "Regression Problems"}, {"arrows": "to", "from": "Reinforcement learning", "title": "subtopic_of", "to": "Learning a model for an MDP"}, {"arrows": "to", "from": "Bayesian Classification", "title": "depends_on", "to": "Class Priors"}, {"arrows": "to", "from": "Parameter Updates", "title": "has_subtopic", "to": "\u03b8 Update Rule"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "subtopic", "to": "Output Layer"}, {"arrows": "to", "from": "Feature Maps and Kernels", "title": "related_to", "to": "Inner Product Computation"}, {"arrows": "to", "from": "Synchronous Updates", "title": "subtopic", "to": "Value Iteration Algorithm"}, {"arrows": "to", "from": "Naive Bayes Spam Filter", "title": "depends_on", "to": "Parameter Estimation"}, {"arrows": "to", "from": "Dynamic Programming Application", "title": "subtopic", "to": "Machine Learning Concepts"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "subtopic", "to": "Maximum Likelihood Estimation (MLE)"}, {"arrows": "to", "from": "Machine Learning Algorithms", "title": "contains", "to": "Mini-batch SGD"}, {"arrows": "to", "from": "Kernel Functions", "title": "related_to", "to": "Computational Efficiency"}, {"arrows": "to", "from": "Multinomial Random Variable", "title": "depends_on", "to": "Maximum Likelihood Estimates"}, {"arrows": "to", "from": "Machine Learning Models", "title": "contains", "to": "Fully-Connected Neural Networks"}, {"arrows": "to", "from": "Batch Normalization Variants", "title": "related_to", "to": "Layer Normalization (LN)"}, {"arrows": "to", "from": "Machine Learning", "title": "related_to", "to": "Contrastive Learning"}, {"arrows": "to", "from": "Support Vector Machines", "title": "subtopic", "to": "Optimal Margin Classifier (Optional)"}, {"arrows": "to", "from": "Cross Validation", "title": "has_subtopic", "to": "Leave-One-Out CV"}, {"arrows": "to", "from": "Learning Model for MDPs", "title": "subtopic", "to": "State Transition Probabilities Estimation"}, {"arrows": "to", "from": "Machine Learning Models", "title": "has_subtopic", "to": "Classification Model"}, {"arrows": "to", "from": "Perceptron Algorithm", "title": "depends_on", "to": "Multi-class Classification"}, {"arrows": "to", "from": "Generalized Linear Models (GLMs)", "title": "subtopic", "to": "Ordinary Least Squares"}, {"arrows": "to", "from": "VE Procedure", "title": "depends_on", "to": "k Parameter"}, {"arrows": "to", "from": "Stochastic Gradient Descent (SGD)", "title": "subtopic", "to": "Gradient Calculation"}, {"arrows": "to", "from": "Convergence Proof", "title": "depends_on", "to": "Jensen\u0027s Inequality"}, {"arrows": "to", "from": "Exponential Family Distributions", "title": "subtopic", "to": "Log Partition Function"}, {"arrows": "to", "from": "Regularization", "title": "depends_on", "to": "Training Loss/Cost Function"}, {"arrows": "to", "from": "Mini-batch SGD", "title": "subtopic", "to": "Mini-batch Gradient Calculation"}, {"arrows": "to", "from": "Neural Networks", "title": "subtopic", "to": "Single Neuron Network"}, {"arrows": "to", "from": "Backpropagation", "title": "subtopic", "to": "Preliminaries on Partial Derivatives"}, {"arrows": "to", "from": "Sample Complexity Bound", "title": "subtopic", "to": "Corollary Proof"}, {"arrows": "to", "from": "Regression Problems", "title": "subtopic", "to": "Bias-Variance Tradeoff"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "contains", "to": "ELBO Optimization"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "related_to", "to": "Linear Quadratic Regulator (LQR)"}, {"arrows": "to", "from": "Objective Function Dual", "title": "subtopic", "to": "Value Dual Problem"}, {"arrows": "to", "from": "Backpropagation", "title": "subtopic_of", "to": "Back-propagation for MLPs"}, {"arrows": "to", "from": "Bayesian Statistics", "title": "has_subtopic", "to": "Prior Distribution"}, {"arrows": "to", "from": "Linear Regression", "title": "subtopic", "to": "Locally Weighted Linear Regression"}, {"arrows": "to", "from": "Machine Learning Algorithms", "title": "subtopic", "to": "Discriminative Learning Algorithms"}, {"arrows": "to", "from": "Coordinate Ascent Algorithm", "title": "related_to", "to": "Sequential Minimal Optimization (SMO) Algorithm"}, {"arrows": "to", "from": "Machine Learning Basics", "title": "depends_on", "to": "Probability Distributions"}, {"arrows": "to", "from": "Sample Complexity", "title": "depends_on", "to": "Generalization Error"}, {"arrows": "to", "from": "Confidence Measure Limitation", "title": "related_to", "to": "Normalization Condition"}, {"arrows": "to", "from": "Gaussian Discriminant Analysis (GDA)", "title": "subtopic", "to": "Machine Learning Models"}, {"arrows": "to", "from": "Naive Bayes", "title": "subtopic", "to": "Event Models for Text Classification"}, {"arrows": "to", "from": "Matrix Multiplication Backward Function", "title": "has_subtopic", "to": "Vectorized Notation"}, {"arrows": "to", "from": "Optimization Problems", "title": "related_to", "to": "KKT Conditions"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "has_subtopic", "to": "k-means Algorithm"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "subtopic", "to": "EM Algorithm"}, {"arrows": "to", "from": "Newton\u0027s Method", "title": "related_to", "to": "Maximizing Functions"}, {"arrows": "to", "from": "M-step Maximization", "title": "has_subtopic", "to": "Parameter Updates"}, {"arrows": "to", "from": "Beta Update Equation", "title": "depends_on", "to": "Inner Product Computation"}, {"arrows": "to", "from": "Kernel Methods", "title": "subtopic", "to": "Properties of Kernels"}, {"arrows": "to", "from": "1-D Convolution Layer", "title": "depends_on", "to": "Filter Vector"}, {"arrows": "to", "from": "Machine Learning Techniques", "title": "contains", "to": "Model Selection"}, {"arrows": "to", "from": "Exponential Family Distributions", "title": "subtopic", "to": "Sufficient Statistic"}, {"arrows": "to", "from": "Bias-Variance Tradeoff", "title": "depends_on", "to": "Expected Test Error"}, {"arrows": "to", "from": "Curse of Dimensionality", "title": "subtopic", "to": "Discretization in MDPs"}, {"arrows": "to", "from": "Implicit Regularization Effect", "title": "subtopic", "to": "Regularization in Deep Learning"}, {"arrows": "to", "from": "Self-Supervised Learning", "title": "related_to", "to": "Supervised Contrastive Algorithms"}, {"arrows": "to", "from": "Logistic Regression", "title": "subtopic", "to": "Conditional Distribution"}, {"arrows": "to", "from": "Neural Networks", "title": "related_to", "to": "Multi-layer Networks"}, {"arrows": "to", "from": "Clustering", "title": "subtopic", "to": "K-Means Algorithm"}, {"arrows": "to", "from": "Inner Product Computation", "title": "subtopic", "to": "Pre-computation Strategy"}, {"arrows": "to", "from": "Loss Function", "title": "depends_on", "to": "Negative Log-Likelihood"}, {"arrows": "to", "from": "MLP Model", "title": "related_to", "to": "Modules and Parameters"}, {"arrows": "to", "from": "Machine Learning Models", "title": "related_to", "to": "Matricization Approach"}, {"arrows": "to", "from": "Partial Derivatives in ML", "title": "related_to", "to": "Multi-Variate Function Challenges"}, {"arrows": "to", "from": "Spam Filter", "title": "depends_on", "to": "Feature Vector"}, {"arrows": "to", "from": "ICA Ambiguities", "title": "has_subtopic", "to": "Sign Changes Irrelevance"}, {"arrows": "to", "from": "Markov Decision Process (MDP)", "title": "subtopic", "to": "Learning in MDPs with Unknown Transitions"}, {"arrows": "to", "from": "Optimization Problems", "title": "subtopic", "to": "Duality Gap"}, {"arrows": "to", "from": "Unsupervised Learning", "title": "subtopic", "to": "Clustering"}, {"arrows": "to", "from": "ICA Ambiguities", "title": "has_subtopic", "to": "Sign Change Ambiguity"}, {"arrows": "to", "from": "Lagrangian Function", "title": "related_to", "to": "Dual Problem"}, {"arrows": "to", "from": "Independent Component Analysis (ICA)", "title": "depends_on", "to": "Mixing Matrix"}, {"arrows": "to", "from": "LQR, DDP and LQG", "title": "has_subtopic", "to": "Linear Quadratic Gaussian (LQG)"}, {"arrows": "to", "from": "Optimizers", "title": "subtopic", "to": "Stochastic Gradient Descent (SGD)"}, {"arrows": "to", "from": "Regularized Loss", "title": "related_to", "to": "\u03bb (Lambda)"}, {"arrows": "to", "from": "Policy Gradient Methods", "title": "has_subtopic", "to": "Trajectory Collection"}, {"arrows": "to", "from": "Regularization and model selection", "title": "has_subtopic", "to": "Regularization"}, {"arrows": "to", "from": "EM_Algorithm", "title": "subtopic", "to": "Single_Example_Optimization"}, {"arrows": "to", "from": "Machine Learning Models", "title": "has_subtopic", "to": "Non-linear Model h_\u03b8(x)"}, {"arrows": "to", "from": "Backpropagation", "title": "depends_on", "to": "Intermediate Values Storage"}, {"arrows": "to", "from": "EM_Algorithm", "title": "has_subtopic", "to": "E_Step"}, {"arrows": "to", "from": "Optimization Constraints", "title": "subtopic", "to": "Box Constraint"}, {"arrows": "to", "from": "Kalman Filter", "title": "depends_on", "to": "Belief States Update"}, {"arrows": "to", "from": "Bias-Variance Tradeoff", "title": "depends_on", "to": "Machine Learning Basics"}, {"arrows": "to", "from": "Optimization Problem", "title": "subtopic", "to": "Convex Quadratic Objective"}, {"arrows": "to", "from": "Weight Matrices and Biases", "title": "related_to", "to": "Total Number of Neurons"}, {"arrows": "to", "from": "Generalized Linear Models", "title": "subtopic", "to": "Constructing GLMs"}, {"arrows": "to", "from": "Model Selection", "title": "related_to", "to": "k-fold Cross Validation"}, {"arrows": "to", "from": "Machine Learning Models", "title": "has_subtopic", "to": "Training Examples"}, {"arrows": "to", "from": "Linear Model Prediction", "title": "depends_on", "to": "Learning Algorithm"}, {"arrows": "to", "from": "Contrastive Learning Framework", "title": "subtopic", "to": "Machine Learning Literature"}, {"arrows": "to", "from": "Markov Decision Processes (MDP)", "title": "subtopic", "to": "Expectation Calculation"}, {"arrows": "to", "from": "Naive Bayes Classifier", "title": "subtopic", "to": "Conditional Independence Assumption"}, {"arrows": "to", "from": "State Transition Probabilities", "title": "subtopic", "to": "Estimating State Transitions"}, {"arrows": "to", "from": "Concave Functions", "title": "related_to", "to": "Jensen\u0027s Inequality"}, {"arrows": "to", "from": "Functional Margin", "title": "subtopic", "to": "Function Margin with Training Set"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "subtopic", "to": "LMS with Features"}, {"arrows": "to", "from": "Variational Inference Review", "title": "subtopic", "to": "Machine Learning Literature"}, {"arrows": "to", "from": "Unsupervised learning", "title": "has_subtopic", "to": "Clustering and the k-means algorithm"}, {"arrows": "to", "from": "Model Selection", "title": "subtopic", "to": "Hold Out Cross Validation"}, {"arrows": "to", "from": "Data Scarcity", "title": "related_to", "to": "k-fold Cross Validation"}, {"arrows": "to", "from": "Functional Margin", "title": "related_to", "to": "Geometric Margin"}, {"arrows": "to", "from": "Dynamics of Model", "title": "subtopic", "to": "Optimal Value Function"}, {"arrows": "to", "from": "Machine Learning Overview", "title": "contains", "to": "Supervised Learning"}, {"arrows": "to", "from": "Pretraining Methods in CV", "title": "subtopic", "to": "Supervised Pretraining"}, {"arrows": "to", "from": "Gaussian Data Example", "title": "has_subtopic", "to": "Rotation Matrix R"}, {"arrows": "to", "from": "Supervised Learning", "title": "subtopic", "to": "Hypothesis"}, {"arrows": "to", "from": "ELBO Interpretation", "title": "subtopic", "to": "Alternative ELBO Formulations"}, {"arrows": "to", "from": "Vectorization in Neural Networks", "title": "related_to", "to": "BLAS Packages"}, {"arrows": "to", "from": "Training Example Independence Assumption", "title": "depends_on", "to": "Machine Learning Concepts"}, {"arrows": "to", "from": "Linearization of Dynamics", "title": "subtopic", "to": "Rewriting Dynamics"}, {"arrows": "to", "from": "Text Classification", "title": "subtopic", "to": "Spam Filter"}, {"arrows": "to", "from": "Kernel Methods", "title": "subtopic", "to": "Feature Maps"}, {"arrows": "to", "from": "Maximum Likelihood Estimation", "title": "depends_on", "to": "Laplace Smoothing"}, {"arrows": "to", "from": "Model Selection", "title": "depends_on", "to": "Cross Validation"}, {"arrows": "to", "from": "Machine Learning Algorithms", "title": "subtopic", "to": "Newton\u0027s Method"}, {"arrows": "to", "from": "Parallelism in Training Examples", "title": "subtopic", "to": "Basic Idea"}, {"arrows": "to", "from": "Reinforcement Learning Overview", "title": "subtopic", "to": "Total Payoff Calculation"}, {"arrows": "to", "from": "Multi-layer Fully-Connected Neural Networks", "title": "subtopic", "to": "Weight Matrices and Biases"}, {"arrows": "to", "from": "Prediction Equation", "title": "depends_on", "to": "Intercept Term Calculation"}, {"arrows": "to", "from": "Machine Learning Overview", "title": "contains", "to": "Fitted Value Iteration"}, {"arrows": "to", "from": "Optimal Parameters Calculation", "title": "subtopic", "to": "Prediction Equation"}, {"arrows": "to", "from": "Logistic Regression", "title": "depends_on", "to": "Negative Log-Likelihood"}, {"arrows": "to", "from": "Classification and Logistic Regression", "title": "subtopic", "to": "Perceptron Learning Algorithm"}, {"arrows": "to", "from": "Latent Variable Models", "title": "subtopic", "to": "EM Algorithm"}, {"arrows": "to", "from": "Machine Learning Algorithms", "title": "related_to", "to": "Neural Networks Overview"}, {"arrows": "to", "from": "Linear Regression", "title": "related_to", "to": "Gradient Descent"}, {"arrows": "to", "from": "Value Iteration", "title": "subtopic", "to": "Expectation Approximation"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "subtopic", "to": "Convolutional Neural Networks (CNNs)"}, {"arrows": "to", "from": "Generalized Linear Models (GLMs)", "title": "subtopic", "to": "Logistic Regression"}, {"arrows": "to", "from": "Backward Functions", "title": "subtopic", "to": "Loss Functions"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "subtopic", "to": "Hidden Units"}, {"arrows": "to", "from": "LQR, DDP and LQG", "title": "has_subtopic", "to": "Linear Quadratic Regulation (LQR)"}, {"arrows": "to", "from": "Bayesian Statistics", "title": "related_to", "to": "MLE"}, {"arrows": "to", "from": "Optimization Constraints", "title": "related_to", "to": "Quadratic Function"}, {"arrows": "to", "from": "Baseline Estimation", "title": "leads_to", "to": "Gradient Estimator Update"}, {"arrows": "to", "from": "Neural Networks Parameters", "title": "subtopic", "to": "Deep Learning"}, {"arrows": "to", "from": "Contour Plots", "title": "related_to", "to": "Multivariate Normal Distribution"}, {"arrows": "to", "from": "Optimal Policy", "title": "subtopic", "to": "Linear Optimal Action"}, {"arrows": "to", "from": "Optimization Problem", "title": "subtopic", "to": "Linear Constraints"}, {"arrows": "to", "from": "Gradient Descent", "title": "subtopic", "to": "Stochastic Gradient Descent"}, {"arrows": "to", "from": "Bias-Variance Tradeoff", "title": "subtopic", "to": "Sample Complexity Bounds"}, {"arrows": "to", "from": "Gradient Descent Methods", "title": "contains", "to": "Batch Gradient Descent"}, {"arrows": "to", "from": "Optimizers and Generalization", "title": "contains", "to": "Implicit Regularization"}, {"arrows": "to", "from": "Machine Learning Adaptation Techniques", "title": "has_subtopic", "to": "In-context Learning"}, {"arrows": "to", "from": "Conditional Probability", "title": "subtopic", "to": "Softmax Function"}, {"arrows": "to", "from": "Policy Definition", "title": "depends_on", "to": "Value Function"}, {"arrows": "to", "from": "Feature Maps and Kernels", "title": "subtopic", "to": "Algorithm Implementation"}, {"arrows": "to", "from": "Likelihood Function", "title": "subtopic", "to": "Probability Distribution"}, {"arrows": "to", "from": "Markov Decision Processes (MDPs)", "title": "contains", "to": "Learning Model for MDPs"}, {"arrows": "to", "from": "Inner Product Computation", "title": "related_to", "to": "Efficient Inner Product Calculation"}, {"arrows": "to", "from": "Continuous State Space", "title": "related_to", "to": "Markov Decision Process (MDP)"}, {"arrows": "to", "from": "Log Likelihood Function", "title": "subtopic", "to": "Gradient Ascent Rule"}, {"arrows": "to", "from": "Gaussian Discriminant Analysis", "title": "subtopic", "to": "GDA Model"}, {"arrows": "to", "from": "Learning Model for MDPs", "title": "example_of", "to": "Inverted Pendulum Problem"}, {"arrows": "to", "from": "Optimizers", "title": "subtopic", "to": "Gradient Descent (GD)"}, {"arrows": "to", "from": "Machine Learning Models", "title": "depends_on", "to": "Conditional Distribution Modeling"}, {"arrows": "to", "from": "Supervised Learning Algorithm", "title": "subtopic", "to": "Linear Regression"}, {"arrows": "to", "from": "LQR Algorithm Steps", "title": "subtopic", "to": "Machine Learning Concepts"}, {"arrows": "to", "from": "Functional Margins", "title": "subtopic", "to": "Machine Learning Concepts"}, {"arrows": "to", "from": "Supervised Learning", "title": "related_to", "to": "Classification Problem"}, {"arrows": "to", "from": "Immediate Reward", "title": "related_to", "to": "Bellman Equations"}, {"arrows": "to", "from": "Sample-wise Double Descent", "title": "depends_on", "to": "Optimal Algorithms"}, {"arrows": "to", "from": "Multi-layer Fully-Connected Neural Networks", "title": "subtopic", "to": "Total Number of Parameters"}, {"arrows": "to", "from": "Normal Equations Method", "title": "contains", "to": "Matrix Derivatives"}, {"arrows": "to", "from": "Transformer Model", "title": "subtopic", "to": "Input-Output Interface"}, {"arrows": "to", "from": "Learning Guarantees", "title": "related_to", "to": "Binary Classification"}, {"arrows": "to", "from": "Machine_Learning_Theory", "title": "subtopic", "to": "Double_Descent_Phenomenon"}, {"arrows": "to", "from": "Jensen\u0027s Inequality", "title": "depends_on", "to": "Evidence Lower Bound (ELBO)"}, {"arrows": "to", "from": "Markov Decision Processes (MDP)", "title": "includes", "to": "Discount Factor"}, {"arrows": "to", "from": "Generalization Error", "title": "depends_on", "to": "PAC Assumptions"}, {"arrows": "to", "from": "VE Procedure", "title": "related_to", "to": "Initialization Option 1"}, {"arrows": "to", "from": "Differential Dynamic Programming (DDP)", "title": "subtopic", "to": "Reward Function Approximation"}, {"arrows": "to", "from": "Stochastic Gradient Descent (SGD)", "title": "depends_on", "to": "Hyperparameters"}, {"arrows": "to", "from": "Loss Functions", "title": "subtopic", "to": "Test Error"}, {"arrows": "to", "from": "Linear Regression", "title": "related_to", "to": "Overfitting"}, {"arrows": "to", "from": "Self-Supervised Loss", "title": "subtopic", "to": "Pretraining Phase"}, {"arrows": "to", "from": "Machine Learning Adaptation Methods", "title": "subtopic", "to": "Pretraining Methods in CV"}, {"arrows": "to", "from": "Logistic Regression", "title": "subtopic", "to": "Robustness of Logistic Regression"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "has_subtopic", "to": "Policy Gradient Methods"}, {"arrows": "to", "from": "Distortion Function", "title": "subtopic", "to": "k-means Algorithm"}, {"arrows": "to", "from": "Polynomial Fitting", "title": "related_to", "to": "Variance"}, {"arrows": "to", "from": "Dual Problem Formulation", "title": "depends_on", "to": "KKT Conditions"}, {"arrows": "to", "from": "Model Parameters (w, b)", "title": "depends_on", "to": "Scaling Constraint on w"}, {"arrows": "to", "from": "Backpropagation", "title": "subtopic", "to": "General Strategy of Backpropagation"}, {"arrows": "to", "from": "Machine Learning Models", "title": "has_subtopic", "to": "Deterministic vs Stochastic Models"}, {"arrows": "to", "from": "Backward Function in Machine Learning", "title": "subtopic", "to": "Chain Rule Application"}, {"arrows": "to", "from": "Generalization", "title": "has_subtopic", "to": "Sample complexity bounds (optional readings)"}, {"arrows": "to", "from": "Optimal Intercept b", "title": "related_to", "to": "Lagrangian Optimization"}, {"arrows": "to", "from": "Machine Learning Algorithms", "title": "depends_on", "to": "Batch Gradient Descent"}, {"arrows": "to", "from": "Policy Gradient Theorem", "title": "depends_on", "to": "Log Probability Derivative"}, {"arrows": "to", "from": "Machine Learning Algorithms", "title": "contains", "to": "Linear Regression"}, {"arrows": "to", "from": "Probability Distribution", "title": "depends_on", "to": "Design Matrix X"}, {"arrows": "to", "from": "Kernels as Similarity Metrics", "title": "subtopic", "to": "Gaussian Kernel"}, {"arrows": "to", "from": "Functional Margin", "title": "depends_on", "to": "Confidence Measure Limitation"}, {"arrows": "to", "from": "Contrastive Learning", "title": "subtopic", "to": "SIMCLR Algorithm"}, {"arrows": "to", "from": "Machine Learning Models", "title": "has_subtopic", "to": "GDA (Generative Discriminative Approach)"}, {"arrows": "to", "from": "Bernoulli Distribution", "title": "subtopic", "to": "Logistic Function"}, {"arrows": "to", "from": "Primal Problem", "title": "related_to", "to": "Primal Constraints"}, {"arrows": "to", "from": "Convex Functions", "title": "depends_on", "to": "Jensen\u0027s Inequality"}, {"arrows": "to", "from": "Independence Assumption", "title": "related_to", "to": "Likelihood Function"}, {"arrows": "to", "from": "Machine Learning Algorithms", "title": "includes", "to": "Locally Weighted Linear Regression"}, {"arrows": "to", "from": "Layer Normalization (LN)", "title": "subtopic", "to": "Affine Transformation in LN"}, {"arrows": "to", "from": "LQR, DDP and LQG", "title": "has_subtopic", "to": "Finite-horizon MDPs"}, {"arrows": "to", "from": "Non-separable Case", "title": "depends_on", "to": "Optimization Problem"}, {"arrows": "to", "from": "EM Algorithm", "title": "related_to", "to": "K-means Clustering"}, {"arrows": "to", "from": "Machine Learning Basics", "title": "has_subtopic", "to": "Empirical Risk Minimization (ERM)"}, {"arrows": "to", "from": "Optimization Problems", "title": "related_to", "to": "SVM Optimization Problem"}, {"arrows": "to", "from": "Deep Learning", "title": "subtopic", "to": "Machine Learning Overview"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "related_to", "to": "Value Function Approximation"}, {"arrows": "to", "from": "ELBO Optimization", "title": "subtopic", "to": "Gradient Ascent in ELBO"}, {"arrows": "to", "from": "Training Set", "title": "depends_on", "to": "Maximum Likelihood Estimation"}, {"arrows": "to", "from": "Gradient Descent", "title": "depends_on", "to": "Learning Rate (\u03b1)"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "depends_on", "to": "Value Function in RL"}, {"arrows": "to", "from": "Convolutional Neural Networks (CNNs)", "title": "subtopic", "to": "1-D Convolution Layer"}, {"arrows": "to", "from": "Layer Normalization", "title": "subtopic", "to": "Scaling-Invariant Property"}, {"arrows": "to", "from": "Projection of Data Points", "title": "subtopic", "to": "Variance Maximization"}, {"arrows": "to", "from": "Supervised Learning", "title": "related_to", "to": "Regression Problem"}, {"arrows": "to", "from": "System Dynamics", "title": "depends_on", "to": "Step 1"}, {"arrows": "to", "from": "Double Descent Phenomenon", "title": "related_to", "to": "Bias-Variance Tradeoff"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "related_to", "to": "Principal Components Analysis (PCA)"}, {"arrows": "to", "from": "Constrained Optimization", "title": "subtopic", "to": "Primal Problem"}, {"arrows": "to", "from": "Mixture of Gaussians Model", "title": "subtopic", "to": "Joint Distribution"}, {"arrows": "to", "from": "Loss Function", "title": "subtopic", "to": "Intermediate Variables"}, {"arrows": "to", "from": "VE Procedure", "title": "subtopic", "to": "Update Rule (15.12)"}, {"arrows": "to", "from": "Binary Classification Problem", "title": "depends_on", "to": "Loss Function"}, {"arrows": "to", "from": "Necessary Conditions for Valid Kernels", "title": "depends_on", "to": "Positive Semi-Definiteness"}, {"arrows": "to", "from": "Machine Learning Models", "title": "subtopic", "to": "5th Degree Polynomial Models"}, {"arrows": "to", "from": "Reinforcement Learning", "title": "depends_on", "to": "Policy Gradient Methods"}, {"arrows": "to", "from": "Support Vector Machine (SVM)", "title": "depends_on", "to": "Optimization Problem"}, {"arrows": "to", "from": "Neural Networks", "title": "related_to", "to": "Single Neuron Model"}, {"arrows": "to", "from": "Machine Learning Techniques", "title": "contains", "to": "Gaussian Mixture Models"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "has_subtopic", "to": "Partially Observable MDPs (POMDP)"}, {"arrows": "to", "from": "Deep Learning", "title": "subtopic", "to": "Modules in Modern Neural Networks"}, {"arrows": "to", "from": "Feature Mapping", "title": "depends_on", "to": "Iterative Update Process"}, {"arrows": "to", "from": "Feature Vector Selection", "title": "depends_on", "to": "Generative Models"}, {"arrows": "to", "from": "Machine Learning Models", "title": "has_subtopic", "to": "Maximum Likelihood Estimation (MLE)"}, {"arrows": "to", "from": "Generalized Linear Models (GLM)", "title": "subtopic", "to": "Bernoulli Distribution"}, {"arrows": "to", "from": "Posterior Distribution", "title": "depends_on", "to": "EM Algorithm"}, {"arrows": "to", "from": "Logistic Regression", "title": "subtopic", "to": "Negative Likelihood Loss Function"}, {"arrows": "to", "from": "Two-Layer Fully-Connected Network", "title": "depends_on", "to": "Weight Matrix W^[1]"}, {"arrows": "to", "from": "Sample Complexity", "title": "related_to", "to": "Hypothesis Class Size"}, {"arrows": "to", "from": "Markov Decision Processes (MDP)", "title": "subtopic", "to": "Infinite Horizon MDPs"}, {"arrows": "to", "from": "Neural Networks", "title": "depends_on", "to": "Parameters (\u03b8)"}, {"arrows": "to", "from": "Probability Estimation", "title": "subtopic", "to": "Maximum Likelihood Estimates"}, {"arrows": "to", "from": "Loss Functions", "title": "subtopic", "to": "Cross-Entropy Loss"}, {"arrows": "to", "from": "Feature Engineering", "title": "subtopic", "to": "Machine Learning Overview"}, {"arrows": "to", "from": "Implementation Subtlety", "title": "subtopic", "to": "Data Matrix Representation"}, {"arrows": "to", "from": "Generalization Error", "title": "related_to", "to": "Training Error"}, {"arrows": "to", "from": "Markov Decision Processes (MDP)", "title": "subtopic", "to": "Optimal Policy Determination"}, {"arrows": "to", "from": "Backward Function for Loss Functions", "title": "subtopic", "to": "Logistic Loss"}, {"arrows": "to", "from": "Vanilla REINFORCE Algorithm", "title": "depends_on", "to": "Empirical Trajectories Estimation"}, {"arrows": "to", "from": "Posterior Approximation", "title": "contains", "to": "Prior Selection"}, {"arrows": "to", "from": "Transfer Learning", "title": "subtopic", "to": "Machine Learning Overview"}, {"arrows": "to", "from": "Kernel Matrix Properties", "title": "related_to", "to": "Examples of Kernels in Practice"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "has_subtopic", "to": "Feature Extraction for Strings"}, {"arrows": "to", "from": "Softplus Function", "title": "related_to", "to": "ReLU Function"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "depends_on", "to": "Support Vector Machines (SVMs)"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "depends_on", "to": "Generalization Error Guarantees"}, {"arrows": "to", "from": "Log-Likelihood Maximization", "title": "depends_on", "to": "EM Algorithm"}, {"arrows": "to", "from": "Learning a model for an MDP", "title": "has_subtopic", "to": "Continuous state MDPs"}, {"arrows": "to", "from": "Pretraining Methods in CV", "title": "subtopic", "to": "Contrastive Learning"}, {"arrows": "to", "from": "Convolutional Layers", "title": "subtopic", "to": "Channel Concept"}, {"arrows": "to", "from": "Fitted Value Iteration", "title": "has_subtopic", "to": "Value Function Approximation"}, {"arrows": "to", "from": "Machine Learning Fundamentals", "title": "depends_on", "to": "Backpropagation Algorithm"}, {"arrows": "to", "from": "Machine Learning Conferences", "title": "related_to", "to": "NeurIPS Conference"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "related_to", "to": "Properties of Kernels"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "subtopic", "to": "Least Squares Regression"}, {"arrows": "to", "from": "Linear Quadratic Regulator (LQR)", "title": "subtopic", "to": "Optimization in RL"}, {"arrows": "to", "from": "Foundation Models Opportunities and Risks", "title": "subtopic", "to": "Machine Learning Literature"}, {"arrows": "to", "from": "Few-Shot Learning Capabilities", "title": "subtopic", "to": "Machine Learning Literature"}, {"arrows": "to", "from": "Layer Normalization (LN)", "title": "subtopic", "to": "LN-S Module"}, {"arrows": "to", "from": "Machine Learning Theory", "title": "depends_on", "to": "Optimization Problems"}, {"arrows": "to", "from": "Matrix Notation in Machine Learning", "title": "related_to", "to": "Generalization to Multiple Layers"}, {"arrows": "to", "from": "Sample Complexity", "title": "depends_on", "to": "Floating Point Representation"}, {"arrows": "to", "from": "EM Algorithm", "title": "subtopic", "to": "E-step"}, {"arrows": "to", "from": "Dual Form of Problem", "title": "related_to", "to": "Kernel Trick"}, {"arrows": "to", "from": "Generalized Linear Model (GLM)", "title": "contains", "to": "Conditional Distribution Assumption"}, {"arrows": "to", "from": "Machine Learning Techniques", "title": "contains", "to": "Principal Component Analysis (PCA)"}, {"arrows": "to", "from": "Contrastive Learning", "title": "related_to", "to": "Positive Pair"}, {"arrows": "to", "from": "PCA", "title": "subtopic", "to": "Applications"}, {"arrows": "to", "from": "Maximum Likelihood Estimation (MLE)", "title": "uses", "to": "Gradient Ascent"}, {"arrows": "to", "from": "Naive Bayes Classifier", "title": "subtopic", "to": "Laplace Smoothing"}, {"arrows": "to", "from": "Independent Component Analysis (ICA)", "title": "related_to", "to": "Cocktail Party Problem"}, {"arrows": "to", "from": "Other Normalization Layers", "title": "subtopic", "to": "Group Normalization"}, {"arrows": "to", "from": "Bias-Variance Tradeoff", "title": "subtopic", "to": "Hypothesis Class"}, {"arrows": "to", "from": "EM_Algorithm", "title": "depends_on", "to": "Likelihood_Estimation"}, {"arrows": "to", "from": "Deep Learning", "title": "subtopic", "to": "Neural Networks"}, {"arrows": "to", "from": "Support Vector Machines (SVMs)", "title": "defines", "to": "Dual Optimization Problem"}, {"arrows": "to", "from": "Support Vector Machines", "title": "subtopic", "to": "Dual Formulation (Optional)"}, {"arrows": "to", "from": "Fitted Value Iteration", "title": "depends_on", "to": "Discrete Action Space"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "subtopic", "to": "Conv1D-S Module"}, {"arrows": "to", "from": "Hypotheses Space (H)", "title": "related_to", "to": "Uniform Convergence"}, {"arrows": "to", "from": "Finetuning Pretrained Models", "title": "related_to", "to": "Optimization Goal"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "related_to", "to": "Underfitting"}, {"arrows": "to", "from": "Self-supervised Learning", "title": "subtopic", "to": "Foundation Models"}, {"arrows": "to", "from": "Modern Neural Networks", "title": "has_subtopic", "to": "Backpropagation"}, {"arrows": "to", "from": "Neural Networks", "title": "subtopic", "to": "Stacking Neurons"}, {"arrows": "to", "from": "Maximum Likelihood Estimation", "title": "subtopic", "to": "Joint Distribution of Sources"}, {"arrows": "to", "from": "Gaussian Distribution", "title": "subtopic", "to": "Density Visualization"}, {"arrows": "to", "from": "Optimization Methods", "title": "subtopic", "to": "Newton\u0027s Method"}, {"arrows": "to", "from": "Coordinate Descent on J", "title": "subtopic", "to": "k-means Algorithm"}, {"arrows": "to", "from": "LQR Algorithm", "title": "depends_on", "to": "Step 3"}, {"arrows": "to", "from": "Gaussian Discriminant Analysis (GDA)", "title": "subtopic", "to": "Multivariate Normal Distribution"}, {"arrows": "to", "from": "Finetuning Pretrained Models", "title": "depends_on", "to": "Prediction Model Structure"}, {"arrows": "to", "from": "ICA Overview", "title": "depends_on", "to": "Maximum Likelihood Estimation"}, {"arrows": "to", "from": "Machine Learning Basics", "title": "has_subtopic", "to": "Function Representation"}, {"arrows": "to", "from": "Support Vector Machines (SVMs)", "title": "subtopic", "to": "Functional Margin"}, {"arrows": "to", "from": "Policy Gradient (REINFORCE)", "title": "depends_on", "to": "Randomized Policy"}, {"arrows": "to", "from": "Modules in Modern Neural Networks", "title": "subtopic", "to": "Matrix Multiplication as a Building Block"}, {"arrows": "to", "from": "Machine Learning", "title": "has_subtopic", "to": "Reinforcement Learning"}, {"arrows": "to", "from": "Logits", "title": "depends_on", "to": "Softmax Function"}, {"arrows": "to", "from": "Machine Learning Algorithms", "title": "contains", "to": "Markov Decision Processes (MDPs)"}, {"arrows": "to", "from": "Markov Decision Process (MDP)", "title": "depends_on", "to": "State Transition"}, {"arrows": "to", "from": "Chapter 16 Introduction", "title": "depends_on", "to": "Finite-horizon MDPs"}, {"arrows": "to", "from": "Loss Function Analysis", "title": "subtopic", "to": "Machine Learning Concepts"}, {"arrows": "to", "from": "Gradient Descent Methods", "title": "contains", "to": "Stochastic Gradient Descent"}, {"arrows": "to", "from": "Newton\u0027s Method", "title": "subtopic", "to": "Algorithm Application"}, {"arrows": "to", "from": "Machine Learning Models", "title": "contains", "to": "Generalized Linear Models (GLM)"}, {"arrows": "to", "from": "Independent components analysis", "title": "subtopic_of", "to": "ICA ambiguities"}, {"arrows": "to", "from": "Markov Decision Processes (MDP)", "title": "subtopic", "to": "Value Iteration"}, {"arrows": "to", "from": "Value Function", "title": "related_to", "to": "Bellman\u0027s Equation"}, {"arrows": "to", "from": "Support Vector Machines (SVM)", "title": "related_to", "to": "Lagrange Duality"}, {"arrows": "to", "from": "Cocktail Party Problem", "title": "related_to", "to": "Mixing Matrix (A)"}, {"arrows": "to", "from": "Probabilistic Interpretation", "title": "related_to", "to": "Regression Problem"}, {"arrows": "to", "from": "Machine Learning Models", "title": "subtopic", "to": "Decision Boundaries"}, {"arrows": "to", "from": "Differential Dynamic Programming (DDP)", "title": "subtopic", "to": "Nominal Trajectory Generation"}, {"arrows": "to", "from": "Backpropagation", "title": "depends_on", "to": "Chain Rule"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "has_subtopic", "to": "EM Algorithms"}, {"arrows": "to", "from": "Normalization Process", "title": "subtopic", "to": "Normalization Formula"}, {"arrows": "to", "from": "Markov Decision Processes (MDP)", "title": "includes", "to": "States"}, {"arrows": "to", "from": "Machine Learning Models", "title": "subtopic", "to": "Model Assumptions"}, {"arrows": "to", "from": "Logistic Regression", "title": "depends_on", "to": "Logit"}, {"arrows": "to", "from": "Linear Quadratic Regulator (LQR)", "title": "subtopic", "to": "Optimal Policy"}, {"arrows": "to", "from": "Support Vector Machines", "title": "subtopic", "to": "Lagrange Duality (Optional)"}, {"arrows": "to", "from": "Probability Vector", "title": "related_to", "to": "Softmax Function"}, {"arrows": "to", "from": "Kernel Functions", "title": "related_to", "to": "Characterization of Valid Kernels"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "related_to", "to": "Feature Map"}, {"arrows": "to", "from": "Jensen\u0027s Inequality", "title": "subtopic", "to": "Theorem Statement"}, {"arrows": "to", "from": "Machine Learning Basics", "title": "subtopic", "to": "Regularization"}, {"arrows": "to", "from": "Markov Decision Processes (MDP)", "title": "subtopic", "to": "Finite Horizon MDPs"}, {"arrows": "to", "from": "Empirical Distribution", "title": "related_to", "to": "Training Loss"}, {"arrows": "to", "from": "Matricization Approach", "title": "depends_on", "to": "Implementation Subtlety"}, {"arrows": "to", "from": "Continuous state MDPs", "title": "has_subtopic", "to": "Value function approximation"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "subtopic", "to": "Bias-Variance Tradeoff"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "contains", "to": "Kernel Functions"}, {"arrows": "to", "from": "Double Descent Phenomenon", "title": "related_to", "to": "Regularization Tuning"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "depends_on", "to": "Hypothesis Space"}, {"arrows": "to", "from": "Softmax Function", "title": "subtopic", "to": "Probabilistic Model"}, {"arrows": "to", "from": "Layer Normalization (LN)", "title": "depends_on", "to": "Scale-Invariant Property"}, {"arrows": "to", "from": "Law of Total Expectation", "title": "subtopic", "to": "Estimator Simplification"}, {"arrows": "to", "from": "Constrained Optimization", "title": "subtopic", "to": "Generalized Lagrangian"}, {"arrows": "to", "from": "ICA Ambiguities", "title": "has_subtopic", "to": "Scaling Factor"}, {"arrows": "to", "from": "Machine Learning Models", "title": "has_subtopic", "to": "Binary Classification"}, {"arrows": "to", "from": "Support Vector Machines (SVM)", "title": "subtopic", "to": "Margins"}, {"arrows": "to", "from": "Machine Learning Adaptation Methods", "title": "subtopic", "to": "Finetuning Pretrained Models"}, {"arrows": "to", "from": "Empirical Risk Minimization (ERM)", "title": "related_to", "to": "Non-ERM Algorithms"}, {"arrows": "to", "from": "Markov Decision Processes (MDPs)", "title": "contains", "to": "Value Iteration"}, {"arrows": "to", "from": "k-means Algorithm", "title": "related_to", "to": "Convergence Properties"}, {"arrows": "to", "from": "Self-supervised learning and foundation models", "title": "has_subtopic", "to": "Pretraining and adaptation"}, {"arrows": "to", "from": "Generative Learning Algorithms", "title": "depends_on", "to": "Bayes Rule"}, {"arrows": "to", "from": "Regularization in Machine Learning", "title": "subtopic", "to": "Sparsity Regularization"}, {"arrows": "to", "from": "Gaussian Distribution", "title": "related_to", "to": "Standard Normal Distribution"}, {"arrows": "to", "from": "Linear Quadratic Regulation (LQR)", "title": "contains", "to": "Quadratic Rewards"}, {"arrows": "to", "from": "Pretrained large language models", "title": "subtopic_of", "to": "Open up the blackbox of Transformers"}, {"arrows": "to", "from": "Support Vector Machines (SVM)", "title": "depends_on", "to": "Optimization Problem"}, {"arrows": "to", "from": "Classification and Logistic Regression", "title": "subtopic", "to": "Multi-class Classification"}, {"arrows": "to", "from": "Supervised Learning Problem", "title": "subtopic", "to": "Discretization in MDPs"}, {"arrows": "to", "from": "Regularization", "title": "subtopic", "to": "Model Complexity"}, {"arrows": "to", "from": "Discretization", "title": "depends_on", "to": "Curse of Dimensionality"}, {"arrows": "to", "from": "Policy Gradients", "title": "related_to", "to": "Trajectory Probability"}, {"arrows": "to", "from": "Alternative ELBO Formulations", "title": "depends_on", "to": "KL Divergence in ELBO"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "subtopic", "to": "Differential Dynamic Programming (DDP)"}, {"arrows": "to", "from": "Derived Features", "title": "subtopic", "to": "Walkability"}, {"arrows": "to", "from": "Total Payoff Calculation", "title": "related_to", "to": "Discount Factor (\u03b3)"}, {"arrows": "to", "from": "Chapter 15 Overview", "title": "depends_on", "to": "Optimal Bellman Equation"}, {"arrows": "to", "from": "ResNet Architecture", "title": "subtopic", "to": "Convolutional Layers"}, {"arrows": "to", "from": "Sufficient Conditions for Valid Kernels", "title": "subtopic", "to": "Mercer\u0027s Theorem"}, {"arrows": "to", "from": "Optimization Problems", "title": "subtopic", "to": "Primal Problem"}, {"arrows": "to", "from": "Conditional Probability Modeling", "title": "subtopic", "to": "Parameterized Model"}, {"arrows": "to", "from": "Policy Gradients", "title": "depends_on", "to": "Reward Function"}, {"arrows": "to", "from": "Gaussian Distribution", "title": "related_to", "to": "Kalman Filter"}, {"arrows": "to", "from": "Gradient Descent Update Rule", "title": "subtopic", "to": "Feature Mapping"}, {"arrows": "to", "from": "Machine Learning Algorithms", "title": "related_to", "to": "Markov Decision Processes (MDP)"}, {"arrows": "to", "from": "SMO Algorithm", "title": "depends_on", "to": "Alpha Value Update"}, {"arrows": "to", "from": "Machine Learning Fundamentals", "title": "subtopic", "to": "Chain Rule for Auto-Differentiation"}, {"arrows": "to", "from": "Value Function", "title": "depends_on", "to": "Policy Execution"}, {"arrows": "to", "from": "Housing Price Prediction", "title": "depends_on", "to": "Derived Features"}, {"arrows": "to", "from": "Model Complexity Measures", "title": "related_to", "to": "Norm of Learned Model"}, {"arrows": "to", "from": "Cross Validation", "title": "subtopic", "to": "Regularization Parameters"}, {"arrows": "to", "from": "EM_Algorithm", "title": "has_subtopic", "to": "M_Step"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "contains", "to": "Regression Problems"}, {"arrows": "to", "from": "Machine Learning Fundamentals", "title": "has_subtopic", "to": "Backward Function in Machine Learning"}, {"arrows": "to", "from": "Distance to Decision Boundary", "title": "subtopic", "to": "Decision Boundary"}, {"arrows": "to", "from": "Multi-layer Fully-Connected Neural Networks", "title": "subtopic", "to": "Notational Consistency"}, {"arrows": "to", "from": "Backpropagation", "title": "subtopic", "to": "Backward Functions for Basic Modules"}, {"arrows": "to", "from": "Classification Model", "title": "depends_on", "to": "Probabilistic Assumptions"}, {"arrows": "to", "from": "Generalization Error Guarantees", "title": "related_to", "to": "Bernoulli Random Variable Z"}, {"arrows": "to", "from": "Markov Decision Process (MDP)", "title": "related_to", "to": "Expected Immediate Reward"}, {"arrows": "to", "from": "Continuous Latent Variables", "title": "subtopic", "to": "Variational Inference"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "has_subtopic", "to": "LQR Extension"}, {"arrows": "to", "from": "Generalization and regularization", "title": "has_subtopic", "to": "Generalization"}, {"arrows": "to", "from": "Cross Validation", "title": "subtopic", "to": "Leave-One-Out Cross Validation"}, {"arrows": "to", "from": "Optimal Margin Classifier", "title": "subtopic", "to": "Maximizing Geometric Margin"}, {"arrows": "to", "from": "Reinforcement Learning Overview", "title": "subtopic", "to": "Policy Definition"}, {"arrows": "to", "from": "Least Squares Regression", "title": "related_to", "to": "Cost Function J(\u03b8)"}, {"arrows": "to", "from": "Machine Learning Models", "title": "related_to", "to": "Vectorization"}, {"arrows": "to", "from": "Regularization and model selection", "title": "has_subtopic", "to": "Model selection via cross validation"}, {"arrows": "to", "from": "ResNet Architecture", "title": "subtopic", "to": "Batch Normalization Variants"}, {"arrows": "to", "from": "Gradient Computation", "title": "subtopic", "to": "Reparameterization Trick"}, {"arrows": "to", "from": "Design Matrix", "title": "subtopic", "to": "Least Squares Revisited"}, {"arrows": "to", "from": "Sufficient Conditions for Valid Kernels", "title": "subtopic", "to": "Testing Kernel Validity"}, {"arrows": "to", "from": "Gradient Descent", "title": "related_to", "to": "Cost Function J(\u03b8)"}, {"arrows": "to", "from": "Logistic Regression", "title": "subtopic", "to": "Logistic Loss Function"}, {"arrows": "to", "from": "Normal Equations", "title": "subtopic", "to": "Matrix Derivatives"}, {"arrows": "to", "from": "Machine Learning Optimization Techniques", "title": "contains", "to": "Coordinate Ascent"}, {"arrows": "to", "from": "Data Normalization", "title": "has_subtopic", "to": "Mean Removal"}, {"arrows": "to", "from": "Lagrangian Function", "title": "related_to", "to": "Primal Problem"}, {"arrows": "to", "from": "Naive Bayes Algorithm", "title": "depends_on", "to": "Conditional Probability"}, {"arrows": "to", "from": "Linear Regression", "title": "subtopic", "to": "Least-Squares Cost Function"}, {"arrows": "to", "from": "Policy Iteration", "title": "depends_on", "to": "Bellman\u0027s Equations"}, {"arrows": "to", "from": "Principal Components Analysis (PCA)", "title": "depends_on", "to": "Data Subspace Identification"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "contains", "to": "Stochastic Gradient Ascent"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "subtopic", "to": "Locally Weighted Linear Regression (LWR)"}, {"arrows": "to", "from": "Gradient Descent", "title": "related_to", "to": "Convex Function"}, {"arrows": "to", "from": "Machine Learning Architectures", "title": "related_to", "to": "ResNet Architecture"}, {"arrows": "to", "from": "Expectation Estimation", "title": "subtopic", "to": "Sample-Based Estimation"}, {"arrows": "to", "from": "Linear Quadratic Regulator (LQR)", "title": "depends_on", "to": "Parameter Estimation"}, {"arrows": "to", "from": "Double Descent Phenomenon", "title": "subtopic", "to": "Sample-wise Double Descent"}, {"arrows": "to", "from": "Differential Dynamic Programming (DDP)", "title": "subtopic", "to": "Linearization of Dynamics"}, {"arrows": "to", "from": "Adaptation Methods", "title": "subtopic", "to": "Zero-shot Learning"}, {"arrows": "to", "from": "Logistic Loss Function", "title": "related_to", "to": "Logit"}, {"arrows": "to", "from": "Machine Learning Overview", "title": "contains", "to": "Bayesian Inference"}, {"arrows": "to", "from": "Machine Learning Techniques", "title": "contains", "to": "Differential Dynamic Programming (DDP)"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "related_to", "to": "Loss Function"}, {"arrows": "to", "from": "Test Error Decomposition", "title": "subtopic", "to": "Bias-Variance Tradeoff"}, {"arrows": "to", "from": "Decision Boundary", "title": "related_to", "to": "Geometric Margins"}, {"arrows": "to", "from": "Policy Iteration", "title": "related_to", "to": "Greedy Policy with Respect to V"}, {"arrows": "to", "from": "Quadratic Assumption", "title": "depends_on", "to": "Dynamics of Model"}, {"arrows": "to", "from": "Learning Guarantees", "title": "subtopic", "to": "Hoeffding Inequality (Chernoff Bound)"}, {"arrows": "to", "from": "Value Function Approximation", "title": "subtopic", "to": "Model or Simulator"}, {"arrows": "to", "from": "Major Axis of Variation", "title": "has_subtopic", "to": "Projection and Variance Maximization"}, {"arrows": "to", "from": "Reward Function", "title": "related_to", "to": "Finite-State MDPs"}, {"arrows": "to", "from": "Machine Learning Models", "title": "contains", "to": "Generative Learning Algorithms"}, {"arrows": "to", "from": "Machine Learning Algorithms", "title": "related_to", "to": "Logistic Regression"}, {"arrows": "to", "from": "Value Iteration", "title": "depends_on", "to": "Convergence of Value Functions"}, {"arrows": "to", "from": "Derived Features", "title": "subtopic", "to": "Family Size"}, {"arrows": "to", "from": "Gaussian Distribution", "title": "subtopic", "to": "Mean"}, {"arrows": "to", "from": "Logistic Regression", "title": "subtopic", "to": "Stochastic Gradient Ascent Rule"}, {"arrows": "to", "from": "Alpha Variables", "title": "depends_on", "to": "Derivation Example"}, {"arrows": "to", "from": "GELU Function", "title": "variant_of", "to": "ReLU Function"}, {"arrows": "to", "from": "Optimization Problems", "title": "subtopic", "to": "Feasibility Conditions"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "subtopic", "to": "Linear Function Over Features"}, {"arrows": "to", "from": "Model Selection via Cross Validation", "title": "contains", "to": "Cross Validation Techniques"}, {"arrows": "to", "from": "EM algorithms", "title": "has_subtopic", "to": "Variational inference and variational auto-encoder (optional reading)"}, {"arrows": "to", "from": "Parameterized Model", "title": "depends_on", "to": "Embeddings and Representations"}, {"arrows": "to", "from": "Newton\u0027s Method", "title": "subtopic", "to": "Multidimensional Generalization"}, {"arrows": "to", "from": "Lagrangian Function", "title": "depends_on", "to": "Optimization Constraints"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "subtopic", "to": "Expectation-Maximization (EM) Algorithm"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "subtopic", "to": "Empirical Risk Minimization"}, {"arrows": "to", "from": "Machine Learning Architectures", "title": "related_to", "to": "Transformer Architecture"}, {"arrows": "to", "from": "Finite-State MDPs", "title": "subtopic", "to": "Value Iteration and Policy Iteration"}, {"arrows": "to", "from": "Underfitting", "title": "depends_on", "to": "Bias-Variance Tradeoff"}, {"arrows": "to", "from": "M-step", "title": "related_to", "to": "ELBO (Evidence Lower Bound)"}, {"arrows": "to", "from": "Generalization", "title": "subtopic_of", "to": "The double descent phenomenon"}, {"arrows": "to", "from": "ReLU Function", "title": "subtopic", "to": "Activation Functions"}, {"arrows": "to", "from": "Machine Learning Loss Functions", "title": "subtopic", "to": "Cross-Entropy Loss Function"}, {"arrows": "to", "from": "Optimal Hypothesis (h*)", "title": "depends_on", "to": "Generalization Error"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "subtopic", "to": "Layer Normalization (LN)"}, {"arrows": "to", "from": "Variance Maximization", "title": "related_to", "to": "k-Dimensional Subspace"}, {"arrows": "to", "from": "Support Vector Machines (SVMs)", "title": "subtopic", "to": "Notation for SVMs"}, {"arrows": "to", "from": "Self-Supervised Learning", "title": "depends_on", "to": "Representation Function"}, {"arrows": "to", "from": "ICA Ambiguities", "title": "has_subtopic", "to": "Gaussian Data Example"}, {"arrows": "to", "from": "Machine Learning Overview", "title": "related_to", "to": "Deep Learning Introduction"}, {"arrows": "to", "from": "Density on x=As=W^-1s", "title": "depends_on", "to": "Cumulative Distribution Function (CDF)"}, {"arrows": "to", "from": "Cost Function", "title": "related_to", "to": "Ordinary Least Squares"}, {"arrows": "to", "from": "Pretraining Phase", "title": "subtopic", "to": "Transfer Learning"}, {"arrows": "to", "from": "Encoder-Decoder Framework", "title": "related_to", "to": "Optimizing Continuous Latent Variables"}, {"arrows": "to", "from": "Machine Learning Models", "title": "subtopic", "to": "Linear Model Limitations"}, {"arrows": "to", "from": "Efficient Evaluation of ELBO", "title": "depends_on", "to": "Optimizing Continuous Latent Variables"}, {"arrows": "to", "from": "Learned Features", "title": "subtopic", "to": "Deep Learning"}, {"arrows": "to", "from": "Generative Learning Algorithms", "title": "subtopic", "to": "Class Priors"}, {"arrows": "to", "from": "Machine Learning Basics", "title": "depends_on", "to": "Linear Regression"}, {"arrows": "to", "from": "Modern Neural Networks", "title": "has_subtopic", "to": "Modules in Modern Neural Networks"}, {"arrows": "to", "from": "Generative Models", "title": "subtopic", "to": "Naive Bayes Classifier"}, {"arrows": "to", "from": "EM Algorithm", "title": "subtopic", "to": "Evidence Lower Bound (ELBO)"}, {"arrows": "to", "from": "Backpropagation", "title": "subtopic", "to": "Loss Function Composition"}, {"arrows": "to", "from": "Support Vectors", "title": "subtopic", "to": "Support Vectors Definition"}, {"arrows": "to", "from": "Model-wise Double Descent", "title": "subtopic", "to": "Implicit Regularization"}, {"arrows": "to", "from": "Optimal Value Function", "title": "subtopic", "to": "Value Iteration Algorithm"}, {"arrows": "to", "from": "In-context Learning", "title": "has_subtopic", "to": "Prompt Construction"}, {"arrows": "to", "from": "Optimization Problem", "title": "related_to", "to": "Lagrangian Function"}, {"arrows": "to", "from": "Hold-out Cross Validation", "title": "depends_on", "to": "Model Selection Based on Validation Error"}, {"arrows": "to", "from": "Vectorization", "title": "depends_on", "to": "Broadcasting"}, {"arrows": "to", "from": "Gaussian Distribution", "title": "subtopic", "to": "Covariance"}, {"arrows": "to", "from": "Kernels in Machine Learning", "title": "depends_on", "to": "Feature Maps"}, {"arrows": "to", "from": "Generative Learning Algorithms", "title": "subtopic", "to": "Naive Bayes Classifier"}, {"arrows": "to", "from": "Support Vector Machines (SVM)", "title": "related_to", "to": "Dual Form Insight"}, {"arrows": "to", "from": "Model Selection", "title": "related_to", "to": "Bias-Variance Tradeoff"}, {"arrows": "to", "from": "Empirical Risk Minimization", "title": "depends_on", "to": "Hypotheses Training"}, {"arrows": "to", "from": "Backpropagation", "title": "subtopic_of", "to": "General strategy of backpropagation"}, {"arrows": "to", "from": "Exponential Family Distributions", "title": "subtopic", "to": "Poisson Distribution"}, {"arrows": "to", "from": "Supervised Learning", "title": "subtopic", "to": "Generative Learning Algorithms"}, {"arrows": "to", "from": "Training Set", "title": "subtopic", "to": "Machine Learning Concepts"}, {"arrows": "to", "from": "Two-Layer Neural Network", "title": "depends_on", "to": "Weight Matrices"}, {"arrows": "to", "from": "Chapter 15 Overview", "title": "subtopic", "to": "Value Iteration Preference"}, {"arrows": "to", "from": "Policy Gradient (REINFORCE)", "title": "related_to", "to": "Expected Total Payoff"}, {"arrows": "to", "from": "Partial Observability", "title": "subtopic", "to": "LQG Framework"}, {"arrows": "to", "from": "Machine Learning Loss Functions", "title": "subtopic", "to": "Logistic Loss Function"}, {"arrows": "to", "from": "Machine Learning Models", "title": "related_to", "to": "Non-linear Functions"}, {"arrows": "to", "from": "Gradient Descent Update Rule", "title": "related_to", "to": "Computational Complexity"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "contains", "to": "Bellman Update"}, {"arrows": "to", "from": "Data Redundancy Detection", "title": "depends_on", "to": "PCA Algorithm Introduction"}, {"arrows": "to", "from": "Expected Test Error", "title": "subtopic", "to": "Mean Squared Error (MSE)"}, {"arrows": "to", "from": "Generalized Linear Models (GLM)", "title": "related_to", "to": "Exponential Family Distributions"}, {"arrows": "to", "from": "Support Vector Machines", "title": "subtopic", "to": "Notation (Optional)"}, {"arrows": "to", "from": "Partially Observable MDPs (POMDP)", "title": "has_subtopic", "to": "Observation Layer"}, {"arrows": "to", "from": "Density Transformation", "title": "subtopic", "to": "General Case"}, {"arrows": "to", "from": "Reward Function", "title": "subtopic", "to": "Reinforcement Learning"}, {"arrows": "to", "from": "Time-Dependent Policies", "title": "depends_on", "to": "Non-Stationary Optimal Policy"}, {"arrows": "to", "from": "Value Iteration Algorithm", "title": "subtopic", "to": "Value Iteration and Policy Iteration"}, {"arrows": "to", "from": "Linear Regression", "title": "depends_on", "to": "Feature Selection"}, {"arrows": "to", "from": "Layer Normalization", "title": "related_to", "to": "Affine Transformation"}, {"arrows": "to", "from": "E-step", "title": "related_to", "to": "ELBO (Evidence Lower Bound)"}, {"arrows": "to", "from": "Exponential Family Distributions", "title": "subtopic", "to": "Canonical Response Function"}, {"arrows": "to", "from": "Weight Calculation", "title": "contains", "to": "Bandwidth Parameter (\u03c4)"}, {"arrows": "to", "from": "Data Preprocessing Assumptions", "title": "depends_on", "to": "Logistic Function Properties"}, {"arrows": "to", "from": "Optimization Problem", "title": "related_to", "to": "Functional Margin"}, {"arrows": "to", "from": "Support Vector Machines", "title": "subtopic", "to": "SMO Algorithm (Optional)"}, {"arrows": "to", "from": "Generalized Linear Models (GLM)", "title": "contains", "to": "Logistic Regression"}, {"arrows": "to", "from": "Sufficient Statistic for Bernoulli", "title": "subtopic", "to": "Bernoulli Distribution"}, {"arrows": "to", "from": "Test Error", "title": "related_to", "to": "Population Distribution"}, {"arrows": "to", "from": "Support Vector Machines", "title": "subtopic", "to": "Functional and Geometric Margins"}, {"arrows": "to", "from": "Leaky ReLU", "title": "variant_of", "to": "ReLU Function"}, {"arrows": "to", "from": "Foundation Models", "title": "subtopic", "to": "Machine Learning Overview"}, {"arrows": "to", "from": "Generalized Linear Model (GLM)", "title": "subtopic", "to": "Poisson Distribution"}, {"arrows": "to", "from": "Classification Problem", "title": "subtopic", "to": "Binary Classification"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "related_to", "to": "Other Normalization Layers"}, {"arrows": "to", "from": "Pretrained Model", "title": "depends_on", "to": "Transfer Learning"}, {"arrows": "to", "from": "Reinforcement learning", "title": "subtopic_of", "to": "Markov decision processes"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "contains", "to": "Kernels in Machine Learning"}, {"arrows": "to", "from": "Function Representation", "title": "subtopic", "to": "LMS Algorithm"}, {"arrows": "to", "from": "Machine Learning Basics", "title": "depends_on", "to": "Classification Problem"}, {"arrows": "to", "from": "Bernoulli Distribution", "title": "subtopic", "to": "Gaussian Discriminant Analysis (GDA)"}, {"arrows": "to", "from": "Machine Learning Models", "title": "related_to", "to": "Naive Bayes (Discrete Features)"}, {"arrows": "to", "from": "Optimization Problems", "title": "subtopic", "to": "Karush-Kuhn-Tucker (KKT) Conditions"}, {"arrows": "to", "from": "Variance Maximization", "title": "related_to", "to": "Empirical Covariance Matrix"}, {"arrows": "to", "from": "Backward Function in Machine Learning", "title": "related_to", "to": "Jacobian Matrix"}, {"arrows": "to", "from": "Policy", "title": "subtopic", "to": "Optimal Policy"}, {"arrows": "to", "from": "Variance Maximization", "title": "subtopic", "to": "Principal Eigenvector"}, {"arrows": "to", "from": "Machine_Learning_Theory", "title": "related_to", "to": "Statistical_Mechanics_of_Learning"}, {"arrows": "to", "from": "Explicit Regularization Techniques", "title": "subtopic", "to": "Regularization in Deep Learning"}, {"arrows": "to", "from": "Log-Likelihood Function", "title": "subtopic", "to": "Maximum Likelihood Estimation (MLE)"}, {"arrows": "to", "from": "Generalized Linear Models (GLMs)", "title": "depends_on", "to": "Assumptions/Design Choices"}, {"arrows": "to", "from": "Machine Learning Algorithms", "title": "contains", "to": "Sequential Minimal Optimization (SMO)"}, {"arrows": "to", "from": "Cross-Entropy Loss", "title": "depends_on", "to": "Softmax Function"}, {"arrows": "to", "from": "Uniform Convergence", "title": "depends_on", "to": "Union Bound Application"}, {"arrows": "to", "from": "Bellman Equations", "title": "subtopic", "to": "Value Function"}, {"arrows": "to", "from": "Machine Learning Algorithms", "title": "subtopic", "to": "Cross Validation"}, {"arrows": "to", "from": "Vapnik\u0027s Theorem", "title": "depends_on", "to": "Uniform Convergence"}, {"arrows": "to", "from": "VC Dimension", "title": "related_to", "to": "Vapnik\u0027s Theorem"}, {"arrows": "to", "from": "Primal Problem", "title": "depends_on", "to": "Objective Function Primal"}, {"arrows": "to", "from": "Optimizers and Generalization", "title": "contains", "to": "Global Minima Variability"}, {"arrows": "to", "from": "Policy Gradients", "title": "depends_on", "to": "Expectation Estimation"}, {"arrows": "to", "from": "Linear Function Over Features", "title": "subtopic", "to": "Cubic Function Representation"}, {"arrows": "to", "from": "Cocktail Party Problem", "title": "depends_on", "to": "Unmixing Matrix (W)"}, {"arrows": "to", "from": "Uniform Convergence", "title": "related_to", "to": "Hypothesis Class"}, {"arrows": "to", "from": "Likelihood Function", "title": "related_to", "to": "Maximum Likelihood Estimation"}, {"arrows": "to", "from": "Linear Quadratic Regulator (LQR)", "title": "depends_on", "to": "Discrete Ricatti Equations"}, {"arrows": "to", "from": "Asynchronous Updates", "title": "subtopic", "to": "Value Iteration Algorithm"}, {"arrows": "to", "from": "Support Vector Machines (SVM)", "title": "subtopic", "to": "Kernels"}, {"arrows": "to", "from": "Gradient Ascent", "title": "related_to", "to": "Reparametrization Technique"}, {"arrows": "to", "from": "LQR, DDP and LQG", "title": "has_subtopic", "to": "From non-linear dynamics to LQR"}, {"arrows": "to", "from": "Regularization in Machine Learning", "title": "subtopic", "to": "L2 Norm Regularization"}, {"arrows": "to", "from": "Recovering w from Alpha", "title": "related_to", "to": "Lagrangian Optimization"}, {"arrows": "to", "from": "Hold-out Cross Validation", "title": "subtopic", "to": "Validation Set S_cv"}, {"arrows": "to", "from": "Dual Form of Problem", "title": "depends_on", "to": "Lagrangian Function"}, {"arrows": "to", "from": "Generative Learning Algorithms", "title": "subtopic", "to": "Naive Bayes"}, {"arrows": "to", "from": "Kernel Methods", "title": "subtopic", "to": "LMS with Kernel Trick"}, {"arrows": "to", "from": "Vectorization in Neural Networks", "title": "depends_on", "to": "Matrix Algebra"}, {"arrows": "to", "from": "ResNet (Residual Network)", "title": "subtopic", "to": "Residual Block"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "contains", "to": "Independent Component Analysis (ICA)"}, {"arrows": "to", "from": "Policy Gradient Theorem", "title": "related_to", "to": "Vanilla REINFORCE Algorithm"}, {"arrows": "to", "from": "EM algorithms", "title": "has_subtopic", "to": "General EM algorithms"}, {"arrows": "to", "from": "Backward Function for Loss Functions", "title": "subtopic", "to": "Efficiency Considerations"}, {"arrows": "to", "from": "EM algorithms", "title": "subtopic_of", "to": "Jensen\u0027s inequality"}, {"arrows": "to", "from": "Fitted Value Iteration", "title": "depends_on", "to": "Supervised Learning Algorithm"}, {"arrows": "to", "from": "Two-Layer Neural Network", "title": "subtopic", "to": "Hidden Layer"}, {"arrows": "to", "from": "Machine Learning Algorithms", "title": "depends_on", "to": "Gradient Descent Update Rule"}, {"arrows": "to", "from": "Laplace Smoothing", "title": "depends_on", "to": "Spam Classification Example"}, {"arrows": "to", "from": "Markov Decision Processes (MDP)", "title": "subtopic", "to": "Comparison Between Algorithms"}, {"arrows": "to", "from": "Gaussian Data Example", "title": "has_subtopic", "to": "Mixing Matrix A"}, {"arrows": "to", "from": "Constructing GLMs", "title": "subtopic", "to": "Ordinary Least Squares"}, {"arrows": "to", "from": "Backpropagation", "title": "subtopic", "to": "Backward Pass"}, {"arrows": "to", "from": "Linearization of Dynamics", "title": "related_to", "to": "Inverted Pendulum Example"}, {"arrows": "to", "from": "Training Error vs Generalization Error", "title": "subtopic", "to": "Sample Size Determination"}, {"arrows": "to", "from": "Regularization", "title": "related_to", "to": "Inductive Bias"}, {"arrows": "to", "from": "Continuous state MDPs", "title": "has_subtopic", "to": "Discretization"}, {"arrows": "to", "from": "Jensen\u0027s Inequality", "title": "subtopic", "to": "Optimizing Q Distribution"}, {"arrows": "to", "from": "Machine Learning Fundamentals", "title": "depends_on", "to": "Backward Function for Loss Functions"}, {"arrows": "to", "from": "Expectation-Maximization Algorithm", "title": "depends_on", "to": "Convergence Guarantees"}, {"arrows": "to", "from": "Optimization Problem", "title": "related_to", "to": "Geometric Margin"}, {"arrows": "to", "from": "Density Transformation", "title": "related_to", "to": "ICA Algorithm"}, {"arrows": "to", "from": "Predict Step", "title": "has_subtopic", "to": "Gaussian Distribution"}, {"arrows": "to", "from": "Log-Likelihood Optimization", "title": "depends_on", "to": "Single Example Case"}, {"arrows": "to", "from": "Machine Learning Models", "title": "has_subtopic", "to": "Linear Model Prediction"}, {"arrows": "to", "from": "Data Scarcity", "title": "related_to", "to": "Leave-One-Out Cross Validation"}, {"arrows": "to", "from": "Markov Decision Processes (MDPs)", "title": "contains", "to": "Policy Iteration"}, {"arrows": "to", "from": "Probability Estimation", "title": "subtopic", "to": "Laplace Smoothing"}, {"arrows": "to", "from": "Bias-Variance Tradeoff", "title": "related_to", "to": "Linear Regression Models"}, {"arrows": "to", "from": "Machine Learning Algorithms", "title": "related_to", "to": "LQR Updates"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "depends_on", "to": "Expectation Maximization (EM)"}, {"arrows": "to", "from": "Machine Learning Algorithms", "title": "depends_on", "to": "Empirical Risk Minimization"}, {"arrows": "to", "from": "Linear Quadratic Regulation (LQR)", "title": "contains", "to": "Linear Transitions"}, {"arrows": "to", "from": "Step 1: Estimate Matrices", "title": "subtopic", "to": "LQR Algorithm Steps"}, {"arrows": "to", "from": "Dual Problem", "title": "depends_on", "to": "Objective Function Dual"}, {"arrows": "to", "from": "Mini-batch SGD", "title": "related_to", "to": "Deep Learning Model Training Steps"}, {"arrows": "to", "from": "Dual Formulation of SVM", "title": "depends_on", "to": "Lagrange Multipliers"}, {"arrows": "to", "from": "Policy Gradient Methods", "title": "has_subtopic", "to": "Baseline Estimation"}, {"arrows": "to", "from": "Reinforcement learning", "title": "has_subtopic", "to": "Connections between Policy and Value Iteration (Optional)"}, {"arrows": "to", "from": "Optimization Problems", "title": "subtopic", "to": "Dual Problem"}, {"arrows": "to", "from": "Machine Learning Theory", "title": "subtopic", "to": "Empirical Risk Minimization (ERM)"}, {"arrows": "to", "from": "Vector y", "title": "subtopic", "to": "Least Squares Revisited"}, {"arrows": "to", "from": "Naive Bayes Algorithm", "title": "depends_on", "to": "Training Set"}, {"arrows": "to", "from": "Ordinary Least Squares", "title": "related_to", "to": "Conditional Distribution"}, {"arrows": "to", "from": "Model Complexity Measures", "title": "related_to", "to": "Number of Parameters"}, {"arrows": "to", "from": "LQR Model Assumptions", "title": "subtopic", "to": "Machine Learning Concepts"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "related_to", "to": "Historical Context and Recent Discoveries"}, {"arrows": "to", "from": "Machine Learning Algorithms", "title": "related_to", "to": "Stochastic Gradient Descent"}, {"arrows": "to", "from": "Model Selection Methods", "title": "subtopic", "to": "Sample Complexity Bounds"}, {"arrows": "to", "from": "Language Problem Methods", "title": "related_to", "to": "Machine Learning Adaptation Methods"}, {"arrows": "to", "from": "General EM algorithms", "title": "subtopic_of", "to": "Other interpretation of ELBO"}, {"arrows": "to", "from": "Labeled Task Dataset", "title": "related_to", "to": "Adaptation Phase"}, {"arrows": "to", "from": "Stacking Neurons", "title": "example_of", "to": "Housing Prediction Example"}, {"arrows": "to", "from": "Transformer Model", "title": "related_to", "to": "Training Process"}, {"arrows": "to", "from": "Regularization", "title": "depends_on", "to": "Overfitting"}, {"arrows": "to", "from": "Exponential Family Distributions", "title": "subtopic", "to": "Natural Parameter"}, {"arrows": "to", "from": "VE Procedure", "title": "related_to", "to": "Initialization Option 2"}, {"arrows": "to", "from": "Probabilistic Interpretation", "title": "subtopic", "to": "Gaussian Distribution Assumption"}, {"arrows": "to", "from": "Regularization and model selection", "title": "has_subtopic", "to": "Implicit regularization effect (optional reading)"}, {"arrows": "to", "from": "Deep Learning", "title": "subtopic", "to": "Backpropagation"}, {"arrows": "to", "from": "Value Iteration and Policy Iteration", "title": "subtopic", "to": "Machine Learning Overview"}, {"arrows": "to", "from": "Weight Matrices and Biases", "title": "related_to", "to": "Total Number of Parameters"}, {"arrows": "to", "from": "Expectation-Maximization Algorithm", "title": "has_subtopic", "to": "M-step Maximization"}, {"arrows": "to", "from": "Support Vector Machines (SVM)", "title": "depends_on", "to": "Sequential Minimal Optimization (SMO) Algorithm"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "depends_on", "to": "Optimization Problems"}, {"arrows": "to", "from": "Regularizer", "title": "subtopic", "to": "\u039b_2 Regularization"}, {"arrows": "to", "from": "Machine Learning Overview", "title": "related_to", "to": "Value Function Approximation"}, {"arrows": "to", "from": "Backpropagation", "title": "subtopic_of", "to": "Backward functions for basic modules"}, {"arrows": "to", "from": "Infinite Horizon MDPs", "title": "depends_on", "to": "Discount Factor \u03b3"}, {"arrows": "to", "from": "Optimal Value Function", "title": "depends_on", "to": "Dynamic Programming"}, {"arrows": "to", "from": "Support Vector Machines (SVMs)", "title": "related_to", "to": "Geometric Margin"}, {"arrows": "to", "from": "5th Degree Polynomial Models", "title": "depends_on", "to": "Generalization Failure"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "contains", "to": "Linear Regression"}, {"arrows": "to", "from": "Optimization Methods", "title": "related_to", "to": "Fisher Scoring"}, {"arrows": "to", "from": "VC Dimension", "title": "subtopic", "to": "Shattering Sets"}, {"arrows": "to", "from": "Bias-Variance Tradeoff", "title": "depends_on", "to": "Training and Test Datasets"}, {"arrows": "to", "from": "Function Representation", "title": "subtopic", "to": "Cost Function"}, {"arrows": "to", "from": "Policy Gradients", "title": "subtopic", "to": "Gradient Ascent"}, {"arrows": "to", "from": "Unsupervised learning", "title": "has_subtopic", "to": "Principal components analysis"}, {"arrows": "to", "from": "SIMCLR Algorithm", "title": "depends_on", "to": "Loss Function"}, {"arrows": "to", "from": "Machine Learning Fundamentals", "title": "depends_on", "to": "Model Evaluation"}, {"arrows": "to", "from": "Multi-class Classification", "title": "subtopic", "to": "Negative Log-likelihood Loss Function (Multi-class)"}, {"arrows": "to", "from": "Leaky ReLU", "title": "subtopic", "to": "Activation Functions"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "related_to", "to": "Probability Distributions"}, {"arrows": "to", "from": "Optimization Constraints", "title": "subtopic", "to": "Alpha Variables"}, {"arrows": "to", "from": "LMS Update Rule", "title": "depends_on", "to": "Error Term"}, {"arrows": "to", "from": "Regularization", "title": "subtopic", "to": "Regularizer"}, {"arrows": "to", "from": "Regularization", "title": "subtopic", "to": "L1 Regularization"}, {"arrows": "to", "from": "Independent Component Analysis (ICA)", "title": "depends_on", "to": "Mixing Matrix (A)"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "subtopic", "to": "Total Parameters Conv1D"}, {"arrows": "to", "from": "Vectorization in Neural Networks", "title": "subtopic", "to": "Two-Layer Fully-Connected Network"}, {"arrows": "to", "from": "Naive Bayes Algorithm", "title": "subtopic", "to": "Parameter Estimation"}, {"arrows": "to", "from": "Machine Learning Basics", "title": "related_to", "to": "Loss Function"}, {"arrows": "to", "from": "Coordinate Ascent", "title": "illustrates", "to": "Quadratic Function Contours"}, {"arrows": "to", "from": "Likelihood_Estimation", "title": "related_to", "to": "Non_Convex_Optimization"}, {"arrows": "to", "from": "Feature Maps", "title": "subtopic", "to": "Deep Learning"}, {"arrows": "to", "from": "Learning Guarantees", "title": "subtopic", "to": "Union Bound Lemma"}, {"arrows": "to", "from": "Logistic Regression", "title": "subtopic", "to": "Probability Calculation"}, {"arrows": "to", "from": "Bias-Variance Trade-off Reconciliation", "title": "subtopic", "to": "Machine Learning Literature"}, {"arrows": "to", "from": "Value Iteration", "title": "subtopic", "to": "Geometric Convergence"}, {"arrows": "to", "from": "Linear Function Approximation", "title": "has_subtopic", "to": "Parameters (Weights)"}, {"arrows": "to", "from": "E-step", "title": "depends_on", "to": "Gaussian Mixture Model"}, {"arrows": "to", "from": "PCA", "title": "subtopic", "to": "Approximation Error Minimization"}, {"arrows": "to", "from": "Multi-class Classification", "title": "depends_on", "to": "Softmax Function"}, {"arrows": "to", "from": "Finite Horizon MDPs", "title": "related_to", "to": "Optimal Policy"}, {"arrows": "to", "from": "Least-Squares Cost Function", "title": "depends_on", "to": "Invertibility of X^TX Matrix"}, {"arrows": "to", "from": "ResNet (Residual Network)", "title": "related_to", "to": "Simplified ResNet Architecture"}, {"arrows": "to", "from": "Log Partition Function for Bernoulli", "title": "subtopic", "to": "Bernoulli Distribution"}, {"arrows": "to", "from": "Backward Function Overview", "title": "has_subtopic", "to": "Activation Functions Backward Function"}, {"arrows": "to", "from": "Fully Bayesian Prediction", "title": "depends_on", "to": "Computational Challenges"}, {"arrows": "to", "from": "Noise Reduction", "title": "contains", "to": "Eigenfaces Method"}, {"arrows": "to", "from": "GELU Function", "title": "subtopic", "to": "Activation Functions"}, {"arrows": "to", "from": "Expectation Maximization (EM)", "title": "subtopic", "to": "Evidence Lower Bound (ELBO)"}, {"arrows": "to", "from": "Regression Problems", "title": "has_subtopic", "to": "Mean-Square Cost Function"}, {"arrows": "to", "from": "Logistic Function", "title": "related_to", "to": "Derivative of Sigmoid"}, {"arrows": "to", "from": "Loss Function", "title": "subtopic", "to": "Average Loss"}, {"arrows": "to", "from": "Machine Learning Fundamentals", "title": "subtopic", "to": "Partial Derivatives in ML"}, {"arrows": "to", "from": "Machine Learning", "title": "contains", "to": "Classification"}, {"arrows": "to", "from": "Sparsity Regularization", "title": "depends_on", "to": "L1 Norm (LASSO)"}, {"arrows": "to", "from": "Training Set", "title": "depends_on", "to": "Posterior Distribution"}, {"arrows": "to", "from": "Principal Component Analysis (PCA)", "title": "subtopic", "to": "Noise Reduction"}, {"arrows": "to", "from": "Regression Problems", "title": "related_to", "to": "Training Dataset"}, {"arrows": "to", "from": "Gradient Descent (GD)", "title": "depends_on", "to": "Learning Rate"}, {"arrows": "to", "from": "Training Examples Representation", "title": "subtopic", "to": "First-Layer Activations"}, {"arrows": "to", "from": "Feature Map", "title": "depends_on", "to": "Attributes"}, {"arrows": "to", "from": "Hypothesis Class Switching", "title": "related_to", "to": "Variance Increase"}, {"arrows": "to", "from": "Density Transformation", "title": "subtopic", "to": "1D Example"}, {"arrows": "to", "from": "Empirical Risk Minimization", "title": "related_to", "to": "Training Error Selection"}, {"arrows": "to", "from": "Kalman Filter", "title": "subtopic", "to": "Step 1"}, {"arrows": "to", "from": "Chain Rule", "title": "subtopic", "to": "Gradient Computation"}, {"arrows": "to", "from": "Learning Theory", "title": "subtopic", "to": "Machine Learning Concepts"}, {"arrows": "to", "from": "Naive Bayes Classifier", "title": "subtopic", "to": "Maximum Likelihood Estimation"}, {"arrows": "to", "from": "Logistic Regression", "title": "subtopic", "to": "Total Loss Function"}, {"arrows": "to", "from": "Independent components analysis", "title": "subtopic_of", "to": "Densities and linear transformations"}, {"arrows": "to", "from": "Expectation Approximation", "title": "related_to", "to": "Gaussian Noise Model"}, {"arrows": "to", "from": "Overfitting", "title": "depends_on", "to": "Bias-Variance Tradeoff"}, {"arrows": "to", "from": "Infinite Hypothesis Classes", "title": "depends_on", "to": "Bit Representation"}, {"arrows": "to", "from": "Model Selection", "title": "depends_on", "to": "Validation Set Size"}, {"arrows": "to", "from": "5th Degree Polynomial Models", "title": "subtopic", "to": "Overfitting"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "subtopic", "to": "Functional Margin"}, {"arrows": "to", "from": "Machine Learning Models", "title": "has_subtopic", "to": "Convolutional Layers"}, {"arrows": "to", "from": "Conditional Probability in Language Models", "title": "subtopic", "to": "Temperature Parameter"}, {"arrows": "to", "from": "Markov Decision Processes (MDP)", "title": "includes", "to": "Actions"}, {"arrows": "to", "from": "Machine Learning Algorithms", "title": "contains", "to": "Parameter Initialization"}, {"arrows": "to", "from": "EM algorithms", "title": "subtopic_of", "to": "EM for mixture of Gaussians"}, {"arrows": "to", "from": "Classification and Logistic Regression", "title": "subtopic", "to": "Logistic Regression"}, {"arrows": "to", "from": "Machine Learning Algorithms", "title": "includes", "to": "Parametric Algorithms"}, {"arrows": "to", "from": "Neural Networks", "title": "related_to", "to": "Biological Inspiration"}, {"arrows": "to", "from": "Gradient Descent", "title": "subtopic", "to": "LMS Update Rule"}, {"arrows": "to", "from": "Generalization and Regularization", "title": "depends_on", "to": "Training Loss Function"}, {"arrows": "to", "from": "Sequential Decision Making", "title": "subtopic", "to": "Reinforcement Learning"}, {"arrows": "to", "from": "Log Likelihood", "title": "depends_on", "to": "Lower Bound Derivation"}, {"arrows": "to", "from": "Deep Learning Introduction", "title": "subtopic", "to": "Supervised Learning with Non-Linear Models"}, {"arrows": "to", "from": "Value Iteration", "title": "subtopic", "to": "Policy Update Rule (15.13)"}, {"arrows": "to", "from": "Spam/Non-Spam Classification", "title": "depends_on", "to": "Word Generation Process"}, {"arrows": "to", "from": "EM Algorithm", "title": "related_to", "to": "Convergence Proof"}, {"arrows": "to", "from": "Objective Function Primal", "title": "subtopic", "to": "Value Primal Problem"}, {"arrows": "to", "from": "Pretrained large language models", "title": "subtopic_of", "to": "Zero-shot learning and in-context learning"}, {"arrows": "to", "from": "Supervised Learning", "title": "subtopic", "to": "Support Vector Machines"}, {"arrows": "to", "from": "State Representation", "title": "subtopic", "to": "Discretization"}, {"arrows": "to", "from": "Training vs Test Distributions", "title": "subtopic", "to": "Machine Learning Basics"}, {"arrows": "to", "from": "Finite State MDPs", "title": "related_to", "to": "Machine Learning Algorithms"}, {"arrows": "to", "from": "High-Dimensional Statistical Analysis", "title": "subtopic", "to": "Machine Learning Literature"}, {"arrows": "to", "from": "Support Vector Machines (SVM)", "title": "subtopic", "to": "Kernel Application"}, {"arrows": "to", "from": "Training Set", "title": "depends_on", "to": "Hypothesis Function"}, {"arrows": "to", "from": "Adaptation Algorithm", "title": "subtopic", "to": "Machine Learning Adaptation Methods"}, {"arrows": "to", "from": "Maximum Likelihood Estimates", "title": "subtopic", "to": "Laplace Smoothing"}, {"arrows": "to", "from": "Machine Learning Models", "title": "subtopic", "to": "MLP (Multi-Layer Perceptron)"}, {"arrows": "to", "from": "Markov Decision Process (MDP)", "title": "depends_on", "to": "Policy Iteration"}, {"arrows": "to", "from": "Kernel Trick", "title": "depends_on", "to": "Inner Products"}, {"arrows": "to", "from": "Linear Probe Approach", "title": "subtopic", "to": "Adaptation Algorithm"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "contains", "to": "Mixture of Gaussians"}, {"arrows": "to", "from": "Partially Observable MDPs (POMDP)", "title": "has_subtopic", "to": "Belief State"}, {"arrows": "to", "from": "Hold Out Cross Validation", "title": "contains", "to": "Retraining on Full Dataset"}, {"arrows": "to", "from": "Binary Classification Problem", "title": "subtopic", "to": "MLP Model"}, {"arrows": "to", "from": "Markov Decision Processes (MDP)", "title": "includes", "to": "State Transition Probabilities"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "subtopic", "to": "Kernel Trick"}, {"arrows": "to", "from": "Kalman Filter", "title": "subtopic", "to": "Step 3"}, {"arrows": "to", "from": "Expectation-Maximization (EM) Algorithm", "title": "depends_on", "to": "M-step Update Rule"}, {"arrows": "to", "from": "Cross Validation", "title": "subtopic", "to": "Polynomial Regression Models"}, {"arrows": "to", "from": "Single Neuron Network", "title": "example_of", "to": "Housing Price Prediction"}, {"arrows": "to", "from": "Probability Estimation", "title": "related_to", "to": "Naive Bayes Classifier"}, {"arrows": "to", "from": "Gaussian Discriminant Analysis (GDA)", "title": "related_to", "to": "Decision Boundaries"}, {"arrows": "to", "from": "Succinct Representation of Distribution Qi", "title": "subtopic", "to": "Optimizing Continuous Latent Variables"}, {"arrows": "to", "from": "Feature Mapping", "title": "related_to", "to": "Kernels as Similarity Metrics"}, {"arrows": "to", "from": "Convergence", "title": "subtopic", "to": "k-means Algorithm"}, {"arrows": "to", "from": "Markov Decision Processes (MDP)", "title": "includes", "to": "Reward Function"}, {"arrows": "to", "from": "Log Probability Derivative", "title": "subtopic", "to": "Trajectory Probability Change"}, {"arrows": "to", "from": "Stochastic Gradient Descent (SGD)", "title": "related_to", "to": "Deep Learning Model Training Steps"}, {"arrows": "to", "from": "Binary Classification", "title": "depends_on", "to": "Logistic Function"}, {"arrows": "to", "from": "Machine Learning Techniques", "title": "depends_on", "to": "Cross Validation"}, {"arrows": "to", "from": "Uniform Convergence", "title": "related_to", "to": "Training Error vs Generalization Error"}, {"arrows": "to", "from": "Reward Function Estimation", "title": "depends_on", "to": "Expectation Maximization"}, {"arrows": "to", "from": "Jensen\u0027s Inequality", "title": "subtopic", "to": "Lower Bound Derivation"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "related_to", "to": "Kernel Methods"}, {"arrows": "to", "from": "Kernel Methods", "title": "subtopic", "to": "Support Vector Machines (SVM)"}, {"arrows": "to", "from": "Backpropagation Algorithm", "title": "subtopic", "to": "Chain Rule Application"}, {"arrows": "to", "from": "Gradient Descent", "title": "subtopic", "to": "Update Rule"}, {"arrows": "to", "from": "Neural Networks", "title": "depends_on", "to": "Regression Problem"}, {"arrows": "to", "from": "Independent Component Analysis (ICA)", "title": "subtopic", "to": "Unmixing Matrix (W)"}, {"arrows": "to", "from": "Gaussian Distribution Assumption", "title": "subtopic", "to": "Mean and Variance Functions"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "subtopic", "to": "ReLU Activation Function"}, {"arrows": "to", "from": "Optimization in RL", "title": "depends_on", "to": "Machine Learning Concepts"}, {"arrows": "to", "from": "Convolutional Layers", "title": "subtopic", "to": "Parameter Sharing"}, {"arrows": "to", "from": "Kernel Methods", "title": "subtopic", "to": "LMS with Features"}, {"arrows": "to", "from": "Constructing GLMs", "title": "subtopic", "to": "Logistic Regression (GLM)"}, {"arrows": "to", "from": "Bayesian Classification", "title": "depends_on", "to": "Conditional Probability"}, {"arrows": "to", "from": "Gaussian Discriminant Analysis", "title": "subtopic", "to": "Discussion: GDA and Logistic Regression"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "subtopic", "to": "Total Parameters Conv2D"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "related_to", "to": "Overfitting"}, {"arrows": "to", "from": "Language Models", "title": "depends_on", "to": "Conditional Probability in Language Models"}, {"arrows": "to", "from": "Likelihood_Estimation", "title": "depends_on", "to": "Latent_Variables"}, {"arrows": "to", "from": "Machine Learning Techniques", "title": "related_to", "to": "Independent Components Analysis (ICA)"}, {"arrows": "to", "from": "Optimization Problem", "title": "depends_on", "to": "Linear Regression"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "subtopic", "to": "Variational Inference"}, {"arrows": "to", "from": "Backward Functions", "title": "subtopic", "to": "Matrix Multiplication Module (MM)"}, {"arrows": "to", "from": "Linear Regression", "title": "subtopic", "to": "Normal Equations"}, {"arrows": "to", "from": "Unsupervised Learning", "title": "related_to", "to": "Mixture of Gaussians Model"}, {"arrows": "to", "from": "Multidimensional Generalization", "title": "depends_on", "to": "Hessian Matrix"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "subtopic", "to": "Geometric Margin"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "subtopic", "to": "Inverted Pendulum Example"}, {"arrows": "to", "from": "Locally Weighted Linear Regression (LWLR)", "title": "depends_on", "to": "Weight Calculation"}, {"arrows": "to", "from": "Self-supervised learning and foundation models", "title": "has_subtopic", "to": "Pretraining methods in computer vision"}, {"arrows": "to", "from": "Hypothesis Function", "title": "subtopic", "to": "Training Error"}, {"arrows": "to", "from": "Optimal Parameters Alpha", "title": "subtopic", "to": "Dual Problem Formulation"}, {"arrows": "to", "from": "k-means Algorithm", "title": "depends_on", "to": "Distortion Function J"}, {"arrows": "to", "from": "Gradient Descent Optimizer", "title": "depends_on", "to": "Minimum Norm Solution"}, {"arrows": "to", "from": "Machine Learning Algorithms", "title": "subtopic", "to": "Gradient Ascent"}, {"arrows": "to", "from": "Zero-shot Learning", "title": "has_subtopic", "to": "Language Model Utilization"}, {"arrows": "to", "from": "Mixture of Gaussians Model", "title": "subtopic", "to": "Model Parameters"}, {"arrows": "to", "from": "Classification Problem", "title": "subtopic", "to": "Logistic Regression"}, {"arrows": "to", "from": "1-D Convolution Layer", "title": "depends_on", "to": "Bias Scalar"}, {"arrows": "to", "from": "Data Augmentation", "title": "subtopic", "to": "Negative Pair"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "subtopic", "to": "Polynomial Fitting"}, {"arrows": "to", "from": "Logistic Regression", "title": "depends_on", "to": "Gradient Calculation"}, {"arrows": "to", "from": "Optimization Problems", "title": "related_to", "to": "Convex Functions"}, {"arrows": "to", "from": "Car Example", "title": "related_to", "to": "Data Redundancy Detection"}, {"arrows": "to", "from": "PCA Algorithm Introduction", "title": "subtopic", "to": "Normalization Process"}, {"arrows": "to", "from": "Foundation Models", "title": "subtopic", "to": "Pretraining Phase"}, {"arrows": "to", "from": "Matrix Notation in Machine Learning", "title": "subtopic", "to": "Vectorization"}, {"arrows": "to", "from": "MAP Estimation", "title": "related_to", "to": "MLE vs MAP"}, {"arrows": "to", "from": "Underfitting", "title": "subtopic", "to": "Machine Learning Basics"}, {"arrows": "to", "from": "From non-linear dynamics to LQR", "title": "has_subtopic", "to": "Linearization of dynamics"}, {"arrows": "to", "from": "Generalized Linear Model (GLM)", "title": "subtopic", "to": "Linear Relationship Assumption"}, {"arrows": "to", "from": "Discrete Latent Variables", "title": "related_to", "to": "Mean Field Assumption"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "subtopic", "to": "Neural Network Inputs"}, {"arrows": "to", "from": "Machine Learning", "title": "related_to", "to": "Convolutional Neural Networks (CNN)"}, {"arrows": "to", "from": "Two-Layer Neural Network", "title": "depends_on", "to": "Bias Vectors"}, {"arrows": "to", "from": "Chapter 9 Regularization and Model Selection", "title": "subtopic", "to": "Regularization"}, {"arrows": "to", "from": "Discriminative Learning Algorithms", "title": "related_to", "to": "Perceptron Algorithm"}, {"arrows": "to", "from": "Constrained Optimization", "title": "depends_on", "to": "Lagrange Multipliers"}, {"arrows": "to", "from": "Linear Regression", "title": "subtopic", "to": "LMS Algorithm"}, {"arrows": "to", "from": "Partial Derivatives in ML", "title": "depends_on", "to": "Scalar Functions and Vectors"}, {"arrows": "to", "from": "Machine Learning Overview", "title": "related_to", "to": "Support Vector Machines (SVM)"}, {"arrows": "to", "from": "Support Vector Machines (SVM)", "title": "subtopic", "to": "Dual Formulation of SVM"}, {"arrows": "to", "from": "Identity Function", "title": "subtopic", "to": "Activation Functions"}, {"arrows": "to", "from": "Locally Weighted Linear Regression", "title": "is_a", "to": "Non-Parametric Algorithms"}, {"arrows": "to", "from": "Machine Learning Basics", "title": "has_subtopic", "to": "Hypothesis Class"}, {"arrows": "to", "from": "Sequential Minimal Optimization (SMO)", "title": "depends_on", "to": "Convergence Criteria"}, {"arrows": "to", "from": "Newton\u0027s Method", "title": "depends_on", "to": "Finding Roots"}, {"arrows": "to", "from": "Supervised Learning", "title": "subtopic", "to": "Kernel Methods"}, {"arrows": "to", "from": "State Transition Probabilities", "title": "related_to", "to": "Finite-State MDPs"}, {"arrows": "to", "from": "Principal Eigenvector", "title": "depends_on", "to": "Lagrange Multipliers"}, {"arrows": "to", "from": "Feature Map", "title": "related_to", "to": "Features Variables"}, {"arrows": "to", "from": "Deep Learning", "title": "subtopic", "to": "Supervised Learning with Non-linear Models"}, {"arrows": "to", "from": "Back-propagation for MLPs", "title": "depends_on", "to": "Gradient Computation"}, {"arrows": "to", "from": "Machine Learning Algorithms", "title": "includes", "to": "Non-Parametric Algorithms"}, {"arrows": "to", "from": "Markov Decision Process (MDP)", "title": "depends_on", "to": "Value Iteration"}, {"arrows": "to", "from": "Parameter Estimation", "title": "depends_on", "to": "Machine Learning Models"}, {"arrows": "to", "from": "Estimator Simplification", "title": "subtopic", "to": "Expectation Calculation"}, {"arrows": "to", "from": "Kernel Functions", "title": "subtopic", "to": "Explicit Definition of Kernels"}, {"arrows": "to", "from": "Bias-variance tradeoff", "title": "subtopic_of", "to": "A mathematical decomposition (for regression)"}, {"arrows": "to", "from": "Reinforcement Learning", "title": "defines_with", "to": "Markov Decision Processes (MDP)"}, {"arrows": "to", "from": "Adaptation Methods", "title": "subtopic", "to": "Finetuning"}, {"arrows": "to", "from": "Unsupervised learning", "title": "has_subtopic", "to": "EM algorithms"}, {"arrows": "to", "from": "Policy Gradients", "title": "subtopic", "to": "Reward Function Estimation"}, {"arrows": "to", "from": "Backpropagation Algorithm", "title": "subtopic", "to": "Gradient Computation"}, {"arrows": "to", "from": "ICA Ambiguities", "title": "has_subtopic", "to": "Permutation Matrix"}, {"arrows": "to", "from": "Optimization Problem", "title": "subtopic", "to": "Optimal Margin Classifier"}, {"arrows": "to", "from": "Machine Learning Models", "title": "subtopic", "to": "ResNet (Residual Network)"}, {"arrows": "to", "from": "Linear Model Limitations", "title": "related_to", "to": "Underfitting"}, {"arrows": "to", "from": "Loss Function", "title": "depends_on", "to": "Gradient Computation"}, {"arrows": "to", "from": "Supervised Learning", "title": "related_to", "to": "Regression Algorithms"}, {"arrows": "to", "from": "Model Creation Methods", "title": "subtopic", "to": "Learning from Data"}, {"arrows": "to", "from": "Regularization", "title": "depends_on", "to": "Regularization Parameter (\u03bb)"}, {"arrows": "to", "from": "Support Vector Machines (SVMs)", "title": "derives_from", "to": "SMO Algorithm"}, {"arrows": "to", "from": "KKT Conditions", "title": "subtopic", "to": "Dual Complementarity Condition"}, {"arrows": "to", "from": "Evidence Lower Bound (ELBO)", "title": "related_to", "to": "Gradient Computation"}, {"arrows": "to", "from": "Implicit Bias Study", "title": "subtopic", "to": "Machine Learning Literature"}, {"arrows": "to", "from": "Lagrange Duality", "title": "depends_on", "to": "Dual Formulation"}, {"arrows": "to", "from": "Linear Regression", "title": "subtopic", "to": "Probabilistic Interpretation"}, {"arrows": "to", "from": "Backpropagation", "title": "depends_on", "to": "Differentiable Circuit"}, {"arrows": "to", "from": "Machine Learning Algorithms", "title": "related_to", "to": "Gradient Descent"}, {"arrows": "to", "from": "Necessary Conditions for Valid Kernels", "title": "related_to", "to": "Kernel Matrix"}, {"arrows": "to", "from": "Kernel Functions", "title": "subtopic", "to": "Necessary Conditions for Valid Kernels"}, {"arrows": "to", "from": "Model Creation Methods", "title": "subtopic", "to": "Physics Simulation"}, {"arrows": "to", "from": "Model Evaluation", "title": "related_to", "to": "Bias-Variance Tradeoff"}, {"arrows": "to", "from": "Other Normalization Layers", "title": "subtopic", "to": "Batch Normalization"}, {"arrows": "to", "from": "Backward Function Overview", "title": "has_subtopic", "to": "Matrix Multiplication Backward Function"}, {"arrows": "to", "from": "Principal Components", "title": "related_to", "to": "Dimensionality Reduction"}, {"arrows": "to", "from": "Optimal Policy in MDPs", "title": "depends_on", "to": "Machine Learning Overview"}, {"arrows": "to", "from": "Other Activation Functions", "title": "depends_on", "to": "Multi-layer Fully-Connected Neural Networks"}, {"arrows": "to", "from": "Support Vector Machines", "title": "depends_on", "to": "Regularization"}, {"arrows": "to", "from": "Hypothesis Class Switching", "title": "depends_on", "to": "Bias Decrease"}, {"arrows": "to", "from": "Training Process", "title": "depends_on", "to": "Cross-Entropy Loss"}, {"arrows": "to", "from": "Linearization of Dynamics", "title": "depends_on", "to": "Taylor Expansion"}, {"arrows": "to", "from": "Kernel Matrix Properties", "title": "subtopic", "to": "Sufficient Conditions for Valid Kernels"}, {"arrows": "to", "from": "Supervised Learning", "title": "subtopic", "to": "Generalized Linear Models"}, {"arrows": "to", "from": "Multi-class Classification", "title": "subtopic", "to": "Logits in Multi-class"}, {"arrows": "to", "from": "Parameter Estimation", "title": "subtopic", "to": "Maximum Likelihood Estimation (MLE)"}]);

                  nodeColors = {};
                  allNodes = nodes.get({ returnType: "Object" });
                  for (nodeId in allNodes) {
                    nodeColors[nodeId] = allNodes[nodeId].color;
                  }
                  allEdges = edges.get({ returnType: "Object" });
                  // adding nodes and edges to the graph
                  data = {nodes: nodes, edges: edges};

                  var options = {
    "configure": {
        "enabled": false
    },
    "edges": {
        "color": {
            "inherit": true
        },
        "smooth": {
            "enabled": true,
            "type": "dynamic"
        }
    },
    "interaction": {
        "dragNodes": true,
        "hideEdgesOnDrag": false,
        "hideNodesOnDrag": false
    },
    "physics": {
        "enabled": true,
        "repulsion": {
            "centralGravity": 0.1,
            "damping": 0.09,
            "nodeDistance": 200,
            "springConstant": 0.05,
            "springLength": 150
        },
        "solver": "repulsion",
        "stabilization": {
            "enabled": true,
            "fit": true,
            "iterations": 1000,
            "onlyDynamicEdges": false,
            "updateInterval": 50
        }
    }
};

                  


                  

                  network = new vis.Network(container, data, options);

                  

                  

                  


                  
                      network.on("stabilizationProgress", function(params) {
                          document.getElementById('loadingBar').removeAttribute("style");
                          var maxWidth = 496;
                          var minWidth = 20;
                          var widthFactor = params.iterations/params.total;
                          var width = Math.max(minWidth,maxWidth * widthFactor);
                          document.getElementById('bar').style.width = width + 'px';
                          document.getElementById('text').innerHTML = Math.round(widthFactor*100) + '%';
                      });
                      network.once("stabilizationIterationsDone", function() {
                          document.getElementById('text').innerHTML = '100%';
                          document.getElementById('bar').style.width = '496px';
                          document.getElementById('loadingBar').style.opacity = 0;
                          // really clean the dom element
                          setTimeout(function () {document.getElementById('loadingBar').style.display = 'none';}, 500);
                      });
                  

                  return network;

              }
              drawGraph();
        </script>
    </body>
</html>