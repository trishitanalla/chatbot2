{
  "nodes": [
    {
      "id": "Supervised Learning",
      "type": "major",
      "parent": null,
      "description": "Learning process where a model is trained on labeled data to predict outcomes."
    },
    {
      "id": "Regression Problem",
      "type": "subnode",
      "parent": "Supervised Learning",
      "description": "Statistical approach to predicting a continuous outcome variable based on one or more predictor variables."
    },
    {
      "id": "Classification Problem",
      "type": "subnode",
      "parent": "Supervised Learning",
      "description": "Type of supervised learning where the target variable takes on discrete values."
    },
    {
      "id": "Hypothesis",
      "type": "subnode",
      "parent": "Supervised Learning",
      "description": "Function learned by a model to predict outcomes based on input data."
    },
    {
      "id": "Linear Regression",
      "type": "major",
      "parent": null,
      "description": "Technique for modeling the relationship between a scalar dependent variable y and one or more explanatory variables X."
    },
    {
      "id": "Feature Selection",
      "type": "subnode",
      "parent": "Linear Regression",
      "description": "Process of selecting which features to include in a model to improve performance."
    },
    {
      "id": "Machine Learning Basics",
      "type": "major",
      "parent": null,
      "description": "Fundamental concepts in machine learning including training and test datasets."
    },
    {
      "id": "Gradient Descent",
      "type": "subnode",
      "parent": "Machine Learning Basics",
      "description": "Optimization algorithm used to minimize a function by iteratively moving towards the minimum value of that function."
    },
    {
      "id": "LMS Update Rule",
      "type": "subnode",
      "parent": "Gradient Descent",
      "description": "Specific update rule derived from cost function for a single training example."
    },
    {
      "id": "Widrow-Hoff Learning Rule",
      "type": "subnode",
      "parent": "LMS Update Rule",
      "description": "Alternative name for the LMS update rule, used in adaptive filters and neural networks."
    },
    {
      "id": "Error Term",
      "type": "subnode",
      "parent": "LMS Update Rule",
      "description": "Difference between actual output and predicted value that guides parameter updates."
    },
    {
      "id": "Batch Gradient Descent",
      "type": "subnode",
      "parent": "Gradient Descent",
      "description": "Version of gradient descent that uses the entire dataset to make a single update."
    },
    {
      "id": "Matrix Derivatives",
      "type": "major",
      "parent": null,
      "description": "Derivation of function f with respect to matrix A."
    },
    {
      "id": "Gradient Calculation",
      "type": "subnode",
      "parent": "Matrix Derivatives",
      "description": "Process of calculating gradients for parameters to update model weights."
    },
    {
      "id": "Least Squares Revisited",
      "type": "major",
      "parent": null,
      "description": "Revisiting least squares using matrix derivatives."
    },
    {
      "id": "Design Matrix",
      "type": "subnode",
      "parent": "Least Squares Revisited",
      "description": "Matrix containing training examples' input values in its rows."
    },
    {
      "id": "Vector y",
      "type": "subnode",
      "parent": "Least Squares Revisited",
      "description": "n-dimensional vector with target values from the training set."
    },
    {
      "id": "Machine Learning Algorithms",
      "type": "major",
      "parent": null,
      "description": "Overview of machine learning algorithms including gradient descent and stochastic methods."
    },
    {
      "id": "Locally Weighted Linear Regression (LWLR)",
      "type": "subnode",
      "parent": "Linear Regression",
      "description": "A variant of linear regression where weights are assigned to data points based on their proximity to the point being predicted."
    },
    {
      "id": "Weight Calculation",
      "type": "subnode",
      "parent": "Locally Weighted Linear Regression (LWLR)",
      "description": "Determines how much influence each training example has on the prediction at a given query point."
    },
    {
      "id": "Bandwidth Parameter (\u03c4)",
      "type": "subnode",
      "parent": "Weight Calculation",
      "description": "Controls the rate of decay in weight as distance from the query point increases."
    },
    {
      "id": "Learning a model for an MDP",
      "type": "major",
      "parent": null,
      "description": "Techniques to estimate the transition dynamics of an environment from data."
    },
    {
      "id": "Continuous state MDPs",
      "type": "subnode",
      "parent": "Learning a model for an MDP",
      "description": "Handling environments with a continuous state space in reinforcement learning."
    },
    {
      "id": "Discretization",
      "type": "subnode",
      "parent": "Continuous state MDPs",
      "description": "Approaches to converting continuous states into discrete representations for RL algorithms."
    },
    {
      "id": "Value function approximation",
      "type": "subnode",
      "parent": "Continuous state MDPs",
      "description": "Methods for estimating value functions in environments with a large or infinite number of states."
    },
    {
      "id": "Connections between Policy and Value Iteration (Optional)",
      "type": "subnode",
      "parent": "Learning a model for an MDP",
      "description": "Theoretical connections and relationships between policy iteration and value iteration methods."
    },
    {
      "id": "LQR, DDP and LQG",
      "type": "major",
      "parent": null,
      "description": "Control theory concepts applied to reinforcement learning problems."
    },
    {
      "id": "Finite-horizon MDPs",
      "type": "subnode",
      "parent": "LQR, DDP and LQG",
      "description": "Analysis of Markov decision processes with a fixed time horizon."
    },
    {
      "id": "Linear Quadratic Regulation (LQR)",
      "type": "subnode",
      "parent": "LQR, DDP and LQG",
      "description": "Special case of finite-horizon setting in reinforcement learning with linear transitions and quadratic rewards."
    },
    {
      "id": "From non-linear dynamics to LQR",
      "type": "subnode",
      "parent": "LQR, DDP and LQG",
      "description": "Approaches to apply LQR in nonlinear dynamic environments"
    },
    {
      "id": "Linearization of dynamics",
      "type": "subnode",
      "parent": "From non-linear dynamics to LQR",
      "description": "Techniques for linearizing nonlinear system dynamics"
    },
    {
      "id": "Differential Dynamic Programming (DDP)",
      "type": "subnode",
      "parent": "From non-linear dynamics to LQR",
      "description": "Optimization technique used for trajectory optimization in robotics and control systems."
    },
    {
      "id": "Linear Quadratic Gaussian (LQG)",
      "type": "subnode",
      "parent": "LQR, DDP and LQG",
      "description": "Combination of LQR with stochastic dynamics and noisy measurements"
    },
    {
      "id": "Policy Gradient (REINFORCE)",
      "type": "major",
      "parent": null,
      "description": "Model-free algorithm for learning randomized policies without value functions."
    },
    {
      "id": "Supervised Learning Introduction",
      "type": "major",
      "parent": null,
      "description": "Introduction to supervised learning concepts and examples"
    },
    {
      "id": "Machine Learning Concepts",
      "type": "major",
      "parent": null,
      "description": "Overview of key concepts in machine learning including EM algorithm and variational inference."
    },
    {
      "id": "Underfitting",
      "type": "subnode",
      "parent": "Linear Regression",
      "description": "Scenario where a model is too simple to capture the underlying pattern of the data."
    },
    {
      "id": "Overfitting",
      "type": "subnode",
      "parent": "Linear Regression",
      "description": "Situation where a model fits noise in training data rather than underlying pattern."
    },
    {
      "id": "Locally Weighted Linear Regression (LWR)",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Regression algorithm that assigns more weight to nearby points when making predictions."
    },
    {
      "id": "Maximum Likelihood Estimation (MLE)",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Process for estimating the parameters that maximize the probability of observing the training data."
    },
    {
      "id": "Least Squares Regression",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Method for fitting a line to data points by minimizing squared errors."
    },
    {
      "id": "Probabilistic Assumptions",
      "type": "subnode",
      "parent": "Maximum Likelihood Estimation (MLE)",
      "description": "Assumptions about the probability distribution of class labels given input features."
    },
    {
      "id": "Cost Function J(\u03b8)",
      "type": "subnode",
      "parent": "Least Squares Regression",
      "description": "Function to be minimized, representing error between predictions and actual values."
    },
    {
      "id": "Function Representation",
      "type": "subnode",
      "parent": "Machine Learning Basics",
      "description": "How functions are represented and approximated in machine learning models."
    },
    {
      "id": "Linear Function Approximation",
      "type": "subnode",
      "parent": "Function Representation",
      "description": "Approximating the target function using linear equations with parameters."
    },
    {
      "id": "Parameters (Weights)",
      "type": "subnode",
      "parent": "Linear Function Approximation",
      "description": "Coefficients that define the form of the linear approximation."
    },
    {
      "id": "Cost Function",
      "type": "subnode",
      "parent": "Function Representation",
      "description": "Measures the performance of a machine learning model by quantifying the error between predicted and actual values."
    },
    {
      "id": "Ordinary Least Squares",
      "type": "subnode",
      "parent": "Cost Function",
      "description": "A special case of GLM where target variable is continuous and modeled as Gaussian distribution."
    },
    {
      "id": "LMS Algorithm",
      "type": "subnode",
      "parent": "Function Representation",
      "description": "Iterative method to minimize the cost function by adjusting parameters."
    },
    {
      "id": "Convex Function",
      "type": "subnode",
      "parent": "Gradient Descent",
      "description": "Function with a single global minimum, ensuring gradient descent convergence."
    },
    {
      "id": "Stochastic Gradient Descent",
      "type": "subnode",
      "parent": "Gradient Descent",
      "description": "Algorithm that uses random samples to update parameters, reducing computational load."
    },
    {
      "id": "Optimization Problem",
      "type": "major",
      "parent": null,
      "description": "Mathematical problem of finding the best solution from all feasible solutions."
    },
    {
      "id": "Gradient Descent Methods",
      "type": "major",
      "parent": null,
      "description": "Techniques for minimizing cost function in machine learning."
    },
    {
      "id": "Learning Rate Decay",
      "type": "subnode",
      "parent": "Stochastic Gradient Descent",
      "description": "Gradually decreases learning rate to ensure convergence."
    },
    {
      "id": "Normal Equations Method",
      "type": "major",
      "parent": null,
      "description": "Direct method for minimizing cost function without iteration."
    },
    {
      "id": "Least-Squares Cost Function",
      "type": "subnode",
      "parent": "Linear Regression",
      "description": "Method to find the best fit line by minimizing the sum of squared residuals."
    },
    {
      "id": "Probabilistic Interpretation",
      "type": "subnode",
      "parent": "Linear Regression",
      "description": "Interpreting linear regression from a probabilistic perspective, assuming Gaussian noise."
    },
    {
      "id": "Invertibility of X^TX Matrix",
      "type": "subnode",
      "parent": "Least-Squares Cost Function",
      "description": "Condition for the matrix to be invertible in least-squares solution."
    },
    {
      "id": "Gaussian Distribution Assumption",
      "type": "subnode",
      "parent": "Probabilistic Interpretation",
      "description": "Assumes Qi is a Gaussian distribution with independent coordinates, mean governed by q(x;phi), variance by v(x;psi)."
    },
    {
      "id": "Normal Equations",
      "type": "subnode",
      "parent": "Linear Regression",
      "description": "Direct method for finding the optimal parameters in linear regression without iteration."
    },
    {
      "id": "Probability Distribution",
      "type": "subnode",
      "parent": "Machine Learning Basics",
      "description": "Distribution of y given x and parameters \u03b8."
    },
    {
      "id": "Design Matrix X",
      "type": "subnode",
      "parent": "Machine Learning Basics",
      "description": "Matrix containing all input variables x(i)."
    },
    {
      "id": "Likelihood Function",
      "type": "subnode",
      "parent": "Probability Distribution",
      "description": "Function that measures how likely a set of parameters is, given observed data."
    },
    {
      "id": "Independence Assumption",
      "type": "subnode",
      "parent": "Likelihood Function",
      "description": "Assumption on \u03b5(i)'s leading to product form likelihood."
    },
    {
      "id": "Maximum Likelihood Estimation",
      "type": "subnode",
      "parent": "Machine Learning Basics",
      "description": "Estimating parameters to maximize probability of observed data under model."
    },
    {
      "id": "Log Likelihood",
      "type": "subnode",
      "parent": "Maximum Likelihood Estimation",
      "description": "Expression for log likelihood in terms of joint probability distribution."
    },
    {
      "id": "Learning Rate (\u03b1)",
      "type": "subnode",
      "parent": "Gradient Descent",
      "description": "Hyperparameter controlling the size of each step in gradient descent."
    },
    {
      "id": "Update Rule",
      "type": "subnode",
      "parent": "Gradient Descent",
      "description": "Rule for updating parameters in each iteration of gradient descent."
    },
    {
      "id": "Locally Weighted Linear Regression",
      "type": "subnode",
      "parent": "Machine Learning Algorithms",
      "description": "A method for learning to estimate future states from current state-action pairs."
    },
    {
      "id": "Non-Parametric Algorithms",
      "type": "subnode",
      "parent": "Machine Learning Algorithms",
      "description": "Algorithms where the amount of information needed grows with the dataset size."
    },
    {
      "id": "Parametric Algorithms",
      "type": "subnode",
      "parent": "Machine Learning Algorithms",
      "description": "Algorithms that have a fixed number of parameters regardless of data size."
    },
    {
      "id": "Binary Classification",
      "type": "subnode",
      "parent": "Classification Problem",
      "description": "Classification task where output is binary, using logistic function to predict probabilities."
    },
    {
      "id": "Logistic Regression",
      "type": "subnode",
      "parent": "Classification Problem",
      "description": "Statistical method for modeling binary dependent variables by estimating probabilities using a logistic function."
    },
    {
      "id": "Linear Regression Approach",
      "type": "subnode",
      "parent": "Classification Problem",
      "description": "Initial attempt to solve classification with linear regression."
    },
    {
      "id": "Logistic Function",
      "type": "subnode",
      "parent": "Logistic Regression",
      "description": "Function used to convert logit values into probability estimates for classification tasks."
    },
    {
      "id": "Derivative of Sigmoid",
      "type": "subnode",
      "parent": "Logistic Function",
      "description": "Calculation showing the derivative of the sigmoid function."
    },
    {
      "id": "Machine Learning Models",
      "type": "major",
      "parent": null,
      "description": "Overview of models used in machine learning including classification and regression."
    },
    {
      "id": "Classification Model",
      "type": "subnode",
      "parent": "Machine Learning Models",
      "description": "A model that predicts categorical class labels based on input features."
    },
    {
      "id": "Log-Likelihood",
      "type": "subnode",
      "parent": "Likelihood Function",
      "description": "Natural logarithm of the likelihood function for easier computation and optimization."
    },
    {
      "id": "Gradient Ascent",
      "type": "subnode",
      "parent": "Maximum Likelihood Estimation (MLE)",
      "description": "Optimization technique used to maximize a function by iteratively moving in the direction of steepest ascent."
    },
    {
      "id": "Logistic Regression Derivation",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Derivation of logistic regression formulae."
    },
    {
      "id": "Perceptron Algorithm",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Attempts to find a linear decision boundary between classes."
    },
    {
      "id": "Multi-class Classification",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Classification problem where response variable can take on multiple values."
    },
    {
      "id": "Classification and Logistic Regression",
      "type": "subnode",
      "parent": "Supervised Learning",
      "description": "Classifying data into discrete categories using logistic regression."
    },
    {
      "id": "Perceptron Learning Algorithm",
      "type": "subnode",
      "parent": "Classification and Logistic Regression",
      "description": "Algorithm for learning linear classifiers."
    },
    {
      "id": "Maximizing l(theta)",
      "type": "subnode",
      "parent": "Classification and Logistic Regression",
      "description": "Alternative algorithm for maximizing the likelihood function."
    },
    {
      "id": "Generalized Linear Models",
      "type": "subnode",
      "parent": "Supervised Learning",
      "description": "Extension of linear models to non-normal distributions."
    },
    {
      "id": "Exponential Family",
      "type": "subnode",
      "parent": "Generalized Linear Models",
      "description": "Family of probability distributions with common properties."
    },
    {
      "id": "Constructing GLMs",
      "type": "subnode",
      "parent": "Generalized Linear Models",
      "description": "Methods for constructing generalized linear models."
    },
    {
      "id": "Logistic Regression (GLM)",
      "type": "subnode",
      "parent": "Constructing GLMs",
      "description": "Application of logistic regression within generalized linear models framework."
    },
    {
      "id": "Generative Learning Algorithms",
      "type": "subnode",
      "parent": "Supervised Learning",
      "description": "Models that generate data based on underlying probability distributions."
    },
    {
      "id": "Gaussian Discriminant Analysis",
      "type": "subnode",
      "parent": "Generative Learning Algorithms",
      "description": "Method for classification using Gaussian distributions."
    },
    {
      "id": "Multivariate Normal Distribution",
      "type": "subnode",
      "parent": "Gaussian Discriminant Analysis",
      "description": "A generalization of the one-dimensional normal distribution to higher dimensions."
    },
    {
      "id": "GDA Model",
      "type": "subnode",
      "parent": "Gaussian Discriminant Analysis",
      "description": "Model for classification using multivariate normal distributions."
    },
    {
      "id": "Discussion: GDA and Logistic Regression",
      "type": "subnode",
      "parent": "Gaussian Discriminant Analysis",
      "description": "Comparison between Gaussian discriminant analysis and logistic regression."
    },
    {
      "id": "Naive Bayes",
      "type": "subnode",
      "parent": "Generative Learning Algorithms",
      "description": "Simplified probabilistic classifier based on applying Bayes' theorem with strong independence assumptions."
    },
    {
      "id": "Laplace Smoothing",
      "type": "subnode",
      "parent": "Naive Bayes",
      "description": "Technique to improve Naive Bayes by handling zero probabilities for unseen events."
    },
    {
      "id": "Event Models for Text Classification",
      "type": "subnode",
      "parent": "Naive Bayes",
      "description": "Models used for classifying text documents based on word occurrences."
    },
    {
      "id": "Kernel Methods",
      "type": "major",
      "parent": null,
      "description": "Techniques that allow algorithms to work in high-dimensional feature spaces without explicit computation of vectors."
    },
    {
      "id": "Feature Maps",
      "type": "subnode",
      "parent": "Kernel Methods",
      "description": "Transformation of input data into higher-dimensional space to fit more complex models."
    },
    {
      "id": "LMS with Features",
      "type": "subnode",
      "parent": "Kernel Methods",
      "description": "Gradient descent algorithm for fitting models using features."
    },
    {
      "id": "LMS with Kernel Trick",
      "type": "subnode",
      "parent": "Kernel Methods",
      "description": "Efficient computation of LMS using kernel functions."
    },
    {
      "id": "Properties of Kernels",
      "type": "subnode",
      "parent": "Kernel Methods",
      "description": "Exploration of properties associated with kernel functions in machine learning."
    },
    {
      "id": "Support Vector Machines",
      "type": "major",
      "parent": null,
      "description": "Algorithm for classification that finds the hyperplane maximizing the margin between classes."
    },
    {
      "id": "Margins: Intuition",
      "type": "subnode",
      "parent": "Support Vector Machines",
      "description": "Understanding the concept of margins in SVMs."
    },
    {
      "id": "Notation (Optional)",
      "type": "subnode",
      "parent": "Support Vector Machines",
      "description": "Mathematical notation used in support vector machines."
    },
    {
      "id": "Functional and Geometric Margins",
      "type": "subnode",
      "parent": "Support Vector Machines",
      "description": "Definitions of functional and geometric margins in SVMs."
    },
    {
      "id": "Optimal Margin Classifier (Optional)",
      "type": "subnode",
      "parent": "Support Vector Machines",
      "description": "Finding the optimal margin classifier for linearly separable data."
    },
    {
      "id": "Lagrange Duality (Optional)",
      "type": "subnode",
      "parent": "Support Vector Machines",
      "description": "Use of Lagrangian duality in solving optimization problems in SVMs."
    },
    {
      "id": "Dual Formulation (Optional)",
      "type": "subnode",
      "parent": "Support Vector Machines",
      "description": "Optimal margin classifier expressed in the dual form."
    },
    {
      "id": "Regularization and Non-separable Case (Optional)",
      "type": "subnode",
      "parent": "Support Vector Machines",
      "description": "Handling non-separable data with regularization techniques."
    },
    {
      "id": "SMO Algorithm (Optional)",
      "type": "subnode",
      "parent": "Support Vector Machines",
      "description": "Sequential minimal optimization algorithm for solving SVM problems."
    },
    {
      "id": "Coordinate Ascent",
      "type": "subnode",
      "parent": "SMO Algorithm (Optional)",
      "description": "An iterative optimization algorithm that optimizes one variable at a time."
    },
    {
      "id": "SMO Details",
      "type": "subnode",
      "parent": "SMO Algorithm (Optional)",
      "description": "Detailed explanation of the sequential minimal optimization process."
    },
    {
      "id": "Deep Learning",
      "type": "major",
      "parent": null,
      "description": "Subfield of machine learning using neural networks to learn representations from data."
    },
    {
      "id": "Supervised Learning with Non-linear Models",
      "type": "subnode",
      "parent": "Deep Learning",
      "description": "Using non-linear models in supervised learning tasks."
    },
    {
      "id": "Neural Networks",
      "type": "subnode",
      "parent": "Deep Learning",
      "description": "Inspired by biological neural networks, used for complex function approximation."
    },
    {
      "id": "Modules in Modern Neural Networks",
      "type": "subnode",
      "parent": "Deep Learning",
      "description": "Introduces various building blocks and ways to combine them in modern neural networks."
    },
    {
      "id": "Backpropagation",
      "type": "subnode",
      "parent": "Deep Learning",
      "description": "Algorithm for training multi-layered neural networks by adjusting weights based on error gradients."
    },
    {
      "id": "Preliminaries on Partial Derivatives",
      "type": "subnode",
      "parent": "Backpropagation",
      "description": "Introduction to partial derivatives necessary for backpropagation."
    },
    {
      "id": "General Strategy of Backpropagation",
      "type": "subnode",
      "parent": "Backpropagation",
      "description": "Overview of the general strategy used in backpropagation algorithm."
    },
    {
      "id": "Backward Functions for Basic Modules",
      "type": "subnode",
      "parent": "Backpropagation",
      "description": "Derivation and implementation of backward functions for basic neural network modules."
    },
    {
      "id": "Back-propagation for MLPs",
      "type": "subnode",
      "parent": "Backpropagation",
      "description": "Process of computing gradients through a multi-layer perceptron network to update weights during training."
    },
    {
      "id": "Stochastic Gradient Ascent Rule",
      "type": "subnode",
      "parent": "Logistic Regression",
      "description": "Update rule for parameters using stochastic gradient ascent."
    },
    {
      "id": "Logistic Loss Function",
      "type": "subnode",
      "parent": "Logistic Regression",
      "description": "Function that measures the performance of a classification model where labels are binary values."
    },
    {
      "id": "Negative Log-Likelihood",
      "type": "subnode",
      "parent": "Logistic Regression",
      "description": "Loss function measuring the dissimilarity between predicted and actual distributions."
    },
    {
      "id": "Logit",
      "type": "subnode",
      "parent": "Logistic Loss Function",
      "description": "Linear combination of input features and parameters before applying the logistic function."
    },
    {
      "id": "Loss Functions",
      "type": "subnode",
      "parent": "Machine Learning Basics",
      "description": "Functions used to measure the performance of a model during training and testing."
    },
    {
      "id": "Cross-Entropy Loss",
      "type": "subnode",
      "parent": "Loss Functions",
      "description": "Summation of negative log-likelihoods over training data for optimization."
    },
    {
      "id": "Softmax Function",
      "type": "subnode",
      "parent": "Cross-Entropy Loss",
      "description": "Function used to convert logits into a probability vector for multi-class classification."
    },
    {
      "id": "Machine Learning",
      "type": "major",
      "parent": null,
      "description": "Field of study that uses algorithms to make predictions or decisions based on data."
    },
    {
      "id": "Classification",
      "type": "subnode",
      "parent": "Machine Learning",
      "description": "Technique for categorizing data into predefined classes."
    },
    {
      "id": "Multinomial Distribution",
      "type": "subnode",
      "parent": "Multi-class Classification",
      "description": "Probability distribution over multiple outcomes."
    },
    {
      "id": "Logits",
      "type": "subnode",
      "parent": "Softmax Function",
      "description": "Inputs to the softmax function before transformation."
    },
    {
      "id": "Probability Vector",
      "type": "subnode",
      "parent": "Softmax Function",
      "description": "Output of softmax, a vector with nonnegative entries summing to 1."
    },
    {
      "id": "Probabilistic Model",
      "type": "major",
      "parent": null,
      "description": "Model using softmax outputs as probabilities for classification tasks."
    },
    {
      "id": "Newton's Method",
      "type": "major",
      "parent": null,
      "description": "Optimization technique using second-order derivatives to find local minima or maxima."
    },
    {
      "id": "Finding Roots",
      "type": "subnode",
      "parent": "Newton's Method",
      "description": "The process of determining the values of x where f(x) = 0."
    },
    {
      "id": "Maximizing Functions",
      "type": "subnode",
      "parent": "Newton's Method",
      "description": "Using Newton's method to find maxima by setting first derivative to zero."
    },
    {
      "id": "Multidimensional Generalization",
      "type": "subnode",
      "parent": "Newton's Method",
      "description": "Extending Newton's method to handle vector-valued functions in logistic regression."
    },
    {
      "id": "Hessian Matrix",
      "type": "subnode",
      "parent": "Multidimensional Generalization",
      "description": "A square matrix of second-order partial derivatives used for multidimensional optimization."
    },
    {
      "id": "Optimization Methods",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Papers discussing optimization methods such as Adam and variational auto-encoding."
    },
    {
      "id": "Fisher Scoring",
      "type": "subnode",
      "parent": "Optimization Methods",
      "description": "A variant of Newton's method used in logistic regression."
    },
    {
      "id": "Generalized Linear Models (GLMs)",
      "type": "major",
      "parent": null,
      "description": "Models that extend linear models to accommodate non-normal distributions and non-linear relationships."
    },
    {
      "id": "Exponential Family Distributions",
      "type": "subnode",
      "parent": "Generalized Linear Models (GLMs)",
      "description": "A class of probability distributions that can be expressed in a specific form involving natural parameters and sufficient statistics."
    },
    {
      "id": "Loss Function",
      "type": "subnode",
      "parent": "Machine Learning Basics",
      "description": "Mathematical function used to measure the performance of a model and guide its training."
    },
    {
      "id": "Cross Entropy Loss",
      "type": "subnode",
      "parent": "Loss Function",
      "description": "A loss function commonly used in logistic regression and neural networks."
    },
    {
      "id": "Algorithm Application",
      "type": "subnode",
      "parent": "Newton's Method",
      "description": "Application of Newton's method in logistic regression for maximizing likelihood."
    },
    {
      "id": "Modern Neural Networks",
      "type": "major",
      "parent": null,
      "description": "Overview of modern neural network architectures and training techniques."
    },
    {
      "id": "Preliminaries on partial derivatives",
      "type": "subnode",
      "parent": "Backpropagation",
      "description": "Introduction to the mathematical concepts needed for backpropagation."
    },
    {
      "id": "General strategy of backpropagation",
      "type": "subnode",
      "parent": "Backpropagation",
      "description": "Overview of how backpropagation works in neural networks."
    },
    {
      "id": "Backward functions for basic modules",
      "type": "subnode",
      "parent": "Backpropagation",
      "description": "Detailed explanation of backward propagation through simple network components."
    },
    {
      "id": "Vectorization over training examples",
      "type": "subnode",
      "parent": "Modern Neural Networks",
      "description": "Techniques for optimizing the computation of gradients across multiple data points."
    },
    {
      "id": "Generalization and regularization",
      "type": "major",
      "parent": null,
      "description": "Strategies to improve model performance on unseen data through generalization and regularization techniques."
    },
    {
      "id": "Generalization",
      "type": "subnode",
      "parent": "Generalization and regularization",
      "description": "Concepts related to improving a model's ability to generalize from training data to new, unseen data."
    },
    {
      "id": "Bias-variance tradeoff",
      "type": "subnode",
      "parent": "Generalization",
      "description": "Discussion on the balance between model complexity and error due to variance or bias."
    },
    {
      "id": "A mathematical decomposition (for regression)",
      "type": "subnode",
      "parent": "Bias-variance tradeoff",
      "description": "Mathematical breakdown of the bias-variance tradeoff in a regression context."
    },
    {
      "id": "The double descent phenomenon",
      "type": "subnode",
      "parent": "Generalization",
      "description": "Phenomenon where model performance improves after reaching a peak due to overfitting."
    },
    {
      "id": "Sample complexity bounds (optional readings)",
      "type": "subnode",
      "parent": "Generalization",
      "description": "Theoretical analysis of the number of samples needed for learning tasks."
    },
    {
      "id": "Regularization and model selection",
      "type": "major",
      "parent": null,
      "description": "Techniques to prevent overfitting by penalizing complex models or selecting optimal hyperparameters."
    },
    {
      "id": "Regularization",
      "type": "subnode",
      "parent": "Regularization and model selection",
      "description": "Technique to prevent overfitting by adding a regularizer term to the loss function."
    },
    {
      "id": "Implicit regularization effect (optional reading)",
      "type": "subnode",
      "parent": "Regularization and model selection",
      "description": "Exploration of how certain algorithms inherently regularize models without explicit penalties."
    },
    {
      "id": "Model selection via cross validation",
      "type": "subnode",
      "parent": "Regularization and model selection",
      "description": "Procedure for choosing the best model based on performance across different data splits."
    },
    {
      "id": "Bayesian statistics and regularization",
      "type": "subnode",
      "parent": "Regularization and model selection",
      "description": "Application of Bayesian methods to regularize models and improve generalization."
    },
    {
      "id": "Unsupervised learning",
      "type": "major",
      "parent": null,
      "description": "Techniques for learning from data without labeled responses."
    },
    {
      "id": "Clustering and the k-means algorithm",
      "type": "subnode",
      "parent": "Unsupervised learning",
      "description": "Introduction to clustering methods with a focus on the k-means algorithm."
    },
    {
      "id": "EM algorithms",
      "type": "subnode",
      "parent": "Unsupervised learning",
      "description": "Expectation-Maximization techniques for parameter estimation in probabilistic models."
    },
    {
      "id": "EM for mixture of Gaussians",
      "type": "subnode",
      "parent": "EM algorithms",
      "description": "Application of EM to Gaussian Mixture Models (GMMs)."
    },
    {
      "id": "Jensen's inequality",
      "type": "subnode",
      "parent": "EM algorithms",
      "description": "Mathematical principle used in the derivation and understanding of EM algorithms."
    },
    {
      "id": "General EM algorithms",
      "type": "subnode",
      "parent": "EM algorithms",
      "description": "Overview of the general framework and applications of EM beyond GMMs."
    },
    {
      "id": "Other interpretation of ELBO",
      "type": "subnode",
      "parent": "General EM algorithms",
      "description": "Alternative perspectives on the Evidence Lower Bound (ELBO) in variational inference."
    },
    {
      "id": "Mixture of Gaussians revisited",
      "type": "subnode",
      "parent": "EM algorithms",
      "description": "Re-examination and advanced topics related to Gaussian Mixture Models using EM."
    },
    {
      "id": "Variational inference and variational auto-encoder (optional reading)",
      "type": "subnode",
      "parent": "EM algorithms",
      "description": "Advanced topic on probabilistic modeling with VAEs and VI."
    },
    {
      "id": "Principal components analysis",
      "type": "subnode",
      "parent": "Unsupervised learning",
      "description": "Dimensionality reduction technique that projects data onto a lower-dimensional space."
    },
    {
      "id": "Independent components analysis",
      "type": "subnode",
      "parent": "Unsupervised learning",
      "description": "Technique for separating mixed signals into their independent sources."
    },
    {
      "id": "ICA ambiguities",
      "type": "subnode",
      "parent": "Independent components analysis",
      "description": "Discussion on the inherent limitations and challenges in ICA."
    },
    {
      "id": "Densities and linear transformations",
      "type": "subnode",
      "parent": "Independent components analysis",
      "description": "Exploration of how densities change under linear transformations relevant to ICA."
    },
    {
      "id": "ICA algorithm",
      "type": "subnode",
      "parent": "Independent components analysis",
      "description": "Detailed explanation of the Independent Components Analysis procedure."
    },
    {
      "id": "Self-supervised learning and foundation models",
      "type": "major",
      "parent": null,
      "description": "Approaches to training large-scale models using self-supervision techniques."
    },
    {
      "id": "Pretraining and adaptation",
      "type": "subnode",
      "parent": "Self-supervised learning and foundation models",
      "description": "Overview of pre-training methods and subsequent fine-tuning for specific tasks."
    },
    {
      "id": "Pretraining methods in computer vision",
      "type": "subnode",
      "parent": "Self-supervised learning and foundation models",
      "description": "Techniques used to train visual recognition systems without labeled data."
    },
    {
      "id": "Pretrained large language models",
      "type": "subnode",
      "parent": "Self-supervised learning and foundation models",
      "description": "Discussion on the development and applications of pre-trained language models."
    },
    {
      "id": "Open up the blackbox of Transformers",
      "type": "subnode",
      "parent": "Pretrained large language models",
      "description": "Exploration of the architecture and workings of Transformer-based models."
    },
    {
      "id": "Zero-shot learning and in-context learning",
      "type": "subnode",
      "parent": "Pretrained large language models",
      "description": "Capabilities of models to perform tasks without explicit training data."
    },
    {
      "id": "Reinforcement Learning and Control",
      "type": "major",
      "parent": null,
      "description": "Techniques for agents to learn optimal behavior through interaction with an environment."
    },
    {
      "id": "Reinforcement learning",
      "type": "subnode",
      "parent": "Reinforcement Learning and Control",
      "description": "Introduction to the principles of reinforcement learning and its applications."
    },
    {
      "id": "Markov decision processes",
      "type": "subnode",
      "parent": "Reinforcement learning",
      "description": "Mathematical framework for modeling sequential decision-making problems."
    },
    {
      "id": "Value iteration and policy iteration",
      "type": "subnode",
      "parent": "Reinforcement learning",
      "description": "Algorithms for finding optimal policies in MDPs through value or policy updates."
    },
    {
      "id": "Generalized Linear Model (GLM)",
      "type": "subnode",
      "parent": "Machine Learning Models",
      "description": "A method to model problems using exponential family distributions."
    },
    {
      "id": "Poisson Distribution",
      "type": "subnode",
      "parent": "Generalized Linear Model (GLM)",
      "description": "Distribution used to model count data such as customer arrivals or page views."
    },
    {
      "id": "Conditional Distribution Assumption",
      "type": "subnode",
      "parent": "Generalized Linear Model (GLM)",
      "description": "Assumes y|x follows an exponential family distribution."
    },
    {
      "id": "Prediction Goal",
      "type": "subnode",
      "parent": "Generalized Linear Model (GLM)",
      "description": "Aims to predict the expected value of T(y) given x."
    },
    {
      "id": "Linear Relationship Assumption",
      "type": "subnode",
      "parent": "Generalized Linear Model (GLM)",
      "description": "Assumes a linear relationship between natural parameter and inputs."
    },
    {
      "id": "Generalized Linear Models (GLM)",
      "type": "subnode",
      "parent": "Machine Learning Models",
      "description": "Models that extend linear regression to accommodate non-normal distributions."
    },
    {
      "id": "Bernoulli Distribution",
      "type": "subnode",
      "parent": "Generalized Linear Models (GLM)",
      "description": "Binary outcome probability distribution, an example of exponential family."
    },
    {
      "id": "Gaussian Distribution",
      "type": "subnode",
      "parent": "Exponential Family Distributions",
      "description": "Describes the distribution of state at time t+1 given observations up to time t."
    },
    {
      "id": "Natural Parameter",
      "type": "subnode",
      "parent": "Exponential Family Distributions",
      "description": "Parameter \u03b7 in the exponential family distribution formula."
    },
    {
      "id": "Sufficient Statistic",
      "type": "subnode",
      "parent": "Exponential Family Distributions",
      "description": "Statistic T(y) that summarizes data relevant to parameter estimation."
    },
    {
      "id": "Log Partition Function",
      "type": "subnode",
      "parent": "Exponential Family Distributions",
      "description": "Function a(\u03b7) ensuring the distribution sums/integrates to 1."
    },
    {
      "id": "Natural Parameter for Bernoulli",
      "type": "subnode",
      "parent": "Bernoulli Distribution",
      "description": "\u03b7 = log(\u03c6/(1-\u03c6)), where \u03c6 is the mean of the distribution."
    },
    {
      "id": "Sufficient Statistic for Bernoulli",
      "type": "subnode",
      "parent": "Bernoulli Distribution",
      "description": "T(y) = y, indicating the outcome itself as sufficient statistic."
    },
    {
      "id": "Log Partition Function for Bernoulli",
      "type": "subnode",
      "parent": "Bernoulli Distribution",
      "description": "a(\u03b7) = log(1 + e^\u03b7), ensuring normalization of distribution."
    },
    {
      "id": "Assumptions/Design Choices",
      "type": "subnode",
      "parent": "Generalized Linear Models (GLMs)",
      "description": "Three foundational principles that lead to the derivation of GLMs."
    },
    {
      "id": "Response Variable",
      "type": "subnode",
      "parent": "Ordinary Least Squares",
      "description": "Continuous target variable in the context of GLM formulation."
    },
    {
      "id": "Conditional Distribution",
      "type": "subnode",
      "parent": "Ordinary Least Squares",
      "description": "Modeling the conditional distribution of y given x and theta."
    },
    {
      "id": "Bayesian Classification",
      "type": "major",
      "parent": null,
      "description": "Classification method using Bayes' theorem to predict class membership probabilities."
    },
    {
      "id": "Class Priors",
      "type": "subnode",
      "parent": "Bayesian Classification",
      "description": "Prior probability of each class before observing data."
    },
    {
      "id": "Conditional Probability",
      "type": "subnode",
      "parent": "Bayesian Classification",
      "description": "Probability of an event given that another event has occurred."
    },
    {
      "id": "Posterior Distribution",
      "type": "subnode",
      "parent": "Bayesian Classification",
      "description": "The posterior distribution of latent variables given observed data under current parameters."
    },
    {
      "id": "Gaussian Discriminant Analysis (GDA)",
      "type": "major",
      "parent": null,
      "description": "A probabilistic model that assumes the data is generated from a mixture of several Gaussian distributions with unknown parameters."
    },
    {
      "id": "Conditional Distribution Modeling",
      "type": "subnode",
      "parent": "Machine Learning Models",
      "description": "Modeling the distribution of y given x."
    },
    {
      "id": "Canonical Response Function",
      "type": "subnode",
      "parent": "Exponential Family Distributions",
      "description": "Function giving the mean of a distribution as a function of its natural parameter."
    },
    {
      "id": "Canonical Link Function",
      "type": "subnode",
      "parent": "Exponential Family Distributions",
      "description": "Inverse of the canonical response function, mapping from expected value to natural parameter."
    },
    {
      "id": "Discriminative Learning Algorithms",
      "type": "subnode",
      "parent": "Machine Learning Algorithms",
      "description": "Algorithms that directly model the decision boundary between classes."
    },
    {
      "id": "Bayes Rule",
      "type": "subnode",
      "parent": "Generative Learning Algorithms",
      "description": "Used to derive posterior distribution p(y|x) from p(x|y) and p(y)."
    },
    {
      "id": "Probability Distributions",
      "type": "subnode",
      "parent": "Machine Learning Basics",
      "description": "Discussion on probability distributions used in ML models."
    },
    {
      "id": "Mean",
      "type": "subnode",
      "parent": "Gaussian Distribution",
      "description": "Definition and calculation of mean in Gaussian distribution."
    },
    {
      "id": "Covariance",
      "type": "subnode",
      "parent": "Gaussian Distribution",
      "description": "Explanation of covariance matrix and its significance."
    },
    {
      "id": "Standard Normal Distribution",
      "type": "subnode",
      "parent": "Gaussian Distribution",
      "description": "Definition and properties of standard normal distribution."
    },
    {
      "id": "Density Visualization",
      "type": "subnode",
      "parent": "Gaussian Distribution",
      "description": "Examples showing how density changes with covariance matrix values."
    },
    {
      "id": "Covariance Matrix",
      "type": "subnode",
      "parent": "Multivariate Normal Distribution",
      "description": "Matrix that describes the variance and covariance between random variables."
    },
    {
      "id": "Contour Plots",
      "type": "subnode",
      "parent": "Multivariate Normal Distribution",
      "description": "Graphical representation of density contours for different values of parameters."
    },
    {
      "id": "Multivariate Normal Distributions for Classes",
      "type": "subnode",
      "parent": "Gaussian Discriminant Analysis (GDA)",
      "description": "Represents different classes with separate mean vectors and a common covariance matrix."
    },
    {
      "id": "Decision Boundaries",
      "type": "subnode",
      "parent": "Machine Learning Models",
      "description": "Boundaries that separate different classes in feature space."
    },
    {
      "id": "Model Assumptions",
      "type": "subnode",
      "parent": "Machine Learning Models",
      "description": "Theoretical assumptions made by models about the data distribution."
    },
    {
      "id": "Model Parameters",
      "type": "subnode",
      "parent": "Gaussian Discriminant Analysis (GDA)",
      "description": "Parameters include prior probability of spam, conditional probabilities of words given spam or non-spam"
    },
    {
      "id": "Log-Likelihood Function",
      "type": "subnode",
      "parent": "Model Parameters",
      "description": "Function used to estimate parameters by maximizing likelihood of observed data given the model."
    },
    {
      "id": "Decision Boundary",
      "type": "subnode",
      "parent": "Gaussian Discriminant Analysis (GDA)",
      "description": "Boundary in feature space where model predicts equal probabilities for both classes."
    },
    {
      "id": "GDA (Generative Discriminative Approach)",
      "type": "subnode",
      "parent": "Machine Learning Models",
      "description": "Model that makes strong assumptions about data distribution."
    },
    {
      "id": "Robustness of Logistic Regression",
      "type": "subnode",
      "parent": "Logistic Regression",
      "description": "Discusses the robust nature and performance in large datasets."
    },
    {
      "id": "Naive Bayes (Discrete Features)",
      "type": "subnode",
      "parent": "Machine Learning Models",
      "description": "Algorithm for classification with discrete-valued features."
    },
    {
      "id": "Feature Vector Selection",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Choosing relevant features for classification tasks."
    },
    {
      "id": "Generative Models",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Models that generate data based on learned parameters."
    },
    {
      "id": "Naive Bayes Classifier",
      "type": "subnode",
      "parent": "Generative Models",
      "description": "A probabilistic classifier based on applying Bayes' theorem with strong (naive) independence assumptions between the features."
    },
    {
      "id": "Conditional Independence Assumption",
      "type": "subnode",
      "parent": "Naive Bayes Classifier",
      "description": "Assumption that features are independent given the class label."
    },
    {
      "id": "Text Classification",
      "type": "subnode",
      "parent": "Machine Learning",
      "description": "Process of categorizing text into predefined categories using machine learning techniques."
    },
    {
      "id": "Spam Filter",
      "type": "subnode",
      "parent": "Text Classification",
      "description": "System that uses machine learning to classify emails as spam or non-spam."
    },
    {
      "id": "Training Set",
      "type": "subnode",
      "parent": "Spam Filter",
      "description": "Collection of examples used for training a model, each with features and labels."
    },
    {
      "id": "Feature Vector",
      "type": "subnode",
      "parent": "Spam Filter",
      "description": "Representation of an email as a vector where each dimension corresponds to the presence or absence of words in the vocabulary."
    },
    {
      "id": "Vocabulary",
      "type": "subnode",
      "parent": "Feature Vector",
      "description": "Set of unique words used to represent emails in feature vectors."
    },
    {
      "id": "Stop Words",
      "type": "subnode",
      "parent": "Training Set",
      "description": "Commonly occurring words that are often excluded from the vocabulary due to their lack of discriminatory power."
    },
    {
      "id": "Naive Bayes Algorithm",
      "type": "major",
      "parent": null,
      "description": "A probabilistic classifier based on applying Bayes' theorem with strong independence assumptions between the features."
    },
    {
      "id": "Bayesian Inference",
      "type": "subnode",
      "parent": "Naive Bayes Algorithm",
      "description": "Statistical method for updating the probability estimate for a hypothesis as more evidence or information becomes available."
    },
    {
      "id": "Parameter Estimation",
      "type": "subnode",
      "parent": "Naive Bayes Algorithm",
      "description": "Process of estimating parameters such as \u03c6\u208a|y=1 and \u03c6\u208a|y=0 from a training set."
    },
    {
      "id": "Binary Features",
      "type": "subnode",
      "parent": "Naive Bayes Algorithm",
      "description": "Features are binary-valued in the basic formulation of Naive Bayes."
    },
    {
      "id": "Multinomial Features",
      "type": "subnode",
      "parent": "Naive Bayes Algorithm",
      "description": "Generalization to features with multiple discrete values."
    },
    {
      "id": "Spam Classification Example",
      "type": "subnode",
      "parent": "Laplace Smoothing",
      "description": "Illustrative example of applying Laplace smoothing in email spam detection."
    },
    {
      "id": "Machine Learning Conferences",
      "type": "major",
      "parent": null,
      "description": "Top machine learning conferences including NeurIPS."
    },
    {
      "id": "NeurIPS Conference",
      "type": "subnode",
      "parent": "Machine Learning Conferences",
      "description": "One of the top machine learning conferences with a submission deadline in May-June."
    },
    {
      "id": "Naive Bayes Spam Filter",
      "type": "major",
      "parent": null,
      "description": "A probabilistic classifier used for spam detection based on Bayesian probability theory."
    },
    {
      "id": "Zero Frequency Problem",
      "type": "subnode",
      "parent": "Parameter Estimation",
      "description": "Issue where the probability of an unseen event is estimated as zero based on finite training data."
    },
    {
      "id": "Probability Estimation",
      "type": "major",
      "parent": null,
      "description": "Discusses the statistical issues with estimating probabilities as zero."
    },
    {
      "id": "Multinomial Random Variable",
      "type": "subnode",
      "parent": "Probability Estimation",
      "description": "A random variable taking values in a finite set of outcomes."
    },
    {
      "id": "Maximum Likelihood Estimates",
      "type": "subnode",
      "parent": "Probability Estimation",
      "description": "Estimates for parameters based on observed data, which can result in zero probabilities."
    },
    {
      "id": "Bernoulli Event Model",
      "type": "subnode",
      "parent": "Naive Bayes Classifier",
      "description": "Model for text classification assuming binary presence or absence of words."
    },
    {
      "id": "Multinomial Event Model",
      "type": "subnode",
      "parent": "Generative Learning Algorithms",
      "description": "Model for generating emails where each word is chosen independently from a multinomial distribution based on spam/non-spam classification"
    },
    {
      "id": "Spam/Non-Spam Classification",
      "type": "subnode",
      "parent": "Multinomial Event Model",
      "description": "Determining whether an email is spam or not before generating its content"
    },
    {
      "id": "Word Generation Process",
      "type": "subnode",
      "parent": "Multinomial Event Model",
      "description": "Process of independently selecting each word from a multinomial distribution over words given the classification"
    },
    {
      "id": "Probability Calculation",
      "type": "subnode",
      "parent": "Multinomial Event Model",
      "description": "Calculating overall probability of an email as product of probabilities for each word and spam/non-spam classification"
    },
    {
      "id": "Feature Map",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Transformation from attributes to features."
    },
    {
      "id": "Linear Function Over Features",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Rewriting a cubic function as a linear combination of feature variables."
    },
    {
      "id": "Attributes",
      "type": "subnode",
      "parent": "Feature Map",
      "description": "Original input values in machine learning problems."
    },
    {
      "id": "Features Variables",
      "type": "subnode",
      "parent": "Feature Map",
      "description": "New set of quantities derived from attributes using a feature map."
    },
    {
      "id": "Cubic Function Representation",
      "type": "subnode",
      "parent": "Linear Function Over Features",
      "description": "Expressing cubic functions as linear combinations in higher dimensions."
    },
    {
      "id": "Gradient Descent Update Rule",
      "type": "subnode",
      "parent": "LMS with Features",
      "description": "Update rule for gradient descent in the context of high-dimensional features."
    },
    {
      "id": "Feature Mapping",
      "type": "subnode",
      "parent": "Gradient Descent Update Rule",
      "description": "Transformation of input data into a higher-dimensional feature space for better model fitting."
    },
    {
      "id": "Kernel Trick",
      "type": "subnode",
      "parent": "Machine Learning Algorithms",
      "description": "Technique used in SVMs to handle non-linearly separable data by transforming it into a higher-dimensional space."
    },
    {
      "id": "Computational Complexity",
      "type": "subnode",
      "parent": "Gradient Descent Update Rule",
      "description": "Discussion on computational challenges with high-dimensional feature mappings."
    },
    {
      "id": "Iterative Update Process",
      "type": "subnode",
      "parent": "Feature Mapping",
      "description": "Process for updating coefficients in feature mapping iteratively."
    },
    {
      "id": "Linear Combination Representation",
      "type": "subnode",
      "parent": "Kernel Trick",
      "description": "Representation of the vector theta as a linear combination of transformed input vectors."
    },
    {
      "id": "Feature Maps and Kernels",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Discussion on feature maps and their corresponding kernel functions."
    },
    {
      "id": "Kernel Function Definition",
      "type": "subnode",
      "parent": "Feature Maps and Kernels",
      "description": "Definition of the kernel function based on inner products."
    },
    {
      "id": "Inner Product Computation",
      "type": "subnode",
      "parent": "Feature Maps and Kernels",
      "description": "Efficient computation of inner products between feature vectors in high-dimensional space."
    },
    {
      "id": "Algorithm Implementation",
      "type": "subnode",
      "parent": "Feature Maps and Kernels",
      "description": "Steps to implement the algorithm using kernel functions."
    },
    {
      "id": "Beta Update Equation",
      "type": "subnode",
      "parent": "Batch Gradient Descent",
      "description": "Equation describing how \beta_i is updated in each iteration."
    },
    {
      "id": "Feature Map Phi",
      "type": "subnode",
      "parent": "Batch Gradient Descent",
      "description": "Mapping function that transforms input data into a higher-dimensional space."
    },
    {
      "id": "Pre-computation Strategy",
      "type": "subnode",
      "parent": "Inner Product Computation",
      "description": "Technique to pre-calculate all pairwise inner products before the main loop starts."
    },
    {
      "id": "Efficient Inner Product Calculation",
      "type": "subnode",
      "parent": "Inner Product Computation",
      "description": "Method for computing inner products without explicitly calculating feature maps."
    },
    {
      "id": "Kernels in Machine Learning",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Introduction to kernel functions and their properties."
    },
    {
      "id": "Kernel Functions",
      "type": "subnode",
      "parent": "Kernels in Machine Learning",
      "description": "Description and properties of kernel functions, their relation to feature maps."
    },
    {
      "id": "Explicit Definition of Kernels",
      "type": "subnode",
      "parent": "Kernel Functions",
      "description": "How kernels can be defined explicitly or implicitly through feature maps."
    },
    {
      "id": "Characterization of Valid Kernels",
      "type": "subnode",
      "parent": "Kernel Functions",
      "description": "Criteria for determining if a function is a valid kernel corresponding to some feature map."
    },
    {
      "id": "Computational Efficiency",
      "type": "subnode",
      "parent": "Kernel Functions",
      "description": "Efficiency of calculating kernel functions compared to direct computation in high-dimensional space."
    },
    {
      "id": "Polynomial Kernels",
      "type": "subnode",
      "parent": "Feature Mapping",
      "description": "Kernels that map data into polynomial feature spaces."
    },
    {
      "id": "Kernels as Similarity Metrics",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Using kernels to measure similarity between inputs."
    },
    {
      "id": "Gaussian Kernel",
      "type": "subnode",
      "parent": "Kernels as Similarity Metrics",
      "description": "A specific kernel function that measures similarity in an infinite dimensional space."
    },
    {
      "id": "Valid Kernels Conditions",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Criteria for a function to be considered a valid kernel."
    },
    {
      "id": "Feature Extraction for Strings",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Techniques to extract features from variable-length string data."
    },
    {
      "id": "Support Vector Machines (SVM)",
      "type": "subnode",
      "parent": "Kernel Methods",
      "description": "A supervised learning model for classification and regression analysis."
    },
    {
      "id": "Kernel Matrix Properties",
      "type": "major",
      "parent": null,
      "description": "Properties of the kernel matrix in machine learning."
    },
    {
      "id": "Sufficient Conditions for Valid Kernels",
      "type": "subnode",
      "parent": "Kernel Matrix Properties",
      "description": "Conditions that are both necessary and sufficient for a function to be a valid kernel."
    },
    {
      "id": "Mercer's Theorem",
      "type": "subnode",
      "parent": "Sufficient Conditions for Valid Kernels",
      "description": "Theorem stating necessary and sufficient conditions for a kernel to be valid."
    },
    {
      "id": "Testing Kernel Validity",
      "type": "subnode",
      "parent": "Sufficient Conditions for Valid Kernels",
      "description": "Methods to test if a given function is a valid kernel."
    },
    {
      "id": "Examples of Kernels in Practice",
      "type": "major",
      "parent": null,
      "description": "Practical applications and examples of kernels in machine learning problems."
    },
    {
      "id": "Necessary Conditions for Valid Kernels",
      "type": "subnode",
      "parent": "Kernel Functions",
      "description": "Conditions a kernel must meet to correspond to some feature mapping."
    },
    {
      "id": "Symmetry Property",
      "type": "subnode",
      "parent": "Necessary Conditions for Valid Kernels",
      "description": "A valid kernel matrix is symmetric."
    },
    {
      "id": "Positive Semi-Definiteness",
      "type": "subnode",
      "parent": "Necessary Conditions for Valid Kernels",
      "description": "A valid kernel matrix must be positive semi-definite."
    },
    {
      "id": "Kernel Matrix",
      "type": "subnode",
      "parent": "Necessary Conditions for Valid Kernels",
      "description": "Matrix representation of a kernel function over a set of points."
    },
    {
      "id": "Functional Margins",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Measure of confidence for predictions based on distance from decision boundary."
    },
    {
      "id": "Geometric Margins",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Concept explaining the geometric interpretation of margins in machine learning models."
    },
    {
      "id": "Support Vector Machines (SVMs)",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "A type of machine learning model used for classification and regression analysis."
    },
    {
      "id": "Notation for SVMs",
      "type": "subnode",
      "parent": "Support Vector Machines (SVMs)",
      "description": "Introduction of notation using w and b parameters for linear classifiers."
    },
    {
      "id": "Functional Margin",
      "type": "subnode",
      "parent": "Support Vector Machines (SVMs)",
      "description": "Measure indicating the confidence and correctness of predictions."
    },
    {
      "id": "Geometric Margin",
      "type": "subnode",
      "parent": "Support Vector Machines (SVMs)",
      "description": "Smallest margin between the decision boundary and closest data points in training set."
    },
    {
      "id": "Confidence Measure Limitation",
      "type": "subnode",
      "parent": "Functional Margin",
      "description": "Explains why functional margin is not a reliable measure of confidence."
    },
    {
      "id": "Normalization Condition",
      "type": "subnode",
      "parent": "Confidence Measure Limitation",
      "description": "Proposed normalization to improve the reliability of the functional margin."
    },
    {
      "id": "Function Margin with Training Set",
      "type": "subnode",
      "parent": "Functional Margin",
      "description": "Definition and calculation of function margin for a set of training examples."
    },
    {
      "id": "Margins",
      "type": "subnode",
      "parent": "Support Vector Machines (SVM)",
      "description": "Concept of margins in SVMs to maximize the distance between classes."
    },
    {
      "id": "Optimal Margin Classifier",
      "type": "subnode",
      "parent": "Support Vector Machines (SVM)",
      "description": "Classifier that maximizes geometric margin to achieve confident predictions on linearly separable data sets."
    },
    {
      "id": "Lagrange Duality",
      "type": "subnode",
      "parent": "Support Vector Machines (SVM)",
      "description": "Theory for transforming constrained optimization problems into dual form."
    },
    {
      "id": "Kernels",
      "type": "subnode",
      "parent": "Support Vector Machines (SVM)",
      "description": "Technique for handling non-linearly separable data in high-dimensional spaces."
    },
    {
      "id": "SMO Algorithm",
      "type": "subnode",
      "parent": "Support Vector Machines (SVM)",
      "description": "Algorithm that updates two Lagrange multipliers simultaneously to solve the dual optimization problem."
    },
    {
      "id": "Vector w",
      "type": "subnode",
      "parent": "Decision Boundary",
      "description": "Orthogonal to the decision boundary and points towards positive class."
    },
    {
      "id": "Distance to Decision Boundary",
      "type": "subnode",
      "parent": "Decision Boundary",
      "description": "Calculated using vector projection and hyperplane equation."
    },
    {
      "id": "Unit Vector w/||w||",
      "type": "subnode",
      "parent": "Vector w",
      "description": "Normalized version of vector w pointing in the same direction."
    },
    {
      "id": "Model Parameters (w, b)",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Parameters used to define the decision boundary in a linear classifier."
    },
    {
      "id": "Scaling Constraint on w",
      "type": "subnode",
      "parent": "Model Parameters (w, b)",
      "description": "Arbitrary scaling constraints that can be imposed on parameter w without changing model behavior."
    },
    {
      "id": "Maximizing Geometric Margin",
      "type": "subnode",
      "parent": "Optimal Margin Classifier",
      "description": "Process of finding decision boundary with maximum margin between positive and negative examples."
    },
    {
      "id": "Support Vector Machine (SVM)",
      "type": "major",
      "parent": null,
      "description": "Binary classification model maximizing margin between classes."
    },
    {
      "id": "Non-Convex Constraint",
      "type": "subnode",
      "parent": "Optimization Problem",
      "description": "Constraint that complicates optimization problem."
    },
    {
      "id": "Scaling Constraint",
      "type": "subnode",
      "parent": "Optimization Problem",
      "description": "Constraint to simplify the objective function."
    },
    {
      "id": "Objective Function Transformation",
      "type": "subnode",
      "parent": "Optimization Problem",
      "description": "Transformation of the original problem into a convex one."
    },
    {
      "id": "Constrained Optimization",
      "type": "major",
      "parent": null,
      "description": "Optimization problems with equality and inequality constraints."
    },
    {
      "id": "Lagrange Multipliers",
      "type": "subnode",
      "parent": "Constrained Optimization",
      "description": "Variables used in the SMO algorithm to optimize the SVM's objective function."
    },
    {
      "id": "Generalized Lagrangian",
      "type": "subnode",
      "parent": "Constrained Optimization",
      "description": "Function combining the objective and constraint functions with multipliers."
    },
    {
      "id": "Primal Problem",
      "type": "subnode",
      "parent": "Constrained Optimization",
      "description": "Optimization problem defined by maximizing over alpha and beta while minimizing over w."
    },
    {
      "id": "\\(\\theta_{\\cal P}(w)\\)",
      "type": "subnode",
      "parent": "Generalized Lagrangian",
      "description": "Maximum value of the generalized Lagrangian for given \\(w\\)."
    },
    {
      "id": "Convex Quadratic Objective",
      "type": "subnode",
      "parent": "Optimization Problem",
      "description": "Objective function in the form of a convex quadratic equation."
    },
    {
      "id": "Linear Constraints",
      "type": "subnode",
      "parent": "Optimization Problem",
      "description": "Constraints that are linear equations or inequalities."
    },
    {
      "id": "Dual Formulation",
      "type": "subnode",
      "parent": "Lagrange Duality",
      "description": "Alternative formulation of the original problem that simplifies computation."
    },
    {
      "id": "Optimization Problems",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Formulation and solution of optimization problems in ML."
    },
    {
      "id": "KKT Conditions",
      "type": "subnode",
      "parent": "Optimization Problems",
      "description": "Conditions ensuring optimality in constrained optimization problems"
    },
    {
      "id": "Dual Complementarity Condition",
      "type": "subnode",
      "parent": "KKT Conditions",
      "description": "Condition indicating active constraints in optimization problems."
    },
    {
      "id": "Support Vectors",
      "type": "subnode",
      "parent": "Optimization Problems",
      "description": "Points that lie on the decision boundary and influence the optimal solution."
    },
    {
      "id": "SVM Optimization Problem",
      "type": "subnode",
      "parent": "Optimization Problems",
      "description": "Primal and dual forms of SVM optimization problem."
    },
    {
      "id": "Dual Problem",
      "type": "major",
      "parent": null,
      "description": "Optimization problem where the order of max and min operations is reversed compared to the primal problem."
    },
    {
      "id": "Objective Function Primal",
      "type": "subnode",
      "parent": "Primal Problem",
      "description": "Function \u03b8\u211aP(w) that represents the objective in the primal problem."
    },
    {
      "id": "Objective Function Dual",
      "type": "subnode",
      "parent": "Dual Problem",
      "description": "Function \u03b8\u211aD(\u03b1,\u03b2) representing the dual optimization's objective."
    },
    {
      "id": "Primal Constraints",
      "type": "subnode",
      "parent": "Primal Problem",
      "description": "Constraints that w must satisfy in the primal problem."
    },
    {
      "id": "Dual Constraints",
      "type": "subnode",
      "parent": "Dual Problem",
      "description": "Non-negativity constraints on \u03b1 for the dual problem."
    },
    {
      "id": "Lagrangian Function",
      "type": "major",
      "parent": null,
      "description": "Function used to derive dual problem in SVMs with L1 regularization."
    },
    {
      "id": "Value Primal Problem",
      "type": "subnode",
      "parent": "Primal Problem",
      "description": "Optimal value p* of the objective function for the primal problem."
    },
    {
      "id": "Value Dual Problem",
      "type": "subnode",
      "parent": "Dual Problem",
      "description": "Optimal value d* of the dual problem's objective function."
    },
    {
      "id": "Machine Learning Theory",
      "type": "major",
      "parent": null,
      "description": "Theoretical foundations of machine learning including generalization and hypothesis classes."
    },
    {
      "id": "Duality Gap",
      "type": "subnode",
      "parent": "Optimization Problems",
      "description": "Difference between primal and dual problem solutions."
    },
    {
      "id": "Convex Functions",
      "type": "subnode",
      "parent": "Optimization Problems",
      "description": "Functions where the line segment between any two points on the graph lies above or on the function."
    },
    {
      "id": "Affine Constraints",
      "type": "subnode",
      "parent": "Optimization Problems",
      "description": "Linear constraints that can be shifted by a constant."
    },
    {
      "id": "Feasibility Conditions",
      "type": "subnode",
      "parent": "Optimization Problems",
      "description": "Conditions ensuring the existence of solutions satisfying all constraints."
    },
    {
      "id": "Karush-Kuhn-Tucker (KKT) Conditions",
      "type": "subnode",
      "parent": "Optimization Problems",
      "description": "Necessary conditions for a solution in nonlinear programming to be optimal."
    },
    {
      "id": "Dual Form of Problem",
      "type": "major",
      "parent": null,
      "description": "Formulation focusing on Lagrange multipliers to solve optimization problem."
    },
    {
      "id": "Inner Products",
      "type": "subnode",
      "parent": "Kernel Trick",
      "description": "Key to the kernel trick, expressed as (x^(i))^T x^(j)."
    },
    {
      "id": "Optimization Constraints",
      "type": "subnode",
      "parent": "Lagrangian Function",
      "description": "Constraints on optimization variables in machine learning models."
    },
    {
      "id": "Support Vectors Definition",
      "type": "subnode",
      "parent": "Support Vectors",
      "description": "Points with non-zero alpha values that define the decision boundary."
    },
    {
      "id": "Lagrangian Optimization",
      "type": "major",
      "parent": null,
      "description": "Optimization involving Lagrangian function for SVMs"
    },
    {
      "id": "Dual Problem Formulation",
      "type": "subnode",
      "parent": "Lagrangian Optimization",
      "description": "Formulating the dual problem from primal constraints and objective"
    },
    {
      "id": "Optimal Parameters Alpha",
      "type": "subnode",
      "parent": "Dual Problem Formulation",
      "description": "Finding optimal alpha values to maximize the dual objective function"
    },
    {
      "id": "Recovering w from Alpha",
      "type": "subnode",
      "parent": "Lagrangian Optimization",
      "description": "Using optimal alphas to find the optimal weight vector w"
    },
    {
      "id": "Optimal Intercept b",
      "type": "subnode",
      "parent": "Lagrangian Optimization",
      "description": "Calculating the intercept term using the primal problem constraints"
    },
    {
      "id": "Optimal Parameters Calculation",
      "type": "subnode",
      "parent": "Support Vector Machines (SVM)",
      "description": "Calculation of optimal parameters w and b from training data."
    },
    {
      "id": "Intercept Term Calculation",
      "type": "subnode",
      "parent": "Optimal Parameters Calculation",
      "description": "Finding the intercept term b using the primal problem solution."
    },
    {
      "id": "Prediction Equation",
      "type": "subnode",
      "parent": "Optimal Parameters Calculation",
      "description": "Equation for prediction based on inner product and support vectors."
    },
    {
      "id": "Dual Form Insight",
      "type": "subnode",
      "parent": "Support Vector Machines (SVM)",
      "description": "Insight gained from the dual form of optimization problem."
    },
    {
      "id": "Kernel Application",
      "type": "subnode",
      "parent": "Support Vector Machines (SVM)",
      "description": "Application of kernels to classification problems in SVMs."
    },
    {
      "id": "Non-separable Case",
      "type": "subnode",
      "parent": "Support Vector Machines",
      "description": "Handling datasets that are not linearly separable with SVMs."
    },
    {
      "id": "L1 Regularization",
      "type": "subnode",
      "parent": "Regularization",
      "description": "Penalizes the sum of absolute values of coefficients to handle outliers."
    },
    {
      "id": "Dual Formulation of SVM",
      "type": "subnode",
      "parent": "Support Vector Machines (SVM)",
      "description": "Optimization problem reformulated to solve for Lagrange multipliers."
    },
    {
      "id": "Sequential Minimal Optimization (SMO) Algorithm",
      "type": "subnode",
      "parent": "Support Vector Machines (SVM)",
      "description": "Efficient algorithm to solve the dual problem of SVM optimization."
    },
    {
      "id": "Coordinate Ascent Algorithm",
      "type": "subnode",
      "parent": "Machine Learning Algorithms",
      "description": "Optimization technique for solving unconstrained problems by iteratively optimizing one variable at a time."
    },
    {
      "id": "Machine Learning Optimization Techniques",
      "type": "major",
      "parent": null,
      "description": "Techniques for optimizing functions in machine learning problems."
    },
    {
      "id": "Quadratic Function Contours",
      "type": "subnode",
      "parent": "Coordinate Ascent",
      "description": "Visual representation of the contours of a quadratic function being optimized."
    },
    {
      "id": "Dual Optimization Problem",
      "type": "subnode",
      "parent": "Support Vector Machines (SVMs)",
      "description": "The optimization problem in the dual form for SVMs."
    },
    {
      "id": "Sequential Minimal Optimization (SMO)",
      "type": "subnode",
      "parent": "Machine Learning Algorithms",
      "description": "Algorithm for solving the optimization problem in SVM efficiently."
    },
    {
      "id": "Convergence Criteria",
      "type": "subnode",
      "parent": "Sequential Minimal Optimization (SMO)",
      "description": "Conditions to determine if SMO has reached a solution."
    },
    {
      "id": "Efficient Update Mechanism",
      "type": "subnode",
      "parent": "Sequential Minimal Optimization (SMO)",
      "description": "Method to update alpha values efficiently in SMO."
    },
    {
      "id": "Constraints Handling",
      "type": "subnode",
      "parent": "Efficient Update Mechanism",
      "description": "Management of constraints during the optimization process."
    },
    {
      "id": "Alpha Variables",
      "type": "subnode",
      "parent": "Optimization Constraints",
      "description": "Variables \u03b11 and \u03b22 used to define constraints."
    },
    {
      "id": "Quadratic Function",
      "type": "subnode",
      "parent": "Optimization Constraints",
      "description": "Function representing the objective in terms of quadratic form."
    },
    {
      "id": "Box Constraint",
      "type": "subnode",
      "parent": "Optimization Constraints",
      "description": "Constraints defining permissible values for \u03b12 within a specified range [L, H]."
    },
    {
      "id": "Derivation Example",
      "type": "subnode",
      "parent": "Alpha Variables",
      "description": "Example of deriving \u03b11 as a function of \u03b12 and y^(2)."
    },
    {
      "id": "Machine Learning Overview",
      "type": "major",
      "parent": null,
      "description": "General introduction to machine learning concepts and algorithms."
    },
    {
      "id": "Alpha Value Update",
      "type": "subnode",
      "parent": "SMO Algorithm",
      "description": "Process of updating alpha values within SMO constraints."
    },
    {
      "id": "Deep Learning Introduction",
      "type": "major",
      "parent": null,
      "description": "Introduction to deep learning concepts and neural networks."
    },
    {
      "id": "Supervised Learning with Non-Linear Models",
      "type": "subnode",
      "parent": "Deep Learning Introduction",
      "description": "Exploration of non-linear models in supervised learning context."
    },
    {
      "id": "Non-linear Model h_\u03b8(x)",
      "type": "subnode",
      "parent": "Machine Learning Models",
      "description": "An abstract non-linear model for regression and classification tasks."
    },
    {
      "id": "Training Examples",
      "type": "subnode",
      "parent": "Machine Learning Models",
      "description": "Set of training data used to learn the model parameters."
    },
    {
      "id": "Regression Problems",
      "type": "subnode",
      "parent": "Machine Learning Models",
      "description": "Problems where output is a real number, using least square cost function."
    },
    {
      "id": "Mean-Square Cost Function",
      "type": "subnode",
      "parent": "Regression Problems",
      "description": "Cost function for regression tasks, averaged over all training examples."
    },
    {
      "id": "Average Loss",
      "type": "subnode",
      "parent": "Loss Function",
      "description": "Total loss divided by number of examples."
    },
    {
      "id": "Conditional Probabilistic Models",
      "type": "subnode",
      "parent": "Negative Log-Likelihood",
      "description": "Models where output distribution depends on input features."
    },
    {
      "id": "Optimizers",
      "type": "major",
      "parent": null,
      "description": "Algorithms for minimizing loss functions."
    },
    {
      "id": "Gradient Descent (GD)",
      "type": "subnode",
      "parent": "Optimizers",
      "description": "Iterative optimization algorithm using gradients of the function."
    },
    {
      "id": "Stochastic Gradient Descent (SGD)",
      "type": "subnode",
      "parent": "Optimizers",
      "description": "Variant of GD that uses a single example for each update."
    },
    {
      "id": "Learning Rate",
      "type": "subnode",
      "parent": "Gradient Descent (GD)",
      "description": "Hyperparameter controlling the size of steps in gradient descent."
    },
    {
      "id": "Negative Likelihood Loss Function",
      "type": "subnode",
      "parent": "Logistic Regression",
      "description": "Loss function derived from negative log-likelihood for binary classification."
    },
    {
      "id": "Total Loss Function",
      "type": "subnode",
      "parent": "Logistic Regression",
      "description": "Average of loss functions over individual training examples."
    },
    {
      "id": "Logits in Multi-class",
      "type": "subnode",
      "parent": "Multi-class Classification",
      "description": "Output of the model before applying softmax function, representing predictions for each class."
    },
    {
      "id": "Negative Log-likelihood Loss Function (Multi-class)",
      "type": "subnode",
      "parent": "Multi-class Classification",
      "description": "Loss function derived from negative log-likelihood in multi-class classification."
    },
    {
      "id": "Single Neuron Network",
      "type": "subnode",
      "parent": "Neural Networks",
      "description": "A basic network with a single neuron for simple predictions."
    },
    {
      "id": "Housing Price Prediction",
      "type": "subnode",
      "parent": "Single Neuron Network",
      "description": "Predicting housing prices based on derived features such as family size, walkability, and school quality."
    },
    {
      "id": "ReLU Function",
      "type": "subnode",
      "parent": "Single Neuron Network",
      "description": "Element-wise non-linear transformation using the Rectified Linear Unit (ReLU) function."
    },
    {
      "id": "Activation Functions",
      "type": "subnode",
      "parent": "Neural Networks",
      "description": "Functions that introduce non-linearity in the network, such as ReLU."
    },
    {
      "id": "Single Neuron Model",
      "type": "subnode",
      "parent": "Neural Networks",
      "description": "Model with a single neuron including weight vector, bias term, and activation function."
    },
    {
      "id": "Stacking Neurons",
      "type": "subnode",
      "parent": "Neural Networks",
      "description": "Process of combining multiple neurons to form complex neural networks."
    },
    {
      "id": "Housing Prediction Example",
      "type": "subnode",
      "parent": "Stacking Neurons",
      "description": "Illustration using housing price prediction with multiple features and layers."
    },
    {
      "id": "Mini-batch SGD",
      "type": "subnode",
      "parent": "Machine Learning Algorithms",
      "description": "Variant of SGD that uses small batches of data for updates."
    },
    {
      "id": "Hyperparameters",
      "type": "subnode",
      "parent": "Stochastic Gradient Descent (SGD)",
      "description": "Learning rate and number of iterations in the algorithm."
    },
    {
      "id": "Mini-batch Hyperparameters",
      "type": "subnode",
      "parent": "Mini-batch SGD",
      "description": "Includes learning rate, batch size, and number of iterations."
    },
    {
      "id": "Parameter Initialization",
      "type": "subnode",
      "parent": "Machine Learning Algorithms",
      "description": "Random initialization of parameters before training starts."
    },
    {
      "id": "Mini-batch Gradient Calculation",
      "type": "subnode",
      "parent": "Mini-batch SGD",
      "description": "Gradient calculation using multiple examples simultaneously."
    },
    {
      "id": "Deep Learning Model Training Steps",
      "type": "subnode",
      "parent": "Machine Learning Algorithms",
      "description": "Steps to train a deep learning model including parametrization and backpropagation."
    },
    {
      "id": "Neural Networks Overview",
      "type": "major",
      "parent": null,
      "description": "Introduction to neural networks as non-linear models for regression and classification problems."
    },
    {
      "id": "Parameters (\u03b8)",
      "type": "subnode",
      "parent": "Neural Networks",
      "description": "Set of parameters that define the model's behavior and are learned during training."
    },
    {
      "id": "Biological Inspiration",
      "type": "subnode",
      "parent": "Neural Networks",
      "description": "Comparison between artificial neural networks and biological neural systems."
    },
    {
      "id": "Two-Layer Neural Network",
      "type": "subnode",
      "parent": "Neural Networks",
      "description": "A simple model with two layers for predicting housing prices using ReLU activation function."
    },
    {
      "id": "Derived Features",
      "type": "subnode",
      "parent": "Housing Price Prediction",
      "description": "Features like family size, walkable neighborhood, and school quality that influence house price."
    },
    {
      "id": "Family Size",
      "type": "subnode",
      "parent": "Derived Features",
      "description": "Size of the household based on house dimensions and number of bedrooms."
    },
    {
      "id": "Walkability",
      "type": "subnode",
      "parent": "Derived Features",
      "description": "Measure of how easily one can walk to amenities like grocery stores in a neighborhood."
    },
    {
      "id": "School Quality",
      "type": "subnode",
      "parent": "Derived Features",
      "description": "Quality of local elementary schools based on neighborhood wealth and zip code."
    },
    {
      "id": "Neural Network Inputs",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Input features for a neural network, such as house dimensions and number of bedrooms."
    },
    {
      "id": "Hidden Units",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Intermediate variables (hidden units) in the neural network that process input features."
    },
    {
      "id": "ReLU Activation Function",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Rectified Linear Unit activation function used for hidden layers to introduce non-linearity."
    },
    {
      "id": "Output Layer",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Final layer of the neural network that produces the output based on processed input features."
    },
    {
      "id": "Vectorization in Neural Networks",
      "type": "major",
      "parent": null,
      "description": "Process of converting loops into matrix operations for efficiency."
    },
    {
      "id": "Matrix Algebra",
      "type": "subnode",
      "parent": "Vectorization in Neural Networks",
      "description": "Use of matrices and vectors to represent neural network computations."
    },
    {
      "id": "BLAS Packages",
      "type": "subnode",
      "parent": "Vectorization in Neural Networks",
      "description": "Highly optimized numerical linear algebra libraries for fast computation."
    },
    {
      "id": "Two-Layer Fully-Connected Network",
      "type": "subnode",
      "parent": "Vectorization in Neural Networks",
      "description": "Example of a neural network structure used to illustrate vectorization concepts."
    },
    {
      "id": "Weight Matrix W^[1]",
      "type": "subnode",
      "parent": "Two-Layer Fully-Connected Network",
      "description": "Matrix representation of weights connecting input layer to hidden layer."
    },
    {
      "id": "Fully-Connected Neural Networks",
      "type": "subnode",
      "parent": "Machine Learning Models",
      "description": "A type of neural network where each neuron is connected to every neuron in the previous layer."
    },
    {
      "id": "Intermediate Variables (a_i)",
      "type": "subnode",
      "parent": "Fully-Connected Neural Networks",
      "description": "Variables that depend on all inputs and are used for computation within layers."
    },
    {
      "id": "Vectorization",
      "type": "subnode",
      "parent": "Machine Learning Models",
      "description": "Process of using matrix and vector notations to simplify expressions and improve computational efficiency."
    },
    {
      "id": "Weight Matrices",
      "type": "subnode",
      "parent": "Two-Layer Neural Network",
      "description": "Explanation of weight matrices in the context of layers W^[1] and W^[2]."
    },
    {
      "id": "Bias Vectors",
      "type": "subnode",
      "parent": "Two-Layer Neural Network",
      "description": "Description of bias vectors b^[1] and b^[2] used in each layer."
    },
    {
      "id": "Hidden Layer",
      "type": "subnode",
      "parent": "Two-Layer Neural Network",
      "description": "The activation a as the hidden layer output from ReLU function."
    },
    {
      "id": "Multi-layer Networks",
      "type": "subnode",
      "parent": "Neural Networks",
      "description": "Introduction to multi-layer fully-connected neural networks with more than two layers."
    },
    {
      "id": "Multi-layer Fully-Connected Neural Networks",
      "type": "major",
      "parent": null,
      "description": "Stacking layers to form deeper neural networks with ReLU activation functions."
    },
    {
      "id": "Weight Matrices and Biases",
      "type": "subnode",
      "parent": "Multi-layer Fully-Connected Neural Networks",
      "description": "Dimensions of weight matrices and biases for compatibility in multi-layer networks."
    },
    {
      "id": "Total Number of Neurons",
      "type": "subnode",
      "parent": "Multi-layer Fully-Connected Neural Networks",
      "description": "Summation of neurons across all layers in the network."
    },
    {
      "id": "Total Number of Parameters",
      "type": "subnode",
      "parent": "Multi-layer Fully-Connected Neural Networks",
      "description": "Calculation of total parameters including weights and biases."
    },
    {
      "id": "Notational Consistency",
      "type": "subnode",
      "parent": "Multi-layer Fully-Connected Neural Networks",
      "description": "Using consistent notation for input and output layers."
    },
    {
      "id": "Other Activation Functions",
      "type": "major",
      "parent": null,
      "description": "Alternative non-linear functions to ReLU in neural networks."
    },
    {
      "id": "Feature Engineering",
      "type": "subnode",
      "parent": "Machine Learning Overview",
      "description": "Process of selecting and transforming raw data into features that improve model performance."
    },
    {
      "id": "Neural Networks Parameters",
      "type": "subnode",
      "parent": "Deep Learning",
      "description": "Parameters in neural networks, including weights and biases."
    },
    {
      "id": "Learned Features",
      "type": "subnode",
      "parent": "Deep Learning",
      "description": "Features automatically discovered by deep learning models without explicit feature engineering."
    },
    {
      "id": "Sigmoid Function",
      "type": "subnode",
      "parent": "Activation Functions",
      "description": "Maps real numbers to (0, 1) range; less commonly used due to vanishing gradient problem."
    },
    {
      "id": "Tanh Function",
      "type": "subnode",
      "parent": "Activation Functions",
      "description": "Similar to sigmoid but maps to (-1, 1); also suffers from vanishing gradients."
    },
    {
      "id": "Leaky ReLU",
      "type": "subnode",
      "parent": "Activation Functions",
      "description": "Variant of ReLU with a small gradient for negative inputs."
    },
    {
      "id": "GELU Function",
      "type": "subnode",
      "parent": "Activation Functions",
      "description": "Gaussian Error Linear Unit; used in advanced NLP models like BERT and GPT."
    },
    {
      "id": "Softplus Function",
      "type": "subnode",
      "parent": "Activation Functions",
      "description": "Smoothed ReLU with a proper second-order derivative but less practical use."
    },
    {
      "id": "Identity Function",
      "type": "subnode",
      "parent": "Activation Functions",
      "description": "Function that outputs the input directly; not used due to lack of non-linearity."
    },
    {
      "id": "Deep Learning Representations",
      "type": "major",
      "parent": null,
      "description": "Discusses how neural networks automatically discover useful features for prediction."
    },
    {
      "id": "House Price Prediction Example",
      "type": "subnode",
      "parent": "Deep Learning Representations",
      "description": "Illustrates the use of fully-connected neural networks in predicting house prices without specifying intermediate quantities."
    },
    {
      "id": "Feature Maps and Representation Transferability",
      "type": "subnode",
      "parent": "Deep Learning Representations",
      "description": "Explains how feature maps from one dataset can be useful for other datasets, indicating essential data information."
    },
    {
      "id": "Complex Features in Neural Networks",
      "type": "subnode",
      "parent": "Deep Learning Representations",
      "description": "Discusses the difficulty of human interpretation of complex features discovered by neural networks."
    },
    {
      "id": "Matrix Multiplication as a Building Block",
      "type": "subnode",
      "parent": "Modules in Modern Neural Networks",
      "description": "Describes matrix multiplication operation with parameters W and b, operating on an input z."
    },
    {
      "id": "MLP Composition of Modules",
      "type": "subnode",
      "parent": "Modules in Modern Neural Networks",
      "description": "Explains how MLP can be written as a composition of multiple matrix multiplication modules and nonlinear activation modules."
    },
    {
      "id": "Layer Normalization",
      "type": "major",
      "parent": null,
      "description": "Normalization technique that normalizes the inputs in each training example independently."
    },
    {
      "id": "LN-S(z)",
      "type": "subnode",
      "parent": "Layer Normalization",
      "description": "Standardized output of layer normalization before affine transformation."
    },
    {
      "id": "Affine Transformation",
      "type": "subnode",
      "parent": "Layer Normalization",
      "description": "Transformation that scales and shifts the standardized output to desired mean and standard deviation."
    },
    {
      "id": "Scaling-Invariant Property",
      "type": "subnode",
      "parent": "Layer Normalization",
      "description": "Property ensuring model invariance under scaling of parameters in subsequent layers."
    },
    {
      "id": "MLP (Multi-Layer Perceptron)",
      "type": "subnode",
      "parent": "Machine Learning Models",
      "description": "A neural network model composed of multiple layers with matrix multiplication and activation functions."
    },
    {
      "id": "Matrix Multiplication Module",
      "type": "subnode",
      "parent": "MLP (Multi-Layer Perceptron)",
      "description": "Basic building block in MLP, involves linear transformation using weights and biases."
    },
    {
      "id": "Nonlinear Activation Module",
      "type": "subnode",
      "parent": "MLP (Multi-Layer Perceptron)",
      "description": "Applies a nonlinear function to the output of matrix multiplication modules."
    },
    {
      "id": "ResNet (Residual Network)",
      "type": "subnode",
      "parent": "Machine Learning Models",
      "description": "Neural network architecture that uses residual blocks for improved training of deep networks."
    },
    {
      "id": "Residual Block",
      "type": "subnode",
      "parent": "ResNet (Residual Network)",
      "description": "Building block in ResNet, adds input directly to output after nonlinear transformations."
    },
    {
      "id": "Simplified ResNet Architecture",
      "type": "subnode",
      "parent": "ResNet (Residual Network)",
      "description": "A composition of residual blocks followed by a matrix multiplication layer."
    },
    {
      "id": "Machine Learning Architectures",
      "type": "major",
      "parent": null,
      "description": "Overview of different architectures in machine learning."
    },
    {
      "id": "ResNet Architecture",
      "type": "subnode",
      "parent": "Machine Learning Architectures",
      "description": "Deep residual network architecture using convolution layers and batch normalization."
    },
    {
      "id": "Convolutional Layers",
      "type": "subnode",
      "parent": "ResNet Architecture",
      "description": "Layer type commonly used in deep learning for image and signal processing."
    },
    {
      "id": "Batch Normalization Variants",
      "type": "subnode",
      "parent": "ResNet Architecture",
      "description": "Different types of batch normalization techniques used in neural networks."
    },
    {
      "id": "Transformer Architecture",
      "type": "subnode",
      "parent": "Machine Learning Architectures",
      "description": "Architecture widely used in modern large language models."
    },
    {
      "id": "Layer Normalization (LN)",
      "type": "subnode",
      "parent": "Batch Normalization Variants",
      "description": "Normalization technique applied to layers in neural networks."
    },
    {
      "id": "LN-S Module",
      "type": "subnode",
      "parent": "Layer Normalization (LN)",
      "description": "Sub-module of layer normalization that normalizes vector to mean zero and standard deviation one."
    },
    {
      "id": "Affine Transformation in LN",
      "type": "subnode",
      "parent": "Layer Normalization (LN)",
      "description": "Transformation using learnable parameters beta and gamma for desired mean and standard deviation."
    },
    {
      "id": "Parameter Sharing",
      "type": "subnode",
      "parent": "Convolutional Layers",
      "description": "Mechanism where the same filter is applied across different positions of input data."
    },
    {
      "id": "Efficiency Comparison",
      "type": "subnode",
      "parent": "Convolutional Layers",
      "description": "Comparison between convolution and generic matrix multiplication in terms of computational efficiency."
    },
    {
      "id": "Channel Concept",
      "type": "subnode",
      "parent": "Convolutional Layers",
      "description": "Concept describing multiple input/output dimensions within a layer."
    },
    {
      "id": "Convolutional Neural Networks (CNN)",
      "type": "subnode",
      "parent": "Machine Learning",
      "description": "Type of neural network used for image and signal processing."
    },
    {
      "id": "1-D Convolution Layer",
      "type": "subnode",
      "parent": "Convolutional Neural Networks (CNN)",
      "description": "Simplified version of 1-dimensional convolution layer used in CNNs."
    },
    {
      "id": "Filter Vector",
      "type": "subnode",
      "parent": "1-D Convolution Layer",
      "description": "Vector of weights used to extract features from input data."
    },
    {
      "id": "Bias Scalar",
      "type": "subnode",
      "parent": "1-D Convolution Layer",
      "description": "Scalar value added to the output of each neuron in the layer."
    },
    {
      "id": "Matrix Multiplication with Shared Parameters",
      "type": "subnode",
      "parent": "1-D Convolution Layer",
      "description": "Operation where convolution is represented as a matrix multiplication with shared parameters."
    },
    {
      "id": "Conv1D-S Module",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "A 1-dimensional convolutional module with specific parameters."
    },
    {
      "id": "Total Parameters Conv1D",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Calculation of total number of parameters in a Conv1D layer."
    },
    {
      "id": "2-D Convolution (Conv2D-S)",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "A 2-dimensional convolutional module with input and output matrices."
    },
    {
      "id": "Total Parameters Conv2D",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Calculation of total number of parameters in a Conv2D layer."
    },
    {
      "id": "Scale-Invariant Property",
      "type": "subnode",
      "parent": "Layer Normalization (LN)",
      "description": "Property of modern DL architectures regarding weight scaling."
    },
    {
      "id": "Other Normalization Layers",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Alternative normalization techniques used in neural networks."
    },
    {
      "id": "Batch Normalization",
      "type": "subnode",
      "parent": "Other Normalization Layers",
      "description": "Normalization technique commonly used in computer vision applications."
    },
    {
      "id": "Group Normalization",
      "type": "subnode",
      "parent": "Other Normalization Layers",
      "description": "Normalization method for groups of channels in neural networks."
    },
    {
      "id": "Convolutional Neural Networks (CNNs)",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Neural network architecture designed for image and sequence data processing."
    },
    {
      "id": "Differentiable Circuit",
      "type": "subnode",
      "parent": "Backpropagation",
      "description": "Composition of arithmetic operations and elementary functions in a network."
    },
    {
      "id": "Gradient Computation",
      "type": "subnode",
      "parent": "Backpropagation",
      "description": "Process of calculating gradients with respect to the loss function and network parameters during backpropagation."
    },
    {
      "id": "Machine Learning Fundamentals",
      "type": "major",
      "parent": null,
      "description": "Basic concepts and principles of machine learning."
    },
    {
      "id": "Backward Function in Machine Learning",
      "type": "subnode",
      "parent": "Machine Learning Fundamentals",
      "description": "Explanation of the backward function used to compute gradients."
    },
    {
      "id": "Jacobian Matrix",
      "type": "subnode",
      "parent": "Backward Function in Machine Learning",
      "description": "Matrix representation of partial derivatives, not fully detailed here."
    },
    {
      "id": "Chain Rule Application",
      "type": "subnode",
      "parent": "Backward Function in Machine Learning",
      "description": "Use of the chain rule to compute gradients through intermediate variables."
    },
    {
      "id": "Partial Derivatives in ML",
      "type": "subnode",
      "parent": "Machine Learning Fundamentals",
      "description": "Understanding partial derivatives in the context of machine learning functions."
    },
    {
      "id": "Chain Rule for Auto-Differentiation",
      "type": "subnode",
      "parent": "Machine Learning Fundamentals",
      "description": "Application of chain rule to compute gradients efficiently in neural networks."
    },
    {
      "id": "Scalar Functions and Vectors",
      "type": "subnode",
      "parent": "Partial Derivatives in ML",
      "description": "Focus on derivatives involving scalar functions with respect to vectors or matrices."
    },
    {
      "id": "Multi-Variate Function Challenges",
      "type": "subnode",
      "parent": "Partial Derivatives in ML",
      "description": "Challenges and computational difficulties of dealing with multi-variate function derivatives."
    },
    {
      "id": "Chain Rule",
      "type": "subnode",
      "parent": "Backpropagation",
      "description": "Mathematical rule for computing derivatives of composite functions."
    },
    {
      "id": "Loss Function Composition",
      "type": "subnode",
      "parent": "Backpropagation",
      "description": "Abstract representation of loss functions as compositions of modules."
    },
    {
      "id": "Auto-Differentiation",
      "type": "subnode",
      "parent": "Backpropagation",
      "description": "Automatic computation of gradients in deep learning frameworks."
    },
    {
      "id": "Deep Learning Packages",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Software libraries for implementing neural networks and machine learning models."
    },
    {
      "id": "Backpropagation Algorithm",
      "type": "subnode",
      "parent": "Machine Learning Fundamentals",
      "description": "Algorithm for computing gradients in neural networks."
    },
    {
      "id": "Efficiency of Backward Functions",
      "type": "subnode",
      "parent": "Backpropagation Algorithm",
      "description": "Efficient computation of backward functions for atomic modules."
    },
    {
      "id": "Backward Functions",
      "type": "subnode",
      "parent": "Neural Networks",
      "description": "Computation of backward functions for modules used in networks."
    },
    {
      "id": "Matrix Multiplication Module (MM)",
      "type": "subnode",
      "parent": "Backward Functions",
      "description": "Calculation of backward function using equation 7.62 and 7.63."
    },
    {
      "id": "Activations",
      "type": "subnode",
      "parent": "Backward Functions",
      "description": "Computation of backward functions for activation modules."
    },
    {
      "id": "Backward Function Overview",
      "type": "major",
      "parent": null,
      "description": "Overview of backward functions in machine learning."
    },
    {
      "id": "Matrix Multiplication Backward Function",
      "type": "subnode",
      "parent": "Backward Function Overview",
      "description": "Details on the backward function for matrix multiplication operations."
    },
    {
      "id": "Vectorized Notation",
      "type": "subnode",
      "parent": "Matrix Multiplication Backward Function",
      "description": "Explanation of vectorized notation used in backward functions."
    },
    {
      "id": "Efficiency Considerations",
      "type": "subnode",
      "parent": "Matrix Multiplication Backward Function",
      "description": "Discussion on computational efficiency for matrix multiplication operations."
    },
    {
      "id": "Activation Functions Backward Function",
      "type": "subnode",
      "parent": "Backward Function Overview",
      "description": "Details on the backward function for activation functions in neural networks."
    },
    {
      "id": "Binary Classification Problem",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "A specific type of classification problem with two classes"
    },
    {
      "id": "MLP Model",
      "type": "subnode",
      "parent": "Binary Classification Problem",
      "description": "Multi-layer perceptron model used in binary classification"
    },
    {
      "id": "Modules and Parameters",
      "type": "subnode",
      "parent": "MLP Model",
      "description": "Description of modules involved in MLP with parameters or fixed operations"
    },
    {
      "id": "Intermediate Variables",
      "type": "subnode",
      "parent": "Loss Function",
      "description": "Variables used during the computation process in a loss function"
    },
    {
      "id": "Forward Pass",
      "type": "subnode",
      "parent": "Backpropagation",
      "description": "Process of computing intermediate variables sequentially"
    },
    {
      "id": "Backward Pass",
      "type": "subnode",
      "parent": "Backpropagation",
      "description": "Calculation of derivatives in reverse order to compute gradients"
    },
    {
      "id": "Intermediate Values Storage",
      "type": "subnode",
      "parent": "Backpropagation",
      "description": "Storing values like $a^{[i]}$ and $z^{[i]}$ after forward pass."
    },
    {
      "id": "Parallelism in Training Examples",
      "type": "major",
      "parent": null,
      "description": "Using matrix notation to handle multiple training examples simultaneously."
    },
    {
      "id": "Basic Idea",
      "type": "subnode",
      "parent": "Parallelism in Training Examples",
      "description": "Concept of evaluating forward and backward passes for multiple examples using matrices."
    },
    {
      "id": "Backward Function for Loss Functions",
      "type": "subnode",
      "parent": "Machine Learning Fundamentals",
      "description": "Explains the backward function computation for different loss functions."
    },
    {
      "id": "Squared Loss (MSE)",
      "type": "subnode",
      "parent": "Backward Function for Loss Functions",
      "description": "Details the backward function for squared loss or mean squared error."
    },
    {
      "id": "Logistic Loss",
      "type": "subnode",
      "parent": "Backward Function for Loss Functions",
      "description": "Explains the backward pass computation for logistic loss."
    },
    {
      "id": "Machine Learning Loss Functions",
      "type": "major",
      "parent": null,
      "description": "Overview of loss functions used in machine learning models."
    },
    {
      "id": "Cross-Entropy Loss Function",
      "type": "subnode",
      "parent": "Machine Learning Loss Functions",
      "description": "Loss function used in classification problems to measure the dissimilarity between predicted and actual probability distributions."
    },
    {
      "id": "Forward Pass in MLP",
      "type": "subnode",
      "parent": "Back-propagation for MLPs",
      "description": "Sequence of operations that transforms input data into predictions using an MLP model."
    },
    {
      "id": "Matrix Notation in Machine Learning",
      "type": "major",
      "parent": null,
      "description": "Overview of using matrix notation for machine learning operations."
    },
    {
      "id": "Training Examples Representation",
      "type": "subnode",
      "parent": "Matrix Notation in Machine Learning",
      "description": "Representation of multiple training examples in matrix form."
    },
    {
      "id": "First-Layer Activations",
      "type": "subnode",
      "parent": "Training Examples Representation",
      "description": "Calculation of first-layer activations for each example using matrix notation."
    },
    {
      "id": "Broadcasting",
      "type": "subnode",
      "parent": "Vectorization",
      "description": "Technique used for adding a scalar or column vector to each column of a matrix."
    },
    {
      "id": "Generalization to Multiple Layers",
      "type": "subnode",
      "parent": "Matrix Notation in Machine Learning",
      "description": "Extension of the matricization approach to multiple layers with implementation subtleties."
    },
    {
      "id": "Matricization Approach",
      "type": "subnode",
      "parent": "Machine Learning Models",
      "description": "Generalizing the approach to multiple layers with implementation subtleties."
    },
    {
      "id": "Implementation Subtlety",
      "type": "subnode",
      "parent": "Matricization Approach",
      "description": "Handling data points as rows in deep learning packages vs columns in papers."
    },
    {
      "id": "Data Matrix Representation",
      "type": "subnode",
      "parent": "Implementation Subtlety",
      "description": "Conversion between row and column vector representations for consistency."
    },
    {
      "id": "Generalization and Regularization",
      "type": "major",
      "parent": null,
      "description": "Analyzing model performance on unseen test data."
    },
    {
      "id": "Training Loss Function",
      "type": "subnode",
      "parent": "Generalization and Regularization",
      "description": "Function used to fit the training dataset for supervised learning problems."
    },
    {
      "id": "Training Loss",
      "type": "subnode",
      "parent": "Loss Functions",
      "description": "The loss calculated on the training dataset, also known as empirical loss or risk."
    },
    {
      "id": "Test Error",
      "type": "subnode",
      "parent": "Loss Functions",
      "description": "Evaluation metric for a model's performance on unseen data, representing true generalization ability."
    },
    {
      "id": "Mean Squared Error (MSE)",
      "type": "subnode",
      "parent": "Training Loss",
      "description": "A specific loss function that measures the average squared difference between predicted and actual values."
    },
    {
      "id": "Empirical Distribution",
      "type": "subnode",
      "parent": "Loss Functions",
      "description": "The distribution of training data, used to approximate population distribution for empirical risk minimization."
    },
    {
      "id": "Population Distribution",
      "type": "subnode",
      "parent": "Loss Functions",
      "description": "True underlying distribution from which test examples are drawn, representing the target generalization goal."
    },
    {
      "id": "Training vs Test Distributions",
      "type": "subnode",
      "parent": "Machine Learning Basics",
      "description": "Difference between training and test distributions and its impact on model performance."
    },
    {
      "id": "Generalization Gap",
      "type": "subnode",
      "parent": "Training vs Test Distributions",
      "description": "Difference between training error and test error indicating model's ability to generalize."
    },
    {
      "id": "Bias-Variance Tradeoff",
      "type": "major",
      "parent": null,
      "description": "Decomposition of test error into bias and variance components for understanding overfitting and underfitting."
    },
    {
      "id": "Double Descent Phenomenon",
      "type": "subnode",
      "parent": "Bias-Variance Tradeoff",
      "description": "Phenomenon where test errors decrease, increase, then decrease again with increasing model parameters or data samples."
    },
    {
      "id": "Training and Test Datasets",
      "type": "subnode",
      "parent": "Bias-Variance Tradeoff",
      "description": "Illustration of datasets used to evaluate model performance."
    },
    {
      "id": "Linear Regression Models",
      "type": "subnode",
      "parent": "Bias-Variance Tradeoff",
      "description": "Examples of linear models and their limitations in capturing non-linear relationships."
    },
    {
      "id": "Quadratic Function Example",
      "type": "subnode",
      "parent": "Training and Test Datasets",
      "description": "Example using a quadratic function to demonstrate bias-variance tradeoff concepts."
    },
    {
      "id": "Polynomial Fitting",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Fitting polynomials to data sets and the issues that arise."
    },
    {
      "id": "Variance",
      "type": "subnode",
      "parent": "Polynomial Fitting",
      "description": "Measure of variability in models trained on different datasets from the same distribution."
    },
    {
      "id": "Linear Model Limitations",
      "type": "subnode",
      "parent": "Machine Learning Models",
      "description": "Explains the limitations of linear models in capturing data structure."
    },
    {
      "id": "Bias Definition",
      "type": "subnode",
      "parent": "Linear Model Limitations",
      "description": "Defines bias as test error with infinite training data."
    },
    {
      "id": "5th Degree Polynomial Models",
      "type": "subnode",
      "parent": "Machine Learning Models",
      "description": "Discusses the behavior of 5th degree polynomial models with training data."
    },
    {
      "id": "Generalization Failure",
      "type": "subnode",
      "parent": "5th Degree Polynomial Models",
      "description": "Describes the failure of 5th degree polynomials to generalize well on test data."
    },
    {
      "id": "Model Complexity",
      "type": "subnode",
      "parent": "Bias-Variance Tradeoff",
      "description": "Measure of the simplicity or complexity of a model, often related to parameters."
    },
    {
      "id": "Test Error Decomposition",
      "type": "subnode",
      "parent": "Bias-Variance Tradeoff",
      "description": "Breakdown of test error into bias and variance components."
    },
    {
      "id": "Bias Term",
      "type": "subnode",
      "parent": "Bias-Variance Tradeoff",
      "description": "Error due to overly simplistic models unable to capture true relationships."
    },
    {
      "id": "Variance Term",
      "type": "subnode",
      "parent": "Bias-Variance Tradeoff",
      "description": "Error due to model's sensitivity to training data variations."
    },
    {
      "id": "Model-wise Double Descent",
      "type": "subnode",
      "parent": "Double Descent Phenomenon",
      "description": "Peak in test error related to model capacity and sample size relationship."
    },
    {
      "id": "Model Evaluation",
      "type": "subnode",
      "parent": "Machine Learning Fundamentals",
      "description": "Assessment of model performance using metrics like MSE."
    },
    {
      "id": "Average Model (h_avg)",
      "type": "subnode",
      "parent": "Mean Squared Error (MSE)",
      "description": "Hypothetical model representing the average of predictions from an infinite number of datasets."
    },
    {
      "id": "Unavoidable Noise (\u03c3^2)",
      "type": "subnode",
      "parent": "Mean Squared Error (MSE)",
      "description": "Inherent noise in data that cannot be reduced by any model improvement."
    },
    {
      "id": "Training Dataset",
      "type": "subnode",
      "parent": "Regression Problems",
      "description": "Set of data used to train a machine learning model."
    },
    {
      "id": "Test Example",
      "type": "subnode",
      "parent": "Regression Problems",
      "description": "Single instance used for evaluating the performance of a trained model."
    },
    {
      "id": "Expected Test Error",
      "type": "subnode",
      "parent": "Bias-Variance Tradeoff",
      "description": "Average error expected on unseen data after training with random datasets."
    },
    {
      "id": "Claim 8.1.1",
      "type": "subnode",
      "parent": "Mean Squared Error (MSE)",
      "description": "Mathematical tool for decomposing MSE into bias and variance terms."
    },
    {
      "id": "Model Complexity and Test Errors",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Relationship between model complexity and test error performance."
    },
    {
      "id": "Overparameterized Models",
      "type": "subnode",
      "parent": "Double Descent Phenomenon",
      "description": "Models with more parameters than necessary for the training dataset."
    },
    {
      "id": "Sample-wise Double Descent",
      "type": "subnode",
      "parent": "Double Descent Phenomenon",
      "description": "Test error pattern observed as the number of training samples increases."
    },
    {
      "id": "Historical Context and Recent Discoveries",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Overview of historical context and recent advancements in understanding double descent phenomena."
    },
    {
      "id": "Optimal Algorithms",
      "type": "subnode",
      "parent": "Sample-wise Double Descent",
      "description": "Algorithms that can achieve lower test errors when samples are limited."
    },
    {
      "id": "Regularization Tuning",
      "type": "subnode",
      "parent": "Double Descent Phenomenon",
      "description": "Optimal regularization helps mitigate double descent issues."
    },
    {
      "id": "Implicit Regularization",
      "type": "subnode",
      "parent": "Model-wise Double Descent",
      "description": "Optimizer effects like gradient descent provide implicit regularization in overparameterized models."
    },
    {
      "id": "Learning Guarantees",
      "type": "subnode",
      "parent": "Machine Learning Theory",
      "description": "Conditions under which learning algorithms work well."
    },
    {
      "id": "Union Bound Lemma",
      "type": "subnode",
      "parent": "Learning Guarantees",
      "description": "Probability bound for union of events."
    },
    {
      "id": "Hoeffding Inequality (Chernoff Bound)",
      "type": "subnode",
      "parent": "Learning Guarantees",
      "description": "Bound on deviation from true value in Bernoulli distribution."
    },
    {
      "id": "Sample Complexity Bounds",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Theoretical bounds on the number of samples needed for learning algorithms to generalize well."
    },
    {
      "id": "Model Selection Methods",
      "type": "subnode",
      "parent": "Sample Complexity Bounds",
      "description": "Techniques for choosing the right level of model complexity based on training data."
    },
    {
      "id": "Generalization Error",
      "type": "subnode",
      "parent": "Sample Complexity Bounds",
      "description": "Error made by a predictive model when making predictions on data not seen during training."
    },
    {
      "id": "Learning Theory",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Theoretical foundations of machine learning, including generalization bounds and sample complexity."
    },
    {
      "id": "Gradient Descent Optimizer",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Optimizer used to find the minimum norm solution for linear models."
    },
    {
      "id": "Minimum Norm Solution",
      "type": "subnode",
      "parent": "Gradient Descent Optimizer",
      "description": "Solution found by gradient descent with zero initialization in overparameterized regime."
    },
    {
      "id": "Model Complexity Measures",
      "type": "subnode",
      "parent": "Double Descent Phenomenon",
      "description": "Different measures of model complexity such as number of parameters or norm."
    },
    {
      "id": "Number of Parameters",
      "type": "subnode",
      "parent": "Model Complexity Measures",
      "description": "Common measure of model complexity, often leading to double descent phenomenon."
    },
    {
      "id": "Norm of Learned Model",
      "type": "subnode",
      "parent": "Model Complexity Measures",
      "description": "Alternative measure that can mitigate the occurrence of double descent."
    },
    {
      "id": "Regularization Strength",
      "type": "subnode",
      "parent": "Double Descent Phenomenon",
      "description": "Impact of regularization on mitigating double descent phenomenon."
    },
    {
      "id": "Hypothesis Function",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Function that maps input data to predictions."
    },
    {
      "id": "Training Error",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Error measured on the dataset used to train a model."
    },
    {
      "id": "Empirical Risk Minimization (ERM)",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Learning algorithm that minimizes the empirical risk over the training set."
    },
    {
      "id": "PAC Assumptions",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Framework for learning theory assumptions, including same-distribution assumption and i.i.d. samples."
    },
    {
      "id": "Hypothesis Class",
      "type": "subnode",
      "parent": "Machine Learning Basics",
      "description": "Set of all possible hypotheses a learning algorithm can choose from."
    },
    {
      "id": "Finite Hypothesis Classes",
      "type": "subnode",
      "parent": "Hypothesis Class",
      "description": "Learning problems with a finite set of hypotheses."
    },
    {
      "id": "Uniform Convergence",
      "type": "major",
      "parent": null,
      "description": "Property ensuring that the difference between empirical and true errors is small with high probability."
    },
    {
      "id": "Training Error vs Generalization Error",
      "type": "subnode",
      "parent": "Uniform Convergence",
      "description": "Discusses how training error relates to generalization error across different hypotheses."
    },
    {
      "id": "Union Bound Application",
      "type": "subnode",
      "parent": "Uniform Convergence",
      "description": "Uses the union bound to extend a probability result from one hypothesis to all in a class."
    },
    {
      "id": "Probability of Error",
      "type": "subnode",
      "parent": "Training Error vs Generalization Error",
      "description": "Analyzes how error probabilities relate to training and generalization errors."
    },
    {
      "id": "Sample Size Determination",
      "type": "subnode",
      "parent": "Training Error vs Generalization Error",
      "description": "Determines the sample size needed for a given probability of error bound."
    },
    {
      "id": "Empirical Risk Minimization",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Process of selecting a hypothesis with the smallest training error."
    },
    {
      "id": "Generalization Error Guarantees",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Strategies for ensuring that training error closely approximates generalization error."
    },
    {
      "id": "Bernoulli Random Variable Z",
      "type": "subnode",
      "parent": "Generalization Error Guarantees",
      "description": "Random variable indicating whether a hypothesis misclassifies an example drawn from the distribution."
    },
    {
      "id": "Training Set Sampling",
      "type": "subnode",
      "parent": "Bernoulli Random Variable Z",
      "description": "Process of drawing examples independently and identically distributed (iid) from \u0394."
    },
    {
      "id": "Hoeffding Inequality",
      "type": "subnode",
      "parent": "Generalization Error Guarantees",
      "description": "Inequality used to bound the probability that training error deviates significantly from generalization error."
    },
    {
      "id": "Sample Complexity",
      "type": "subnode",
      "parent": "Machine Learning Theory",
      "description": "Number of training examples needed for a certain level of performance guarantee."
    },
    {
      "id": "Hypotheses Space (H)",
      "type": "subnode",
      "parent": "Machine Learning Theory",
      "description": "Set of all possible hypotheses or models considered in a learning problem."
    },
    {
      "id": "Optimal Hypothesis (h*)",
      "type": "subnode",
      "parent": "Machine Learning Theory",
      "description": "Hypothesis with the lowest true error over hypothesis space H."
    },
    {
      "id": "Hypothesis Class Switching",
      "type": "major",
      "parent": null,
      "description": "Discussion on switching to a larger hypothesis class and its effects."
    },
    {
      "id": "Bias Decrease",
      "type": "subnode",
      "parent": "Hypothesis Class Switching",
      "description": "Explains how bias decreases when moving to a larger hypothesis class."
    },
    {
      "id": "Variance Increase",
      "type": "subnode",
      "parent": "Hypothesis Class Switching",
      "description": "Describes the increase in variance with a larger hypothesis class."
    },
    {
      "id": "Sample Complexity Bound",
      "type": "major",
      "parent": null,
      "description": "Derivation of sample complexity bound for finite hypothesis classes."
    },
    {
      "id": "Corollary Proof",
      "type": "subnode",
      "parent": "Sample Complexity Bound",
      "description": "Proof involving fixed \u03b4 and \u03b3 to derive n's value."
    },
    {
      "id": "Infinite Hypothesis Classes",
      "type": "major",
      "parent": null,
      "description": "Introduction to dealing with hypothesis classes parameterized by real numbers."
    },
    {
      "id": "Bit Representation",
      "type": "subnode",
      "parent": "Infinite Hypothesis Classes",
      "description": "Discussion on the finite representation of real numbers in computers using bits."
    },
    {
      "id": "Floating Point Representation",
      "type": "subnode",
      "parent": "Sample Complexity",
      "description": "Use of 64-bit floating point numbers in parameter representation."
    },
    {
      "id": "Hypothesis Class Size",
      "type": "subnode",
      "parent": "Sample Complexity",
      "description": "Size of the hypothesis class based on model parameters."
    },
    {
      "id": "Non-ERM Algorithms",
      "type": "subnode",
      "parent": "Empirical Risk Minimization (ERM)",
      "description": "Learning algorithms that do not rely solely on empirical risk minimization."
    },
    {
      "id": "Hypothesis Space",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "The set of all possible hypotheses that a learner can choose from."
    },
    {
      "id": "Parameterization of Hypotheses",
      "type": "subnode",
      "parent": "Hypothesis Space",
      "description": "Different ways to parameterize the same hypothesis space."
    },
    {
      "id": "VC Dimension",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Measure of the capacity of a statistical model, defined as the cardinality of the largest set of points that the model can shatter."
    },
    {
      "id": "Shattering Sets",
      "type": "subnode",
      "parent": "VC Dimension",
      "description": "A hypothesis class can shatter a set if it can realize any labeling on the set."
    },
    {
      "id": "Vapnik's Theorem",
      "type": "major",
      "parent": null,
      "description": "Theorem linking VC dimension to generalization error bounds"
    },
    {
      "id": "Corollary on Training Examples",
      "type": "major",
      "parent": null,
      "description": "Number of examples needed for learning is linear in VC dimension"
    },
    {
      "id": "Regularization in Deep Learning",
      "type": "major",
      "parent": null,
      "description": "Overview of regularization techniques and their impact on deep learning models."
    },
    {
      "id": "Explicit Regularization Techniques",
      "type": "subnode",
      "parent": "Regularization in Deep Learning",
      "description": "Techniques such as weight decay, dropout, data augmentation, spectral norm regularization, and Lipschitzness regularization."
    },
    {
      "id": "Implicit Regularization Effect",
      "type": "subnode",
      "parent": "Regularization in Deep Learning",
      "description": "The impact of optimizers on model parameters beyond explicit regularization."
    },
    {
      "id": "Regularization in Machine Learning",
      "type": "major",
      "parent": null,
      "description": "Techniques to prevent overfitting by adding a penalty for complexity."
    },
    {
      "id": "Sparsity Regularization",
      "type": "subnode",
      "parent": "Regularization in Machine Learning",
      "description": "Imposing sparsity on model parameters to reduce complexity and improve generalization."
    },
    {
      "id": "L1 Norm (LASSO)",
      "type": "subnode",
      "parent": "Sparsity Regularization",
      "description": "A common relaxation for \u03b8\u20960, promoting sparsity in linear models."
    },
    {
      "id": "L2 Norm Regularization",
      "type": "subnode",
      "parent": "Regularization in Machine Learning",
      "description": "Penalizes the squared magnitude of coefficients to prevent overfitting."
    },
    {
      "id": "Deep Learning Regularization Techniques",
      "type": "subnode",
      "parent": "Regularization in Machine Learning",
      "description": "Various methods to regularize neural networks and improve generalization."
    },
    {
      "id": "Optimizers and Generalization",
      "type": "major",
      "parent": null,
      "description": "Discussion on how optimizers affect model generalization."
    },
    {
      "id": "Global Minima Variability",
      "type": "subnode",
      "parent": "Optimizers and Generalization",
      "description": "Different global minima can lead to different generalization performance."
    },
    {
      "id": "Model Selection via Cross Validation",
      "type": "major",
      "parent": null,
      "description": "Process of selecting models using cross validation techniques."
    },
    {
      "id": "Cross Validation Techniques",
      "type": "subnode",
      "parent": "Model Selection via Cross Validation",
      "description": "Techniques used for model selection through cross validation."
    },
    {
      "id": "Regularized Loss",
      "type": "subnode",
      "parent": "Regularization",
      "description": "Combination of training loss and regularizer term, used in model evaluation."
    },
    {
      "id": "Regularizer",
      "type": "subnode",
      "parent": "Regularization",
      "description": "Function that measures model complexity, often \u03b8 norm."
    },
    {
      "id": "\u03bb (Lambda)",
      "type": "subnode",
      "parent": "Regularized Loss",
      "description": "Parameter controlling the trade-off between loss and regularizer."
    },
    {
      "id": "\u039b_2 Regularization",
      "type": "subnode",
      "parent": "Regularizer",
      "description": "Encourages small \u03b8 norm, also known as weight decay."
    },
    {
      "id": "Weight Decay",
      "type": "subnode",
      "parent": "\u039b_2 Regularization",
      "description": "Effect of gradient descent on regularized loss in deep learning."
    },
    {
      "id": "Inductive Bias",
      "type": "subnode",
      "parent": "Regularization",
      "description": "Structures or biases imposed by regularization to guide model learning."
    },
    {
      "id": "Chapter 9 Regularization and Model Selection",
      "type": "major",
      "parent": null,
      "description": "Focuses on techniques for controlling model complexity."
    },
    {
      "id": "Training Loss/Cost Function",
      "type": "subnode",
      "parent": "Regularization",
      "description": "Function used to evaluate the performance of a model during training."
    },
    {
      "id": "Regularizer Term",
      "type": "subnode",
      "parent": "Regularization",
      "description": "Additional term added to the loss function to control complexity and prevent overfitting."
    },
    {
      "id": "Regularization Parameter (\u03bb)",
      "type": "subnode",
      "parent": "Regularization",
      "description": "Hyperparameter controlling the influence of the regularizer on the overall loss function."
    },
    {
      "id": "Model Selection",
      "type": "major",
      "parent": null,
      "description": "Process of choosing the best model based on validation techniques."
    },
    {
      "id": "Cross Validation",
      "type": "subnode",
      "parent": "Model Selection",
      "description": "Technique to evaluate models and select the one with the best performance."
    },
    {
      "id": "Polynomial Regression Models",
      "type": "subnode",
      "parent": "Cross Validation",
      "description": "Models with varying degrees of polynomial terms for regression analysis."
    },
    {
      "id": "Regularization Parameters",
      "type": "subnode",
      "parent": "Cross Validation",
      "description": "Parameters like C in SVM that control model complexity and prevent overfitting."
    },
    {
      "id": "Machine Learning Techniques",
      "type": "major",
      "parent": null,
      "description": "Various techniques used in machine learning for data analysis and pattern recognition."
    },
    {
      "id": "Validation Set Size",
      "type": "subnode",
      "parent": "Model Selection",
      "description": "Determining an appropriate size for the validation set in machine learning."
    },
    {
      "id": "Hold Out Cross Validation",
      "type": "subnode",
      "parent": "Model Selection",
      "description": "A method where a portion of data is held out as a validation set."
    },
    {
      "id": "k-fold Cross Validation",
      "type": "subnode",
      "parent": "Model Selection",
      "description": "Divides the dataset into k parts, trains on k-1 and tests on 1 part repeatedly."
    },
    {
      "id": "Retraining on Full Dataset",
      "type": "subnode",
      "parent": "Hold Out Cross Validation",
      "description": "Optionally retrain selected model on entire training set after validation."
    },
    {
      "id": "Leave-One-Out Cross Validation",
      "type": "subnode",
      "parent": "Cross Validation",
      "description": "Uses each data point as a test set once while training on all others."
    },
    {
      "id": "Data Scarcity",
      "type": "subnode",
      "parent": "Machine Learning Techniques",
      "description": "Situation where the amount of available data is limited, affecting model evaluation methods."
    },
    {
      "id": "Leave-One-Out CV",
      "type": "subnode",
      "parent": "Cross Validation",
      "description": "Method where one training example is held out at a time."
    },
    {
      "id": "Bayesian Statistics",
      "type": "major",
      "parent": null,
      "description": "Approach to parameter estimation that treats parameters as random variables."
    },
    {
      "id": "MLE",
      "type": "subnode",
      "parent": "Bayesian Statistics",
      "description": "Estimation method where parameters are viewed as constant but unknown values."
    },
    {
      "id": "Prior Distribution",
      "type": "subnode",
      "parent": "Bayesian Statistics",
      "description": "Distribution expressing prior beliefs about the parameters before seeing data."
    },
    {
      "id": "Hold-out Cross Validation",
      "type": "subnode",
      "parent": "Cross Validation",
      "description": "Technique splitting data into training and validation sets for better error estimation."
    },
    {
      "id": "Training Set S",
      "type": "subnode",
      "parent": "Machine Learning Algorithms",
      "description": "Dataset used to train models in machine learning tasks."
    },
    {
      "id": "Hypotheses Training",
      "type": "subnode",
      "parent": "Empirical Risk Minimization",
      "description": "Process of training each model on the full dataset S."
    },
    {
      "id": "Training Error Selection",
      "type": "subnode",
      "parent": "Empirical Risk Minimization",
      "description": "Selection based on minimum training error, often leading to overfitting."
    },
    {
      "id": "Validation Set S_cv",
      "type": "subnode",
      "parent": "Hold-out Cross Validation",
      "description": "Subset of data used for validating model performance after training."
    },
    {
      "id": "Model Selection Based on Validation Error",
      "type": "subnode",
      "parent": "Hold-out Cross Validation",
      "description": "Choosing the best hypothesis based on its error on the validation set."
    },
    {
      "id": "Bayesian Machine Learning",
      "type": "major",
      "parent": null,
      "description": "Predictions made using posterior distribution on parameters."
    },
    {
      "id": "Predictive Distribution",
      "type": "subnode",
      "parent": "Bayesian Machine Learning",
      "description": "Probability distribution of predictions for new examples based on posterior distribution."
    },
    {
      "id": "Fully Bayesian Prediction",
      "type": "subnode",
      "parent": "Bayesian Machine Learning",
      "description": "Prediction method that averages over the posterior distribution of parameters."
    },
    {
      "id": "Computational Challenges",
      "type": "subnode",
      "parent": "Bayesian Machine Learning",
      "description": "Difficulties in computing high-dimensional integrals for posterior distributions."
    },
    {
      "id": "k-means Algorithm",
      "type": "major",
      "parent": null,
      "description": "Clustering algorithm that partitions data into k clusters."
    },
    {
      "id": "Initialization",
      "type": "subnode",
      "parent": "k-means Algorithm",
      "description": "Randomly selecting initial cluster centroids."
    },
    {
      "id": "Convergence",
      "type": "subnode",
      "parent": "k-means Algorithm",
      "description": "Guaranteed to converge in a certain sense."
    },
    {
      "id": "Distortion Function",
      "type": "subnode",
      "parent": "k-means Algorithm",
      "description": "Measures sum of squared distances between examples and cluster centroids."
    },
    {
      "id": "Coordinate Descent on J",
      "type": "subnode",
      "parent": "k-means Algorithm",
      "description": "Minimizing distortion function iteratively with respect to c and mu."
    },
    {
      "id": "Distortion Function J",
      "type": "subnode",
      "parent": "k-means Algorithm",
      "description": "Function measuring the quality of clustering in k-means."
    },
    {
      "id": "Convergence Properties",
      "type": "subnode",
      "parent": "k-means Algorithm",
      "description": "Properties related to convergence and local optima in k-means."
    },
    {
      "id": "EM Algorithms",
      "type": "major",
      "parent": null,
      "description": "Expectation-Maximization algorithm used in probabilistic modeling."
    },
    {
      "id": "EM for Mixture of Gaussians",
      "type": "subnode",
      "parent": "EM Algorithms",
      "description": "Application of EM to model data with Gaussian distributions."
    },
    {
      "id": "Posterior Approximation",
      "type": "subnode",
      "parent": "Bayesian Inference",
      "description": "Techniques for approximating the posterior distribution when exact computation is infeasible."
    },
    {
      "id": "MAP Estimation",
      "type": "subnode",
      "parent": "Posterior Approximation",
      "description": "Estimate parameters by maximizing the posterior probability, incorporating prior knowledge."
    },
    {
      "id": "MLE vs MAP",
      "type": "subnode",
      "parent": "MAP Estimation",
      "description": "Comparison between maximum likelihood and maximum a posteriori estimation methods."
    },
    {
      "id": "Prior Selection",
      "type": "subnode",
      "parent": "Posterior Approximation",
      "description": "Choosing appropriate prior distributions for Bayesian models, e.g., Gaussian distribution."
    },
    {
      "id": "Unsupervised Learning",
      "type": "major",
      "parent": null,
      "description": "Learning from data without labeled responses; finding hidden structure in unlabeled data sets."
    },
    {
      "id": "Clustering",
      "type": "subnode",
      "parent": "Unsupervised Learning",
      "description": "Techniques for grouping a set of objects into clusters based on their similarity."
    },
    {
      "id": "K-Means Algorithm",
      "type": "subnode",
      "parent": "Clustering",
      "description": "A popular unsupervised learning algorithm that partitions data points into k clusters."
    },
    {
      "id": "Mixture of Gaussians Model",
      "type": "subnode",
      "parent": "Unsupervised Learning",
      "description": "Model using multiple Gaussian distributions with latent variables"
    },
    {
      "id": "Latent Variables",
      "type": "subnode",
      "parent": "Mixture of Gaussians Model",
      "description": "Hidden random variables that influence the observed data"
    },
    {
      "id": "Joint Distribution",
      "type": "subnode",
      "parent": "Mixture of Gaussians Model",
      "description": "Distribution modeling both latent and observable variables"
    },
    {
      "id": "Likelihood Estimation",
      "type": "subnode",
      "parent": "Model Parameters",
      "description": "Estimating parameters by maximizing likelihood of observed data"
    },
    {
      "id": "EM Algorithm",
      "type": "subnode",
      "parent": "Machine Learning Models",
      "description": "Iterative method for finding maximum likelihood or maximum a posteriori estimates in statistical models with latent variables."
    },
    {
      "id": "E-step",
      "type": "subnode",
      "parent": "EM Algorithm",
      "description": "Estimation step where posterior distribution of hidden variables is computed given observed data and current parameters."
    },
    {
      "id": "M-step",
      "type": "subnode",
      "parent": "EM Algorithm",
      "description": "Maximization step where model parameters are updated to maximize the expected log-likelihood found in E-step."
    },
    {
      "id": "Gaussian Mixture Model",
      "type": "subnode",
      "parent": "E-step",
      "description": "Model used for clustering data into multiple Gaussian distributions with different means and covariances."
    },
    {
      "id": "Soft Assignments",
      "type": "subnode",
      "parent": "EM Algorithm",
      "description": "Assigns probabilities to each cluster instead of hard assignments, allowing for probabilistic membership in clusters."
    },
    {
      "id": "K-means Clustering",
      "type": "subnode",
      "parent": "EM Algorithm",
      "description": "Clustering algorithm that assigns data points to the nearest centroid; contrasted with EM's soft assignments."
    },
    {
      "id": "Expectation-Maximization Algorithm",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Algorithm used for finding maximum likelihood or maximum a posteriori estimates of parameters in statistical models where the model depends on unobserved latent variables."
    },
    {
      "id": "Convergence Guarantees",
      "type": "subnode",
      "parent": "Expectation-Maximization Algorithm",
      "description": "Conditions and proofs ensuring the algorithm's convergence to optimal solutions."
    },
    {
      "id": "Jensen's Inequality",
      "type": "major",
      "parent": null,
      "description": "Mathematical result used to prove the monotonic increase of log-likelihood in each iteration of EM."
    },
    {
      "id": "Strict Convexity",
      "type": "subnode",
      "parent": "Convex Functions",
      "description": "Condition for a convex function to be strictly convex, ensuring unique minimum points."
    },
    {
      "id": "Theorem Statement",
      "type": "subnode",
      "parent": "Jensen's Inequality",
      "description": "Formal statement of Jensen's inequality involving expectations and convex functions."
    },
    {
      "id": "Concave Functions",
      "type": "subnode",
      "parent": "Jensen's Inequality",
      "description": "Functions where E[f(X)] <= f(E[X]) if f is concave"
    },
    {
      "id": "Latent Variable Models",
      "type": "subnode",
      "parent": "EM Algorithm",
      "description": "Models with unobserved variables that influence observed data"
    },
    {
      "id": "Log-Likelihood Maximization",
      "type": "subnode",
      "parent": "EM Algorithm",
      "description": "Process of finding parameter values that maximize the probability of observed data under a statistical model."
    },
    {
      "id": "Machine_Learning_Concepts",
      "type": "major",
      "parent": null,
      "description": "Overview of machine learning concepts and algorithms."
    },
    {
      "id": "EM_Algorithm",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Efficient method for maximum likelihood estimation in probabilistic models."
    },
    {
      "id": "Likelihood_Estimation",
      "type": "subnode",
      "parent": "EM_Algorithm",
      "description": "Process of estimating parameters to maximize the likelihood function."
    },
    {
      "id": "Non_Convex_Optimization",
      "type": "subnode",
      "parent": "Likelihood_Estimation",
      "description": "Challenges in optimizing non-convex functions for parameter estimation."
    },
    {
      "id": "E_Step",
      "type": "subnode",
      "parent": "EM_Algorithm",
      "description": "Expectation step where a lower bound on the likelihood is constructed."
    },
    {
      "id": "M_Step",
      "type": "subnode",
      "parent": "EM_Algorithm",
      "description": "Maximization step where the lower bound is optimized to update parameters."
    },
    {
      "id": "Latent_Variables",
      "type": "subnode",
      "parent": "Likelihood_Estimation",
      "description": "Random variables that are not directly observed but influence the model."
    },
    {
      "id": "Single_Example_Optimization",
      "type": "subnode",
      "parent": "EM_Algorithm",
      "description": "Simplification of EM algorithm for optimizing likelihood of a single example."
    },
    {
      "id": "Evidence Lower Bound (ELBO)",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Objective function used in variational inference to approximate complex probability distributions."
    },
    {
      "id": "Lower Bound Derivation",
      "type": "subnode",
      "parent": "Jensen's Inequality",
      "description": "Deriving a lower bound on the log-likelihood using Jensen's inequality."
    },
    {
      "id": "Optimizing Q Distribution",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Choosing an optimal distribution Q to make the lower bound tight for given parameters."
    },
    {
      "id": "Log-Likelihood Optimization",
      "type": "subnode",
      "parent": "EM Algorithm",
      "description": "Optimizing the log-likelihood function under the EM framework."
    },
    {
      "id": "Single Example Case",
      "type": "subnode",
      "parent": "Log-Likelihood Optimization",
      "description": "Discussion of optimizing for a single training example."
    },
    {
      "id": "Multiple Examples Case",
      "type": "subnode",
      "parent": "Log-Likelihood Optimization",
      "description": "Extending the optimization to multiple training examples."
    },
    {
      "id": "E-step Calculation",
      "type": "subnode",
      "parent": "Expectation-Maximization Algorithm",
      "description": "Calculates the probability of latent variables given observed data and current parameter estimates."
    },
    {
      "id": "M-step Maximization",
      "type": "subnode",
      "parent": "Expectation-Maximization Algorithm",
      "description": "Maximizes the expected log-likelihood found in the E step as a function of the parameters."
    },
    {
      "id": "Parameter Updates",
      "type": "subnode",
      "parent": "M-step Maximization",
      "description": "Updates for \u03c6, \u03bc, and \u03a3 based on maximizing expected log-likelihood."
    },
    {
      "id": "\u03b8 Update Rule",
      "type": "subnode",
      "parent": "Parameter Updates",
      "description": "Rule for updating parameters in the M-step to maximize likelihood function."
    },
    {
      "id": "ELBO Interpretation",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Various interpretations of Evidence Lower Bound (ELBO)."
    },
    {
      "id": "Alternative ELBO Formulations",
      "type": "subnode",
      "parent": "ELBO Interpretation",
      "description": "Different mathematical formulations of the ELBO equation."
    },
    {
      "id": "KL Divergence in ELBO",
      "type": "subnode",
      "parent": "Alternative ELBO Formulations",
      "description": "Explanation of KL divergence within ELBO context."
    },
    {
      "id": "Mixture of Gaussians",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Application of EM algorithm to Gaussian mixture models for parameter estimation."
    },
    {
      "id": "EM Algorithm Steps",
      "type": "subnode",
      "parent": "Mixture of Gaussians",
      "description": "E-step and M-step processes in the context of Gaussian mixtures."
    },
    {
      "id": "Convergence Proof",
      "type": "subnode",
      "parent": "EM Algorithm",
      "description": "Proof showing EM algorithm monotonically increases log-likelihood until convergence."
    },
    {
      "id": "ELBO (Evidence Lower Bound)",
      "type": "major",
      "parent": null,
      "description": "Objective function used in variational inference and EM algorithm as a lower bound on the log-likelihood."
    },
    {
      "id": "Expectation-Maximization (EM) Algorithm",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Iterative method for finding maximum likelihood or maximum a posteriori estimates of parameters in statistical models, where the model depends on unobserved latent variables."
    },
    {
      "id": "M-step Update Rule",
      "type": "subnode",
      "parent": "Expectation-Maximization (EM) Algorithm",
      "description": "Rule used to update parameters during the maximization step of the EM algorithm."
    },
    {
      "id": "Lagrangian Method",
      "type": "subnode",
      "parent": "M-step Update Rule",
      "description": "Mathematical technique for finding local maxima and minima of a function subject to equality constraints."
    },
    {
      "id": "Variational Inference",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Technique used in machine learning to approximate posterior distributions over unobserved variables, especially useful when exact inference is computationally infeasible."
    },
    {
      "id": "Variational Auto-Encoder (VAE)",
      "type": "subnode",
      "parent": "Variational Inference",
      "description": "Type of generative model that uses variational inference to learn a latent variable model for a set of observed data."
    },
    {
      "id": "ELBO",
      "type": "subnode",
      "parent": "Variational Inference",
      "description": "Evidence Lower Bound used to optimize variational inference."
    },
    {
      "id": "Mean Field Assumption",
      "type": "subnode",
      "parent": "Variational Inference",
      "description": "Assumption that latent variables are independent, simplifying the optimization problem."
    },
    {
      "id": "Discrete Latent Variables",
      "type": "subnode",
      "parent": "Mean Field Assumption",
      "description": "Application of mean field assumption to discrete latent variable models."
    },
    {
      "id": "Continuous Latent Variables",
      "type": "subnode",
      "parent": "Variational Inference",
      "description": "Handling continuous variables requires additional techniques beyond mean field assumptions."
    },
    {
      "id": "Re-parametrization Trick",
      "type": "subnode",
      "parent": "VAE",
      "description": "Method to enable backpropagation through stochastic nodes."
    },
    {
      "id": "Gaussian Mixture Models",
      "type": "subnode",
      "parent": "Machine Learning Techniques",
      "description": "Model for clustering data into multiple Gaussian distributions."
    },
    {
      "id": "Optimizing Continuous Latent Variables",
      "type": "major",
      "parent": null,
      "description": "Process of optimizing parameters for continuous latent variables in machine learning models."
    },
    {
      "id": "Succinct Representation of Distribution Qi",
      "type": "subnode",
      "parent": "Optimizing Continuous Latent Variables",
      "description": "Using a Gaussian distribution to represent Qi succinctly over an infinite number of points."
    },
    {
      "id": "Mean and Variance Functions",
      "type": "subnode",
      "parent": "Succinct Representation of Distribution Qi",
      "description": "Functions q(x;phi) and v(x;psi) map from dimension d to k, parameterized by phi and psi."
    },
    {
      "id": "Encoder-Decoder Framework",
      "type": "subnode",
      "parent": "Optimizing Continuous Latent Variables",
      "description": "In variational auto-encoders, q and v are often neural networks acting as encoders, g(z;theta) as decoder."
    },
    {
      "id": "Efficient Evaluation of ELBO",
      "type": "subnode",
      "parent": "Optimizing Continuous Latent Variables",
      "description": "Verification process to ensure efficient evaluation of Evidence Lower Bound for fixed Q."
    },
    {
      "id": "ELBO Optimization",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Techniques for optimizing the Evidence Lower Bound (ELBO)."
    },
    {
      "id": "Gradient Ascent in ELBO",
      "type": "subnode",
      "parent": "ELBO Optimization",
      "description": "Using gradient ascent for optimizing parameters in ELBO."
    },
    {
      "id": "Gaussian Distributions",
      "type": "subnode",
      "parent": "Efficient Evaluation of ELBO",
      "description": "Utilizing Gaussian distributions to efficiently evaluate ELBO values."
    },
    {
      "id": "Expectation Maximization (EM)",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Technique for finding maximum likelihood or maximum a posteriori estimates of parameters in statistical models where the model depends on unobserved latent variables."
    },
    {
      "id": "Reparameterization Trick",
      "type": "subnode",
      "parent": "Gradient Computation",
      "description": "Technique to compute gradients through stochastic variables by re-expressing them in a differentiable form."
    },
    {
      "id": "Data Normalization",
      "type": "major",
      "parent": null,
      "description": "Process of standardizing data attributes to ensure comparability and zero mean."
    },
    {
      "id": "Mean Removal",
      "type": "subnode",
      "parent": "Data Normalization",
      "description": "Subtracting the mean from each feature to center the data around zero."
    },
    {
      "id": "Variance Scaling",
      "type": "subnode",
      "parent": "Data Normalization",
      "description": "Dividing by standard deviation to ensure unit variance for comparability."
    },
    {
      "id": "Major Axis of Variation",
      "type": "major",
      "parent": null,
      "description": "Direction in which the data shows maximum variation after normalization."
    },
    {
      "id": "Projection and Variance Maximization",
      "type": "subnode",
      "parent": "Major Axis of Variation",
      "description": "Finding unit vector u to maximize variance when data is projected onto it."
    },
    {
      "id": "Gradient Estimation",
      "type": "subnode",
      "parent": "Reparameterization Trick",
      "description": "Process of estimating gradients for optimization in probabilistic models."
    },
    {
      "id": "Principal Components Analysis (PCA)",
      "type": "major",
      "parent": null,
      "description": "Dimensionality reduction technique that identifies the subspace where data approximately lies."
    },
    {
      "id": "Data Subspace Identification",
      "type": "subnode",
      "parent": "Principal Components Analysis (PCA)",
      "description": "Process of identifying a lower-dimensional space in which the data can be represented accurately."
    },
    {
      "id": "Data Redundancy Detection",
      "type": "major",
      "parent": null,
      "description": "Identifying and removing redundant data attributes."
    },
    {
      "id": "PCA Algorithm Introduction",
      "type": "subnode",
      "parent": "Data Redundancy Detection",
      "description": "Introduction to Principal Component Analysis for detecting redundancy."
    },
    {
      "id": "Normalization Process",
      "type": "subnode",
      "parent": "PCA Algorithm Introduction",
      "description": "Preprocessing step before PCA involving mean and variance adjustment."
    },
    {
      "id": "Car Example",
      "type": "subnode",
      "parent": "Data Redundancy Detection",
      "description": "Example illustrating linear dependency in car attributes."
    },
    {
      "id": "Pilot Survey Example",
      "type": "subnode",
      "parent": "Data Redundancy Detection",
      "description": "Survey data example showing correlation between piloting skill and enjoyment."
    },
    {
      "id": "Normalization Formula",
      "type": "subnode",
      "parent": "Normalization Process",
      "description": "Formula for normalizing features to have mean 0 and variance 1."
    },
    {
      "id": "Principal Component Analysis (PCA)",
      "type": "major",
      "parent": null,
      "description": "Dimensionality reduction technique that transforms data into principal components."
    },
    {
      "id": "Projection of Data Points",
      "type": "subnode",
      "parent": "Principal Component Analysis (PCA)",
      "description": "Projecting data points onto a unit vector to reduce dimensionality."
    },
    {
      "id": "Variance Maximization",
      "type": "subnode",
      "parent": "Projection of Data Points",
      "description": "Maximizing the variance of projections for optimal direction selection."
    },
    {
      "id": "Empirical Covariance Matrix",
      "type": "subnode",
      "parent": "Variance Maximization",
      "description": "Matrix representing the covariance between data points."
    },
    {
      "id": "Principal Eigenvector",
      "type": "subnode",
      "parent": "Variance Maximization",
      "description": "Eigenvector corresponding to the largest eigenvalue of the covariance matrix."
    },
    {
      "id": "k-Dimensional Subspace",
      "type": "subnode",
      "parent": "Variance Maximization",
      "description": "Projection of data into a lower-dimensional space using top k eigenvectors."
    },
    {
      "id": "PCA",
      "type": "major",
      "parent": null,
      "description": "Principal Component Analysis for dimensionality reduction"
    },
    {
      "id": "Eigenvectors of Sigma",
      "type": "subnode",
      "parent": "PCA",
      "description": "Top k eigenvectors used to form a new orthogonal basis"
    },
    {
      "id": "Dimensionality Reduction",
      "type": "subnode",
      "parent": "PCA",
      "description": "Reduces data from d dimensions to k dimensions"
    },
    {
      "id": "Principal Components",
      "type": "subnode",
      "parent": "PCA",
      "description": "First k eigenvectors of Sigma, representing the new basis"
    },
    {
      "id": "Approximation Error Minimization",
      "type": "subnode",
      "parent": "PCA",
      "description": "Derivation based on minimizing error from projection onto k-dimensional subspace"
    },
    {
      "id": "Applications",
      "type": "subnode",
      "parent": "PCA",
      "description": "Various uses including data compression and visualization"
    },
    {
      "id": "Independent Component Analysis (ICA)",
      "type": "major",
      "parent": null,
      "description": "Technique for separating mixed signals into independent components."
    },
    {
      "id": "Cocktail Party Problem",
      "type": "subnode",
      "parent": "Independent Component Analysis (ICA)",
      "description": "Motivating example involving separating mixed audio signals into individual sources."
    },
    {
      "id": "Mixing Matrix (A)",
      "type": "subnode",
      "parent": "Independent Component Analysis (ICA)",
      "description": "Matrix representing the mixing process of original sources into observed data."
    },
    {
      "id": "Unmixing Matrix (W)",
      "type": "subnode",
      "parent": "Independent Component Analysis (ICA)",
      "description": "Inverse matrix used to recover original sources from mixed signals."
    },
    {
      "id": "ICA Ambiguities",
      "type": "major",
      "parent": null,
      "description": "Discussion on the limitations and uncertainties in recovering the unmixing matrix W."
    },
    {
      "id": "Data Visualization",
      "type": "subnode",
      "parent": "Principal Component Analysis (PCA)",
      "description": "Plotting transformed data to identify clusters and similarities."
    },
    {
      "id": "Dimension Reduction",
      "type": "subnode",
      "parent": "Principal Component Analysis (PCA)",
      "description": "Reducing dataset dimensions for computational efficiency and overfitting prevention."
    },
    {
      "id": "Noise Reduction",
      "type": "subnode",
      "parent": "Principal Component Analysis (PCA)",
      "description": "Estimating intrinsic features from noisy data to improve signal clarity."
    },
    {
      "id": "Eigenfaces Method",
      "type": "subnode",
      "parent": "Noise Reduction",
      "description": "Applying PCA to face images for dimension reduction and noise removal."
    },
    {
      "id": "Independent Components Analysis (ICA)",
      "type": "major",
      "parent": null,
      "description": "Technique for finding independent components in data, differing from PCA in its objectives."
    },
    {
      "id": "Permutation Matrix",
      "type": "subnode",
      "parent": "ICA Ambiguities",
      "description": "A matrix used to permute vectors"
    },
    {
      "id": "Scaling Ambiguity",
      "type": "subnode",
      "parent": "ICA Ambiguities",
      "description": "Ambiguity in scaling factors of sources and mixing matrix"
    },
    {
      "id": "Sign Change Ambiguity",
      "type": "subnode",
      "parent": "ICA Ambiguities",
      "description": "Ambiguity due to sign changes in source signals"
    },
    {
      "id": "Density Transformation",
      "type": "major",
      "parent": null,
      "description": "Transformation of density functions under linear transformations."
    },
    {
      "id": "1D Example",
      "type": "subnode",
      "parent": "Density Transformation",
      "description": "Illustration with a 1-dimensional example."
    },
    {
      "id": "General Case",
      "type": "subnode",
      "parent": "Density Transformation",
      "description": "Extension to vector-valued distributions and general matrices."
    },
    {
      "id": "ICA Algorithm",
      "type": "major",
      "parent": null,
      "description": "Derivation of an Independent Component Analysis algorithm based on maximum likelihood estimation."
    },
    {
      "id": "Scaling Factor",
      "type": "subnode",
      "parent": "ICA Ambiguities",
      "description": "Impact of scaling a speaker's speech signal by a positive factor."
    },
    {
      "id": "Sign Changes Irrelevance",
      "type": "subnode",
      "parent": "ICA Ambiguities",
      "description": "Explanation that sign changes in the signal do not affect the outcome."
    },
    {
      "id": "Non-Gaussian Sources",
      "type": "subnode",
      "parent": "ICA Ambiguities",
      "description": "Sources are non-Gaussian, which resolves ambiguities."
    },
    {
      "id": "Gaussian Data Example",
      "type": "subnode",
      "parent": "ICA Ambiguities",
      "description": "Example with Gaussian data showing rotational symmetry and ambiguity."
    },
    {
      "id": "Mixing Matrix A",
      "type": "subnode",
      "parent": "Gaussian Data Example",
      "description": "Introduction of mixing matrix A in the context of Gaussian data."
    },
    {
      "id": "Rotation Matrix R",
      "type": "subnode",
      "parent": "Gaussian Data Example",
      "description": "Explanation of rotation matrix R and its properties."
    },
    {
      "id": "Mixed Data x'",
      "type": "subnode",
      "parent": "Gaussian Data Example",
      "description": "Observation of mixed data under different mixing matrices A'."
    },
    {
      "id": "Mixing Matrix",
      "type": "subnode",
      "parent": "Independent Component Analysis (ICA)",
      "description": "Matrix representing the mixing process in ICA."
    },
    {
      "id": "Gaussian Data Limitation",
      "type": "subnode",
      "parent": "Independent Component Analysis (ICA)",
      "description": "Limitations of ICA when data is Gaussian distributed."
    },
    {
      "id": "Rotationally Symmetric Distributions",
      "type": "subnode",
      "parent": "Gaussian Data Limitation",
      "description": "Property of multivariate standard normal distribution affecting ICA."
    },
    {
      "id": "Densities and Linear Transformations",
      "type": "major",
      "parent": null,
      "description": "Effect of linear transformations on probability densities in machine learning."
    },
    {
      "id": "Linear Transformation Impact",
      "type": "subnode",
      "parent": "Densities and Linear Transformations",
      "description": "Impact of linear transformations on the density function of random variables."
    },
    {
      "id": "ICA Overview",
      "type": "major",
      "parent": null,
      "description": "Introduction to Independent Component Analysis concepts and principles."
    },
    {
      "id": "Joint Distribution of Sources",
      "type": "subnode",
      "parent": "Maximum Likelihood Estimation",
      "description": "Modeling the joint distribution as a product of marginals for independent sources."
    },
    {
      "id": "Density on x=As=W^-1s",
      "type": "subnode",
      "parent": "Joint Distribution of Sources",
      "description": "Deriving density function based on transformation from source to mixed signals."
    },
    {
      "id": "Cumulative Distribution Function (CDF)",
      "type": "subnode",
      "parent": "Density on x=As=W^-1s",
      "description": "Definition and properties of CDF for real-valued random variables."
    },
    {
      "id": "Sigmoid Function as Default Density",
      "type": "subnode",
      "parent": "Cumulative Distribution Function (CDF)",
      "description": "Using sigmoid function to define density for sources in ICA due to its desirable properties."
    },
    {
      "id": "Data Preprocessing Assumptions",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Assumption that data has zero mean or can be expected to have zero mean."
    },
    {
      "id": "Logistic Function Properties",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Properties of the logistic function and its derivative."
    },
    {
      "id": "Log Likelihood Function",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Function used to evaluate the performance of a model based on training data."
    },
    {
      "id": "Gradient Ascent Rule",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Rule for updating parameters during training using gradient ascent method."
    },
    {
      "id": "Training Example Independence Assumption",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Assumption that training examples are independent of each other, and its implications on model performance."
    },
    {
      "id": "Stochastic Gradient Ascent",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Optimization technique used for minimizing loss functions in machine learning models."
    },
    {
      "id": "Self-supervised Learning",
      "type": "major",
      "parent": null,
      "description": "Learning paradigm where the model learns from unlabeled data using self-generated supervisory signals."
    },
    {
      "id": "Foundation Models",
      "type": "subnode",
      "parent": "Self-supervised Learning",
      "description": "Large-scale models pre-trained on broad datasets and adaptable to various downstream tasks."
    },
    {
      "id": "Pretraining Phase",
      "type": "subnode",
      "parent": "Foundation Models",
      "description": "Training a model on an unlabeled dataset to learn general representations."
    },
    {
      "id": "Adaptation Phase",
      "type": "subnode",
      "parent": "Foundation Models",
      "description": "Phase where the pre-trained model is fine-tuned for specific tasks with limited labeled data."
    },
    {
      "id": "Transfer Learning",
      "type": "subnode",
      "parent": "Machine Learning Overview",
      "description": "Using a pre-trained model as the starting point for fine-tuning on a new task."
    },
    {
      "id": "Unlabeled Dataset",
      "type": "subnode",
      "parent": "Pretraining Phase",
      "description": "Dataset used during pretraining phase, typically large and unlabeled."
    },
    {
      "id": "Labeled Task Dataset",
      "type": "subnode",
      "parent": "Adaptation Phase",
      "description": "Dataset with labeled data for fine-tuning the model on specific tasks."
    },
    {
      "id": "Pretrained Model",
      "type": "subnode",
      "parent": "Transfer Learning",
      "description": "Model trained in pretraining phase, used as a starting point for adaptation."
    },
    {
      "id": "Self-Supervised Loss",
      "type": "subnode",
      "parent": "Pretraining Phase",
      "description": "Loss function that uses the data itself to provide supervision during pretraining."
    },
    {
      "id": "Machine Learning Adaptation Methods",
      "type": "major",
      "parent": null,
      "description": "Overview of methods for adapting machine learning models to new tasks."
    },
    {
      "id": "Labeled Dataset",
      "type": "subnode",
      "parent": "Machine Learning Adaptation Methods",
      "description": "Dataset used in downstream tasks with labeled examples."
    },
    {
      "id": "Zero-Shot Learning",
      "type": "subnode",
      "parent": "Machine Learning Adaptation Methods",
      "description": "Scenario where no labeled data is available for the task."
    },
    {
      "id": "Few-Shhot Learning",
      "type": "subnode",
      "parent": "Machine Learning Adaptation Methods",
      "description": "Situation with a small number of labeled examples (1-50)."
    },
    {
      "id": "Adaptation Algorithm",
      "type": "subnode",
      "parent": "Machine Learning Adaptation Methods",
      "description": "Algorithm that takes in a downstream dataset and pretrained model to output an adapted model."
    },
    {
      "id": "Linear Probe Approach",
      "type": "subnode",
      "parent": "Adaptation Algorithm",
      "description": "Uses a linear head on top of the representation for prediction without modifying the pretrained model."
    },
    {
      "id": "Finetuning Algorithm",
      "type": "subnode",
      "parent": "Adaptation Algorithm",
      "description": "Further finetunes the pretrained model along with the downstream prediction model."
    },
    {
      "id": "Language Problem Methods",
      "type": "subnode",
      "parent": "Machine Learning Adaptation Methods",
      "description": "Specific methods for language problems introduced in 14.3.2."
    },
    {
      "id": "Self-Supervised Learning",
      "type": "major",
      "parent": null,
      "description": "Training models using only unlabeled data."
    },
    {
      "id": "Representation Function",
      "type": "subnode",
      "parent": "Self-Supervised Learning",
      "description": "Maps semantically similar images to similar representations."
    },
    {
      "id": "Supervised Contrastive Algorithms",
      "type": "subnode",
      "parent": "Self-Supervised Learning",
      "description": "Works well with labeled pretraining datasets."
    },
    {
      "id": "Data Augmentation",
      "type": "subnode",
      "parent": "Self-Supervised Learning",
      "description": "Generates pairs of augmented images from the same original image."
    },
    {
      "id": "Positive Pair",
      "type": "subnode",
      "parent": "Data Augmentation",
      "description": "Pair of samples that are semantically similar in the dataset."
    },
    {
      "id": "Negative Pair",
      "type": "subnode",
      "parent": "Data Augmentation",
      "description": "Randomly selected augmented images from different original images, not necessarily semantically related."
    },
    {
      "id": "Finetuning Pretrained Models",
      "type": "subnode",
      "parent": "Machine Learning Adaptation Methods",
      "description": "Process of adjusting pretrained model parameters for specific tasks."
    },
    {
      "id": "Prediction Model Structure",
      "type": "subnode",
      "parent": "Finetuning Pretrained Models",
      "description": "Structure involving both fixed and trainable parts of the model."
    },
    {
      "id": "Optimization Goal",
      "type": "subnode",
      "parent": "Finetuning Pretrained Models",
      "description": "Objective function to minimize for fitting downstream data."
    },
    {
      "id": "Pretraining Methods in CV",
      "type": "major",
      "parent": null,
      "description": "Techniques used to pretrain models specifically for computer vision tasks."
    },
    {
      "id": "Supervised Pretraining",
      "type": "subnode",
      "parent": "Pretraining Methods in CV",
      "description": "Training with labeled data to initialize model parameters."
    },
    {
      "id": "Contrastive Learning",
      "type": "subnode",
      "parent": "Pretraining Methods in CV",
      "description": "Self-supervised learning using unlabeled data to find similar image representations."
    },
    {
      "id": "Loss Function Analysis",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Analysis of loss functions and their impact on model behavior."
    },
    {
      "id": "Pretrained Large Language Models",
      "type": "major",
      "parent": null,
      "description": "Overview of pretraining models in natural language processing."
    },
    {
      "id": "Language Model Probability Distribution",
      "type": "subnode",
      "parent": "Pretrained Large Language Models",
      "description": "Explanation of the probability distribution used in language modeling."
    },
    {
      "id": "SIMCLR Algorithm",
      "type": "subnode",
      "parent": "Contrastive Learning",
      "description": "Specific algorithm based on contrastive learning principle introduced in 2020."
    },
    {
      "id": "Augmentation Techniques",
      "type": "subnode",
      "parent": "SIMCLR Algorithm",
      "description": "Methods for creating variations of input data to improve model robustness."
    },
    {
      "id": "Conditional Probability Modeling",
      "type": "subnode",
      "parent": "Machine Learning Models",
      "description": "Modeling the probability of an event given that another event has occurred."
    },
    {
      "id": "Parameterized Model",
      "type": "subnode",
      "parent": "Conditional Probability Modeling",
      "description": "A model where parameters are used to adjust predictions based on input data."
    },
    {
      "id": "Embeddings and Representations",
      "type": "subnode",
      "parent": "Parameterized Model",
      "description": "Numerical representations of categorical variables, such as words."
    },
    {
      "id": "Transformer Model",
      "type": "subnode",
      "parent": "Machine Learning Models",
      "description": "A model architecture for handling sequence data with self-attention mechanisms."
    },
    {
      "id": "Input-Output Interface",
      "type": "subnode",
      "parent": "Transformer Model",
      "description": "The way input sequences are transformed into output sequences using embeddings and a blackbox function."
    },
    {
      "id": "Training Process",
      "type": "subnode",
      "parent": "Transformer Model",
      "description": "Involves minimizing the negative log-likelihood of data under a probabilistic model."
    },
    {
      "id": "Autoregressive Text Decoding",
      "type": "major",
      "parent": null,
      "description": "Process of generating text sequentially using a trained Transformer model."
    },
    {
      "id": "Machine Learning Adaptation Techniques",
      "type": "major",
      "parent": null,
      "description": "Techniques for adapting machine learning models to new tasks or domains."
    },
    {
      "id": "Zero-shot Learning",
      "type": "subnode",
      "parent": "Machine Learning Adaptation Techniques",
      "description": "Adapting a model to perform tasks without any input-output pairs from the task."
    },
    {
      "id": "In-context Learning",
      "type": "subnode",
      "parent": "Machine Learning Adaptation Techniques",
      "description": "Learning from a small set of examples provided in the context."
    },
    {
      "id": "Language Model Utilization",
      "type": "subnode",
      "parent": "Zero-shot Learning",
      "description": "Methods to decode answers from language models in zero-shot settings."
    },
    {
      "id": "Prompt Construction",
      "type": "subnode",
      "parent": "In-context Learning",
      "description": "Creating prompts by concatenating labeled examples and test data for model generation."
    },
    {
      "id": "Reinforcement Learning",
      "type": "major",
      "parent": null,
      "description": "Field of machine learning concerned with how software agents ought to take actions in an environment to maximize some notion of cumulative reward."
    },
    {
      "id": "Sequential Decision Making",
      "type": "subnode",
      "parent": "Reinforcement Learning",
      "description": "Decision making in sequences without explicit supervision."
    },
    {
      "id": "Reward Function",
      "type": "subnode",
      "parent": "Reinforcement Learning",
      "description": "Function that assigns a scalar value to each possible input or state-action pair."
    },
    {
      "id": "Language Models",
      "type": "subnode",
      "parent": "Machine Learning Techniques",
      "description": "Models that generate text based on learned patterns from large datasets."
    },
    {
      "id": "Conditional Probability in Language Models",
      "type": "subnode",
      "parent": "Language Models",
      "description": "Probability of generating the next token given previous tokens."
    },
    {
      "id": "Temperature Parameter",
      "type": "subnode",
      "parent": "Conditional Probability in Language Models",
      "description": "Parameter to adjust the randomness or determinism of generated text."
    },
    {
      "id": "Adaptation Methods",
      "type": "subnode",
      "parent": "Machine Learning Techniques",
      "description": "Techniques for adapting pretrained models to new tasks without additional training data."
    },
    {
      "id": "Finetuning",
      "type": "subnode",
      "parent": "Adaptation Methods",
      "description": "Adjusting model parameters based on specific task data."
    },
    {
      "id": "Policy Execution",
      "type": "major",
      "parent": null,
      "description": "Process of selecting actions based on a policy in given states."
    },
    {
      "id": "Value Function",
      "type": "subnode",
      "parent": "Policy Execution",
      "description": "Function that calculates the expected sum of discounted rewards for a given state under a policy."
    },
    {
      "id": "Bellman Equations",
      "type": "subnode",
      "parent": "Value Function",
      "description": "Set of equations used to solve for the value function in an MDP."
    },
    {
      "id": "Immediate Reward",
      "type": "subnode",
      "parent": "Bellman Equations",
      "description": "Reward received immediately upon entering a state."
    },
    {
      "id": "Optimal Value Function",
      "type": "subnode",
      "parent": "Value Function",
      "description": "Finding the optimal value function in a finite-horizon setting."
    },
    {
      "id": "Bellman's Equation",
      "type": "subnode",
      "parent": "Value Function",
      "description": "Use of Bellman's equation for solving dynamic programming problems."
    },
    {
      "id": "Policy",
      "type": "major",
      "parent": null,
      "description": "Strategy that defines an action for each state in a given environment."
    },
    {
      "id": "Optimal Policy",
      "type": "subnode",
      "parent": "Policy",
      "description": "Strategy that maximizes the expected cumulative reward over time, which can be stationary or non-stationary depending on the MDP type."
    },
    {
      "id": "Markov Decision Processes (MDP)",
      "type": "subnode",
      "parent": "Reinforcement Learning",
      "description": "Models decision-making scenarios where outcomes are partly random and partly under the control of a decision maker."
    },
    {
      "id": "States",
      "type": "subnode",
      "parent": "Markov Decision Processes (MDP)",
      "description": "Set of all possible conditions or configurations in an environment."
    },
    {
      "id": "Actions",
      "type": "subnode",
      "parent": "Markov Decision Processes (MDP)",
      "description": "Set of all possible actions that can be taken from a given state."
    },
    {
      "id": "State Transition Probabilities",
      "type": "subnode",
      "parent": "Markov Decision Processes (MDP)",
      "description": "Knowledge of state transition probabilities in the context of solving MDPs."
    },
    {
      "id": "Discount Factor",
      "type": "subnode",
      "parent": "Markov Decision Processes (MDP)",
      "description": "Parameter that determines the present value of future rewards in reinforcement learning."
    },
    {
      "id": "Reinforcement Learning Overview",
      "type": "major",
      "parent": null,
      "description": "Introduction to reinforcement learning concepts and processes."
    },
    {
      "id": "Markov Decision Process (MDP)",
      "type": "subnode",
      "parent": "Reinforcement Learning Overview",
      "description": "A framework for modeling decision making in situations where outcomes are partly random and partly under the control of a decision maker."
    },
    {
      "id": "State Transition",
      "type": "subnode",
      "parent": "Markov Decision Process (MDP)",
      "description": "The process by which states change based on actions taken."
    },
    {
      "id": "Total Payoff Calculation",
      "type": "subnode",
      "parent": "Reinforcement Learning Overview",
      "description": "Calculation of total rewards over time, considering discount factors."
    },
    {
      "id": "Discount Factor (\u03b3)",
      "type": "subnode",
      "parent": "Total Payoff Calculation",
      "description": "A factor used to discount future rewards based on their temporal distance from the present."
    },
    {
      "id": "Policy Definition",
      "type": "subnode",
      "parent": "Reinforcement Learning Overview",
      "description": "Definition of a policy as a function mapping states to actions."
    },
    {
      "id": "Markov Decision Processes (MDPs)",
      "type": "subnode",
      "parent": "Machine Learning Algorithms",
      "description": "Models for decision-making problems under uncertainty."
    },
    {
      "id": "Value Iteration",
      "type": "subnode",
      "parent": "Markov Decision Processes (MDPs)",
      "description": "Algorithm for finding optimal policies in reinforcement learning by iteratively updating value functions."
    },
    {
      "id": "Policy Iteration",
      "type": "subnode",
      "parent": "Markov Decision Processes (MDPs)",
      "description": "Alternative algorithm for finding optimal policies in MDPs through successive policy evaluations and improvements."
    },
    {
      "id": "Learning Model for MDPs",
      "type": "subnode",
      "parent": "Markov Decision Processes (MDPs)",
      "description": "Estimating transition probabilities and rewards from data in MDP problems."
    },
    {
      "id": "Inverted Pendulum Problem",
      "type": "subnode",
      "parent": "Learning Model for MDPs",
      "description": "Example problem used to illustrate learning models in MDPs."
    },
    {
      "id": "State Transition Probabilities Estimation",
      "type": "subnode",
      "parent": "Learning Model for MDPs",
      "description": "Method of estimating transition probabilities from observed state-action pairs and outcomes."
    },
    {
      "id": "Optimal Policy in MDPs",
      "type": "subnode",
      "parent": "Machine Learning Overview",
      "description": "Discussion on the existence of a single optimal policy for all states in an MDP."
    },
    {
      "id": "Value Iteration and Policy Iteration",
      "type": "subnode",
      "parent": "Machine Learning Overview",
      "description": "Efficient algorithms for solving finite-state Markov Decision Processes (MDPs)."
    },
    {
      "id": "Finite-State MDPs",
      "type": "subnode",
      "parent": "Value Iteration and Policy Iteration",
      "description": "Consideration of MDPs with a finite number of states and actions."
    },
    {
      "id": "Value Iteration Algorithm",
      "type": "subnode",
      "parent": "Value Iteration and Policy Iteration",
      "description": "Algorithm for computing the optimal value function using iterative methods."
    },
    {
      "id": "Synchronous Updates",
      "type": "subnode",
      "parent": "Value Iteration Algorithm",
      "description": "Method of updating all state values simultaneously before applying them."
    },
    {
      "id": "Asynchronous Updates",
      "type": "subnode",
      "parent": "Value Iteration Algorithm",
      "description": "Technique for updating state values one at a time in sequence."
    },
    {
      "id": "Convergence of Value Functions",
      "type": "subnode",
      "parent": "Value Iteration",
      "description": "Process by which value functions approach the optimal values in iterative algorithms."
    },
    {
      "id": "Optimal Policy Determination",
      "type": "subnode",
      "parent": "Markov Decision Processes (MDP)",
      "description": "Finding the best policy given an MDP, often through value or policy iteration."
    },
    {
      "id": "Bellman's Equations",
      "type": "subnode",
      "parent": "Policy Iteration",
      "description": "Set of equations used to determine optimal policies in decision-making processes."
    },
    {
      "id": "Greedy Policy with Respect to V",
      "type": "subnode",
      "parent": "Policy Iteration",
      "description": "Policy derived from the current value function that maximizes expected rewards."
    },
    {
      "id": "Comparison Between Algorithms",
      "type": "subnode",
      "parent": "Markov Decision Processes (MDP)",
      "description": "Discussion on pros and cons of different algorithms for solving MDPs."
    },
    {
      "id": "Estimating State Transitions",
      "type": "subnode",
      "parent": "State Transition Probabilities",
      "description": "Methods for estimating transition probabilities based on observed data."
    },
    {
      "id": "Expected Immediate Reward",
      "type": "subnode",
      "parent": "Markov Decision Process (MDP)",
      "description": "The expected reward received when an agent is in a particular state."
    },
    {
      "id": "Learning in MDPs with Unknown Transitions",
      "type": "subnode",
      "parent": "Markov Decision Process (MDP)",
      "description": "Approach for learning policies when state transition probabilities are not known."
    },
    {
      "id": "Continuous State Space",
      "type": "major",
      "parent": null,
      "description": "Extension of MDPs to handle continuous states rather than discrete ones."
    },
    {
      "id": "Continuous State MDPs",
      "type": "subnode",
      "parent": "Machine Learning Algorithms",
      "description": "MDPs with infinite state spaces, such as car or helicopter states."
    },
    {
      "id": "Finite State MDPs",
      "type": "subnode",
      "parent": "Machine Learning Algorithms",
      "description": "MDPs with a finite number of states, contrasting with continuous ones."
    },
    {
      "id": "Model Creation Methods",
      "type": "major",
      "parent": null,
      "description": "Different methods to obtain a model for state transitions in machine learning."
    },
    {
      "id": "Physics Simulation",
      "type": "subnode",
      "parent": "Model Creation Methods",
      "description": "Using physical laws or software packages to simulate system behavior."
    },
    {
      "id": "Open Dynamics Engine",
      "type": "subnode",
      "parent": "Physics Simulation",
      "description": "Free/open-source physics simulator for simulating mechanical systems."
    },
    {
      "id": "Learning from Data",
      "type": "subnode",
      "parent": "Model Creation Methods",
      "description": "Inferring state transition probabilities from collected data in an MDP."
    },
    {
      "id": "Discretization in MDPs",
      "type": "major",
      "parent": null,
      "description": "Process of converting continuous state space into discrete states for easier computation."
    },
    {
      "id": "Supervised Learning Problem",
      "type": "subnode",
      "parent": "Discretization in MDPs",
      "description": "Example of fitting a function using linear regression and piecewise constant representation."
    },
    {
      "id": "Piecewise Constant Representation",
      "type": "subnode",
      "parent": "Discretization in MDPs",
      "description": "Representation that assumes value is constant within each discretized interval."
    },
    {
      "id": "Curse of Dimensionality",
      "type": "subnode",
      "parent": "Discretization in MDPs",
      "description": "Problem where the volume of the state space increases exponentially with dimensionality, making it hard to represent accurately."
    },
    {
      "id": "State Representation",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Methods for representing states in machine learning problems."
    },
    {
      "id": "Value Function Approximation",
      "type": "major",
      "parent": null,
      "description": "Approximating the value function using supervised learning methods such as linear regression."
    },
    {
      "id": "Model or Simulator",
      "type": "subnode",
      "parent": "Value Function Approximation",
      "description": "Black-box model providing next-state transitions based on current state and action."
    },
    {
      "id": "Linear Model Prediction",
      "type": "subnode",
      "parent": "Machine Learning Models",
      "description": "Predicting the next state using a linear model based on current state and action."
    },
    {
      "id": "Learning Algorithm",
      "type": "subnode",
      "parent": "Linear Model Prediction",
      "description": "Algorithm used to estimate parameters A and B in the linear prediction model."
    },
    {
      "id": "Deterministic vs Stochastic Models",
      "type": "subnode",
      "parent": "Machine Learning Models",
      "description": "Comparison between deterministic and stochastic models for predicting next states."
    },
    {
      "id": "Non-linear Functions",
      "type": "subnode",
      "parent": "Machine Learning Models",
      "description": "Use of non-linear functions in state transition prediction models."
    },
    {
      "id": "Fitted Value Iteration",
      "type": "subnode",
      "parent": "Machine Learning Overview",
      "description": "Algorithm for approximating the value function through iterative sampling and regression."
    },
    {
      "id": "Regression Algorithms",
      "type": "subnode",
      "parent": "Supervised Learning",
      "description": "Techniques to predict continuous outcomes from input data."
    },
    {
      "id": "Non-linear Feature Mappings",
      "type": "subnode",
      "parent": "Machine Learning Models",
      "description": "Feature mappings that transform states and actions into non-linear features."
    },
    {
      "id": "Discrete Action Space",
      "type": "subnode",
      "parent": "Fitted Value Iteration",
      "description": "Assumption of a small, discrete set of actions available at each state."
    },
    {
      "id": "Supervised Learning Algorithm",
      "type": "subnode",
      "parent": "Fitted Value Iteration",
      "description": "Use of linear or non-linear regression to approximate the value function based on state features."
    },
    {
      "id": "State Sampling",
      "type": "subnode",
      "parent": "Fitted Value Iteration",
      "description": "Random sampling of states for value function approximation."
    },
    {
      "id": "Action Evaluation",
      "type": "subnode",
      "parent": "Fitted Value Iteration",
      "description": "Evaluation of actions based on expected future rewards and state transitions."
    },
    {
      "id": "Expectation Approximation",
      "type": "subnode",
      "parent": "Value Iteration",
      "description": "Techniques for estimating expectations in reinforcement learning algorithms, such as sampling and deterministic noise removal."
    },
    {
      "id": "Deterministic Simulators",
      "type": "subnode",
      "parent": "Expectation Approximation",
      "description": "Simulations where the next state is determined solely by current state and action without random noise."
    },
    {
      "id": "Gaussian Noise Model",
      "type": "subnode",
      "parent": "Expectation Approximation",
      "description": "Modeling simulator transitions with a deterministic function plus Gaussian noise for approximation purposes."
    },
    {
      "id": "Bellman Updates",
      "type": "subnode",
      "parent": "Policy Iteration",
      "description": "Iterative process of updating the value function based on Bellman's equation to converge towards an optimal policy."
    },
    {
      "id": "VE Procedure",
      "type": "subnode",
      "parent": "Policy Iteration",
      "description": "Procedure used to evaluate value function under a given policy."
    },
    {
      "id": "k Parameter",
      "type": "subnode",
      "parent": "VE Procedure",
      "description": "Hyperparameter controlling the number of iterations in VE procedure."
    },
    {
      "id": "Initialization Option 1",
      "type": "subnode",
      "parent": "VE Procedure",
      "description": "Initialize value function to zero for all states."
    },
    {
      "id": "Initialization Option 2",
      "type": "subnode",
      "parent": "VE Procedure",
      "description": "Initialize value function based on previous iterations' values."
    },
    {
      "id": "Update Rule (15.12)",
      "type": "subnode",
      "parent": "VE Procedure",
      "description": "Rule for updating state values using Bellman equation."
    },
    {
      "id": "Policy Update Rule (15.13)",
      "type": "subnode",
      "parent": "Value Iteration",
      "description": "Rule for updating policy based on value function estimates."
    },
    {
      "id": "Chapter 15 Overview",
      "type": "major",
      "parent": null,
      "description": "Overview of MDPs and value/policy iteration methods."
    },
    {
      "id": "Optimal Bellman Equation",
      "type": "subnode",
      "parent": "Chapter 15 Overview",
      "description": "Equation defining the optimal value function for an optimal policy."
    },
    {
      "id": "Policy Iteration Speedup",
      "type": "subnode",
      "parent": "Chapter 15 Overview",
      "description": "Discussion on how policy iteration can be faster than repeated single-step updates."
    },
    {
      "id": "Value Iteration Preference",
      "type": "subnode",
      "parent": "Chapter 15 Overview",
      "description": "When value iteration is preferred over policy iteration due to computational constraints."
    },
    {
      "id": "Chapter 16 Introduction",
      "type": "major",
      "parent": null,
      "description": "Introduction to LQR, DDP and LQG concepts in MDPs."
    },
    {
      "id": "Optimal Value Function Recovery",
      "type": "subnode",
      "parent": "Finite-horizon MDPs",
      "description": "Recovering the optimal policy from the optimal value function."
    },
    {
      "id": "Finite Horizon MDPs",
      "type": "subnode",
      "parent": "Markov Decision Processes (MDP)",
      "description": "Models considering a finite sequence of decisions without the need for a discount factor."
    },
    {
      "id": "Time-Dependent Policies",
      "type": "subnode",
      "parent": "Finite Horizon MDPs",
      "description": "Policies that change over time in response to the environment."
    },
    {
      "id": "Non-Stationary Optimal Policy",
      "type": "subnode",
      "parent": "Time-Dependent Policies",
      "description": "Optimal policies vary based on remaining steps and current state."
    },
    {
      "id": "Dynamic Environment Models",
      "type": "subnode",
      "parent": "Finite Horizon MDPs",
      "description": "Models that account for changing dynamics over time."
    },
    {
      "id": "Expectation Calculation",
      "type": "subnode",
      "parent": "Markov Decision Processes (MDP)",
      "description": "Calculating expectations under certain conditions that simplify the expression for policy gradients."
    },
    {
      "id": "Rewards Dependency on States and Actions",
      "type": "subnode",
      "parent": "Markov Decision Processes (MDP)",
      "description": "Description of how rewards depend on both states and actions in an MDP."
    },
    {
      "id": "Infinite Horizon MDPs",
      "type": "subnode",
      "parent": "Markov Decision Processes (MDP)",
      "description": "Models considering an infinite sequence of decisions with a discount factor for future rewards."
    },
    {
      "id": "Discount Factor \u03b3",
      "type": "subnode",
      "parent": "Infinite Horizon MDPs",
      "description": "Parameter used to ensure convergence in infinite horizon models by discounting future rewards."
    },
    {
      "id": "Value Function in RL",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Definition and computation of value functions in reinforcement learning."
    },
    {
      "id": "Dynamic Programming",
      "type": "subnode",
      "parent": "Value Function in RL",
      "description": "Application of dynamic programming to reinforcement learning problems."
    },
    {
      "id": "Bellman Update",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Operator used to update value functions in reinforcement learning."
    },
    {
      "id": "Geometric Convergence",
      "type": "subnode",
      "parent": "Value Iteration",
      "description": "Rate at which the approximation error decreases with each iteration."
    },
    {
      "id": "Continuous Setting",
      "type": "subnode",
      "parent": "Linear Quadratic Regulation (LQR)",
      "description": "Model assumptions for LQR including state and action spaces as real vectors."
    },
    {
      "id": "Linear Transitions",
      "type": "subnode",
      "parent": "Linear Quadratic Regulation (LQR)",
      "description": "Assumption of linear dynamics in the system with Gaussian noise."
    },
    {
      "id": "Quadratic Rewards",
      "type": "subnode",
      "parent": "Linear Quadratic Regulation (LQR)",
      "description": "Rewards defined as quadratic functions of state and action vectors."
    },
    {
      "id": "Quadratic Assumption",
      "type": "subnode",
      "parent": "Optimal Value Function",
      "description": "Assumes that the value functions are quadratic for simplification."
    },
    {
      "id": "Dynamics of Model",
      "type": "subnode",
      "parent": "Optimal Value Function",
      "description": "Incorporates model dynamics into the optimal value function calculation."
    },
    {
      "id": "Linear Optimal Action",
      "type": "subnode",
      "parent": "Optimal Policy",
      "description": "Derives the formula for the optimal action which is a linear function of the state."
    },
    {
      "id": "LQR Model Assumptions",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Assumptions made in the Linear Quadratic Regulator model."
    },
    {
      "id": "LQR Algorithm Steps",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Steps involved in implementing the LQR algorithm."
    },
    {
      "id": "Step 1: Estimate Matrices",
      "type": "subnode",
      "parent": "LQR Algorithm Steps",
      "description": "Estimating matrices A, B, and Sigma using linear regression."
    },
    {
      "id": "Step 2: Derive Optimal Policy",
      "type": "subnode",
      "parent": "LQR Algorithm Steps",
      "description": "Deriving the optimal policy given known model parameters."
    },
    {
      "id": "Dynamic Programming Application",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Application of dynamic programming to compute V_t* in LQR context."
    },
    {
      "id": "Linear Quadratic Regulator (LQR)",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "A method to find optimal control policies in linear systems with quadratic cost functions."
    },
    {
      "id": "Discrete Ricatti Equations",
      "type": "subnode",
      "parent": "Linear Quadratic Regulator (LQR)",
      "description": "Set of equations used to solve the LQR problem iteratively."
    },
    {
      "id": "Inverted Pendulum Example",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Example system demonstrating the application of linearization techniques."
    },
    {
      "id": "Linearization of Dynamics",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Process of approximating nonlinear dynamics with a linear model for easier analysis."
    },
    {
      "id": "Taylor Expansion",
      "type": "subnode",
      "parent": "Linearization of Dynamics",
      "description": "Mathematical technique used to approximate functions using polynomials, crucial in linearizing systems."
    },
    {
      "id": "Optimization in RL",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Techniques for optimizing policies in reinforcement learning."
    },
    {
      "id": "LQG Framework",
      "type": "major",
      "parent": null,
      "description": "Extension of LQR to handle stochastic systems and partial observability."
    },
    {
      "id": "Partial Observability",
      "type": "subnode",
      "parent": "LQG Framework",
      "description": "Situation where the full state is not observable, requiring models like LQG."
    },
    {
      "id": "Nominal Trajectory Generation",
      "type": "subnode",
      "parent": "Differential Dynamic Programming (DDP)",
      "description": "Creating an initial approximate path using a naive controller."
    },
    {
      "id": "Rewriting Dynamics",
      "type": "subnode",
      "parent": "Linearization of Dynamics",
      "description": "Expressing the state transition using matrices A and B for non-stationary settings."
    },
    {
      "id": "Reward Function Approximation",
      "type": "subnode",
      "parent": "Differential Dynamic Programming (DDP)",
      "description": "Using Taylor expansion to approximate rewards around nominal trajectory points."
    },
    {
      "id": "Partially Observable MDPs (POMDP)",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "MDPs with an additional observation layer to handle partial observability."
    },
    {
      "id": "Observation Layer",
      "type": "subnode",
      "parent": "Partially Observable MDPs (POMDP)",
      "description": "Introduces new variable o_t representing observations given the current state s_t."
    },
    {
      "id": "Belief State",
      "type": "subnode",
      "parent": "Partially Observable MDPs (POMDP)",
      "description": "Maintains a distribution over states based on past observations to inform policy decisions."
    },
    {
      "id": "LQR Extension",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Extension of Linear Quadratic Regulator to handle partial observability scenarios."
    },
    {
      "id": "Kalman Filter",
      "type": "subnode",
      "parent": "Belief State",
      "description": "Algorithm used for efficient computation and updating of belief states over time."
    },
    {
      "id": "LQR Updates",
      "type": "subnode",
      "parent": "Machine Learning Algorithms",
      "description": "Backward pass computations for optimal control policies."
    },
    {
      "id": "Randomized Policy",
      "type": "subnode",
      "parent": "Policy Gradient (REINFORCE)",
      "description": "Learning policy that outputs actions probabilistically based on state input."
    },
    {
      "id": "Expected Total Payoff",
      "type": "subnode",
      "parent": "Policy Gradient (REINFORCE)",
      "description": "Objective function for optimizing policy parameters over trajectories."
    },
    {
      "id": "Predict Step",
      "type": "subnode",
      "parent": "Kalman Filter",
      "description": "Estimates the next state based on current state distribution."
    },
    {
      "id": "Update Step",
      "type": "subnode",
      "parent": "Kalman Filter",
      "description": "Refine state estimate with new observation at each time step."
    },
    {
      "id": "Belief States Update",
      "type": "subnode",
      "parent": "Kalman Filter",
      "description": "Updates belief states through predict and update steps iteratively."
    },
    {
      "id": "Kalman Gain",
      "type": "subnode",
      "parent": "Update Step",
      "description": "Matrix used in update step to refine state estimate based on new observation."
    },
    {
      "id": "Step 1",
      "type": "subnode",
      "parent": "Kalman Filter",
      "description": "Initial step to set up the system dynamics and noise model."
    },
    {
      "id": "Step 2",
      "type": "subnode",
      "parent": "Kalman Filter",
      "description": "Use mean of distribution as state approximation."
    },
    {
      "id": "Step 3",
      "type": "subnode",
      "parent": "Kalman Filter",
      "description": "Set action based on approximated state and LQR algorithm."
    },
    {
      "id": "System Dynamics",
      "type": "major",
      "parent": null,
      "description": "Model describing how the system evolves over time and is affected by noise."
    },
    {
      "id": "LQR Algorithm",
      "type": "major",
      "parent": null,
      "description": "Linear Quadratic Regulator algorithm used to determine optimal control actions."
    },
    {
      "id": "Policy Gradients",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Techniques for optimizing policies in reinforcement learning environments."
    },
    {
      "id": "Expectation Estimation",
      "type": "subnode",
      "parent": "Policy Gradients",
      "description": "Estimating the expected value of a function under a policy distribution."
    },
    {
      "id": "Sample-Based Estimation",
      "type": "subnode",
      "parent": "Expectation Estimation",
      "description": "Using samples to estimate the gradient of expected values."
    },
    {
      "id": "Log Probability Calculation",
      "type": "subnode",
      "parent": "Gradient Calculation",
      "description": "Calculating log probabilities for policy gradients."
    },
    {
      "id": "Reward Function Estimation",
      "type": "subnode",
      "parent": "Policy Gradients",
      "description": "Estimating gradients without knowing the exact form of the reward function."
    },
    {
      "id": "Expectation Maximization",
      "type": "subnode",
      "parent": "Reward Function Estimation",
      "description": "Using expectations of rewards over policy distributions to estimate gradients."
    },
    {
      "id": "Reparametrization Technique",
      "type": "subnode",
      "parent": "Gradient Ascent",
      "description": "Technique used in VAEs for gradient estimation, not applicable here."
    },
    {
      "id": "REINFORCE Algorithm",
      "type": "subnode",
      "parent": "Reward Function Estimation",
      "description": "Algorithm for estimating gradients of policy performance without knowing the reward function."
    },
    {
      "id": "Policy Gradient Theorem",
      "type": "major",
      "parent": null,
      "description": "Theorem that connects policy gradients to expected payoff."
    },
    {
      "id": "Log Probability Derivative",
      "type": "subnode",
      "parent": "Policy Gradient Theorem",
      "description": "Derivative of log probability with respect to parameters \u03b8."
    },
    {
      "id": "Vanilla REINFORCE Algorithm",
      "type": "subnode",
      "parent": "Policy Gradient Theorem",
      "description": "Algorithm that updates policy parameters using estimated gradients."
    },
    {
      "id": "Trajectory Probability Change",
      "type": "subnode",
      "parent": "Log Probability Derivative",
      "description": "Change in trajectory probability due to parameter changes."
    },
    {
      "id": "Empirical Trajectories Estimation",
      "type": "subnode",
      "parent": "Vanilla REINFORCE Algorithm",
      "description": "Estimating gradients using sample trajectories."
    },
    {
      "id": "Trajectory Probability",
      "type": "subnode",
      "parent": "Policy Gradients",
      "description": "Probability of an agent following a specific sequence of actions and states."
    },
    {
      "id": "Expectation Equations",
      "type": "subnode",
      "parent": "Policy Gradients",
      "description": "Mathematical expressions for expected values in reinforcement learning scenarios."
    },
    {
      "id": "Simplification of Formula (17.8)",
      "type": "subnode",
      "parent": "Expectation Equations",
      "description": "Derivation and simplification process based on constant reward assumption."
    },
    {
      "id": "Policy Gradient Methods",
      "type": "subnode",
      "parent": "Reinforcement Learning",
      "description": "Techniques for optimizing the parameters of a policy directly based on the performance measure, such as expected return or discounted sum of rewards."
    },
    {
      "id": "Law of Total Expectation",
      "type": "subnode",
      "parent": "Policy Gradient Methods",
      "description": "A theorem in probability theory that allows one to compute the expectation of any function of a random variable by conditioning on another random variable."
    },
    {
      "id": "Estimator Simplification",
      "type": "subnode",
      "parent": "Law of Total Expectation",
      "description": "Simplifying an estimator using the law of total expectation, making it easier to understand and compute."
    },
    {
      "id": "Baseline Estimation",
      "type": "subnode",
      "parent": "Policy Gradient Methods",
      "description": "Techniques to reduce variance in policy gradient estimators using baselines."
    },
    {
      "id": "Trajectory Collection",
      "type": "subnode",
      "parent": "Policy Gradient Methods",
      "description": "Process of collecting data through interaction with an environment to train policies."
    },
    {
      "id": "Gradient Estimator Update",
      "type": "subnode",
      "parent": "Baseline Estimation",
      "description": "Updating policy parameters based on the gradient estimator using baselines."
    },
    {
      "id": "Machine_Learning_Theory",
      "type": "major",
      "parent": null,
      "description": "Theoretical foundations of machine learning including generalization and double descent phenomena."
    },
    {
      "id": "Double_Descent_Phenomenon",
      "type": "subnode",
      "parent": "Machine_Learning_Theory",
      "description": "Phenomenon where model performance initially improves then worsens before improving again with increased complexity."
    },
    {
      "id": "Statistical_Mechanics_of_Learning",
      "type": "subnode",
      "parent": "Machine_Learning_Theory",
      "description": "Application of statistical mechanics principles to understand learning processes and generalization in neural networks."
    },
    {
      "id": "Machine Learning Literature",
      "type": "major",
      "parent": null,
      "description": "Collection of key papers and reviews in machine learning."
    },
    {
      "id": "Bias-Variance Trade-off Reconciliation",
      "type": "subnode",
      "parent": "Machine Learning Literature",
      "description": "Paper discussing the reconciliation between modern ML practice and classical bias-variance trade-off."
    },
    {
      "id": "Double Descent for Weak Features",
      "type": "subnode",
      "parent": "Machine Learning Literature",
      "description": "Study on double descent phenomenon in machine learning models with weak features."
    },
    {
      "id": "Variational Inference Review",
      "type": "subnode",
      "parent": "Machine Learning Literature",
      "description": "Review paper discussing variational inference methods for statisticians."
    },
    {
      "id": "Foundation Models Opportunities and Risks",
      "type": "subnode",
      "parent": "Machine Learning Literature",
      "description": "Discussion on the opportunities and risks associated with foundation models in ML."
    },
    {
      "id": "Few-Shot Learning Capabilities",
      "type": "subnode",
      "parent": "Machine Learning Literature",
      "description": "Research highlighting the few-shot learning capabilities of language models."
    },
    {
      "id": "Contrastive Learning Framework",
      "type": "subnode",
      "parent": "Machine Learning Literature",
      "description": "Framework for contrastive learning to improve visual representation in machine learning."
    },
    {
      "id": "BERT Pre-training Methodology",
      "type": "subnode",
      "parent": "Machine Learning Literature",
      "description": "Introduction of BERT, a deep bidirectional transformer model for language understanding."
    },
    {
      "id": "Implicit Bias Study",
      "type": "subnode",
      "parent": "Machine Learning Literature",
      "description": "Research on the implicit bias in machine learning models due to noise covariance shape."
    },
    {
      "id": "High-Dimensional Statistical Analysis",
      "type": "subnode",
      "parent": "Machine Learning Literature",
      "description": "Discussion on surprising phenomena in high-dimensional statistical analysis and machine learning."
    },
    {
      "id": "Machine Learning Papers",
      "type": "major",
      "parent": null,
      "description": "Collection of research papers related to machine learning and statistical learning theory."
    },
    {
      "id": "Implicit Bias in Machine Learning",
      "type": "subnode",
      "parent": "Machine Learning Papers",
      "description": "Research on the implicit bias introduced by different noise covariances and ridgeless least squares interpolation."
    },
    {
      "id": "Deep Residual Learning",
      "type": "subnode",
      "parent": "Machine Learning Papers",
      "description": "Introduction to deep residual learning for image recognition, a key technique in deep neural networks."
    },
    {
      "id": "Theoretical Guarantees for Deep Reinforcement Learning",
      "type": "subnode",
      "parent": "Machine Learning Papers",
      "description": "Research on theoretical frameworks for model-based deep reinforcement learning with guarantees."
    },
    {
      "id": "Generalization Error Analysis",
      "type": "subnode",
      "parent": "Machine Learning Papers",
      "description": "Analysis of generalization error in random features regression and linear regression models."
    }
  ],
  "edges": [
    {
      "from": "Independent components analysis",
      "to": "ICA algorithm",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine Learning Algorithms",
      "to": "Kernel Trick",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Linearization of Dynamics",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Theory",
      "to": "Learning Guarantees",
      "relationship": "depends_on"
    },
    {
      "from": "Exponential Family Distributions",
      "to": "Gaussian Distribution",
      "relationship": "subtopic"
    },
    {
      "from": "Reward Function Estimation",
      "to": "REINFORCE Algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Auto-Differentiation",
      "to": "Deep Learning Packages",
      "relationship": "depends_on"
    },
    {
      "from": "Fitted Value Iteration",
      "to": "Continuous State Space",
      "relationship": "depends_on"
    },
    {
      "from": "Mean Field Assumption",
      "to": "Variational Inference",
      "relationship": "subtopic"
    },
    {
      "from": "Cumulative Distribution Function (CDF)",
      "to": "Sigmoid Function as Default Density",
      "relationship": "related_to"
    },
    {
      "from": "Backward Function for Loss Functions",
      "to": "Squared Loss (MSE)",
      "relationship": "subtopic"
    },
    {
      "from": "Dual Formulation of SVM",
      "to": "KKT Conditions",
      "relationship": "related_to"
    },
    {
      "from": "Regularization in Machine Learning",
      "to": "Deep Learning Regularization Techniques",
      "relationship": "subtopic"
    },
    {
      "from": "Policy Gradients",
      "to": "Expectation Equations",
      "relationship": "has_subtopic"
    },
    {
      "from": "Independent Components Analysis (ICA)",
      "to": "Cocktail Party Problem",
      "relationship": "subtopic"
    },
    {
      "from": "Softplus Function",
      "to": "Activation Functions",
      "relationship": "subtopic"
    },
    {
      "from": "Regularization",
      "to": "Regularized Loss",
      "relationship": "depends_on"
    },
    {
      "from": "Decision Boundary",
      "to": "Functional Margins",
      "relationship": "related_to"
    },
    {
      "from": "Loss Function",
      "to": "Cross Entropy Loss",
      "relationship": "subtopic"
    },
    {
      "from": "Transformer Model",
      "to": "Autoregressive Text Decoding",
      "relationship": "related_to"
    },
    {
      "from": "Linear Quadratic Regulation (LQR)",
      "to": "Continuous Setting",
      "relationship": "subtopic"
    },
    {
      "from": "Backpropagation",
      "to": "Preliminaries on partial derivatives",
      "relationship": "subtopic_of"
    },
    {
      "from": "Double Descent for Weak Features",
      "to": "Machine Learning Literature",
      "relationship": "subtopic"
    },
    {
      "from": "\u039b_2 Regularization",
      "to": "Weight Decay",
      "relationship": "depends_on"
    },
    {
      "from": "Mean and Variance Functions",
      "to": "Succinct Representation of Distribution Qi",
      "relationship": "subtopic"
    },
    {
      "from": "Independent Component Analysis (ICA)",
      "to": "Gaussian Data Limitation",
      "relationship": "related_to"
    },
    {
      "from": "Bernoulli Distribution",
      "to": "Exponential Family Distributions",
      "relationship": "related_to"
    },
    {
      "from": "Kalman Filter",
      "to": "Predict Step",
      "relationship": "has_subtopic"
    },
    {
      "from": "Regularization",
      "to": "Bias-Variance Tradeoff",
      "relationship": "related_to"
    },
    {
      "from": "Ordinary Least Squares",
      "to": "Response Variable",
      "relationship": "related_to"
    },
    {
      "from": "Variational Inference",
      "to": "ELBO",
      "relationship": "contains"
    },
    {
      "from": "Initialization",
      "to": "k-means Algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Two-Layer Neural Network",
      "to": "ReLU Function",
      "relationship": "depends_on"
    },
    {
      "from": "Optimization Problem",
      "to": "Objective Function Transformation",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Overview",
      "to": "Unsupervised Learning",
      "relationship": "contains"
    },
    {
      "from": "Machine Learning Algorithms",
      "to": "Generative Learning Algorithms",
      "relationship": "subtopic"
    },
    {
      "from": "Joint Distribution of Sources",
      "to": "Density on x=As=W^-1s",
      "relationship": "subtopic"
    },
    {
      "from": "Deep Learning Representations",
      "to": "Feature Maps and Representation Transferability",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "VC Dimension",
      "relationship": "related_to"
    },
    {
      "from": "Foundation Models",
      "to": "Adaptation Phase",
      "relationship": "subtopic"
    },
    {
      "from": "Principal Component Analysis (PCA)",
      "to": "Dimension Reduction",
      "relationship": "subtopic"
    },
    {
      "from": "Tanh Function",
      "to": "Activation Functions",
      "relationship": "subtopic"
    },
    {
      "from": "EM Algorithm",
      "to": "Machine Learning Models",
      "relationship": "related_to"
    },
    {
      "from": "Kernel Functions",
      "to": "Feature Mapping",
      "relationship": "subtopic"
    },
    {
      "from": "Mini-batch SGD",
      "to": "Mini-batch Hyperparameters",
      "relationship": "depends_on"
    },
    {
      "from": "Fitted Value Iteration",
      "to": "State Sampling",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Feature Mapping",
      "relationship": "subtopic"
    },
    {
      "from": "Convolutional Layers",
      "to": "Efficiency Comparison",
      "relationship": "subtopic"
    },
    {
      "from": "Value Function",
      "to": "Optimal Value Function",
      "relationship": "defines"
    },
    {
      "from": "Modules in Modern Neural Networks",
      "to": "MLP Composition of Modules",
      "relationship": "subtopic"
    },
    {
      "from": "Support Vector Machines",
      "to": "Non-separable Case",
      "relationship": "related_to"
    },
    {
      "from": "Fully-Connected Neural Networks",
      "to": "Two-Layer Neural Network",
      "relationship": "example_of"
    },
    {
      "from": "Stochastic Gradient Descent",
      "to": "Learning Rate Decay",
      "relationship": "related_to"
    },
    {
      "from": "Backward Functions",
      "to": "Activations",
      "relationship": "subtopic"
    },
    {
      "from": "Polynomial Fitting",
      "to": "Overfitting",
      "relationship": "depends_on"
    },
    {
      "from": "EM Algorithms",
      "to": "EM for Mixture of Gaussians",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Techniques",
      "to": "EM Algorithms",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Optimization Constraints",
      "relationship": "depends_on"
    },
    {
      "from": "Model Evaluation",
      "to": "Mean Squared Error (MSE)",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Generalization Error",
      "relationship": "related_to"
    },
    {
      "from": "Model Parameters",
      "to": "Likelihood Estimation",
      "relationship": "depends_on"
    },
    {
      "from": "Mean Squared Error (MSE)",
      "to": "Average Model (h_avg)",
      "relationship": "depends_on"
    },
    {
      "from": "Bias-Variance Tradeoff",
      "to": "Variance Term",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Basics",
      "to": "Gradient Descent",
      "relationship": "related_to"
    },
    {
      "from": "Support Vector Machines (SVM)",
      "to": "Optimal Margin Classifier",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "EM_Algorithm",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine Learning Basics",
      "to": "Loss Functions",
      "relationship": "contains"
    },
    {
      "from": "Unsupervised learning",
      "to": "Independent components analysis",
      "relationship": "has_subtopic"
    },
    {
      "from": "Language Model Probability Distribution",
      "to": "Pretrained Large Language Models",
      "relationship": "subtopic"
    },
    {
      "from": "MLP (Multi-Layer Perceptron)",
      "to": "Matrix Multiplication Module",
      "relationship": "depends_on"
    },
    {
      "from": "Mixture of Gaussians Model",
      "to": "Latent Variables",
      "relationship": "subtopic"
    },
    {
      "from": "SMO Algorithm",
      "to": "Lagrange Multipliers",
      "relationship": "uses"
    },
    {
      "from": "Reinforcement learning",
      "to": "Value iteration and policy iteration",
      "relationship": "subtopic_of"
    },
    {
      "from": "SMO Algorithm (Optional)",
      "to": "SMO Details",
      "relationship": "subtopic"
    },
    {
      "from": "Geometric Margin",
      "to": "Decision Boundary",
      "relationship": "depends_on"
    },
    {
      "from": "Reinforcement Learning and Control",
      "to": "Reinforcement learning",
      "relationship": "has_subtopic"
    },
    {
      "from": "Bias-Variance Tradeoff",
      "to": "Bias Term",
      "relationship": "subtopic"
    },
    {
      "from": "Naive Bayes Algorithm",
      "to": "Bayesian Inference",
      "relationship": "related_to"
    },
    {
      "from": "Bernoulli Random Variable Z",
      "to": "Training Set Sampling",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Algorithms",
      "to": "Deep Learning Model Training Steps",
      "relationship": "contains"
    },
    {
      "from": "Machine Learning Algorithms",
      "to": "Policy Gradient (REINFORCE)",
      "relationship": "subtopic"
    },
    {
      "from": "Finetuning Algorithm",
      "to": "Adaptation Algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Double Descent Phenomenon",
      "to": "Model-wise Double Descent",
      "relationship": "has_subtopic"
    },
    {
      "from": "Double Descent Phenomenon",
      "to": "Model Complexity Measures",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Support Vector Machines (SVM)",
      "relationship": "related_to"
    },
    {
      "from": "Model Parameters",
      "to": "Log Likelihood Function",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Valid Kernels Conditions",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Backpropagation",
      "relationship": "depends_on"
    },
    {
      "from": "Optimization Problem",
      "to": "Scaling Constraint",
      "relationship": "subtopic"
    },
    {
      "from": "Log Likelihood",
      "to": "Maximum Likelihood Estimation",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Gradient Descent Optimizer",
      "relationship": "related_to"
    },
    {
      "from": "Model Complexity and Test Errors",
      "to": "Double Descent Phenomenon",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Algorithms",
      "to": "Stochastic Gradient Descent (SGD)",
      "relationship": "contains"
    },
    {
      "from": "Logistic Regression",
      "to": "Decision Boundaries",
      "relationship": "related_to"
    },
    {
      "from": "Belief State",
      "to": "Kalman Filter",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Models",
      "to": "Locally Weighted Linear Regression",
      "relationship": "has_subtopic"
    },
    {
      "from": "Parameter Estimation",
      "to": "Zero Frequency Problem",
      "relationship": "subtopic"
    },
    {
      "from": "Generalized Lagrangian",
      "to": "\\(\\theta_{\\cal P}(w)\\)",
      "relationship": "depends_on"
    },
    {
      "from": "Expectation Equations",
      "to": "Simplification of Formula (17.8)",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Policy Gradients",
      "relationship": "has_subtopic"
    },
    {
      "from": "Derived Features",
      "to": "School Quality",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Markov Decision Processes (MDP)",
      "relationship": "contains"
    },
    {
      "from": "Covariance Matrix",
      "to": "Multivariate Normal Distribution",
      "relationship": "depends_on"
    },
    {
      "from": "Labeled Dataset",
      "to": "Machine Learning Adaptation Methods",
      "relationship": "related_to"
    },
    {
      "from": "Self-supervised learning and foundation models",
      "to": "Pretrained large language models",
      "relationship": "has_subtopic"
    },
    {
      "from": "Zero-Shot Learning",
      "to": "Machine Learning Adaptation Methods",
      "relationship": "subtopic"
    },
    {
      "from": "Model Parameters",
      "to": "Likelihood Function",
      "relationship": "related_to"
    },
    {
      "from": "Baseline Estimation",
      "to": "Value Function Approximation",
      "relationship": "depends_on"
    },
    {
      "from": "Back-propagation for MLPs",
      "to": "Forward Pass in MLP",
      "relationship": "depends_on"
    },
    {
      "from": "Maximum Likelihood Estimation (MLE)",
      "to": "Likelihood Function",
      "relationship": "uses"
    },
    {
      "from": "Training Error vs Generalization Error",
      "to": "Probability of Error",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Models",
      "to": "Logistic Regression",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine Learning Models",
      "to": "Generalized Linear Model (GLM)",
      "relationship": "contains"
    },
    {
      "from": "Finite Horizon MDPs",
      "to": "Time-Dependent Policies",
      "relationship": "related_to"
    },
    {
      "from": "Feature Vector",
      "to": "Vocabulary",
      "relationship": "subtopic"
    },
    {
      "from": "Gradient Calculation",
      "to": "Log Probability Calculation",
      "relationship": "subtopic"
    },
    {
      "from": "Adaptation Methods",
      "to": "In-context Learning",
      "relationship": "subtopic"
    },
    {
      "from": "Eigenvectors of Sigma",
      "to": "Dimensionality Reduction",
      "relationship": "depends_on"
    },
    {
      "from": "Few-Shhot Learning",
      "to": "Machine Learning Adaptation Methods",
      "relationship": "subtopic"
    },
    {
      "from": "Optimal Value Function",
      "to": "Bellman's Equation",
      "relationship": "related_to"
    },
    {
      "from": "Backpropagation",
      "to": "Forward Pass",
      "relationship": "related_to"
    },
    {
      "from": "Backpropagation",
      "to": "Gradient Computation",
      "relationship": "subtopic"
    },
    {
      "from": "Fitted Value Iteration",
      "to": "Feature Mapping",
      "relationship": "related_to"
    },
    {
      "from": "Sample Complexity Bounds",
      "to": "Machine Learning Concepts",
      "relationship": "subtopic"
    },
    {
      "from": "Markov Decision Process (MDP)",
      "to": "State Transition Probabilities",
      "relationship": "depends_on"
    },
    {
      "from": "Maximum Likelihood Estimation",
      "to": "Machine Learning Basics",
      "relationship": "subtopic"
    },
    {
      "from": "Neural Networks",
      "to": "Two-Layer Neural Network",
      "relationship": "subtopic"
    },
    {
      "from": "Feature Mapping",
      "to": "Polynomial Kernels",
      "relationship": "contains"
    },
    {
      "from": "Sigmoid Function",
      "to": "Activation Functions",
      "relationship": "subtopic"
    },
    {
      "from": "Convex Functions",
      "to": "Strict Convexity",
      "relationship": "related_to"
    },
    {
      "from": "Optimization Problems",
      "to": "Affine Constraints",
      "relationship": "related_to"
    },
    {
      "from": "Normal Equations",
      "to": "Least Squares Revisited",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Binary Classification Problem",
      "relationship": "related_to"
    },
    {
      "from": "Generalization Error",
      "to": "Sample Complexity Bounds",
      "relationship": "subtopic"
    },
    {
      "from": "Geometric Margins",
      "to": "Machine Learning Concepts",
      "relationship": "subtopic"
    },
    {
      "from": "Model Parameters",
      "to": "Log-Likelihood Function",
      "relationship": "subtopic"
    },
    {
      "from": "Necessary Conditions for Valid Kernels",
      "to": "Symmetry Property",
      "relationship": "depends_on"
    },
    {
      "from": "Gradient Calculation",
      "to": "Matrix Derivatives",
      "relationship": "subtopic"
    },
    {
      "from": "Transformer Model",
      "to": "Conditional Probability",
      "relationship": "depends_on"
    },
    {
      "from": "Data Normalization",
      "to": "Variance Scaling",
      "relationship": "has_subtopic"
    },
    {
      "from": "Overfitting",
      "to": "Machine Learning Basics",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Housing Price Prediction",
      "relationship": "related_to"
    },
    {
      "from": "Reparameterization Trick",
      "to": "Gradient Estimation",
      "relationship": "subtopic"
    },
    {
      "from": "Value Iteration and Policy Iteration",
      "to": "Discretization in MDPs",
      "relationship": "subtopic"
    },
    {
      "from": "Kernels in Machine Learning",
      "to": "Kernel Functions",
      "relationship": "contains"
    },
    {
      "from": "Densities and Linear Transformations",
      "to": "Linear Transformation Impact",
      "relationship": "contains"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Double Descent Phenomenon",
      "relationship": "subtopic"
    },
    {
      "from": "Naive Bayes Algorithm",
      "to": "Binary Features",
      "relationship": "depends_on"
    },
    {
      "from": "Discriminative Learning Algorithms",
      "to": "Logistic Regression",
      "relationship": "related_to"
    },
    {
      "from": "BERT Pre-training Methodology",
      "to": "Machine Learning Literature",
      "relationship": "subtopic"
    },
    {
      "from": "Implicit Bias in Machine Learning",
      "to": "Surprises in High-Dimensional Ridgeless Least Squares Interpolation",
      "relationship": "subtopic"
    },
    {
      "from": "Hypothesis Class",
      "to": "Finite Hypothesis Classes",
      "relationship": "subtopic"
    },
    {
      "from": "Function Representation",
      "to": "Linear Function Approximation",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Adaptation Techniques",
      "to": "Zero-shot Learning",
      "relationship": "has_subtopic"
    },
    {
      "from": "Regularization",
      "to": "Regularizer Term",
      "relationship": "subtopic"
    },
    {
      "from": "Convergence Criteria",
      "to": "KKT Conditions",
      "relationship": "related_to"
    },
    {
      "from": "Generalization Gap",
      "to": "Training vs Test Distributions",
      "relationship": "related_to"
    },
    {
      "from": "Support Vector Machines",
      "to": "Regularization and Non-separable Case (Optional)",
      "relationship": "subtopic"
    },
    {
      "from": "Naive Bayes Algorithm",
      "to": "Discretization",
      "relationship": "subtopic"
    },
    {
      "from": "ELBO Optimization",
      "to": "Efficient Evaluation of ELBO",
      "relationship": "subtopic"
    },
    {
      "from": "Generalization Error Guarantees",
      "to": "Hoeffding Inequality",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Models",
      "to": "Conditional Probability Modeling",
      "relationship": "contains"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "2-D Convolution (Conv2D-S)",
      "relationship": "subtopic"
    },
    {
      "from": "Regularization in Machine Learning",
      "to": "Kernel Methods",
      "relationship": "related_to"
    },
    {
      "from": "Adaptation Phase",
      "to": "Transfer Learning",
      "relationship": "subtopic"
    },
    {
      "from": "Optimal Parameters Calculation",
      "to": "Intercept Term Calculation",
      "relationship": "depends_on"
    },
    {
      "from": "Naive Bayes Algorithm",
      "to": "Multinomial Features",
      "relationship": "related_to"
    },
    {
      "from": "Mixture of Gaussians",
      "to": "EM Algorithm Steps",
      "relationship": "subtopic"
    },
    {
      "from": "Step 2: Derive Optimal Policy",
      "to": "LQR Algorithm Steps",
      "relationship": "subtopic"
    },
    {
      "from": "Classification Problem",
      "to": "Linear Regression Approach",
      "relationship": "related_to"
    },
    {
      "from": "Log-Likelihood Optimization",
      "to": "Multiple Examples Case",
      "relationship": "related_to"
    },
    {
      "from": "Expectation Approximation",
      "to": "Deterministic Simulators",
      "relationship": "related_to"
    },
    {
      "from": "Variational Inference",
      "to": "Variational Auto-Encoder (VAE)",
      "relationship": "subtopic"
    },
    {
      "from": "EM Algorithm",
      "to": "Soft Assignments",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Models",
      "to": "Non-linear Feature Mappings",
      "relationship": "has_subtopic"
    },
    {
      "from": "Bayesian Inference",
      "to": "Posterior Approximation",
      "relationship": "subtopic"
    },
    {
      "from": "Generalized Linear Models",
      "to": "Exponential Family",
      "relationship": "subtopic"
    },
    {
      "from": "Principal Component Analysis (PCA)",
      "to": "Projection of Data Points",
      "relationship": "depends_on"
    },
    {
      "from": "Backpropagation Algorithm",
      "to": "Efficiency of Backward Functions",
      "relationship": "related_to"
    },
    {
      "from": "Probability Distributions",
      "to": "Gaussian Distribution",
      "relationship": "subtopic"
    },
    {
      "from": "Optimization Problems",
      "to": "Support Vectors",
      "relationship": "depends_on"
    },
    {
      "from": "LMS Update Rule",
      "to": "Widrow-Hoff Learning Rule",
      "relationship": "same_as"
    },
    {
      "from": "Markov Decision Processes (MDP)",
      "to": "Rewards Dependency on States and Actions",
      "relationship": "subtopic"
    },
    {
      "from": "ICA Ambiguities",
      "to": "Scaling Ambiguity",
      "relationship": "has_subtopic"
    },
    {
      "from": "Probability Distributions",
      "to": "Log Likelihood",
      "relationship": "depends_on"
    },
    {
      "from": "Negative Log-Likelihood",
      "to": "Conditional Probabilistic Models",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Models",
      "to": "Neural Networks",
      "relationship": "contains"
    },
    {
      "from": "Mean Squared Error (MSE)",
      "to": "Unavoidable Noise (\u03c3^2)",
      "relationship": "subtopic"
    },
    {
      "from": "Conditional Distribution Modeling",
      "to": "Bernoulli Distribution",
      "relationship": "subtopic"
    },
    {
      "from": "Naive Bayes",
      "to": "Laplace Smoothing",
      "relationship": "subtopic"
    },
    {
      "from": "MLP (Multi-Layer Perceptron)",
      "to": "Nonlinear Activation Module",
      "relationship": "depends_on"
    },
    {
      "from": "SIMCLR Algorithm",
      "to": "Augmentation Techniques",
      "relationship": "depends_on"
    },
    {
      "from": "Multi-class Classification",
      "to": "Multinomial Distribution",
      "relationship": "depends_on"
    },
    {
      "from": "Double Descent Phenomenon",
      "to": "Regularization Strength",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Basics",
      "to": "Gradient Descent",
      "relationship": "depends_on"
    },
    {
      "from": "Generalization",
      "to": "Bias-variance tradeoff",
      "relationship": "subtopic_of"
    },
    {
      "from": "Dual Problem",
      "to": "Dual Constraints",
      "relationship": "related_to"
    },
    {
      "from": "Finite Horizon MDPs",
      "to": "Dynamic Environment Models",
      "relationship": "subtopic"
    },
    {
      "from": "Generalized Linear Model (GLM)",
      "to": "Exponential Family Distributions",
      "relationship": "related_to"
    },
    {
      "from": "Policy Iteration",
      "to": "Bellman Updates",
      "relationship": "subtopic"
    },
    {
      "from": "Finite-horizon MDPs",
      "to": "Optimal Value Function Recovery",
      "relationship": "related_to"
    },
    {
      "from": "Support Vector Machines",
      "to": "Margins: Intuition",
      "relationship": "subtopic"
    },
    {
      "from": "Linear Model Limitations",
      "to": "Bias Definition",
      "relationship": "depends_on"
    },
    {
      "from": "Neural Networks",
      "to": "Classification Problem",
      "relationship": "depends_on"
    },
    {
      "from": "Batch Gradient Descent",
      "to": "Feature Map Phi",
      "relationship": "related_to"
    },
    {
      "from": "Generalization Error",
      "to": "Uniform Convergence",
      "relationship": "depends_on"
    },
    {
      "from": "Posterior Approximation",
      "to": "MAP Estimation",
      "relationship": "subtopic"
    },
    {
      "from": "Discretization",
      "to": "Continuous State MDPs",
      "relationship": "subtopic"
    },
    {
      "from": "ICA Ambiguities",
      "to": "Non-Gaussian Sources",
      "relationship": "has_subtopic"
    },
    {
      "from": "Cross Validation",
      "to": "k-fold Cross Validation",
      "relationship": "subtopic"
    },
    {
      "from": "Layer Normalization",
      "to": "LN-S(z)",
      "relationship": "depends_on"
    },
    {
      "from": "Training Error",
      "to": "Empirical Risk Minimization (ERM)",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Feature Selection",
      "relationship": "subtopic"
    },
    {
      "from": "LMS with Features",
      "to": "Gradient Descent Update Rule",
      "relationship": "depends_on"
    },
    {
      "from": "Deep Learning Representations",
      "to": "Complex Features in Neural Networks",
      "relationship": "subtopic"
    },
    {
      "from": "Matrix Multiplication Backward Function",
      "to": "Efficiency Considerations",
      "relationship": "has_subtopic"
    },
    {
      "from": "Data Augmentation",
      "to": "Positive Pair",
      "relationship": "subtopic"
    },
    {
      "from": "Regression Problems",
      "to": "Test Example",
      "relationship": "related_to"
    },
    {
      "from": "Value Function in RL",
      "to": "Optimal Value Function",
      "relationship": "subtopic"
    },
    {
      "from": "1-D Convolution Layer",
      "to": "Matrix Multiplication with Shared Parameters",
      "relationship": "subtopic"
    },
    {
      "from": "Training and Test Datasets",
      "to": "Quadratic Function Example",
      "relationship": "subtopic"
    },
    {
      "from": "Loss Functions",
      "to": "Training Loss",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Techniques",
      "to": "Adaptation Methods",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Naive Bayes Classifier",
      "relationship": "related_to"
    },
    {
      "from": "Mean Squared Error (MSE)",
      "to": "Claim 8.1.1",
      "relationship": "depends_on"
    },
    {
      "from": "Implicit Bias in Machine Learning",
      "to": "Shape Matters: Understanding the Implicit Bias of the Noise Covariance",
      "relationship": "related_to"
    },
    {
      "from": "Gaussian Data Limitation",
      "to": "Rotationally Symmetric Distributions",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Feature Maps and Kernels",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Jensen's Inequality",
      "relationship": "subtopic"
    },
    {
      "from": "Neural Networks",
      "to": "Activation Functions",
      "relationship": "depends_on"
    },
    {
      "from": "SMO Algorithm (Optional)",
      "to": "Coordinate Ascent",
      "relationship": "subtopic"
    },
    {
      "from": "Posterior Distribution",
      "to": "Predictive Distribution",
      "relationship": "related_to"
    },
    {
      "from": "LQG Framework",
      "to": "Machine Learning Concepts",
      "relationship": "depends_on"
    },
    {
      "from": "Modern Neural Networks",
      "to": "Vectorization over training examples",
      "relationship": "has_subtopic"
    },
    {
      "from": "Multinomial Event Model",
      "to": "Probability Calculation",
      "relationship": "subtopic"
    },
    {
      "from": "Exponential Family Distributions",
      "to": "Canonical Link Function",
      "relationship": "subtopic"
    },
    {
      "from": "Bayesian Classification",
      "to": "Posterior Distribution",
      "relationship": "subtopic"
    },
    {
      "from": "Probability Estimation",
      "to": "Multinomial Random Variable",
      "relationship": "subtopic"
    },
    {
      "from": "Fitted Value Iteration",
      "to": "Action Evaluation",
      "relationship": "depends_on"
    },
    {
      "from": "Model Complexity",
      "to": "Bias-Variance Tradeoff",
      "relationship": "related_to"
    },
    {
      "from": "Fully-Connected Neural Networks",
      "to": "ReLU Activation Function",
      "relationship": "uses"
    },
    {
      "from": "Machine Learning Algorithms",
      "to": "Coordinate Ascent Algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Piecewise Constant Representation",
      "to": "Discretization in MDPs",
      "relationship": "subtopic"
    },
    {
      "from": "Convolutional Neural Networks (CNN)",
      "to": "1-D Convolution Layer",
      "relationship": "subtopic"
    },
    {
      "from": "Supervised Learning",
      "to": "Deep Learning",
      "relationship": "subtopic"
    },
    {
      "from": "From non-linear dynamics to LQR",
      "to": "Differential Dynamic Programming (DDP)",
      "relationship": "has_subtopic"
    },
    {
      "from": "Training Loss",
      "to": "Mean Squared Error (MSE)",
      "relationship": "depends_on"
    },
    {
      "from": "Sequential Minimal Optimization (SMO)",
      "to": "Efficient Update Mechanism",
      "relationship": "subtopic"
    },
    {
      "from": "Linear Regression",
      "to": "Cost Function",
      "relationship": "subtopic"
    },
    {
      "from": "Linear Regression",
      "to": "Locally Weighted Linear Regression (LWLR)",
      "relationship": "subtopic"
    },
    {
      "from": "Hessian Matrix",
      "to": "Optimization in RL",
      "relationship": "related_to"
    },
    {
      "from": "Kalman Filter",
      "to": "Update Step",
      "relationship": "subtopic"
    },
    {
      "from": "Optimization Problem",
      "to": "Non-Convex Constraint",
      "relationship": "subtopic"
    },
    {
      "from": "Classification and Logistic Regression",
      "to": "Maximizing l(theta)",
      "relationship": "subtopic"
    },
    {
      "from": "Markov Decision Processes (MDP)",
      "to": "Policy Iteration",
      "relationship": "subtopic"
    },
    {
      "from": "Unit Vector w/||w||",
      "to": "Vector w",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Algorithms",
      "to": "Kernel Trick",
      "relationship": "related_to"
    },
    {
      "from": "Backward Function for Loss Functions",
      "to": "Cross-Entropy Loss",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Evidence Lower Bound (ELBO)",
      "relationship": "subtopic"
    },
    {
      "from": "Bayesian Machine Learning",
      "to": "Fully Bayesian Prediction",
      "relationship": "subtopic"
    },
    {
      "from": "Policy Iteration",
      "to": "VE Procedure",
      "relationship": "depends_on"
    },
    {
      "from": "Neural Networks",
      "to": "Backward Functions",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Locally Weighted Linear Regression (LWLR)",
      "relationship": "subtopic"
    },
    {
      "from": "Efficient Evaluation of ELBO",
      "to": "Gaussian Distributions",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "ELBO Interpretation",
      "relationship": "contains"
    },
    {
      "from": "Logistic Regression Derivation",
      "to": "Perceptron Algorithm",
      "relationship": "related_to"
    },
    {
      "from": "Generative Learning Algorithms",
      "to": "Multinomial Event Model",
      "relationship": "subtopic"
    },
    {
      "from": "Generalized Linear Models (GLMs)",
      "to": "Exponential Family Distributions",
      "relationship": "depends_on"
    },
    {
      "from": "Unlabeled Dataset",
      "to": "Pretraining Phase",
      "relationship": "related_to"
    },
    {
      "from": "Deep Learning Representations",
      "to": "House Price Prediction Example",
      "relationship": "subtopic"
    },
    {
      "from": "Supervised Learning",
      "to": "Linear Regression",
      "relationship": "subtopic"
    },
    {
      "from": "Physics Simulation",
      "to": "Open Dynamics Engine",
      "relationship": "related_to"
    },
    {
      "from": "M-step Update Rule",
      "to": "Lagrangian Method",
      "relationship": "related_to"
    },
    {
      "from": "Contrastive Learning",
      "to": "Negative Pair",
      "relationship": "related_to"
    },
    {
      "from": "Policy Gradient Methods",
      "to": "Law of Total Expectation",
      "relationship": "related_to"
    },
    {
      "from": "Feature Maps and Kernels",
      "to": "Kernel Function Definition",
      "relationship": "subtopic"
    },
    {
      "from": "Regularization",
      "to": "Regularized Loss",
      "relationship": "related_to"
    },
    {
      "from": "Gaussian Data Example",
      "to": "Mixed Data x'",
      "relationship": "has_subtopic"
    },
    {
      "from": "Multi-layer Fully-Connected Neural Networks",
      "to": "Total Number of Neurons",
      "relationship": "subtopic"
    },
    {
      "from": "Efficient Update Mechanism",
      "to": "Constraints Handling",
      "relationship": "depends_on"
    },
    {
      "from": "Reinforcement learning",
      "to": "Continuous state MDPs",
      "relationship": "has_subtopic"
    },
    {
      "from": "Training Set",
      "to": "Stop Words",
      "relationship": "related_to"
    },
    {
      "from": "Pilot Survey Example",
      "to": "Data Redundancy Detection",
      "relationship": "related_to"
    },
    {
      "from": "Kalman Filter",
      "to": "Step 2",
      "relationship": "subtopic"
    },
    {
      "from": "EM algorithms",
      "to": "Mixture of Gaussians revisited",
      "relationship": "subtopic_of"
    },
    {
      "from": "Batch Gradient Descent",
      "to": "Beta Update Equation",
      "relationship": "subtopic"
    },
    {
      "from": "Double Descent Phenomenon",
      "to": "Overparameterized Models",
      "relationship": "related_to"
    },
    {
      "from": "Sigmoid Function",
      "to": "Tanh Function",
      "relationship": "related_to"
    },
    {
      "from": "Natural Parameter for Bernoulli",
      "to": "Bernoulli Distribution",
      "relationship": "subtopic"
    },
    {
      "from": "Linear Regression",
      "to": "Underfitting",
      "relationship": "related_to"
    },
    {
      "from": "Classification",
      "to": "Binary Classification",
      "relationship": "subtopic"
    },
    {
      "from": "Multivariate Normal Distributions for Classes",
      "to": "Gaussian Discriminant Analysis (GDA)",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Models",
      "to": "Transformer Model",
      "relationship": "contains"
    },
    {
      "from": "Spam Filter",
      "to": "Training Set",
      "relationship": "depends_on"
    },
    {
      "from": "EM Algorithm",
      "to": "M-step",
      "relationship": "subtopic"
    },
    {
      "from": "Supervised Learning",
      "to": "Classification and Logistic Regression",
      "relationship": "subtopic"
    },
    {
      "from": "Cross-Entropy Loss",
      "to": "Probabilistic Model",
      "relationship": "related_to"
    },
    {
      "from": "Regularization and model selection",
      "to": "Bayesian statistics and regularization",
      "relationship": "has_subtopic"
    },
    {
      "from": "Maximum Likelihood Estimation (MLE)",
      "to": "Probabilistic Assumptions",
      "relationship": "depends_on"
    },
    {
      "from": "Generative Learning Algorithms",
      "to": "Gaussian Discriminant Analysis",
      "relationship": "subtopic"
    },
    {
      "from": "Gradient Descent",
      "to": "Batch Gradient Descent",
      "relationship": "subtopic"
    },
    {
      "from": "Negative Log-Likelihood",
      "to": "Probabilistic Model",
      "relationship": "depends_on"
    },
    {
      "from": "Cross Validation",
      "to": "Hold-out Cross Validation",
      "relationship": "subtopic"
    },
    {
      "from": "Expectation-Maximization (EM) Algorithm",
      "to": "Reparameterization Trick",
      "relationship": "related_to"
    },
    {
      "from": "Gaussian Discriminant Analysis (GDA)",
      "to": "Decision Boundary",
      "relationship": "related_to"
    },
    {
      "from": "Support Vector Machines (SVM)",
      "to": "SMO Algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Expectation-Maximization Algorithm",
      "to": "E-step Calculation",
      "relationship": "has_subtopic"
    },
    {
      "from": "Update Step",
      "to": "Kalman Gain",
      "relationship": "has_subtopic"
    },
    {
      "from": "Gaussian Discriminant Analysis (GDA)",
      "to": "Model Parameters",
      "relationship": "depends_on"
    },
    {
      "from": "Learning a model for an MDP",
      "to": "Connections between Policy and Value Iteration (Optional)",
      "relationship": "has_subtopic"
    },
    {
      "from": "Kernel Trick",
      "to": "Linear Combination Representation",
      "relationship": "subtopic"
    },
    {
      "from": "Logistic Regression",
      "to": "Logistic Function",
      "relationship": "depends_on"
    },
    {
      "from": "Kernel Functions",
      "to": "Sufficient Conditions for Valid Kernels",
      "relationship": "subtopic"
    },
    {
      "from": "Likelihood Function",
      "to": "Log-Likelihood",
      "relationship": "related_to"
    },
    {
      "from": "Logistic Regression",
      "to": "Margins",
      "relationship": "related_to"
    },
    {
      "from": "Value Iteration",
      "to": "Continuous State MDPs",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Value Iteration",
      "relationship": "contains"
    },
    {
      "from": "Single Neuron Network",
      "to": "ReLU Function",
      "relationship": "uses"
    },
    {
      "from": "Naive Bayes Classifier",
      "to": "Bernoulli Event Model",
      "relationship": "related_to"
    },
    {
      "from": "Gaussian Discriminant Analysis",
      "to": "Multivariate Normal Distribution",
      "relationship": "subtopic"
    },
    {
      "from": "Fully-Connected Neural Networks",
      "to": "Intermediate Variables (a_i)",
      "relationship": "depends_on"
    },
    {
      "from": "Vector w",
      "to": "Decision Boundary",
      "relationship": "related_to"
    },
    {
      "from": "Principal Component Analysis (PCA)",
      "to": "Data Visualization",
      "relationship": "subtopic"
    },
    {
      "from": "Generalized Linear Model (GLM)",
      "to": "Prediction Goal",
      "relationship": "subtopic"
    },
    {
      "from": "Classification",
      "to": "Multi-class Classification",
      "relationship": "subtopic"
    },
    {
      "from": "Hypothesis Space",
      "to": "Parameterization of Hypotheses",
      "relationship": "subtopic"
    },
    {
      "from": "Chapter 15 Overview",
      "to": "Policy Iteration Speedup",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Models",
      "to": "Regression Problems",
      "relationship": "has_subtopic"
    },
    {
      "from": "Reinforcement learning",
      "to": "Learning a model for an MDP",
      "relationship": "subtopic_of"
    },
    {
      "from": "Bayesian Classification",
      "to": "Class Priors",
      "relationship": "depends_on"
    },
    {
      "from": "Parameter Updates",
      "to": "\u03b8 Update Rule",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Output Layer",
      "relationship": "subtopic"
    },
    {
      "from": "Feature Maps and Kernels",
      "to": "Inner Product Computation",
      "relationship": "related_to"
    },
    {
      "from": "Synchronous Updates",
      "to": "Value Iteration Algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Naive Bayes Spam Filter",
      "to": "Parameter Estimation",
      "relationship": "depends_on"
    },
    {
      "from": "Dynamic Programming Application",
      "to": "Machine Learning Concepts",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Maximum Likelihood Estimation (MLE)",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Algorithms",
      "to": "Mini-batch SGD",
      "relationship": "contains"
    },
    {
      "from": "Kernel Functions",
      "to": "Computational Efficiency",
      "relationship": "related_to"
    },
    {
      "from": "Multinomial Random Variable",
      "to": "Maximum Likelihood Estimates",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Models",
      "to": "Fully-Connected Neural Networks",
      "relationship": "contains"
    },
    {
      "from": "Batch Normalization Variants",
      "to": "Layer Normalization (LN)",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning",
      "to": "Contrastive Learning",
      "relationship": "related_to"
    },
    {
      "from": "Support Vector Machines",
      "to": "Optimal Margin Classifier (Optional)",
      "relationship": "subtopic"
    },
    {
      "from": "Cross Validation",
      "to": "Leave-One-Out CV",
      "relationship": "has_subtopic"
    },
    {
      "from": "Learning Model for MDPs",
      "to": "State Transition Probabilities Estimation",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Models",
      "to": "Classification Model",
      "relationship": "has_subtopic"
    },
    {
      "from": "Perceptron Algorithm",
      "to": "Multi-class Classification",
      "relationship": "depends_on"
    },
    {
      "from": "Generalized Linear Models (GLMs)",
      "to": "Ordinary Least Squares",
      "relationship": "subtopic"
    },
    {
      "from": "VE Procedure",
      "to": "k Parameter",
      "relationship": "depends_on"
    },
    {
      "from": "Stochastic Gradient Descent (SGD)",
      "to": "Gradient Calculation",
      "relationship": "subtopic"
    },
    {
      "from": "Convergence Proof",
      "to": "Jensen's Inequality",
      "relationship": "depends_on"
    },
    {
      "from": "Exponential Family Distributions",
      "to": "Log Partition Function",
      "relationship": "subtopic"
    },
    {
      "from": "VAE",
      "to": "Re-parametrization Trick",
      "relationship": "subtopic"
    },
    {
      "from": "Regularization",
      "to": "Training Loss/Cost Function",
      "relationship": "depends_on"
    },
    {
      "from": "Mini-batch SGD",
      "to": "Mini-batch Gradient Calculation",
      "relationship": "subtopic"
    },
    {
      "from": "Neural Networks",
      "to": "Single Neuron Network",
      "relationship": "subtopic"
    },
    {
      "from": "Backpropagation",
      "to": "Preliminaries on Partial Derivatives",
      "relationship": "subtopic"
    },
    {
      "from": "Sample Complexity Bound",
      "to": "Corollary Proof",
      "relationship": "subtopic"
    },
    {
      "from": "Regression Problems",
      "to": "Bias-Variance Tradeoff",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "ELBO Optimization",
      "relationship": "contains"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Linear Quadratic Regulator (LQR)",
      "relationship": "related_to"
    },
    {
      "from": "Objective Function Dual",
      "to": "Value Dual Problem",
      "relationship": "subtopic"
    },
    {
      "from": "Deep Residual Learning",
      "to": "Deep Residual Learning for Image Recognition",
      "relationship": "related_to"
    },
    {
      "from": "Backpropagation",
      "to": "Back-propagation for MLPs",
      "relationship": "subtopic_of"
    },
    {
      "from": "VAE",
      "to": "Variational Inference",
      "relationship": "subtopic"
    },
    {
      "from": "Bayesian Statistics",
      "to": "Prior Distribution",
      "relationship": "has_subtopic"
    },
    {
      "from": "Linear Regression",
      "to": "Locally Weighted Linear Regression",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Algorithms",
      "to": "Discriminative Learning Algorithms",
      "relationship": "subtopic"
    },
    {
      "from": "Coordinate Ascent Algorithm",
      "to": "Sequential Minimal Optimization (SMO) Algorithm",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Basics",
      "to": "Probability Distributions",
      "relationship": "depends_on"
    },
    {
      "from": "Sample Complexity",
      "to": "Generalization Error",
      "relationship": "depends_on"
    },
    {
      "from": "Confidence Measure Limitation",
      "to": "Normalization Condition",
      "relationship": "related_to"
    },
    {
      "from": "Gaussian Discriminant Analysis (GDA)",
      "to": "Machine Learning Models",
      "relationship": "subtopic"
    },
    {
      "from": "Naive Bayes",
      "to": "Event Models for Text Classification",
      "relationship": "subtopic"
    },
    {
      "from": "Matrix Multiplication Backward Function",
      "to": "Vectorized Notation",
      "relationship": "has_subtopic"
    },
    {
      "from": "Optimization Problems",
      "to": "KKT Conditions",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "k-means Algorithm",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "EM Algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Newton's Method",
      "to": "Maximizing Functions",
      "relationship": "related_to"
    },
    {
      "from": "M-step Maximization",
      "to": "Parameter Updates",
      "relationship": "has_subtopic"
    },
    {
      "from": "Beta Update Equation",
      "to": "Inner Product Computation",
      "relationship": "depends_on"
    },
    {
      "from": "Kernel Methods",
      "to": "Properties of Kernels",
      "relationship": "subtopic"
    },
    {
      "from": "1-D Convolution Layer",
      "to": "Filter Vector",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Techniques",
      "to": "Model Selection",
      "relationship": "contains"
    },
    {
      "from": "Exponential Family Distributions",
      "to": "Sufficient Statistic",
      "relationship": "subtopic"
    },
    {
      "from": "Bias-Variance Tradeoff",
      "to": "Expected Test Error",
      "relationship": "depends_on"
    },
    {
      "from": "Curse of Dimensionality",
      "to": "Discretization in MDPs",
      "relationship": "subtopic"
    },
    {
      "from": "Implicit Regularization Effect",
      "to": "Regularization in Deep Learning",
      "relationship": "subtopic"
    },
    {
      "from": "Self-Supervised Learning",
      "to": "Supervised Contrastive Algorithms",
      "relationship": "related_to"
    },
    {
      "from": "Logistic Regression",
      "to": "Conditional Distribution",
      "relationship": "subtopic"
    },
    {
      "from": "Neural Networks",
      "to": "Multi-layer Networks",
      "relationship": "related_to"
    },
    {
      "from": "Clustering",
      "to": "K-Means Algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Inner Product Computation",
      "to": "Pre-computation Strategy",
      "relationship": "subtopic"
    },
    {
      "from": "Loss Function",
      "to": "Negative Log-Likelihood",
      "relationship": "depends_on"
    },
    {
      "from": "MLP Model",
      "to": "Modules and Parameters",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Models",
      "to": "Matricization Approach",
      "relationship": "related_to"
    },
    {
      "from": "Partial Derivatives in ML",
      "to": "Multi-Variate Function Challenges",
      "relationship": "related_to"
    },
    {
      "from": "Spam Filter",
      "to": "Feature Vector",
      "relationship": "depends_on"
    },
    {
      "from": "ICA Ambiguities",
      "to": "Sign Changes Irrelevance",
      "relationship": "has_subtopic"
    },
    {
      "from": "Markov Decision Process (MDP)",
      "to": "Learning in MDPs with Unknown Transitions",
      "relationship": "subtopic"
    },
    {
      "from": "Optimization Problems",
      "to": "Duality Gap",
      "relationship": "subtopic"
    },
    {
      "from": "Unsupervised Learning",
      "to": "Clustering",
      "relationship": "subtopic"
    },
    {
      "from": "ICA Ambiguities",
      "to": "Sign Change Ambiguity",
      "relationship": "has_subtopic"
    },
    {
      "from": "Lagrangian Function",
      "to": "Dual Problem",
      "relationship": "related_to"
    },
    {
      "from": "Independent Component Analysis (ICA)",
      "to": "Mixing Matrix",
      "relationship": "depends_on"
    },
    {
      "from": "LQR, DDP and LQG",
      "to": "Linear Quadratic Gaussian (LQG)",
      "relationship": "has_subtopic"
    },
    {
      "from": "Optimizers",
      "to": "Stochastic Gradient Descent (SGD)",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Models",
      "to": "Neural Networks",
      "relationship": "depends_on"
    },
    {
      "from": "Regularized Loss",
      "to": "\u03bb (Lambda)",
      "relationship": "related_to"
    },
    {
      "from": "Policy Gradient Methods",
      "to": "Trajectory Collection",
      "relationship": "has_subtopic"
    },
    {
      "from": "Regularization and model selection",
      "to": "Regularization",
      "relationship": "has_subtopic"
    },
    {
      "from": "EM_Algorithm",
      "to": "Single_Example_Optimization",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Models",
      "to": "Non-linear Model h_\u03b8(x)",
      "relationship": "has_subtopic"
    },
    {
      "from": "Backpropagation",
      "to": "Intermediate Values Storage",
      "relationship": "depends_on"
    },
    {
      "from": "EM_Algorithm",
      "to": "E_Step",
      "relationship": "has_subtopic"
    },
    {
      "from": "Optimization Constraints",
      "to": "Box Constraint",
      "relationship": "subtopic"
    },
    {
      "from": "Kalman Filter",
      "to": "Belief States Update",
      "relationship": "depends_on"
    },
    {
      "from": "Bias-Variance Tradeoff",
      "to": "Machine Learning Basics",
      "relationship": "depends_on"
    },
    {
      "from": "Optimization Problem",
      "to": "Convex Quadratic Objective",
      "relationship": "subtopic"
    },
    {
      "from": "Weight Matrices and Biases",
      "to": "Total Number of Neurons",
      "relationship": "related_to"
    },
    {
      "from": "Generalized Linear Models",
      "to": "Constructing GLMs",
      "relationship": "subtopic"
    },
    {
      "from": "Model Selection",
      "to": "k-fold Cross Validation",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Models",
      "to": "Training Examples",
      "relationship": "has_subtopic"
    },
    {
      "from": "Linear Model Prediction",
      "to": "Learning Algorithm",
      "relationship": "depends_on"
    },
    {
      "from": "Contrastive Learning Framework",
      "to": "Machine Learning Literature",
      "relationship": "subtopic"
    },
    {
      "from": "Markov Decision Processes (MDP)",
      "to": "Expectation Calculation",
      "relationship": "subtopic"
    },
    {
      "from": "Naive Bayes Classifier",
      "to": "Conditional Independence Assumption",
      "relationship": "subtopic"
    },
    {
      "from": "State Transition Probabilities",
      "to": "Estimating State Transitions",
      "relationship": "subtopic"
    },
    {
      "from": "Concave Functions",
      "to": "Jensen's Inequality",
      "relationship": "related_to"
    },
    {
      "from": "Functional Margin",
      "to": "Function Margin with Training Set",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "LMS with Features",
      "relationship": "subtopic"
    },
    {
      "from": "Variational Inference Review",
      "to": "Machine Learning Literature",
      "relationship": "subtopic"
    },
    {
      "from": "Unsupervised learning",
      "to": "Clustering and the k-means algorithm",
      "relationship": "has_subtopic"
    },
    {
      "from": "Model Selection",
      "to": "Hold Out Cross Validation",
      "relationship": "subtopic"
    },
    {
      "from": "Data Scarcity",
      "to": "k-fold Cross Validation",
      "relationship": "related_to"
    },
    {
      "from": "Functional Margin",
      "to": "Geometric Margin",
      "relationship": "related_to"
    },
    {
      "from": "Dynamics of Model",
      "to": "Optimal Value Function",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Overview",
      "to": "Supervised Learning",
      "relationship": "contains"
    },
    {
      "from": "Pretraining Methods in CV",
      "to": "Supervised Pretraining",
      "relationship": "subtopic"
    },
    {
      "from": "Gaussian Data Example",
      "to": "Rotation Matrix R",
      "relationship": "has_subtopic"
    },
    {
      "from": "Supervised Learning",
      "to": "Hypothesis",
      "relationship": "subtopic"
    },
    {
      "from": "ELBO Interpretation",
      "to": "Alternative ELBO Formulations",
      "relationship": "subtopic"
    },
    {
      "from": "Vectorization in Neural Networks",
      "to": "BLAS Packages",
      "relationship": "related_to"
    },
    {
      "from": "Training Example Independence Assumption",
      "to": "Machine Learning Concepts",
      "relationship": "depends_on"
    },
    {
      "from": "Linearization of Dynamics",
      "to": "Rewriting Dynamics",
      "relationship": "subtopic"
    },
    {
      "from": "Text Classification",
      "to": "Spam Filter",
      "relationship": "subtopic"
    },
    {
      "from": "Kernel Methods",
      "to": "Feature Maps",
      "relationship": "subtopic"
    },
    {
      "from": "Maximum Likelihood Estimation",
      "to": "Laplace Smoothing",
      "relationship": "depends_on"
    },
    {
      "from": "Model Selection",
      "to": "Cross Validation",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Algorithms",
      "to": "Newton's Method",
      "relationship": "subtopic"
    },
    {
      "from": "Parallelism in Training Examples",
      "to": "Basic Idea",
      "relationship": "subtopic"
    },
    {
      "from": "Reinforcement Learning Overview",
      "to": "Total Payoff Calculation",
      "relationship": "subtopic"
    },
    {
      "from": "Multi-layer Fully-Connected Neural Networks",
      "to": "Weight Matrices and Biases",
      "relationship": "subtopic"
    },
    {
      "from": "Prediction Equation",
      "to": "Intercept Term Calculation",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Overview",
      "to": "Fitted Value Iteration",
      "relationship": "contains"
    },
    {
      "from": "Optimal Parameters Calculation",
      "to": "Prediction Equation",
      "relationship": "subtopic"
    },
    {
      "from": "Logistic Regression",
      "to": "Negative Log-Likelihood",
      "relationship": "depends_on"
    },
    {
      "from": "Classification and Logistic Regression",
      "to": "Perceptron Learning Algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Latent Variable Models",
      "to": "EM Algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Algorithms",
      "to": "Neural Networks Overview",
      "relationship": "related_to"
    },
    {
      "from": "Linear Regression",
      "to": "Gradient Descent",
      "relationship": "related_to"
    },
    {
      "from": "Value Iteration",
      "to": "Expectation Approximation",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Convolutional Neural Networks (CNNs)",
      "relationship": "subtopic"
    },
    {
      "from": "Generalized Linear Models (GLMs)",
      "to": "Logistic Regression",
      "relationship": "subtopic"
    },
    {
      "from": "Backward Functions",
      "to": "Loss Functions",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Hidden Units",
      "relationship": "subtopic"
    },
    {
      "from": "LQR, DDP and LQG",
      "to": "Linear Quadratic Regulation (LQR)",
      "relationship": "has_subtopic"
    },
    {
      "from": "Bayesian Statistics",
      "to": "MLE",
      "relationship": "related_to"
    },
    {
      "from": "Optimization Constraints",
      "to": "Quadratic Function",
      "relationship": "related_to"
    },
    {
      "from": "Baseline Estimation",
      "to": "Gradient Estimator Update",
      "relationship": "leads_to"
    },
    {
      "from": "Neural Networks Parameters",
      "to": "Deep Learning",
      "relationship": "subtopic"
    },
    {
      "from": "Contour Plots",
      "to": "Multivariate Normal Distribution",
      "relationship": "related_to"
    },
    {
      "from": "Optimal Policy",
      "to": "Linear Optimal Action",
      "relationship": "subtopic"
    },
    {
      "from": "Bias-Variance Tradeoff",
      "to": "Variance Term",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine Learning Models",
      "to": "Gaussian Discriminant Analysis (GDA)",
      "relationship": "subtopic"
    },
    {
      "from": "Optimization Problem",
      "to": "Linear Constraints",
      "relationship": "subtopic"
    },
    {
      "from": "Gradient Descent",
      "to": "Stochastic Gradient Descent",
      "relationship": "subtopic"
    },
    {
      "from": "Bias-Variance Tradeoff",
      "to": "Sample Complexity Bounds",
      "relationship": "subtopic"
    },
    {
      "from": "Gradient Descent Methods",
      "to": "Batch Gradient Descent",
      "relationship": "contains"
    },
    {
      "from": "Optimizers and Generalization",
      "to": "Implicit Regularization",
      "relationship": "contains"
    },
    {
      "from": "Machine Learning Adaptation Techniques",
      "to": "In-context Learning",
      "relationship": "has_subtopic"
    },
    {
      "from": "Conditional Probability",
      "to": "Softmax Function",
      "relationship": "subtopic"
    },
    {
      "from": "Policy Definition",
      "to": "Value Function",
      "relationship": "depends_on"
    },
    {
      "from": "Feature Maps and Kernels",
      "to": "Algorithm Implementation",
      "relationship": "subtopic"
    },
    {
      "from": "Likelihood Function",
      "to": "Probability Distribution",
      "relationship": "subtopic"
    },
    {
      "from": "Markov Decision Processes (MDPs)",
      "to": "Learning Model for MDPs",
      "relationship": "contains"
    },
    {
      "from": "Inner Product Computation",
      "to": "Efficient Inner Product Calculation",
      "relationship": "related_to"
    },
    {
      "from": "Continuous State Space",
      "to": "Markov Decision Process (MDP)",
      "relationship": "related_to"
    },
    {
      "from": "Log Likelihood Function",
      "to": "Gradient Ascent Rule",
      "relationship": "subtopic"
    },
    {
      "from": "Gaussian Discriminant Analysis",
      "to": "GDA Model",
      "relationship": "subtopic"
    },
    {
      "from": "Learning Model for MDPs",
      "to": "Inverted Pendulum Problem",
      "relationship": "example_of"
    },
    {
      "from": "Optimizers",
      "to": "Gradient Descent (GD)",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Models",
      "to": "Conditional Distribution Modeling",
      "relationship": "depends_on"
    },
    {
      "from": "Supervised Learning Algorithm",
      "to": "Linear Regression",
      "relationship": "subtopic"
    },
    {
      "from": "LQR Algorithm Steps",
      "to": "Machine Learning Concepts",
      "relationship": "subtopic"
    },
    {
      "from": "Functional Margins",
      "to": "Machine Learning Concepts",
      "relationship": "subtopic"
    },
    {
      "from": "Supervised Learning",
      "to": "Classification Problem",
      "relationship": "related_to"
    },
    {
      "from": "Immediate Reward",
      "to": "Bellman Equations",
      "relationship": "related_to"
    },
    {
      "from": "Sample-wise Double Descent",
      "to": "Optimal Algorithms",
      "relationship": "depends_on"
    },
    {
      "from": "Multi-layer Fully-Connected Neural Networks",
      "to": "Total Number of Parameters",
      "relationship": "subtopic"
    },
    {
      "from": "Normal Equations Method",
      "to": "Matrix Derivatives",
      "relationship": "contains"
    },
    {
      "from": "Transformer Model",
      "to": "Input-Output Interface",
      "relationship": "subtopic"
    },
    {
      "from": "Learning Guarantees",
      "to": "Binary Classification",
      "relationship": "related_to"
    },
    {
      "from": "Machine_Learning_Theory",
      "to": "Double_Descent_Phenomenon",
      "relationship": "subtopic"
    },
    {
      "from": "Jensen's Inequality",
      "to": "Evidence Lower Bound (ELBO)",
      "relationship": "depends_on"
    },
    {
      "from": "Markov Decision Processes (MDP)",
      "to": "Discount Factor",
      "relationship": "includes"
    },
    {
      "from": "Generalization Error",
      "to": "PAC Assumptions",
      "relationship": "depends_on"
    },
    {
      "from": "VE Procedure",
      "to": "Initialization Option 1",
      "relationship": "related_to"
    },
    {
      "from": "Differential Dynamic Programming (DDP)",
      "to": "Reward Function Approximation",
      "relationship": "subtopic"
    },
    {
      "from": "Stochastic Gradient Descent (SGD)",
      "to": "Hyperparameters",
      "relationship": "depends_on"
    },
    {
      "from": "Loss Functions",
      "to": "Test Error",
      "relationship": "subtopic"
    },
    {
      "from": "Linear Regression",
      "to": "Overfitting",
      "relationship": "related_to"
    },
    {
      "from": "Self-Supervised Loss",
      "to": "Pretraining Phase",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Adaptation Methods",
      "to": "Pretraining Methods in CV",
      "relationship": "subtopic"
    },
    {
      "from": "Logistic Regression",
      "to": "Robustness of Logistic Regression",
      "relationship": "subtopic"
    },
    {
      "from": "Double Descent Phenomenon",
      "to": "Model-wise Double Descent",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Policy Gradient Methods",
      "relationship": "has_subtopic"
    },
    {
      "from": "Distortion Function",
      "to": "k-means Algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Polynomial Fitting",
      "to": "Variance",
      "relationship": "related_to"
    },
    {
      "from": "Dual Problem Formulation",
      "to": "KKT Conditions",
      "relationship": "depends_on"
    },
    {
      "from": "Model Parameters (w, b)",
      "to": "Scaling Constraint on w",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Jensen's Inequality",
      "relationship": "related_to"
    },
    {
      "from": "Backpropagation",
      "to": "General Strategy of Backpropagation",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Models",
      "to": "Deterministic vs Stochastic Models",
      "relationship": "has_subtopic"
    },
    {
      "from": "Backward Function in Machine Learning",
      "to": "Chain Rule Application",
      "relationship": "subtopic"
    },
    {
      "from": "Generalization",
      "to": "Sample complexity bounds (optional readings)",
      "relationship": "has_subtopic"
    },
    {
      "from": "Optimal Intercept b",
      "to": "Lagrangian Optimization",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Algorithms",
      "to": "Batch Gradient Descent",
      "relationship": "depends_on"
    },
    {
      "from": "Policy Gradient Theorem",
      "to": "Log Probability Derivative",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Algorithms",
      "to": "Linear Regression",
      "relationship": "contains"
    },
    {
      "from": "Probability Distribution",
      "to": "Design Matrix X",
      "relationship": "depends_on"
    },
    {
      "from": "Kernels as Similarity Metrics",
      "to": "Gaussian Kernel",
      "relationship": "subtopic"
    },
    {
      "from": "Functional Margin",
      "to": "Confidence Measure Limitation",
      "relationship": "depends_on"
    },
    {
      "from": "Contrastive Learning",
      "to": "SIMCLR Algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Models",
      "to": "GDA (Generative Discriminative Approach)",
      "relationship": "has_subtopic"
    },
    {
      "from": "Bernoulli Distribution",
      "to": "Logistic Function",
      "relationship": "subtopic"
    },
    {
      "from": "Primal Problem",
      "to": "Primal Constraints",
      "relationship": "related_to"
    },
    {
      "from": "Convex Functions",
      "to": "Jensen's Inequality",
      "relationship": "depends_on"
    },
    {
      "from": "Independence Assumption",
      "to": "Likelihood Function",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Algorithms",
      "to": "Locally Weighted Linear Regression",
      "relationship": "includes"
    },
    {
      "from": "Layer Normalization (LN)",
      "to": "Affine Transformation in LN",
      "relationship": "subtopic"
    },
    {
      "from": "LQR, DDP and LQG",
      "to": "Finite-horizon MDPs",
      "relationship": "has_subtopic"
    },
    {
      "from": "Non-separable Case",
      "to": "Optimization Problem",
      "relationship": "depends_on"
    },
    {
      "from": "EM Algorithm",
      "to": "K-means Clustering",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Basics",
      "to": "Empirical Risk Minimization (ERM)",
      "relationship": "has_subtopic"
    },
    {
      "from": "Optimization Problems",
      "to": "SVM Optimization Problem",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Models",
      "to": "Logistic Regression",
      "relationship": "subtopic"
    },
    {
      "from": "Deep Learning",
      "to": "Machine Learning Overview",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Value Function Approximation",
      "relationship": "related_to"
    },
    {
      "from": "ELBO Optimization",
      "to": "Gradient Ascent in ELBO",
      "relationship": "subtopic"
    },
    {
      "from": "Training Set",
      "to": "Maximum Likelihood Estimation",
      "relationship": "depends_on"
    },
    {
      "from": "Gradient Descent",
      "to": "Learning Rate (\u03b1)",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Value Function in RL",
      "relationship": "depends_on"
    },
    {
      "from": "Convolutional Neural Networks (CNNs)",
      "to": "1-D Convolution Layer",
      "relationship": "subtopic"
    },
    {
      "from": "Layer Normalization",
      "to": "Scaling-Invariant Property",
      "relationship": "subtopic"
    },
    {
      "from": "Projection of Data Points",
      "to": "Variance Maximization",
      "relationship": "subtopic"
    },
    {
      "from": "Supervised Learning",
      "to": "Regression Problem",
      "relationship": "related_to"
    },
    {
      "from": "System Dynamics",
      "to": "Step 1",
      "relationship": "depends_on"
    },
    {
      "from": "Double Descent Phenomenon",
      "to": "Bias-Variance Tradeoff",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Principal Components Analysis (PCA)",
      "relationship": "related_to"
    },
    {
      "from": "Constrained Optimization",
      "to": "Primal Problem",
      "relationship": "subtopic"
    },
    {
      "from": "Mixture of Gaussians Model",
      "to": "Joint Distribution",
      "relationship": "subtopic"
    },
    {
      "from": "Loss Function",
      "to": "Intermediate Variables",
      "relationship": "subtopic"
    },
    {
      "from": "VE Procedure",
      "to": "Update Rule (15.12)",
      "relationship": "subtopic"
    },
    {
      "from": "Binary Classification Problem",
      "to": "Loss Function",
      "relationship": "depends_on"
    },
    {
      "from": "Necessary Conditions for Valid Kernels",
      "to": "Positive Semi-Definiteness",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Models",
      "to": "5th Degree Polynomial Models",
      "relationship": "subtopic"
    },
    {
      "from": "Reinforcement Learning",
      "to": "Policy Gradient Methods",
      "relationship": "depends_on"
    },
    {
      "from": "Support Vector Machine (SVM)",
      "to": "Optimization Problem",
      "relationship": "depends_on"
    },
    {
      "from": "Neural Networks",
      "to": "Single Neuron Model",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Techniques",
      "to": "Gaussian Mixture Models",
      "relationship": "contains"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Partially Observable MDPs (POMDP)",
      "relationship": "has_subtopic"
    },
    {
      "from": "Deep Learning",
      "to": "Modules in Modern Neural Networks",
      "relationship": "subtopic"
    },
    {
      "from": "Feature Mapping",
      "to": "Iterative Update Process",
      "relationship": "depends_on"
    },
    {
      "from": "Feature Vector Selection",
      "to": "Generative Models",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Models",
      "to": "Maximum Likelihood Estimation (MLE)",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Markov Decision Processes (MDP)",
      "relationship": "related_to"
    },
    {
      "from": "Generalized Linear Models (GLM)",
      "to": "Bernoulli Distribution",
      "relationship": "subtopic"
    },
    {
      "from": "Posterior Distribution",
      "to": "EM Algorithm",
      "relationship": "depends_on"
    },
    {
      "from": "Logistic Regression",
      "to": "Negative Likelihood Loss Function",
      "relationship": "subtopic"
    },
    {
      "from": "Two-Layer Fully-Connected Network",
      "to": "Weight Matrix W^[1]",
      "relationship": "depends_on"
    },
    {
      "from": "Sample Complexity",
      "to": "Hypothesis Class Size",
      "relationship": "related_to"
    },
    {
      "from": "Markov Decision Processes (MDP)",
      "to": "Infinite Horizon MDPs",
      "relationship": "subtopic"
    },
    {
      "from": "Neural Networks",
      "to": "Parameters (\u03b8)",
      "relationship": "depends_on"
    },
    {
      "from": "Probability Estimation",
      "to": "Maximum Likelihood Estimates",
      "relationship": "subtopic"
    },
    {
      "from": "Loss Functions",
      "to": "Cross-Entropy Loss",
      "relationship": "subtopic"
    },
    {
      "from": "Feature Engineering",
      "to": "Machine Learning Overview",
      "relationship": "subtopic"
    },
    {
      "from": "Implementation Subtlety",
      "to": "Data Matrix Representation",
      "relationship": "subtopic"
    },
    {
      "from": "Generalization Error",
      "to": "Training Error",
      "relationship": "related_to"
    },
    {
      "from": "Markov Decision Processes (MDP)",
      "to": "Optimal Policy Determination",
      "relationship": "subtopic"
    },
    {
      "from": "Backward Function for Loss Functions",
      "to": "Logistic Loss",
      "relationship": "subtopic"
    },
    {
      "from": "Vanilla REINFORCE Algorithm",
      "to": "Empirical Trajectories Estimation",
      "relationship": "depends_on"
    },
    {
      "from": "Posterior Approximation",
      "to": "Prior Selection",
      "relationship": "contains"
    },
    {
      "from": "Transfer Learning",
      "to": "Machine Learning Overview",
      "relationship": "subtopic"
    },
    {
      "from": "Kernel Matrix Properties",
      "to": "Examples of Kernels in Practice",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Feature Extraction for Strings",
      "relationship": "has_subtopic"
    },
    {
      "from": "Softplus Function",
      "to": "ReLU Function",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Support Vector Machines (SVMs)",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Generalization Error Guarantees",
      "relationship": "depends_on"
    },
    {
      "from": "Log-Likelihood Maximization",
      "to": "EM Algorithm",
      "relationship": "depends_on"
    },
    {
      "from": "Learning a model for an MDP",
      "to": "Continuous state MDPs",
      "relationship": "has_subtopic"
    },
    {
      "from": "Pretraining Methods in CV",
      "to": "Contrastive Learning",
      "relationship": "subtopic"
    },
    {
      "from": "Convolutional Layers",
      "to": "Channel Concept",
      "relationship": "subtopic"
    },
    {
      "from": "Fitted Value Iteration",
      "to": "Value Function Approximation",
      "relationship": "has_subtopic"
    },
    {
      "from": "Linear Regression",
      "to": "Gradient Descent",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Fundamentals",
      "to": "Backpropagation Algorithm",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Conferences",
      "to": "NeurIPS Conference",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Properties of Kernels",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Least Squares Regression",
      "relationship": "subtopic"
    },
    {
      "from": "Linear Quadratic Regulator (LQR)",
      "to": "Optimization in RL",
      "relationship": "subtopic"
    },
    {
      "from": "Foundation Models Opportunities and Risks",
      "to": "Machine Learning Literature",
      "relationship": "subtopic"
    },
    {
      "from": "Few-Shot Learning Capabilities",
      "to": "Machine Learning Literature",
      "relationship": "subtopic"
    },
    {
      "from": "Layer Normalization (LN)",
      "to": "LN-S Module",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Theory",
      "to": "Optimization Problems",
      "relationship": "depends_on"
    },
    {
      "from": "Matrix Notation in Machine Learning",
      "to": "Generalization to Multiple Layers",
      "relationship": "related_to"
    },
    {
      "from": "Sample Complexity",
      "to": "Floating Point Representation",
      "relationship": "depends_on"
    },
    {
      "from": "EM Algorithm",
      "to": "E-step",
      "relationship": "subtopic"
    },
    {
      "from": "Dual Form of Problem",
      "to": "Kernel Trick",
      "relationship": "related_to"
    },
    {
      "from": "Generalized Linear Model (GLM)",
      "to": "Conditional Distribution Assumption",
      "relationship": "contains"
    },
    {
      "from": "Machine Learning Techniques",
      "to": "Principal Component Analysis (PCA)",
      "relationship": "contains"
    },
    {
      "from": "Contrastive Learning",
      "to": "Positive Pair",
      "relationship": "related_to"
    },
    {
      "from": "PCA",
      "to": "Applications",
      "relationship": "subtopic"
    },
    {
      "from": "Maximum Likelihood Estimation (MLE)",
      "to": "Gradient Ascent",
      "relationship": "uses"
    },
    {
      "from": "Naive Bayes Classifier",
      "to": "Laplace Smoothing",
      "relationship": "subtopic"
    },
    {
      "from": "Independent Component Analysis (ICA)",
      "to": "Cocktail Party Problem",
      "relationship": "related_to"
    },
    {
      "from": "Other Normalization Layers",
      "to": "Group Normalization",
      "relationship": "subtopic"
    },
    {
      "from": "Bias-Variance Tradeoff",
      "to": "Hypothesis Class",
      "relationship": "subtopic"
    },
    {
      "from": "EM_Algorithm",
      "to": "Likelihood_Estimation",
      "relationship": "depends_on"
    },
    {
      "from": "Deep Learning",
      "to": "Neural Networks",
      "relationship": "subtopic"
    },
    {
      "from": "Support Vector Machines (SVMs)",
      "to": "Dual Optimization Problem",
      "relationship": "defines"
    },
    {
      "from": "Support Vector Machines",
      "to": "Dual Formulation (Optional)",
      "relationship": "subtopic"
    },
    {
      "from": "Fitted Value Iteration",
      "to": "Discrete Action Space",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Conv1D-S Module",
      "relationship": "subtopic"
    },
    {
      "from": "Hypotheses Space (H)",
      "to": "Uniform Convergence",
      "relationship": "related_to"
    },
    {
      "from": "Finetuning Pretrained Models",
      "to": "Optimization Goal",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Underfitting",
      "relationship": "related_to"
    },
    {
      "from": "Self-supervised Learning",
      "to": "Foundation Models",
      "relationship": "subtopic"
    },
    {
      "from": "Modern Neural Networks",
      "to": "Backpropagation",
      "relationship": "has_subtopic"
    },
    {
      "from": "Exponential Family Distributions",
      "to": "Bernoulli Distribution",
      "relationship": "subtopic"
    },
    {
      "from": "Neural Networks",
      "to": "Stacking Neurons",
      "relationship": "subtopic"
    },
    {
      "from": "Maximum Likelihood Estimation",
      "to": "Joint Distribution of Sources",
      "relationship": "subtopic"
    },
    {
      "from": "Gaussian Distribution",
      "to": "Density Visualization",
      "relationship": "subtopic"
    },
    {
      "from": "Optimization Methods",
      "to": "Newton's Method",
      "relationship": "subtopic"
    },
    {
      "from": "Coordinate Descent on J",
      "to": "k-means Algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "LQR Algorithm",
      "to": "Step 3",
      "relationship": "depends_on"
    },
    {
      "from": "Gaussian Discriminant Analysis (GDA)",
      "to": "Multivariate Normal Distribution",
      "relationship": "subtopic"
    },
    {
      "from": "Finetuning Pretrained Models",
      "to": "Prediction Model Structure",
      "relationship": "depends_on"
    },
    {
      "from": "ICA Overview",
      "to": "Maximum Likelihood Estimation",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Basics",
      "to": "Function Representation",
      "relationship": "has_subtopic"
    },
    {
      "from": "Support Vector Machines (SVMs)",
      "to": "Functional Margin",
      "relationship": "subtopic"
    },
    {
      "from": "Policy Gradient (REINFORCE)",
      "to": "Randomized Policy",
      "relationship": "depends_on"
    },
    {
      "from": "Modules in Modern Neural Networks",
      "to": "Matrix Multiplication as a Building Block",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning",
      "to": "Reinforcement Learning",
      "relationship": "has_subtopic"
    },
    {
      "from": "Logits",
      "to": "Softmax Function",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Algorithms",
      "to": "Markov Decision Processes (MDPs)",
      "relationship": "contains"
    },
    {
      "from": "Markov Decision Process (MDP)",
      "to": "State Transition",
      "relationship": "depends_on"
    },
    {
      "from": "Chapter 16 Introduction",
      "to": "Finite-horizon MDPs",
      "relationship": "depends_on"
    },
    {
      "from": "LMS Update Rule",
      "to": "Widrow-Hoff Learning Rule",
      "relationship": "related_to"
    },
    {
      "from": "Loss Function Analysis",
      "to": "Machine Learning Concepts",
      "relationship": "subtopic"
    },
    {
      "from": "Uniform Convergence",
      "to": "Generalization Error",
      "relationship": "subtopic"
    },
    {
      "from": "Gradient Descent Methods",
      "to": "Stochastic Gradient Descent",
      "relationship": "contains"
    },
    {
      "from": "Newton's Method",
      "to": "Algorithm Application",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Models",
      "to": "Generalized Linear Models (GLM)",
      "relationship": "contains"
    },
    {
      "from": "Independent components analysis",
      "to": "ICA ambiguities",
      "relationship": "subtopic_of"
    },
    {
      "from": "Markov Decision Processes (MDP)",
      "to": "Value Iteration",
      "relationship": "subtopic"
    },
    {
      "from": "Value Function",
      "to": "Bellman's Equation",
      "relationship": "related_to"
    },
    {
      "from": "Support Vector Machines (SVM)",
      "to": "Lagrange Duality",
      "relationship": "related_to"
    },
    {
      "from": "Cocktail Party Problem",
      "to": "Mixing Matrix (A)",
      "relationship": "related_to"
    },
    {
      "from": "Optimization Methods",
      "to": "Adam: A Method for Stochastic Optimization",
      "relationship": "related_to"
    },
    {
      "from": "Probabilistic Interpretation",
      "to": "Regression Problem",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Models",
      "to": "Decision Boundaries",
      "relationship": "subtopic"
    },
    {
      "from": "Differential Dynamic Programming (DDP)",
      "to": "Nominal Trajectory Generation",
      "relationship": "subtopic"
    },
    {
      "from": "Backpropagation",
      "to": "Chain Rule",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "EM Algorithms",
      "relationship": "has_subtopic"
    },
    {
      "from": "Normalization Process",
      "to": "Normalization Formula",
      "relationship": "subtopic"
    },
    {
      "from": "Markov Decision Processes (MDP)",
      "to": "States",
      "relationship": "includes"
    },
    {
      "from": "Machine Learning Models",
      "to": "Model Assumptions",
      "relationship": "subtopic"
    },
    {
      "from": "Logistic Regression",
      "to": "Logit",
      "relationship": "depends_on"
    },
    {
      "from": "Linear Quadratic Regulator (LQR)",
      "to": "Optimal Policy",
      "relationship": "subtopic"
    },
    {
      "from": "Support Vector Machines",
      "to": "Lagrange Duality (Optional)",
      "relationship": "subtopic"
    },
    {
      "from": "Probability Vector",
      "to": "Softmax Function",
      "relationship": "related_to"
    },
    {
      "from": "Kernel Functions",
      "to": "Characterization of Valid Kernels",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Feature Map",
      "relationship": "related_to"
    },
    {
      "from": "Jensen's Inequality",
      "to": "Theorem Statement",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Basics",
      "to": "Regularization",
      "relationship": "subtopic"
    },
    {
      "from": "Markov Decision Processes (MDP)",
      "to": "Finite Horizon MDPs",
      "relationship": "subtopic"
    },
    {
      "from": "Empirical Distribution",
      "to": "Training Loss",
      "relationship": "related_to"
    },
    {
      "from": "Matricization Approach",
      "to": "Implementation Subtlety",
      "relationship": "depends_on"
    },
    {
      "from": "Continuous state MDPs",
      "to": "Value function approximation",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Bias-Variance Tradeoff",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Kernel Functions",
      "relationship": "contains"
    },
    {
      "from": "Double Descent Phenomenon",
      "to": "Regularization Tuning",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Hypothesis Space",
      "relationship": "depends_on"
    },
    {
      "from": "Softmax Function",
      "to": "Probabilistic Model",
      "relationship": "subtopic"
    },
    {
      "from": "Layer Normalization (LN)",
      "to": "Scale-Invariant Property",
      "relationship": "depends_on"
    },
    {
      "from": "Law of Total Expectation",
      "to": "Estimator Simplification",
      "relationship": "subtopic"
    },
    {
      "from": "Constrained Optimization",
      "to": "Generalized Lagrangian",
      "relationship": "subtopic"
    },
    {
      "from": "ICA Ambiguities",
      "to": "Scaling Factor",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine Learning Models",
      "to": "Binary Classification",
      "relationship": "has_subtopic"
    },
    {
      "from": "Support Vector Machines (SVM)",
      "to": "Margins",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Bias-Variance Tradeoff",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Adaptation Methods",
      "to": "Finetuning Pretrained Models",
      "relationship": "subtopic"
    },
    {
      "from": "Double Descent Phenomenon",
      "to": "Bias-Variance Tradeoff",
      "relationship": "subtopic"
    },
    {
      "from": "Empirical Risk Minimization (ERM)",
      "to": "Non-ERM Algorithms",
      "relationship": "related_to"
    },
    {
      "from": "Markov Decision Processes (MDPs)",
      "to": "Value Iteration",
      "relationship": "contains"
    },
    {
      "from": "k-means Algorithm",
      "to": "Convergence Properties",
      "relationship": "related_to"
    },
    {
      "from": "Self-supervised learning and foundation models",
      "to": "Pretraining and adaptation",
      "relationship": "has_subtopic"
    },
    {
      "from": "Generative Learning Algorithms",
      "to": "Bayes Rule",
      "relationship": "depends_on"
    },
    {
      "from": "Regularization in Machine Learning",
      "to": "Sparsity Regularization",
      "relationship": "subtopic"
    },
    {
      "from": "Gaussian Distribution",
      "to": "Standard Normal Distribution",
      "relationship": "related_to"
    },
    {
      "from": "Linear Quadratic Regulation (LQR)",
      "to": "Quadratic Rewards",
      "relationship": "contains"
    },
    {
      "from": "Pretrained large language models",
      "to": "Open up the blackbox of Transformers",
      "relationship": "subtopic_of"
    },
    {
      "from": "Support Vector Machines (SVM)",
      "to": "Optimization Problem",
      "relationship": "depends_on"
    },
    {
      "from": "Classification and Logistic Regression",
      "to": "Multi-class Classification",
      "relationship": "subtopic"
    },
    {
      "from": "Supervised Learning Problem",
      "to": "Discretization in MDPs",
      "relationship": "subtopic"
    },
    {
      "from": "Continuous state MDPs",
      "to": "Value function approximation",
      "relationship": "subtopic_of"
    },
    {
      "from": "Regularization",
      "to": "Model Complexity",
      "relationship": "subtopic"
    },
    {
      "from": "Discretization",
      "to": "Curse of Dimensionality",
      "relationship": "depends_on"
    },
    {
      "from": "Policy Gradients",
      "to": "Trajectory Probability",
      "relationship": "related_to"
    },
    {
      "from": "Alternative ELBO Formulations",
      "to": "KL Divergence in ELBO",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Differential Dynamic Programming (DDP)",
      "relationship": "subtopic"
    },
    {
      "from": "Derived Features",
      "to": "Walkability",
      "relationship": "subtopic"
    },
    {
      "from": "Total Payoff Calculation",
      "to": "Discount Factor (\u03b3)",
      "relationship": "related_to"
    },
    {
      "from": "Chapter 15 Overview",
      "to": "Optimal Bellman Equation",
      "relationship": "depends_on"
    },
    {
      "from": "ResNet Architecture",
      "to": "Convolutional Layers",
      "relationship": "subtopic"
    },
    {
      "from": "Sufficient Conditions for Valid Kernels",
      "to": "Mercer's Theorem",
      "relationship": "subtopic"
    },
    {
      "from": "Optimization Problems",
      "to": "Primal Problem",
      "relationship": "subtopic"
    },
    {
      "from": "Conditional Probability Modeling",
      "to": "Parameterized Model",
      "relationship": "subtopic"
    },
    {
      "from": "Policy Gradients",
      "to": "Reward Function",
      "relationship": "depends_on"
    },
    {
      "from": "Gaussian Distribution",
      "to": "Kalman Filter",
      "relationship": "related_to"
    },
    {
      "from": "Gradient Descent Update Rule",
      "to": "Feature Mapping",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Algorithms",
      "to": "Markov Decision Processes (MDP)",
      "relationship": "related_to"
    },
    {
      "from": "SMO Algorithm",
      "to": "Alpha Value Update",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Fundamentals",
      "to": "Chain Rule for Auto-Differentiation",
      "relationship": "subtopic"
    },
    {
      "from": "Value Function",
      "to": "Policy Execution",
      "relationship": "depends_on"
    },
    {
      "from": "Generalization Error Analysis",
      "to": "The Generalization Error of Random Features Regression: Precise Asymptotics and the Double Descent Curve",
      "relationship": "subtopic"
    },
    {
      "from": "Housing Price Prediction",
      "to": "Derived Features",
      "relationship": "depends_on"
    },
    {
      "from": "Model Complexity Measures",
      "to": "Norm of Learned Model",
      "relationship": "related_to"
    },
    {
      "from": "Cross Validation",
      "to": "Regularization Parameters",
      "relationship": "subtopic"
    },
    {
      "from": "EM_Algorithm",
      "to": "M_Step",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Regression Problems",
      "relationship": "contains"
    },
    {
      "from": "Machine Learning Fundamentals",
      "to": "Backward Function in Machine Learning",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Backpropagation",
      "relationship": "contains"
    },
    {
      "from": "Distance to Decision Boundary",
      "to": "Decision Boundary",
      "relationship": "subtopic"
    },
    {
      "from": "Multi-layer Fully-Connected Neural Networks",
      "to": "Notational Consistency",
      "relationship": "subtopic"
    },
    {
      "from": "Backpropagation",
      "to": "Backward Functions for Basic Modules",
      "relationship": "subtopic"
    },
    {
      "from": "Classification Model",
      "to": "Probabilistic Assumptions",
      "relationship": "depends_on"
    },
    {
      "from": "Generalization Error Guarantees",
      "to": "Bernoulli Random Variable Z",
      "relationship": "related_to"
    },
    {
      "from": "Markov Decision Process (MDP)",
      "to": "Expected Immediate Reward",
      "relationship": "related_to"
    },
    {
      "from": "Continuous Latent Variables",
      "to": "Variational Inference",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "LQR Extension",
      "relationship": "has_subtopic"
    },
    {
      "from": "Generalization and regularization",
      "to": "Generalization",
      "relationship": "has_subtopic"
    },
    {
      "from": "Cross Validation",
      "to": "Leave-One-Out Cross Validation",
      "relationship": "subtopic"
    },
    {
      "from": "Optimal Margin Classifier",
      "to": "Maximizing Geometric Margin",
      "relationship": "subtopic"
    },
    {
      "from": "Reinforcement Learning Overview",
      "to": "Policy Definition",
      "relationship": "subtopic"
    },
    {
      "from": "Least Squares Regression",
      "to": "Cost Function J(\u03b8)",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Models",
      "to": "Vectorization",
      "relationship": "related_to"
    },
    {
      "from": "Regularization and model selection",
      "to": "Model selection via cross validation",
      "relationship": "has_subtopic"
    },
    {
      "from": "ResNet Architecture",
      "to": "Batch Normalization Variants",
      "relationship": "subtopic"
    },
    {
      "from": "Jensen's Inequality",
      "to": "Convex Functions",
      "relationship": "subtopic"
    },
    {
      "from": "Gradient Computation",
      "to": "Reparameterization Trick",
      "relationship": "subtopic"
    },
    {
      "from": "Design Matrix",
      "to": "Least Squares Revisited",
      "relationship": "subtopic"
    },
    {
      "from": "Sufficient Conditions for Valid Kernels",
      "to": "Testing Kernel Validity",
      "relationship": "subtopic"
    },
    {
      "from": "Gradient Descent",
      "to": "Cost Function J(\u03b8)",
      "relationship": "related_to"
    },
    {
      "from": "Logistic Regression",
      "to": "Logistic Loss Function",
      "relationship": "subtopic"
    },
    {
      "from": "Normal Equations",
      "to": "Matrix Derivatives",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Optimization Techniques",
      "to": "Coordinate Ascent",
      "relationship": "contains"
    },
    {
      "from": "Data Normalization",
      "to": "Mean Removal",
      "relationship": "has_subtopic"
    },
    {
      "from": "Lagrangian Function",
      "to": "Primal Problem",
      "relationship": "related_to"
    },
    {
      "from": "Naive Bayes Algorithm",
      "to": "Conditional Probability",
      "relationship": "depends_on"
    },
    {
      "from": "Linear Regression",
      "to": "Least-Squares Cost Function",
      "relationship": "subtopic"
    },
    {
      "from": "Policy Iteration",
      "to": "Bellman's Equations",
      "relationship": "depends_on"
    },
    {
      "from": "Principal Components Analysis (PCA)",
      "to": "Data Subspace Identification",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Stochastic Gradient Ascent",
      "relationship": "contains"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Locally Weighted Linear Regression (LWR)",
      "relationship": "subtopic"
    },
    {
      "from": "Gradient Descent",
      "to": "Convex Function",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Architectures",
      "to": "ResNet Architecture",
      "relationship": "related_to"
    },
    {
      "from": "Expectation Estimation",
      "to": "Sample-Based Estimation",
      "relationship": "subtopic"
    },
    {
      "from": "Linear Quadratic Regulator (LQR)",
      "to": "Parameter Estimation",
      "relationship": "depends_on"
    },
    {
      "from": "Double Descent Phenomenon",
      "to": "Sample-wise Double Descent",
      "relationship": "subtopic"
    },
    {
      "from": "Differential Dynamic Programming (DDP)",
      "to": "Linearization of Dynamics",
      "relationship": "subtopic"
    },
    {
      "from": "Adaptation Methods",
      "to": "Zero-shot Learning",
      "relationship": "subtopic"
    },
    {
      "from": "Logistic Loss Function",
      "to": "Logit",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Overview",
      "to": "Bayesian Inference",
      "relationship": "contains"
    },
    {
      "from": "Machine Learning Techniques",
      "to": "Differential Dynamic Programming (DDP)",
      "relationship": "contains"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Loss Function",
      "relationship": "related_to"
    },
    {
      "from": "Test Error Decomposition",
      "to": "Bias-Variance Tradeoff",
      "relationship": "subtopic"
    },
    {
      "from": "Decision Boundary",
      "to": "Geometric Margins",
      "relationship": "related_to"
    },
    {
      "from": "Policy Iteration",
      "to": "Greedy Policy with Respect to V",
      "relationship": "related_to"
    },
    {
      "from": "Quadratic Assumption",
      "to": "Dynamics of Model",
      "relationship": "depends_on"
    },
    {
      "from": "Learning Guarantees",
      "to": "Hoeffding Inequality (Chernoff Bound)",
      "relationship": "subtopic"
    },
    {
      "from": "Value Function Approximation",
      "to": "Model or Simulator",
      "relationship": "subtopic"
    },
    {
      "from": "Major Axis of Variation",
      "to": "Projection and Variance Maximization",
      "relationship": "has_subtopic"
    },
    {
      "from": "Reward Function",
      "to": "Finite-State MDPs",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Models",
      "to": "Generative Learning Algorithms",
      "relationship": "contains"
    },
    {
      "from": "Machine Learning Algorithms",
      "to": "Logistic Regression",
      "relationship": "related_to"
    },
    {
      "from": "Value Iteration",
      "to": "Convergence of Value Functions",
      "relationship": "depends_on"
    },
    {
      "from": "Derived Features",
      "to": "Family Size",
      "relationship": "subtopic"
    },
    {
      "from": "Gaussian Distribution",
      "to": "Mean",
      "relationship": "subtopic"
    },
    {
      "from": "Logistic Regression",
      "to": "Stochastic Gradient Ascent Rule",
      "relationship": "subtopic"
    },
    {
      "from": "Alpha Variables",
      "to": "Derivation Example",
      "relationship": "depends_on"
    },
    {
      "from": "GELU Function",
      "to": "ReLU Function",
      "relationship": "variant_of"
    },
    {
      "from": "Optimization Problems",
      "to": "Feasibility Conditions",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Linear Function Over Features",
      "relationship": "subtopic"
    },
    {
      "from": "Model Selection via Cross Validation",
      "to": "Cross Validation Techniques",
      "relationship": "contains"
    },
    {
      "from": "EM algorithms",
      "to": "Variational inference and variational auto-encoder (optional reading)",
      "relationship": "has_subtopic"
    },
    {
      "from": "Parameterized Model",
      "to": "Embeddings and Representations",
      "relationship": "depends_on"
    },
    {
      "from": "Gradient Computation",
      "to": "Backpropagation",
      "relationship": "related_to"
    },
    {
      "from": "Newton's Method",
      "to": "Multidimensional Generalization",
      "relationship": "subtopic"
    },
    {
      "from": "Lagrangian Function",
      "to": "Optimization Constraints",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Expectation-Maximization (EM) Algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Empirical Risk Minimization",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Architectures",
      "to": "Transformer Architecture",
      "relationship": "related_to"
    },
    {
      "from": "Finite-State MDPs",
      "to": "Value Iteration and Policy Iteration",
      "relationship": "subtopic"
    },
    {
      "from": "Underfitting",
      "to": "Bias-Variance Tradeoff",
      "relationship": "depends_on"
    },
    {
      "from": "M-step",
      "to": "ELBO (Evidence Lower Bound)",
      "relationship": "related_to"
    },
    {
      "from": "Generalization",
      "to": "The double descent phenomenon",
      "relationship": "subtopic_of"
    },
    {
      "from": "ReLU Function",
      "to": "Activation Functions",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Expectation-Maximization (EM) Algorithm",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Loss Functions",
      "to": "Cross-Entropy Loss Function",
      "relationship": "subtopic"
    },
    {
      "from": "Optimal Hypothesis (h*)",
      "to": "Generalization Error",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Layer Normalization (LN)",
      "relationship": "subtopic"
    },
    {
      "from": "Variance Maximization",
      "to": "k-Dimensional Subspace",
      "relationship": "related_to"
    },
    {
      "from": "Support Vector Machines (SVMs)",
      "to": "Notation for SVMs",
      "relationship": "subtopic"
    },
    {
      "from": "Self-Supervised Learning",
      "to": "Representation Function",
      "relationship": "depends_on"
    },
    {
      "from": "ICA Ambiguities",
      "to": "Gaussian Data Example",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine Learning Overview",
      "to": "Deep Learning Introduction",
      "relationship": "related_to"
    },
    {
      "from": "ELBO",
      "to": "Variational Inference",
      "relationship": "depends_on"
    },
    {
      "from": "Density on x=As=W^-1s",
      "to": "Cumulative Distribution Function (CDF)",
      "relationship": "depends_on"
    },
    {
      "from": "Cost Function",
      "to": "Ordinary Least Squares",
      "relationship": "related_to"
    },
    {
      "from": "Pretraining Phase",
      "to": "Transfer Learning",
      "relationship": "subtopic"
    },
    {
      "from": "Encoder-Decoder Framework",
      "to": "Optimizing Continuous Latent Variables",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Models",
      "to": "Linear Model Limitations",
      "relationship": "subtopic"
    },
    {
      "from": "Efficient Evaluation of ELBO",
      "to": "Optimizing Continuous Latent Variables",
      "relationship": "depends_on"
    },
    {
      "from": "Learned Features",
      "to": "Deep Learning",
      "relationship": "subtopic"
    },
    {
      "from": "Generative Learning Algorithms",
      "to": "Class Priors",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Basics",
      "to": "Linear Regression",
      "relationship": "depends_on"
    },
    {
      "from": "Modern Neural Networks",
      "to": "Modules in Modern Neural Networks",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine Learning Models",
      "to": "Logistic Regression",
      "relationship": "related_to"
    },
    {
      "from": "Generative Models",
      "to": "Naive Bayes Classifier",
      "relationship": "subtopic"
    },
    {
      "from": "EM Algorithm",
      "to": "Evidence Lower Bound (ELBO)",
      "relationship": "subtopic"
    },
    {
      "from": "Backpropagation",
      "to": "Loss Function Composition",
      "relationship": "subtopic"
    },
    {
      "from": "Support Vectors",
      "to": "Support Vectors Definition",
      "relationship": "subtopic"
    },
    {
      "from": "Model-wise Double Descent",
      "to": "Implicit Regularization",
      "relationship": "subtopic"
    },
    {
      "from": "Optimal Value Function",
      "to": "Value Iteration Algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "In-context Learning",
      "to": "Prompt Construction",
      "relationship": "has_subtopic"
    },
    {
      "from": "Optimization Problem",
      "to": "Lagrangian Function",
      "relationship": "related_to"
    },
    {
      "from": "Hold-out Cross Validation",
      "to": "Model Selection Based on Validation Error",
      "relationship": "depends_on"
    },
    {
      "from": "Vectorization",
      "to": "Broadcasting",
      "relationship": "depends_on"
    },
    {
      "from": "Gaussian Distribution",
      "to": "Covariance",
      "relationship": "subtopic"
    },
    {
      "from": "Kernels in Machine Learning",
      "to": "Feature Maps",
      "relationship": "depends_on"
    },
    {
      "from": "Generative Learning Algorithms",
      "to": "Naive Bayes Classifier",
      "relationship": "subtopic"
    },
    {
      "from": "Support Vector Machines (SVM)",
      "to": "Dual Form Insight",
      "relationship": "related_to"
    },
    {
      "from": "Model Selection",
      "to": "Bias-Variance Tradeoff",
      "relationship": "related_to"
    },
    {
      "from": "Empirical Risk Minimization",
      "to": "Hypotheses Training",
      "relationship": "depends_on"
    },
    {
      "from": "Backpropagation",
      "to": "General strategy of backpropagation",
      "relationship": "subtopic_of"
    },
    {
      "from": "Exponential Family Distributions",
      "to": "Poisson Distribution",
      "relationship": "subtopic"
    },
    {
      "from": "Supervised Learning",
      "to": "Generative Learning Algorithms",
      "relationship": "subtopic"
    },
    {
      "from": "Training Set",
      "to": "Machine Learning Concepts",
      "relationship": "subtopic"
    },
    {
      "from": "Two-Layer Neural Network",
      "to": "Weight Matrices",
      "relationship": "depends_on"
    },
    {
      "from": "Chapter 15 Overview",
      "to": "Value Iteration Preference",
      "relationship": "subtopic"
    },
    {
      "from": "Probability Calculation",
      "to": "Multinomial Event Model",
      "relationship": "subtopic"
    },
    {
      "from": "Policy Gradient (REINFORCE)",
      "to": "Expected Total Payoff",
      "relationship": "related_to"
    },
    {
      "from": "Partial Observability",
      "to": "LQG Framework",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Loss Functions",
      "to": "Logistic Loss Function",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Models",
      "to": "Non-linear Functions",
      "relationship": "related_to"
    },
    {
      "from": "Gradient Descent Update Rule",
      "to": "Computational Complexity",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Bellman Update",
      "relationship": "contains"
    },
    {
      "from": "Data Redundancy Detection",
      "to": "PCA Algorithm Introduction",
      "relationship": "depends_on"
    },
    {
      "from": "Expected Test Error",
      "to": "Mean Squared Error (MSE)",
      "relationship": "subtopic"
    },
    {
      "from": "Generalized Linear Models (GLM)",
      "to": "Exponential Family Distributions",
      "relationship": "related_to"
    },
    {
      "from": "Support Vector Machines",
      "to": "Notation (Optional)",
      "relationship": "subtopic"
    },
    {
      "from": "Partially Observable MDPs (POMDP)",
      "to": "Observation Layer",
      "relationship": "has_subtopic"
    },
    {
      "from": "Density Transformation",
      "to": "General Case",
      "relationship": "subtopic"
    },
    {
      "from": "Reward Function",
      "to": "Reinforcement Learning",
      "relationship": "subtopic"
    },
    {
      "from": "Time-Dependent Policies",
      "to": "Non-Stationary Optimal Policy",
      "relationship": "depends_on"
    },
    {
      "from": "Value Iteration Algorithm",
      "to": "Value Iteration and Policy Iteration",
      "relationship": "subtopic"
    },
    {
      "from": "Linear Regression",
      "to": "Feature Selection",
      "relationship": "depends_on"
    },
    {
      "from": "Layer Normalization",
      "to": "Affine Transformation",
      "relationship": "related_to"
    },
    {
      "from": "Kalman Filter",
      "to": "Predict Step",
      "relationship": "subtopic"
    },
    {
      "from": "E-step",
      "to": "ELBO (Evidence Lower Bound)",
      "relationship": "related_to"
    },
    {
      "from": "Exponential Family Distributions",
      "to": "Canonical Response Function",
      "relationship": "subtopic"
    },
    {
      "from": "Weight Calculation",
      "to": "Bandwidth Parameter (\u03c4)",
      "relationship": "contains"
    },
    {
      "from": "Data Preprocessing Assumptions",
      "to": "Logistic Function Properties",
      "relationship": "depends_on"
    },
    {
      "from": "Optimization Problem",
      "to": "Functional Margin",
      "relationship": "related_to"
    },
    {
      "from": "Support Vector Machines",
      "to": "SMO Algorithm (Optional)",
      "relationship": "subtopic"
    },
    {
      "from": "Generalized Linear Models (GLM)",
      "to": "Logistic Regression",
      "relationship": "contains"
    },
    {
      "from": "Sufficient Statistic for Bernoulli",
      "to": "Bernoulli Distribution",
      "relationship": "subtopic"
    },
    {
      "from": "Test Error",
      "to": "Population Distribution",
      "relationship": "related_to"
    },
    {
      "from": "Support Vector Machines",
      "to": "Functional and Geometric Margins",
      "relationship": "subtopic"
    },
    {
      "from": "Leaky ReLU",
      "to": "ReLU Function",
      "relationship": "variant_of"
    },
    {
      "from": "Foundation Models",
      "to": "Machine Learning Overview",
      "relationship": "subtopic"
    },
    {
      "from": "Generalized Linear Model (GLM)",
      "to": "Poisson Distribution",
      "relationship": "subtopic"
    },
    {
      "from": "Classification Problem",
      "to": "Binary Classification",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Other Normalization Layers",
      "relationship": "related_to"
    },
    {
      "from": "Pretrained Model",
      "to": "Transfer Learning",
      "relationship": "depends_on"
    },
    {
      "from": "Reinforcement learning",
      "to": "Markov decision processes",
      "relationship": "subtopic_of"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Kernels in Machine Learning",
      "relationship": "contains"
    },
    {
      "from": "Function Representation",
      "to": "LMS Algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Basics",
      "to": "Classification Problem",
      "relationship": "depends_on"
    },
    {
      "from": "Bernoulli Distribution",
      "to": "Gaussian Discriminant Analysis (GDA)",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Models",
      "to": "Naive Bayes (Discrete Features)",
      "relationship": "related_to"
    },
    {
      "from": "Optimization Problems",
      "to": "Karush-Kuhn-Tucker (KKT) Conditions",
      "relationship": "subtopic"
    },
    {
      "from": "Variance Maximization",
      "to": "Empirical Covariance Matrix",
      "relationship": "related_to"
    },
    {
      "from": "Backward Function in Machine Learning",
      "to": "Jacobian Matrix",
      "relationship": "related_to"
    },
    {
      "from": "Policy",
      "to": "Optimal Policy",
      "relationship": "subtopic"
    },
    {
      "from": "Variance Maximization",
      "to": "Principal Eigenvector",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Theory",
      "to": "Statistical_Mechanics_of_Learning",
      "relationship": "related_to"
    },
    {
      "from": "Explicit Regularization Techniques",
      "to": "Regularization in Deep Learning",
      "relationship": "subtopic"
    },
    {
      "from": "Log-Likelihood Function",
      "to": "Maximum Likelihood Estimation (MLE)",
      "relationship": "subtopic"
    },
    {
      "from": "Generalized Linear Models (GLMs)",
      "to": "Assumptions/Design Choices",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Algorithms",
      "to": "Sequential Minimal Optimization (SMO)",
      "relationship": "contains"
    },
    {
      "from": "Cross-Entropy Loss",
      "to": "Softmax Function",
      "relationship": "depends_on"
    },
    {
      "from": "Uniform Convergence",
      "to": "Union Bound Application",
      "relationship": "depends_on"
    },
    {
      "from": "Bellman Equations",
      "to": "Value Function",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Algorithms",
      "to": "Cross Validation",
      "relationship": "subtopic"
    },
    {
      "from": "Vapnik's Theorem",
      "to": "Uniform Convergence",
      "relationship": "depends_on"
    },
    {
      "from": "Bias-Variance Tradeoff",
      "to": "Bias Term",
      "relationship": "has_subtopic"
    },
    {
      "from": "VC Dimension",
      "to": "Vapnik's Theorem",
      "relationship": "related_to"
    },
    {
      "from": "Primal Problem",
      "to": "Objective Function Primal",
      "relationship": "depends_on"
    },
    {
      "from": "Optimizers and Generalization",
      "to": "Global Minima Variability",
      "relationship": "contains"
    },
    {
      "from": "Policy Gradients",
      "to": "Expectation Estimation",
      "relationship": "depends_on"
    },
    {
      "from": "Exponential Family Distributions",
      "to": "Bernoulli Distribution",
      "relationship": "related_to"
    },
    {
      "from": "Linear Function Over Features",
      "to": "Cubic Function Representation",
      "relationship": "subtopic"
    },
    {
      "from": "Cocktail Party Problem",
      "to": "Unmixing Matrix (W)",
      "relationship": "depends_on"
    },
    {
      "from": "Uniform Convergence",
      "to": "Hypothesis Class",
      "relationship": "related_to"
    },
    {
      "from": "Likelihood Function",
      "to": "Maximum Likelihood Estimation",
      "relationship": "related_to"
    },
    {
      "from": "Linear Quadratic Regulator (LQR)",
      "to": "Discrete Ricatti Equations",
      "relationship": "depends_on"
    },
    {
      "from": "Asynchronous Updates",
      "to": "Value Iteration Algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Support Vector Machines (SVM)",
      "to": "Kernels",
      "relationship": "subtopic"
    },
    {
      "from": "Gradient Ascent",
      "to": "Reparametrization Technique",
      "relationship": "related_to"
    },
    {
      "from": "LQR, DDP and LQG",
      "to": "From non-linear dynamics to LQR",
      "relationship": "has_subtopic"
    },
    {
      "from": "Regularization in Machine Learning",
      "to": "L2 Norm Regularization",
      "relationship": "subtopic"
    },
    {
      "from": "Recovering w from Alpha",
      "to": "Lagrangian Optimization",
      "relationship": "related_to"
    },
    {
      "from": "Hold-out Cross Validation",
      "to": "Validation Set S_cv",
      "relationship": "subtopic"
    },
    {
      "from": "Dual Form of Problem",
      "to": "Lagrangian Function",
      "relationship": "depends_on"
    },
    {
      "from": "Generative Learning Algorithms",
      "to": "Naive Bayes",
      "relationship": "subtopic"
    },
    {
      "from": "Kernel Methods",
      "to": "LMS with Kernel Trick",
      "relationship": "subtopic"
    },
    {
      "from": "Vectorization in Neural Networks",
      "to": "Matrix Algebra",
      "relationship": "depends_on"
    },
    {
      "from": "ResNet (Residual Network)",
      "to": "Residual Block",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Independent Component Analysis (ICA)",
      "relationship": "contains"
    },
    {
      "from": "Policy Gradient Theorem",
      "to": "Vanilla REINFORCE Algorithm",
      "relationship": "related_to"
    },
    {
      "from": "EM algorithms",
      "to": "General EM algorithms",
      "relationship": "has_subtopic"
    },
    {
      "from": "Backward Function for Loss Functions",
      "to": "Efficiency Considerations",
      "relationship": "subtopic"
    },
    {
      "from": "EM algorithms",
      "to": "Jensen's inequality",
      "relationship": "subtopic_of"
    },
    {
      "from": "Fitted Value Iteration",
      "to": "Supervised Learning Algorithm",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Backpropagation",
      "relationship": "subtopic"
    },
    {
      "from": "E-step",
      "to": "EM Algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Two-Layer Neural Network",
      "to": "Hidden Layer",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Algorithms",
      "to": "Gradient Descent Update Rule",
      "relationship": "depends_on"
    },
    {
      "from": "Laplace Smoothing",
      "to": "Spam Classification Example",
      "relationship": "depends_on"
    },
    {
      "from": "Markov Decision Processes (MDP)",
      "to": "Comparison Between Algorithms",
      "relationship": "subtopic"
    },
    {
      "from": "Gaussian Data Example",
      "to": "Mixing Matrix A",
      "relationship": "has_subtopic"
    },
    {
      "from": "Constructing GLMs",
      "to": "Ordinary Least Squares",
      "relationship": "subtopic"
    },
    {
      "from": "Backpropagation",
      "to": "Backward Pass",
      "relationship": "subtopic"
    },
    {
      "from": "Linearization of Dynamics",
      "to": "Inverted Pendulum Example",
      "relationship": "related_to"
    },
    {
      "from": "Training Error vs Generalization Error",
      "to": "Sample Size Determination",
      "relationship": "subtopic"
    },
    {
      "from": "Regularization",
      "to": "Inductive Bias",
      "relationship": "related_to"
    },
    {
      "from": "Continuous state MDPs",
      "to": "Discretization",
      "relationship": "has_subtopic"
    },
    {
      "from": "Jensen's Inequality",
      "to": "Optimizing Q Distribution",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Fundamentals",
      "to": "Backward Function for Loss Functions",
      "relationship": "depends_on"
    },
    {
      "from": "Expectation-Maximization Algorithm",
      "to": "Convergence Guarantees",
      "relationship": "depends_on"
    },
    {
      "from": "Optimization Problem",
      "to": "Geometric Margin",
      "relationship": "related_to"
    },
    {
      "from": "Density Transformation",
      "to": "ICA Algorithm",
      "relationship": "related_to"
    },
    {
      "from": "Predict Step",
      "to": "Gaussian Distribution",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine Learning Techniques",
      "to": "VAE",
      "relationship": "contains"
    },
    {
      "from": "Log-Likelihood Optimization",
      "to": "Single Example Case",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Models",
      "to": "Linear Model Prediction",
      "relationship": "has_subtopic"
    },
    {
      "from": "Data Scarcity",
      "to": "Leave-One-Out Cross Validation",
      "relationship": "related_to"
    },
    {
      "from": "Markov Decision Processes (MDPs)",
      "to": "Policy Iteration",
      "relationship": "contains"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Policy Gradients",
      "relationship": "depends_on"
    },
    {
      "from": "Probability Estimation",
      "to": "Laplace Smoothing",
      "relationship": "subtopic"
    },
    {
      "from": "Bias-Variance Tradeoff",
      "to": "Linear Regression Models",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Algorithms",
      "to": "LQR Updates",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Expectation Maximization (EM)",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Algorithms",
      "to": "Empirical Risk Minimization",
      "relationship": "depends_on"
    },
    {
      "from": "Linear Quadratic Regulation (LQR)",
      "to": "Linear Transitions",
      "relationship": "contains"
    },
    {
      "from": "Step 1: Estimate Matrices",
      "to": "LQR Algorithm Steps",
      "relationship": "subtopic"
    },
    {
      "from": "Optimization Methods",
      "to": "Auto-Encoding Variational Bayes",
      "relationship": "subtopic"
    },
    {
      "from": "Dual Problem",
      "to": "Objective Function Dual",
      "relationship": "depends_on"
    },
    {
      "from": "Mini-batch SGD",
      "to": "Deep Learning Model Training Steps",
      "relationship": "related_to"
    },
    {
      "from": "Dual Formulation of SVM",
      "to": "Lagrange Multipliers",
      "relationship": "depends_on"
    },
    {
      "from": "Policy Gradient Methods",
      "to": "Baseline Estimation",
      "relationship": "has_subtopic"
    },
    {
      "from": "Activation Functions",
      "to": "ReLU Function",
      "relationship": "subtopic"
    },
    {
      "from": "Reinforcement learning",
      "to": "Connections between Policy and Value Iteration (Optional)",
      "relationship": "has_subtopic"
    },
    {
      "from": "Optimization Problems",
      "to": "Dual Problem",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Theory",
      "to": "Empirical Risk Minimization (ERM)",
      "relationship": "subtopic"
    },
    {
      "from": "Vector y",
      "to": "Least Squares Revisited",
      "relationship": "subtopic"
    },
    {
      "from": "Naive Bayes Algorithm",
      "to": "Training Set",
      "relationship": "depends_on"
    },
    {
      "from": "Continuous state MDPs",
      "to": "Discretization",
      "relationship": "subtopic_of"
    },
    {
      "from": "Ordinary Least Squares",
      "to": "Conditional Distribution",
      "relationship": "related_to"
    },
    {
      "from": "Model Complexity Measures",
      "to": "Number of Parameters",
      "relationship": "related_to"
    },
    {
      "from": "LQR Model Assumptions",
      "to": "Machine Learning Concepts",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Historical Context and Recent Discoveries",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Algorithms",
      "to": "Stochastic Gradient Descent",
      "relationship": "related_to"
    },
    {
      "from": "Model Selection Methods",
      "to": "Sample Complexity Bounds",
      "relationship": "subtopic"
    },
    {
      "from": "Language Problem Methods",
      "to": "Machine Learning Adaptation Methods",
      "relationship": "related_to"
    },
    {
      "from": "General EM algorithms",
      "to": "Other interpretation of ELBO",
      "relationship": "subtopic_of"
    },
    {
      "from": "Labeled Task Dataset",
      "to": "Adaptation Phase",
      "relationship": "related_to"
    },
    {
      "from": "Stacking Neurons",
      "to": "Housing Prediction Example",
      "relationship": "example_of"
    },
    {
      "from": "Transformer Model",
      "to": "Training Process",
      "relationship": "related_to"
    },
    {
      "from": "Regularization",
      "to": "Overfitting",
      "relationship": "depends_on"
    },
    {
      "from": "Exponential Family Distributions",
      "to": "Natural Parameter",
      "relationship": "subtopic"
    },
    {
      "from": "VE Procedure",
      "to": "Initialization Option 2",
      "relationship": "related_to"
    },
    {
      "from": "Probabilistic Interpretation",
      "to": "Gaussian Distribution Assumption",
      "relationship": "subtopic"
    },
    {
      "from": "Regularization and model selection",
      "to": "Implicit regularization effect (optional reading)",
      "relationship": "has_subtopic"
    },
    {
      "from": "Deep Learning",
      "to": "Backpropagation",
      "relationship": "subtopic"
    },
    {
      "from": "Value Iteration and Policy Iteration",
      "to": "Machine Learning Overview",
      "relationship": "subtopic"
    },
    {
      "from": "Weight Matrices and Biases",
      "to": "Total Number of Parameters",
      "relationship": "related_to"
    },
    {
      "from": "Expectation-Maximization Algorithm",
      "to": "M-step Maximization",
      "relationship": "has_subtopic"
    },
    {
      "from": "M-step",
      "to": "EM Algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Support Vector Machines (SVM)",
      "to": "Sequential Minimal Optimization (SMO) Algorithm",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Optimization Problems",
      "relationship": "depends_on"
    },
    {
      "from": "Regularizer",
      "to": "\u039b_2 Regularization",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Overview",
      "to": "Value Function Approximation",
      "relationship": "related_to"
    },
    {
      "from": "Backpropagation",
      "to": "Backward functions for basic modules",
      "relationship": "subtopic_of"
    },
    {
      "from": "Infinite Horizon MDPs",
      "to": "Discount Factor \u03b3",
      "relationship": "depends_on"
    },
    {
      "from": "Optimal Value Function",
      "to": "Dynamic Programming",
      "relationship": "depends_on"
    },
    {
      "from": "Support Vector Machines (SVMs)",
      "to": "Geometric Margin",
      "relationship": "related_to"
    },
    {
      "from": "5th Degree Polynomial Models",
      "to": "Generalization Failure",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Geometric Margins",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Linear Regression",
      "relationship": "contains"
    },
    {
      "from": "Optimization Methods",
      "to": "Fisher Scoring",
      "relationship": "related_to"
    },
    {
      "from": "VC Dimension",
      "to": "Shattering Sets",
      "relationship": "subtopic"
    },
    {
      "from": "Bias-Variance Tradeoff",
      "to": "Training and Test Datasets",
      "relationship": "depends_on"
    },
    {
      "from": "Function Representation",
      "to": "Cost Function",
      "relationship": "subtopic"
    },
    {
      "from": "Policy Gradients",
      "to": "Gradient Ascent",
      "relationship": "subtopic"
    },
    {
      "from": "VC Dimension",
      "to": "Shattering Sets",
      "relationship": "depends_on"
    },
    {
      "from": "Unsupervised learning",
      "to": "Principal components analysis",
      "relationship": "has_subtopic"
    },
    {
      "from": "SIMCLR Algorithm",
      "to": "Loss Function",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Fundamentals",
      "to": "Model Evaluation",
      "relationship": "depends_on"
    },
    {
      "from": "Multi-class Classification",
      "to": "Negative Log-likelihood Loss Function (Multi-class)",
      "relationship": "subtopic"
    },
    {
      "from": "Leaky ReLU",
      "to": "Activation Functions",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Probability Distributions",
      "relationship": "related_to"
    },
    {
      "from": "Optimization Constraints",
      "to": "Alpha Variables",
      "relationship": "subtopic"
    },
    {
      "from": "LMS Update Rule",
      "to": "Error Term",
      "relationship": "depends_on"
    },
    {
      "from": "Regularization",
      "to": "Regularizer",
      "relationship": "subtopic"
    },
    {
      "from": "Regularization",
      "to": "L1 Regularization",
      "relationship": "subtopic"
    },
    {
      "from": "Independent Component Analysis (ICA)",
      "to": "Mixing Matrix (A)",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Total Parameters Conv1D",
      "relationship": "subtopic"
    },
    {
      "from": "Vectorization in Neural Networks",
      "to": "Two-Layer Fully-Connected Network",
      "relationship": "subtopic"
    },
    {
      "from": "Naive Bayes Algorithm",
      "to": "Parameter Estimation",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Basics",
      "to": "Loss Function",
      "relationship": "related_to"
    },
    {
      "from": "Coordinate Ascent",
      "to": "Quadratic Function Contours",
      "relationship": "illustrates"
    },
    {
      "from": "Likelihood_Estimation",
      "to": "Non_Convex_Optimization",
      "relationship": "related_to"
    },
    {
      "from": "Feature Maps",
      "to": "Deep Learning",
      "relationship": "subtopic"
    },
    {
      "from": "Learning Guarantees",
      "to": "Union Bound Lemma",
      "relationship": "subtopic"
    },
    {
      "from": "Logistic Regression",
      "to": "Probability Calculation",
      "relationship": "subtopic"
    },
    {
      "from": "Bias-Variance Trade-off Reconciliation",
      "to": "Machine Learning Literature",
      "relationship": "subtopic"
    },
    {
      "from": "Value Iteration",
      "to": "Geometric Convergence",
      "relationship": "subtopic"
    },
    {
      "from": "Linear Function Approximation",
      "to": "Parameters (Weights)",
      "relationship": "has_subtopic"
    },
    {
      "from": "E-step",
      "to": "Gaussian Mixture Model",
      "relationship": "depends_on"
    },
    {
      "from": "PCA",
      "to": "Approximation Error Minimization",
      "relationship": "subtopic"
    },
    {
      "from": "Multi-class Classification",
      "to": "Softmax Function",
      "relationship": "depends_on"
    },
    {
      "from": "Finite Horizon MDPs",
      "to": "Optimal Policy",
      "relationship": "related_to"
    },
    {
      "from": "Least-Squares Cost Function",
      "to": "Invertibility of X^TX Matrix",
      "relationship": "depends_on"
    },
    {
      "from": "ResNet (Residual Network)",
      "to": "Simplified ResNet Architecture",
      "relationship": "related_to"
    },
    {
      "from": "Log Partition Function for Bernoulli",
      "to": "Bernoulli Distribution",
      "relationship": "subtopic"
    },
    {
      "from": "Backward Function Overview",
      "to": "Activation Functions Backward Function",
      "relationship": "has_subtopic"
    },
    {
      "from": "Fully Bayesian Prediction",
      "to": "Computational Challenges",
      "relationship": "depends_on"
    },
    {
      "from": "Noise Reduction",
      "to": "Eigenfaces Method",
      "relationship": "contains"
    },
    {
      "from": "GELU Function",
      "to": "Activation Functions",
      "relationship": "subtopic"
    },
    {
      "from": "Expectation Maximization (EM)",
      "to": "Evidence Lower Bound (ELBO)",
      "relationship": "subtopic"
    },
    {
      "from": "Regression Problems",
      "to": "Mean-Square Cost Function",
      "relationship": "has_subtopic"
    },
    {
      "from": "Logistic Function",
      "to": "Derivative of Sigmoid",
      "relationship": "related_to"
    },
    {
      "from": "Double Descent Phenomenon",
      "to": "Machine Learning Concepts",
      "relationship": "subtopic"
    },
    {
      "from": "Loss Function",
      "to": "Average Loss",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Fundamentals",
      "to": "Partial Derivatives in ML",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning",
      "to": "Classification",
      "relationship": "contains"
    },
    {
      "from": "Sparsity Regularization",
      "to": "L1 Norm (LASSO)",
      "relationship": "depends_on"
    },
    {
      "from": "Training Set",
      "to": "Posterior Distribution",
      "relationship": "depends_on"
    },
    {
      "from": "Principal Component Analysis (PCA)",
      "to": "Noise Reduction",
      "relationship": "subtopic"
    },
    {
      "from": "Regression Problems",
      "to": "Training Dataset",
      "relationship": "related_to"
    },
    {
      "from": "Gradient Descent (GD)",
      "to": "Learning Rate",
      "relationship": "depends_on"
    },
    {
      "from": "Training Examples Representation",
      "to": "First-Layer Activations",
      "relationship": "subtopic"
    },
    {
      "from": "Feature Map",
      "to": "Attributes",
      "relationship": "depends_on"
    },
    {
      "from": "Hypothesis Class Switching",
      "to": "Variance Increase",
      "relationship": "related_to"
    },
    {
      "from": "Density Transformation",
      "to": "1D Example",
      "relationship": "subtopic"
    },
    {
      "from": "Empirical Risk Minimization",
      "to": "Training Error Selection",
      "relationship": "related_to"
    },
    {
      "from": "Kalman Filter",
      "to": "Step 1",
      "relationship": "subtopic"
    },
    {
      "from": "Chain Rule",
      "to": "Gradient Computation",
      "relationship": "subtopic"
    },
    {
      "from": "Learning Theory",
      "to": "Machine Learning Concepts",
      "relationship": "subtopic"
    },
    {
      "from": "Naive Bayes Classifier",
      "to": "Maximum Likelihood Estimation",
      "relationship": "subtopic"
    },
    {
      "from": "Logistic Regression",
      "to": "Total Loss Function",
      "relationship": "subtopic"
    },
    {
      "from": "Independent components analysis",
      "to": "Densities and linear transformations",
      "relationship": "subtopic_of"
    },
    {
      "from": "Expectation Approximation",
      "to": "Gaussian Noise Model",
      "relationship": "related_to"
    },
    {
      "from": "Overfitting",
      "to": "Bias-Variance Tradeoff",
      "relationship": "depends_on"
    },
    {
      "from": "Infinite Hypothesis Classes",
      "to": "Bit Representation",
      "relationship": "depends_on"
    },
    {
      "from": "Model Selection",
      "to": "Validation Set Size",
      "relationship": "depends_on"
    },
    {
      "from": "5th Degree Polynomial Models",
      "to": "Overfitting",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Functional Margin",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Models",
      "to": "Convolutional Layers",
      "relationship": "has_subtopic"
    },
    {
      "from": "Conditional Probability in Language Models",
      "to": "Temperature Parameter",
      "relationship": "subtopic"
    },
    {
      "from": "Markov Decision Processes (MDP)",
      "to": "Actions",
      "relationship": "includes"
    },
    {
      "from": "Machine Learning Algorithms",
      "to": "Parameter Initialization",
      "relationship": "contains"
    },
    {
      "from": "EM algorithms",
      "to": "EM for mixture of Gaussians",
      "relationship": "subtopic_of"
    },
    {
      "from": "Classification and Logistic Regression",
      "to": "Logistic Regression",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Algorithms",
      "to": "Parametric Algorithms",
      "relationship": "includes"
    },
    {
      "from": "Neural Networks",
      "to": "Biological Inspiration",
      "relationship": "related_to"
    },
    {
      "from": "Gradient Descent",
      "to": "LMS Update Rule",
      "relationship": "subtopic"
    },
    {
      "from": "Generalization and Regularization",
      "to": "Training Loss Function",
      "relationship": "depends_on"
    },
    {
      "from": "Sequential Decision Making",
      "to": "Reinforcement Learning",
      "relationship": "subtopic"
    },
    {
      "from": "Log Likelihood",
      "to": "Lower Bound Derivation",
      "relationship": "depends_on"
    },
    {
      "from": "Deep Learning Introduction",
      "to": "Supervised Learning with Non-Linear Models",
      "relationship": "subtopic"
    },
    {
      "from": "Value Iteration",
      "to": "Policy Update Rule (15.13)",
      "relationship": "subtopic"
    },
    {
      "from": "Spam/Non-Spam Classification",
      "to": "Word Generation Process",
      "relationship": "depends_on"
    },
    {
      "from": "EM Algorithm",
      "to": "Convergence Proof",
      "relationship": "related_to"
    },
    {
      "from": "Objective Function Primal",
      "to": "Value Primal Problem",
      "relationship": "subtopic"
    },
    {
      "from": "Pretrained large language models",
      "to": "Zero-shot learning and in-context learning",
      "relationship": "subtopic_of"
    },
    {
      "from": "Supervised Learning",
      "to": "Support Vector Machines",
      "relationship": "subtopic"
    },
    {
      "from": "State Representation",
      "to": "Discretization",
      "relationship": "subtopic"
    },
    {
      "from": "Training vs Test Distributions",
      "to": "Machine Learning Basics",
      "relationship": "subtopic"
    },
    {
      "from": "Finite State MDPs",
      "to": "Machine Learning Algorithms",
      "relationship": "related_to"
    },
    {
      "from": "High-Dimensional Statistical Analysis",
      "to": "Machine Learning Literature",
      "relationship": "subtopic"
    },
    {
      "from": "Support Vector Machines (SVM)",
      "to": "Kernel Application",
      "relationship": "subtopic"
    },
    {
      "from": "Training Set",
      "to": "Hypothesis Function",
      "relationship": "depends_on"
    },
    {
      "from": "Adaptation Algorithm",
      "to": "Machine Learning Adaptation Methods",
      "relationship": "subtopic"
    },
    {
      "from": "Kalman Filter",
      "to": "Update Step",
      "relationship": "has_subtopic"
    },
    {
      "from": "Maximum Likelihood Estimates",
      "to": "Laplace Smoothing",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Models",
      "to": "MLP (Multi-Layer Perceptron)",
      "relationship": "subtopic"
    },
    {
      "from": "Markov Decision Process (MDP)",
      "to": "Policy Iteration",
      "relationship": "depends_on"
    },
    {
      "from": "Kernel Trick",
      "to": "Inner Products",
      "relationship": "depends_on"
    },
    {
      "from": "Linear Probe Approach",
      "to": "Adaptation Algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Mixture of Gaussians",
      "relationship": "contains"
    },
    {
      "from": "Partially Observable MDPs (POMDP)",
      "to": "Belief State",
      "relationship": "has_subtopic"
    },
    {
      "from": "Hold Out Cross Validation",
      "to": "Retraining on Full Dataset",
      "relationship": "contains"
    },
    {
      "from": "Binary Classification Problem",
      "to": "MLP Model",
      "relationship": "subtopic"
    },
    {
      "from": "Markov Decision Processes (MDP)",
      "to": "State Transition Probabilities",
      "relationship": "includes"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Kernel Trick",
      "relationship": "subtopic"
    },
    {
      "from": "Kalman Filter",
      "to": "Step 3",
      "relationship": "subtopic"
    },
    {
      "from": "Expectation-Maximization (EM) Algorithm",
      "to": "M-step Update Rule",
      "relationship": "depends_on"
    },
    {
      "from": "Cross Validation",
      "to": "Polynomial Regression Models",
      "relationship": "subtopic"
    },
    {
      "from": "Single Neuron Network",
      "to": "Housing Price Prediction",
      "relationship": "example_of"
    },
    {
      "from": "Probability Estimation",
      "to": "Naive Bayes Classifier",
      "relationship": "related_to"
    },
    {
      "from": "Gaussian Discriminant Analysis (GDA)",
      "to": "Decision Boundaries",
      "relationship": "related_to"
    },
    {
      "from": "Succinct Representation of Distribution Qi",
      "to": "Optimizing Continuous Latent Variables",
      "relationship": "subtopic"
    },
    {
      "from": "Feature Mapping",
      "to": "Kernels as Similarity Metrics",
      "relationship": "related_to"
    },
    {
      "from": "Convergence",
      "to": "k-means Algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Markov Decision Processes (MDP)",
      "to": "Reward Function",
      "relationship": "includes"
    },
    {
      "from": "Log Probability Derivative",
      "to": "Trajectory Probability Change",
      "relationship": "subtopic"
    },
    {
      "from": "Stochastic Gradient Descent (SGD)",
      "to": "Deep Learning Model Training Steps",
      "relationship": "related_to"
    },
    {
      "from": "Binary Classification",
      "to": "Logistic Function",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Techniques",
      "to": "Cross Validation",
      "relationship": "depends_on"
    },
    {
      "from": "Uniform Convergence",
      "to": "Training Error vs Generalization Error",
      "relationship": "related_to"
    },
    {
      "from": "Reward Function Estimation",
      "to": "Expectation Maximization",
      "relationship": "depends_on"
    },
    {
      "from": "Jensen's Inequality",
      "to": "Lower Bound Derivation",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Kernel Methods",
      "relationship": "related_to"
    },
    {
      "from": "Kernel Methods",
      "to": "Support Vector Machines (SVM)",
      "relationship": "subtopic"
    },
    {
      "from": "Backpropagation Algorithm",
      "to": "Chain Rule Application",
      "relationship": "subtopic"
    },
    {
      "from": "Gradient Descent",
      "to": "Update Rule",
      "relationship": "subtopic"
    },
    {
      "from": "Neural Networks",
      "to": "Regression Problem",
      "relationship": "depends_on"
    },
    {
      "from": "Independent Component Analysis (ICA)",
      "to": "Unmixing Matrix (W)",
      "relationship": "subtopic"
    },
    {
      "from": "Gaussian Distribution Assumption",
      "to": "Mean and Variance Functions",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "ReLU Activation Function",
      "relationship": "subtopic"
    },
    {
      "from": "Optimization in RL",
      "to": "Machine Learning Concepts",
      "relationship": "depends_on"
    },
    {
      "from": "Convolutional Layers",
      "to": "Parameter Sharing",
      "relationship": "subtopic"
    },
    {
      "from": "Kernel Methods",
      "to": "LMS with Features",
      "relationship": "subtopic"
    },
    {
      "from": "Constructing GLMs",
      "to": "Logistic Regression (GLM)",
      "relationship": "subtopic"
    },
    {
      "from": "Bias-Variance Tradeoff",
      "to": "Variance Term",
      "relationship": "contains"
    },
    {
      "from": "Bayesian Classification",
      "to": "Conditional Probability",
      "relationship": "depends_on"
    },
    {
      "from": "Gaussian Discriminant Analysis",
      "to": "Discussion: GDA and Logistic Regression",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Total Parameters Conv2D",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Overfitting",
      "relationship": "related_to"
    },
    {
      "from": "Language Models",
      "to": "Conditional Probability in Language Models",
      "relationship": "depends_on"
    },
    {
      "from": "Likelihood_Estimation",
      "to": "Latent_Variables",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Techniques",
      "to": "Independent Components Analysis (ICA)",
      "relationship": "related_to"
    },
    {
      "from": "Optimization Problem",
      "to": "Linear Regression",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Variational Inference",
      "relationship": "subtopic"
    },
    {
      "from": "Backward Functions",
      "to": "Matrix Multiplication Module (MM)",
      "relationship": "subtopic"
    },
    {
      "from": "Theoretical Guarantees for Deep Reinforcement Learning",
      "to": "Algorithmic Framework for Model-Based Deep Reinforcement Learning with Theoretical Guarantees",
      "relationship": "related_to"
    },
    {
      "from": "Linear Regression",
      "to": "Normal Equations",
      "relationship": "subtopic"
    },
    {
      "from": "Unsupervised Learning",
      "to": "Mixture of Gaussians Model",
      "relationship": "related_to"
    },
    {
      "from": "Multidimensional Generalization",
      "to": "Hessian Matrix",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Geometric Margin",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Inverted Pendulum Example",
      "relationship": "subtopic"
    },
    {
      "from": "Locally Weighted Linear Regression (LWLR)",
      "to": "Weight Calculation",
      "relationship": "depends_on"
    },
    {
      "from": "Self-supervised learning and foundation models",
      "to": "Pretraining methods in computer vision",
      "relationship": "has_subtopic"
    },
    {
      "from": "Hypothesis Function",
      "to": "Training Error",
      "relationship": "subtopic"
    },
    {
      "from": "Optimal Parameters Alpha",
      "to": "Dual Problem Formulation",
      "relationship": "subtopic"
    },
    {
      "from": "k-means Algorithm",
      "to": "Distortion Function J",
      "relationship": "depends_on"
    },
    {
      "from": "Bias-Variance Tradeoff",
      "to": "Bias Term",
      "relationship": "contains"
    },
    {
      "from": "Gradient Descent Optimizer",
      "to": "Minimum Norm Solution",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Algorithms",
      "to": "Gradient Ascent",
      "relationship": "subtopic"
    },
    {
      "from": "Zero-shot Learning",
      "to": "Language Model Utilization",
      "relationship": "has_subtopic"
    },
    {
      "from": "Mixture of Gaussians Model",
      "to": "Model Parameters",
      "relationship": "subtopic"
    },
    {
      "from": "Classification Problem",
      "to": "Logistic Regression",
      "relationship": "subtopic"
    },
    {
      "from": "1-D Convolution Layer",
      "to": "Bias Scalar",
      "relationship": "depends_on"
    },
    {
      "from": "Data Augmentation",
      "to": "Negative Pair",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Polynomial Fitting",
      "relationship": "subtopic"
    },
    {
      "from": "Logistic Regression",
      "to": "Gradient Calculation",
      "relationship": "depends_on"
    },
    {
      "from": "Optimization Problems",
      "to": "Convex Functions",
      "relationship": "related_to"
    },
    {
      "from": "Car Example",
      "to": "Data Redundancy Detection",
      "relationship": "related_to"
    },
    {
      "from": "PCA Algorithm Introduction",
      "to": "Normalization Process",
      "relationship": "subtopic"
    },
    {
      "from": "Foundation Models",
      "to": "Pretraining Phase",
      "relationship": "subtopic"
    },
    {
      "from": "Matrix Notation in Machine Learning",
      "to": "Vectorization",
      "relationship": "subtopic"
    },
    {
      "from": "MAP Estimation",
      "to": "MLE vs MAP",
      "relationship": "related_to"
    },
    {
      "from": "Underfitting",
      "to": "Machine Learning Basics",
      "relationship": "subtopic"
    },
    {
      "from": "From non-linear dynamics to LQR",
      "to": "Linearization of dynamics",
      "relationship": "has_subtopic"
    },
    {
      "from": "Generalized Linear Model (GLM)",
      "to": "Linear Relationship Assumption",
      "relationship": "subtopic"
    },
    {
      "from": "Discrete Latent Variables",
      "to": "Mean Field Assumption",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Neural Network Inputs",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning",
      "to": "Convolutional Neural Networks (CNN)",
      "relationship": "related_to"
    },
    {
      "from": "Two-Layer Neural Network",
      "to": "Bias Vectors",
      "relationship": "depends_on"
    },
    {
      "from": "Chapter 9 Regularization and Model Selection",
      "to": "Regularization",
      "relationship": "subtopic"
    },
    {
      "from": "Discriminative Learning Algorithms",
      "to": "Perceptron Algorithm",
      "relationship": "related_to"
    },
    {
      "from": "Constrained Optimization",
      "to": "Lagrange Multipliers",
      "relationship": "depends_on"
    },
    {
      "from": "Linear Regression",
      "to": "LMS Algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Partial Derivatives in ML",
      "to": "Scalar Functions and Vectors",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Overview",
      "to": "Support Vector Machines (SVM)",
      "relationship": "related_to"
    },
    {
      "from": "Backpropagation",
      "to": "Back-propagation for MLPs",
      "relationship": "subtopic"
    },
    {
      "from": "Support Vector Machines (SVM)",
      "to": "Dual Formulation of SVM",
      "relationship": "subtopic"
    },
    {
      "from": "Multi-class Classification",
      "to": "Softmax Function",
      "relationship": "uses"
    },
    {
      "from": "Identity Function",
      "to": "Activation Functions",
      "relationship": "subtopic"
    },
    {
      "from": "Locally Weighted Linear Regression",
      "to": "Non-Parametric Algorithms",
      "relationship": "is_a"
    },
    {
      "from": "Machine Learning Basics",
      "to": "Hypothesis Class",
      "relationship": "has_subtopic"
    },
    {
      "from": "Sequential Minimal Optimization (SMO)",
      "to": "Convergence Criteria",
      "relationship": "depends_on"
    },
    {
      "from": "Newton's Method",
      "to": "Finding Roots",
      "relationship": "depends_on"
    },
    {
      "from": "Supervised Learning",
      "to": "Kernel Methods",
      "relationship": "subtopic"
    },
    {
      "from": "Identity Function",
      "to": "Activation Functions",
      "relationship": "depends_on"
    },
    {
      "from": "State Transition Probabilities",
      "to": "Finite-State MDPs",
      "relationship": "related_to"
    },
    {
      "from": "Principal Eigenvector",
      "to": "Lagrange Multipliers",
      "relationship": "depends_on"
    },
    {
      "from": "Feature Map",
      "to": "Features Variables",
      "relationship": "related_to"
    },
    {
      "from": "Deep Learning",
      "to": "Supervised Learning with Non-linear Models",
      "relationship": "subtopic"
    },
    {
      "from": "Back-propagation for MLPs",
      "to": "Gradient Computation",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Algorithms",
      "to": "Non-Parametric Algorithms",
      "relationship": "includes"
    },
    {
      "from": "Markov Decision Process (MDP)",
      "to": "Value Iteration",
      "relationship": "depends_on"
    },
    {
      "from": "Parameter Estimation",
      "to": "Machine Learning Models",
      "relationship": "depends_on"
    },
    {
      "from": "Estimator Simplification",
      "to": "Expectation Calculation",
      "relationship": "subtopic"
    },
    {
      "from": "Kernel Functions",
      "to": "Explicit Definition of Kernels",
      "relationship": "subtopic"
    },
    {
      "from": "Bias-variance tradeoff",
      "to": "A mathematical decomposition (for regression)",
      "relationship": "subtopic_of"
    },
    {
      "from": "Reinforcement Learning",
      "to": "Markov Decision Processes (MDP)",
      "relationship": "defines_with"
    },
    {
      "from": "Adaptation Methods",
      "to": "Finetuning",
      "relationship": "subtopic"
    },
    {
      "from": "Unsupervised learning",
      "to": "EM algorithms",
      "relationship": "has_subtopic"
    },
    {
      "from": "Policy Gradients",
      "to": "Reward Function Estimation",
      "relationship": "subtopic"
    },
    {
      "from": "Backpropagation Algorithm",
      "to": "Gradient Computation",
      "relationship": "subtopic"
    },
    {
      "from": "Generalization Error Analysis",
      "to": "More Data Can Hurt for Linear Regression: Sample-Wise Double Descent",
      "relationship": "related_to"
    },
    {
      "from": "ICA Ambiguities",
      "to": "Permutation Matrix",
      "relationship": "has_subtopic"
    },
    {
      "from": "Optimization Problem",
      "to": "Optimal Margin Classifier",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Models",
      "to": "ResNet (Residual Network)",
      "relationship": "subtopic"
    },
    {
      "from": "Linear Model Limitations",
      "to": "Underfitting",
      "relationship": "related_to"
    },
    {
      "from": "Loss Function",
      "to": "Gradient Computation",
      "relationship": "depends_on"
    },
    {
      "from": "Supervised Learning",
      "to": "Regression Algorithms",
      "relationship": "related_to"
    },
    {
      "from": "Model Creation Methods",
      "to": "Learning from Data",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Backpropagation",
      "relationship": "related_to"
    },
    {
      "from": "Regularization",
      "to": "Regularization Parameter (\u03bb)",
      "relationship": "depends_on"
    },
    {
      "from": "Support Vector Machines (SVMs)",
      "to": "SMO Algorithm",
      "relationship": "derives_from"
    },
    {
      "from": "KKT Conditions",
      "to": "Dual Complementarity Condition",
      "relationship": "subtopic"
    },
    {
      "from": "Evidence Lower Bound (ELBO)",
      "to": "Gradient Computation",
      "relationship": "related_to"
    },
    {
      "from": "Implicit Bias Study",
      "to": "Machine Learning Literature",
      "relationship": "subtopic"
    },
    {
      "from": "Lagrange Duality",
      "to": "Dual Formulation",
      "relationship": "depends_on"
    },
    {
      "from": "Linear Regression",
      "to": "Probabilistic Interpretation",
      "relationship": "subtopic"
    },
    {
      "from": "Backpropagation",
      "to": "Differentiable Circuit",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Algorithms",
      "to": "Gradient Descent",
      "relationship": "related_to"
    },
    {
      "from": "Necessary Conditions for Valid Kernels",
      "to": "Kernel Matrix",
      "relationship": "related_to"
    },
    {
      "from": "Kernel Functions",
      "to": "Necessary Conditions for Valid Kernels",
      "relationship": "subtopic"
    },
    {
      "from": "Model Creation Methods",
      "to": "Physics Simulation",
      "relationship": "subtopic"
    },
    {
      "from": "Model Evaluation",
      "to": "Bias-Variance Tradeoff",
      "relationship": "related_to"
    },
    {
      "from": "Other Normalization Layers",
      "to": "Batch Normalization",
      "relationship": "subtopic"
    },
    {
      "from": "Backward Function Overview",
      "to": "Matrix Multiplication Backward Function",
      "relationship": "has_subtopic"
    },
    {
      "from": "Value Function Approximation",
      "to": "Fitted Value Iteration",
      "relationship": "subtopic"
    },
    {
      "from": "Principal Components",
      "to": "Dimensionality Reduction",
      "relationship": "related_to"
    },
    {
      "from": "Optimal Policy in MDPs",
      "to": "Machine Learning Overview",
      "relationship": "depends_on"
    },
    {
      "from": "Other Activation Functions",
      "to": "Multi-layer Fully-Connected Neural Networks",
      "relationship": "depends_on"
    },
    {
      "from": "Support Vector Machines",
      "to": "Regularization",
      "relationship": "depends_on"
    },
    {
      "from": "Hypothesis Class Switching",
      "to": "Bias Decrease",
      "relationship": "depends_on"
    },
    {
      "from": "Training Process",
      "to": "Cross-Entropy Loss",
      "relationship": "depends_on"
    },
    {
      "from": "Linearization of Dynamics",
      "to": "Taylor Expansion",
      "relationship": "depends_on"
    },
    {
      "from": "Kernel Matrix Properties",
      "to": "Sufficient Conditions for Valid Kernels",
      "relationship": "subtopic"
    },
    {
      "from": "Supervised Learning",
      "to": "Generalized Linear Models",
      "relationship": "subtopic"
    },
    {
      "from": "Multi-class Classification",
      "to": "Logits in Multi-class",
      "relationship": "subtopic"
    },
    {
      "from": "Parameter Estimation",
      "to": "Maximum Likelihood Estimation (MLE)",
      "relationship": "subtopic"
    }
  ]
}