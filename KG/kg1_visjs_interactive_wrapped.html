
<!DOCTYPE html>
<html>
<head>
    <title>KG1 Visualization (Vis.js Interactive - Wrapped Labels)</title>
    <script type="text/javascript" src="https://unpkg.com/vis-network/standalone/umd/vis-network.min.js"></script>
    <style type="text/css">
        body { font-family: sans-serif; margin: 0; display: flex; height: 100vh; }
        #networkContainer { flex-grow: 1; height: 100%; border-right: 1px solid lightgray; position: relative; }
        #mynetwork { width: 100%; height: 100%; }
        #detailsContainer { width: 300px; height: 100%; overflow-y: auto; padding: 15px; box-sizing: border-box; background-color: #f8f8f8; }
        #detailsContainer h3 { margin-top: 0; border-bottom: 1px solid #ccc; padding-bottom: 5px; }
        #detailsContainer p { font-size: 0.9em; line-height: 1.4; }
        .loading-overlay {
            position: absolute; top: 0; left: 0; width: 100%; height: 100%;
            background-color: rgba(255, 255, 255, 0.8); z-index: 10;
            display: flex; justify-content: center; align-items: center; font-size: 1.2em;
        }
        /* Optional: Adjust node font size if needed */
        /*
        .vis-network .vis-node {
            font-size: 12px;
        }
        */
    </style>
</head>
<body>
    <div id="networkContainer">
        <div id="mynetwork"></div>
        <div id="loadingOverlay" class="loading-overlay">Loading & Initial Layout...</div>
    </div>
    <div id="detailsContainer">
        <h3>Node Details</h3>
        <div id="nodeDetails">Select a node to see details.</div>
    </div>

    <script type="text/javascript">
        // --- Embedded Data (Generated by Python) ---
        const nodesData = [{"id": "I Supervised learning", "label": "I Supervised\nlearning", "title": "<b>I Supervised learning</b> (major)<hr>Overview of supervised learning techniques and models.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Overview of supervised learning techniques and models.", "node_type": "major"}, {"id": "1 Linear regression", "label": "1 Linear\nregression", "title": "<b>1 Linear regression</b> (subnode)<hr>Introduction to linear regression including LMS algorithm and normal equations.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Introduction to linear regression including LMS algorithm and normal equations.", "node_type": "subnode"}, {"id": "1.1 LMS algorithm", "label": "1.1 LMS\nalgorithm", "title": "<b>1.1 LMS algorithm</b> (subnode)<hr>Least Mean Squares algorithm for linear regression.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Least Mean Squares algorithm for linear regression.", "node_type": "subnode"}, {"id": "1.2 The normal equations", "label": "1.2 The normal\nequations", "title": "<b>1.2 The normal equations</b> (subnode)<hr>Derivation and use of the normal equation method in linear regression.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Derivation and use of the normal equation method in linear regression.", "node_type": "subnode"}, {"id": "1.2.1 Matrix derivatives", "label": "1.2.1 Matrix\nderivatives", "title": "<b>1.2.1 Matrix derivatives</b> (subnode)<hr>Matrix calculus involved in deriving the normal equations.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Matrix calculus involved in deriving the normal equations.", "node_type": "subnode"}, {"id": "1.2.2 Least squares revisited", "label": "1.2.2 Least\nsquares\nrevisited", "title": "<b>1.2.2 Least squares revisited</b> (subnode)<hr>Revisiting least squares method with matrix algebra.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Revisiting least squares method with matrix algebra.", "node_type": "subnode"}, {"id": "1.3 Probabilistic interpretation", "label": "1.3\nProbabilistic\ninterpretation", "title": "<b>1.3 Probabilistic interpretation</b> (subnode)<hr>Probabilistic view of linear regression models.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Probabilistic view of linear regression models.", "node_type": "subnode"}, {"id": "1.4 Locally weighted linear regression (optional reading)", "label": "1.4 Locally\nweighted linear\nregression\n(optional\nreading)", "title": "<b>1.4 Locally weighted linear regression (optional reading)</b> (subnode)<hr>Advanced topic: locally weighted linear regression for non-stationary data.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Advanced topic: locally weighted linear regression for non-stationary data.", "node_type": "subnode"}, {"id": "2 Classification and logistic regression", "label": "2\nClassification\nand logistic\nregression", "title": "<b>2 Classification and logistic regression</b> (subnode)<hr>Introduction to classification problems and logistic regression.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Introduction to classification problems and logistic regression.", "node_type": "subnode"}, {"id": "2.1 Logistic regression", "label": "2.1 Logistic\nregression", "title": "<b>2.1 Logistic regression</b> (subnode)<hr>Logistic function for binary classification.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Logistic function for binary classification.", "node_type": "subnode"}, {"id": "2.2 Digression: the perceptron learning algorithm", "label": "2.2 Digression:\nthe perceptron\nlearning\nalgorithm", "title": "<b>2.2 Digression: the perceptron learning algorithm</b> (subnode)<hr>Perceptron algorithm as a precursor to modern neural networks.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Perceptron algorithm as a precursor to modern neural networks.", "node_type": "subnode"}, {"id": "2.3 Multi-class classification", "label": "2.3 Multi-class\nclassification", "title": "<b>2.3 Multi-class classification</b> (subnode)<hr>Techniques for handling more than two classes in classification problems.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Techniques for handling more than two classes in classification problems.", "node_type": "subnode"}, {"id": "2.4 Another algorithm for maximizing \u03bb(\u03b8)", "label": "2.4 Another\nalgorithm for\nmaximizing \u03bb(\u03b8)", "title": "<b>2.4 Another algorithm for maximizing \u03bb(\u03b8)</b> (subnode)<hr>Alternative methods to maximize the likelihood function in logistic regression.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Alternative methods to maximize the likelihood function in logistic regression.", "node_type": "subnode"}, {"id": "3 Generalized linear models", "label": "3 Generalized\nlinear models", "title": "<b>3 Generalized linear models</b> (subnode)<hr>Introduction to generalized linear models (GLMs).", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Introduction to generalized linear models (GLMs).", "node_type": "subnode"}, {"id": "3.1 The exponential family", "label": "3.1 The\nexponential\nfamily", "title": "<b>3.1 The exponential family</b> (subnode)<hr>Overview of the exponential family in statistics.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Overview of the exponential family in statistics.", "node_type": "subnode"}, {"id": "3.2 Constructing GLMs", "label": "3.2\nConstructing\nGLMs", "title": "<b>3.2 Constructing GLMs</b> (subnode)<hr>Steps to construct generalized linear models from exponential families.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Steps to construct generalized linear models from exponential families.", "node_type": "subnode"}, {"id": "3.2.1 Ordinary least squares", "label": "3.2.1 Ordinary\nleast squares", "title": "<b>3.2.1 Ordinary least squares</b> (subnode)<hr>Ordinary least squares as a special case of GLM for Gaussian distribution.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Ordinary least squares as a special case of GLM for Gaussian distribution.", "node_type": "subnode"}, {"id": "3.2.2 Logistic regression", "label": "3.2.2 Logistic\nregression", "title": "<b>3.2.2 Logistic regression</b> (subnode)<hr>Logistic regression as an example of GLM for binary classification.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Logistic regression as an example of GLM for binary classification.", "node_type": "subnode"}, {"id": "4 Generative learning algorithms", "label": "4 Generative\nlearning\nalgorithms", "title": "<b>4 Generative learning algorithms</b> (subnode)<hr>Introduction to generative models in machine learning.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Introduction to generative models in machine learning.", "node_type": "subnode"}, {"id": "4.1 Gaussian discriminant analysis", "label": "4.1 Gaussian\ndiscriminant\nanalysis", "title": "<b>4.1 Gaussian discriminant analysis</b> (subnode)<hr>Gaussian Discriminant Analysis for classification tasks.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Gaussian Discriminant Analysis for classification tasks.", "node_type": "subnode"}, {"id": "4.1.1 The multivariate normal distribution", "label": "4.1.1 The\nmultivariate\nnormal\ndistribution", "title": "<b>4.1.1 The multivariate normal distribution</b> (subnode)<hr>Properties and applications of the multivariate normal distribution.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Properties and applications of the multivariate normal distribution.", "node_type": "subnode"}, {"id": "4.1.2 The Gaussian discriminant analysis model", "label": "4.1.2 The\nGaussian\ndiscriminant\nanalysis model", "title": "<b>4.1.2 The Gaussian discriminant analysis model</b> (subnode)<hr>Model formulation for GDA using multivariate normals.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Model formulation for GDA using multivariate normals.", "node_type": "subnode"}, {"id": "4.1.3 Discussion: GDA and logistic regression", "label": "4.1.3\nDiscussion: GDA\nand logistic\nregression", "title": "<b>4.1.3 Discussion: GDA and logistic regression</b> (subnode)<hr>Comparison between GDA and logistic regression models.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Comparison between GDA and logistic regression models.", "node_type": "subnode"}, {"id": "4.2 Naive bayes (Option Reading)", "label": "4.2 Naive bayes\n(Option\nReading)", "title": "<b>4.2 Naive bayes (Option Reading)</b> (subnode)<hr>Naive Bayes classifier for text classification tasks.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Naive Bayes classifier for text classification tasks.", "node_type": "subnode"}, {"id": "4.2.1 Laplace smoothing", "label": "4.2.1 Laplace\nsmoothing", "title": "<b>4.2.1 Laplace smoothing</b> (subnode)<hr>Technique to handle zero probabilities in naive Bayes classifiers.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Technique to handle zero probabilities in naive Bayes classifiers.", "node_type": "subnode"}, {"id": "4.2.2 Event models for text classification", "label": "4.2.2 Event\nmodels for text\nclassification", "title": "<b>4.2.2 Event models for text classification</b> (subnode)<hr>Models based on word events for document categorization.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Models based on word events for document categorization.", "node_type": "subnode"}, {"id": "5 Kernel methods", "label": "5 Kernel\nmethods", "title": "<b>5 Kernel methods</b> (subnode)<hr>Techniques using kernel functions to extend linear models.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Techniques using kernel functions to extend linear models.", "node_type": "subnode"}, {"id": "5.1 Feature maps", "label": "5.1 Feature\nmaps", "title": "<b>5.1 Feature maps</b> (subnode)<hr>Mapping data into higher-dimensional feature spaces.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Mapping data into higher-dimensional feature spaces.", "node_type": "subnode"}, {"id": "5.2 LMS (least mean squares) with features", "label": "5.2 LMS (least\nmean squares)\nwith features", "title": "<b>5.2 LMS (least mean squares) with features</b> (subnode)<hr>LMS algorithm applied in the context of feature maps.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "LMS algorithm applied in the context of feature maps.", "node_type": "subnode"}, {"id": "5.3 LMS with the kernel trick", "label": "5.3 LMS with\nthe kernel\ntrick", "title": "<b>5.3 LMS with the kernel trick</b> (subnode)<hr>Using kernels to implicitly perform LMS in high-dimensional spaces.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Using kernels to implicitly perform LMS in high-dimensional spaces.", "node_type": "subnode"}, {"id": "5.4 Properties of kernels", "label": "5.4 Properties\nof kernels", "title": "<b>5.4 Properties of kernels</b> (subnode)<hr>Mathematical properties and requirements for valid kernel functions.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Mathematical properties and requirements for valid kernel functions.", "node_type": "subnode"}, {"id": "6 Support vector machines", "label": "6 Support\nvector machines", "title": "<b>6 Support vector machines</b> (subnode)<hr>Introduction to support vector machines (SVMs) for classification tasks.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Introduction to support vector machines (SVMs) for classification tasks.", "node_type": "subnode"}, {"id": "II Deep learning", "label": "II Deep\nlearning", "title": "<b>II Deep learning</b> (major)<hr>Overview of deep learning techniques and neural networks.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Overview of deep learning techniques and neural networks.", "node_type": "major"}, {"id": "7 Deep learning", "label": "7 Deep learning", "title": "<b>7 Deep learning</b> (subnode)<hr>Introduction to deep learning concepts and architectures.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Introduction to deep learning concepts and architectures.", "node_type": "subnode"}, {"id": "7.1 Supervised learning with non-linear models", "label": "7.1 Supervised\nlearning with\nnon-linear\nmodels", "title": "<b>7.1 Supervised learning with non-linear models</b> (subnode)<hr>Supervised learning using non-linear models in deep learning.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Supervised learning using non-linear models in deep learning.", "node_type": "subnode"}, {"id": "7.2 Neural networks", "label": "7.2 Neural\nnetworks", "title": "<b>7.2 Neural networks</b> (subnode)<hr>Architecture and training of neural network models.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Architecture and training of neural network models.", "node_type": "subnode"}, {"id": "7.3 Modules in Modern Neural Networks", "label": "7.3 Modules in\nModern Neural\nNetworks", "title": "<b>7.3 Modules in Modern Neural Networks</b> (subnode)<hr>Discussion on various modules used in modern deep learning architectures.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Discussion on various modules used in modern deep learning architectures.", "node_type": "subnode"}, {"id": "7.4 Backpropagation", "label": "7.4\nBackpropagation", "title": "<b>7.4 Backpropagation</b> (subnode)<hr>Algorithm for computing gradients in neural networks using backpropagation.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Algorithm for computing gradients in neural networks using backpropagation.", "node_type": "subnode"}, {"id": "7.4.1 Preliminaries on partial derivatives", "label": "7.4.1\nPreliminaries\non partial\nderivatives", "title": "<b>7.4.1 Preliminaries on partial derivatives</b> (subnode)<hr>Basics of partial derivatives needed for understanding backpropagation.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Basics of partial derivatives needed for understanding backpropagation.", "node_type": "subnode"}, {"id": "7.4.2 General strategy of backpropagation", "label": "7.4.2 General\nstrategy of\nbackpropagation", "title": "<b>7.4.2 General strategy of backpropagation</b> (subnode)<hr>Overview of the general approach to implementing backpropagation.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Overview of the general approach to implementing backpropagation.", "node_type": "subnode"}, {"id": "7.4.3 Backward functions for basic modules", "label": "7.4.3 Backward\nfunctions for\nbasic modules", "title": "<b>7.4.3 Backward functions for basic modules</b> (subnode)<hr>Derivation and implementation of backward functions for simple neural network layers.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Derivation and implementation of backward functions for simple neural network layers.", "node_type": "subnode"}, {"id": "7.4.4 Back-propagation for MLPs", "label": "7.4.4 Back-\npropagation for\nMLPs", "title": "<b>7.4.4 Back-propagation for MLPs</b> (subnode)<hr>Detailed explanation of backpropagation in multi-layer perceptrons (MLPs).", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Detailed explanation of backpropagation in multi-layer perceptrons (MLPs).", "node_type": "subnode"}, {"id": "Neural Networks", "label": "Neural Networks", "title": "<b>Neural Networks</b> (major)<hr>Non-linear models using matrix multiplications and non-linear operations to solve complex problems.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Non-linear models using matrix multiplications and non-linear operations to solve complex problems.", "node_type": "major"}, {"id": "Modules in Modern Neural Networks", "label": "Modules in\nModern Neural\nNetworks", "title": "<b>Modules in Modern Neural Networks</b> (subnode)<hr>Discussion on the components used in modern neural networks.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Discussion on the components used in modern neural networks.", "node_type": "subnode"}, {"id": "Backpropagation", "label": "Backpropagation", "title": "<b>Backpropagation</b> (subnode)<hr>Introduction to backpropagation for efficient gradient computation in neural networks.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Introduction to backpropagation for efficient gradient computation in neural networks.", "node_type": "subnode"}, {"id": "Preliminaries on partial derivatives", "label": "Preliminaries\non partial\nderivatives", "title": "<b>Preliminaries on partial derivatives</b> (subnode)<hr>Introduction to the mathematical concepts needed for backpropagation.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Introduction to the mathematical concepts needed for backpropagation.", "node_type": "subnode"}, {"id": "General strategy of backpropagation", "label": "General\nstrategy of\nbackpropagation", "title": "<b>General strategy of backpropagation</b> (subnode)<hr>Overview of the algorithmic steps in backpropagation.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Overview of the algorithmic steps in backpropagation.", "node_type": "subnode"}, {"id": "Backward functions for basic modules", "label": "Backward\nfunctions for\nbasic modules", "title": "<b>Backward functions for basic modules</b> (subnode)<hr>Explanation of backward propagation for simple network components.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Explanation of backward propagation for simple network components.", "node_type": "subnode"}, {"id": "Back-propagation for MLPs", "label": "Back-\npropagation for\nMLPs", "title": "<b>Back-propagation for MLPs</b> (subnode)<hr>Specific application of backpropagation to multi-layer perceptrons.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Specific application of backpropagation to multi-layer perceptrons.", "node_type": "subnode"}, {"id": "Vectorization over training examples", "label": "Vectorization\nover training\nexamples", "title": "<b>Vectorization over training examples</b> (subnode)<hr>Techniques for efficient computation in neural networks using vector operations.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Techniques for efficient computation in neural networks using vector operations.", "node_type": "subnode"}, {"id": "Generalization and regularization", "label": "Generalization\nand\nregularization", "title": "<b>Generalization and regularization</b> (major)<hr>Topics related to improving model performance on unseen data.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Topics related to improving model performance on unseen data.", "node_type": "major"}, {"id": "Generalization", "label": "Generalization", "title": "<b>Generalization</b> (subnode)<hr>Concepts and techniques for enhancing the generalizability of machine learning models.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Concepts and techniques for enhancing the generalizability of machine learning models.", "node_type": "subnode"}, {"id": "Bias-variance tradeoff", "label": "Bias-variance\ntradeoff", "title": "<b>Bias-variance tradeoff</b> (subnode)<hr>Balancing model complexity to avoid overfitting or underfitting.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Balancing model complexity to avoid overfitting or underfitting.", "node_type": "subnode"}, {"id": "A mathematical decomposition (for regression)", "label": "A mathematical\ndecomposition\n(for\nregression)", "title": "<b>A mathematical decomposition (for regression)</b> (subnode)<hr>Mathematical analysis of bias and variance in regression models.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Mathematical analysis of bias and variance in regression models.", "node_type": "subnode"}, {"id": "The double descent phenomenon", "label": "The double\ndescent\nphenomenon", "title": "<b>The double descent phenomenon</b> (subnode)<hr>Observation that model performance can improve after initial degradation with increased complexity.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Observation that model performance can improve after initial degradation with increased complexity.", "node_type": "subnode"}, {"id": "Sample complexity bounds (optional readings)", "label": "Sample\ncomplexity\nbounds\n(optional\nreadings)", "title": "<b>Sample complexity bounds (optional readings)</b> (subnode)<hr>Theoretical analysis of the number of samples needed for learning tasks.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Theoretical analysis of the number of samples needed for learning tasks.", "node_type": "subnode"}, {"id": "Regularization and model selection", "label": "Regularization\nand model\nselection", "title": "<b>Regularization and model selection</b> (major)<hr>Techniques to prevent overfitting by penalizing complexity in models.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Techniques to prevent overfitting by penalizing complexity in models.", "node_type": "major"}, {"id": "Regularization", "label": "Regularization", "title": "<b>Regularization</b> (subnode)<hr>Technique to control model complexity and prevent overfitting", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Technique to control model complexity and prevent overfitting", "node_type": "subnode"}, {"id": "Implicit regularization effect (optional reading)", "label": "Implicit\nregularization\neffect\n(optional\nreading)", "title": "<b>Implicit regularization effect (optional reading)</b> (subnode)<hr>Natural tendency of optimization algorithms to regularize models.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Natural tendency of optimization algorithms to regularize models.", "node_type": "subnode"}, {"id": "Model selection via cross validation", "label": "Model selection\nvia cross\nvalidation", "title": "<b>Model selection via cross validation</b> (subnode)<hr>Procedure for choosing the best model based on performance metrics.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Procedure for choosing the best model based on performance metrics.", "node_type": "subnode"}, {"id": "Bayesian statistics and regularization", "label": "Bayesian\nstatistics and\nregularization", "title": "<b>Bayesian statistics and regularization</b> (subnode)<hr>Application of Bayesian principles to regularize models.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Application of Bayesian principles to regularize models.", "node_type": "subnode"}, {"id": "Unsupervised learning", "label": "Unsupervised\nlearning", "title": "<b>Unsupervised learning</b> (major)<hr>Techniques for learning from data without labeled responses.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Techniques for learning from data without labeled responses.", "node_type": "major"}, {"id": "Clustering and the k-means algorithm", "label": "Clustering and\nthe k-means\nalgorithm", "title": "<b>Clustering and the k-means algorithm</b> (subnode)<hr>Algorithm for partitioning data into clusters based on similarity.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Algorithm for partitioning data into clusters based on similarity.", "node_type": "subnode"}, {"id": "EM algorithms", "label": "EM algorithms", "title": "<b>EM algorithms</b> (subnode)<hr>Expectation-Maximization algorithm for parameter estimation in probabilistic models.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Expectation-Maximization algorithm for parameter estimation in probabilistic models.", "node_type": "subnode"}, {"id": "EM for mixture of Gaussians", "label": "EM for mixture\nof Gaussians", "title": "<b>EM for mixture of Gaussians</b> (subnode)<hr>Application of EM to model data as a combination of Gaussian distributions.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Application of EM to model data as a combination of Gaussian distributions.", "node_type": "subnode"}, {"id": "Jensen's inequality", "label": "Jensen's\ninequality", "title": "<b>Jensen's inequality</b> (subnode)<hr>Mathematical result used in the derivation and application of the EM algorithm.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Mathematical result used in the derivation and application of the EM algorithm.", "node_type": "subnode"}, {"id": "General EM algorithms", "label": "General EM\nalgorithms", "title": "<b>General EM algorithms</b> (subnode)<hr>Discussion on the general framework and variations of the EM algorithm.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Discussion on the general framework and variations of the EM algorithm.", "node_type": "subnode"}, {"id": "Other interpretation of ELBO", "label": "Other\ninterpretation\nof ELBO", "title": "<b>Other interpretation of ELBO</b> (subnode)<hr>Alternative understanding of the Evidence Lower Bound in variational inference.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Alternative understanding of the Evidence Lower Bound in variational inference.", "node_type": "subnode"}, {"id": "Mixture of Gaussians revisited", "label": "Mixture of\nGaussians\nrevisited", "title": "<b>Mixture of Gaussians revisited</b> (subnode)<hr>Re-examination and extension of mixture models using Gaussian distributions.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Re-examination and extension of mixture models using Gaussian distributions.", "node_type": "subnode"}, {"id": "Variational inference and variational auto-encoder (optional reading)", "label": "Variational\ninference and\nvariational\nauto-encoder\n(optional\nreading)", "title": "<b>Variational inference and variational auto-encoder (optional reading)</b> (subnode)<hr>Advanced topic on probabilistic modeling with latent variables.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Advanced topic on probabilistic modeling with latent variables.", "node_type": "subnode"}, {"id": "Principal components analysis", "label": "Principal\ncomponents\nanalysis", "title": "<b>Principal components analysis</b> (subnode)<hr>Dimensionality reduction technique for identifying principal components in data.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Dimensionality reduction technique for identifying principal components in data.", "node_type": "subnode"}, {"id": "Independent components analysis", "label": "Independent\ncomponents\nanalysis", "title": "<b>Independent components analysis</b> (subnode)<hr>Technique to separate mixed signals into their independent sources.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Technique to separate mixed signals into their independent sources.", "node_type": "subnode"}, {"id": "ICA ambiguities", "label": "ICA ambiguities", "title": "<b>ICA ambiguities</b> (subnode)<hr>Discussion on the inherent limitations and challenges in ICA.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Discussion on the inherent limitations and challenges in ICA.", "node_type": "subnode"}, {"id": "Densities and linear transformations", "label": "Densities and\nlinear\ntransformations", "title": "<b>Densities and linear transformations</b> (subnode)<hr>Mathematical foundations for understanding densities and transformations in ICA.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Mathematical foundations for understanding densities and transformations in ICA.", "node_type": "subnode"}, {"id": "ICA algorithm", "label": "ICA algorithm", "title": "<b>ICA algorithm</b> (subnode)<hr>Detailed explanation of the Independent Components Analysis procedure.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Detailed explanation of the Independent Components Analysis procedure.", "node_type": "subnode"}, {"id": "Self-supervised learning and foundation models", "label": "Self-supervised\nlearning and\nfoundation\nmodels", "title": "<b>Self-supervised learning and foundation models</b> (major)<hr>Techniques for training models on large datasets without explicit supervision.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Techniques for training models on large datasets without explicit supervision.", "node_type": "major"}, {"id": "Pretraining and adaptation", "label": "Pretraining and\nadaptation", "title": "<b>Pretraining and adaptation</b> (subnode)<hr>Overview of pre-training methods and their role in model adaptation.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Overview of pre-training methods and their role in model adaptation.", "node_type": "subnode"}, {"id": "Pretraining methods in computer vision", "label": "Pretraining\nmethods in\ncomputer vision", "title": "<b>Pretraining methods in computer vision</b> (subnode)<hr>Specific approaches to self-supervised learning for visual data.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Specific approaches to self-supervised learning for visual data.", "node_type": "subnode"}, {"id": "Pretrained large language models", "label": "Pretrained\nlarge language\nmodels", "title": "<b>Pretrained large language models</b> (subnode)<hr>Discussion on the development and applications of pre-trained language models.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Discussion on the development and applications of pre-trained language models.", "node_type": "subnode"}, {"id": "Open up the blackbox of Transformers", "label": "Open up the\nblackbox of\nTransformers", "title": "<b>Open up the blackbox of Transformers</b> (subnode)<hr>Exploration of the architecture and workings of Transformer-based models.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Exploration of the architecture and workings of Transformer-based models.", "node_type": "subnode"}, {"id": "Zero-shot learning and in-context learning", "label": "Zero-shot\nlearning and\nin-context\nlearning", "title": "<b>Zero-shot learning and in-context learning</b> (subnode)<hr>Capabilities of models to perform tasks without prior training data.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Capabilities of models to perform tasks without prior training data.", "node_type": "subnode"}, {"id": "Reinforcement Learning and Control", "label": "Reinforcement\nLearning and\nControl", "title": "<b>Reinforcement Learning and Control</b> (major)<hr>Techniques for agents to learn optimal actions through interaction with an environment.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Techniques for agents to learn optimal actions through interaction with an environment.", "node_type": "major"}, {"id": "Reinforcement learning", "label": "Reinforcement\nlearning", "title": "<b>Reinforcement learning</b> (subnode)<hr>Introduction to the principles of reinforcement learning.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Introduction to the principles of reinforcement learning.", "node_type": "subnode"}, {"id": "Markov decision processes", "label": "Markov decision\nprocesses", "title": "<b>Markov decision processes</b> (subnode)<hr>Mathematical framework for modeling sequential decision-making problems.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Mathematical framework for modeling sequential decision-making problems.", "node_type": "subnode"}, {"id": "Value iteration and policy iteration", "label": "Value iteration\nand policy\niteration", "title": "<b>Value iteration and policy iteration</b> (subnode)<hr>Algorithms for finding optimal policies in MDPs.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Algorithms for finding optimal policies in MDPs.", "node_type": "subnode"}, {"id": "Learning a model for an MDP", "label": "Learning a\nmodel for an\nMDP", "title": "<b>Learning a model for an MDP</b> (subnode)<hr>Techniques to estimate the transition dynamics of an environment.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Techniques to estimate the transition dynamics of an environment.", "node_type": "subnode"}, {"id": "Continuous state MDPs", "label": "Continuous\nstate MDPs", "title": "<b>Continuous state MDPs</b> (subnode)<hr>Discussion on handling continuous states in reinforcement learning problems.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Discussion on handling continuous states in reinforcement learning problems.", "node_type": "subnode"}, {"id": "Discretization", "label": "Discretization", "title": "<b>Discretization</b> (subnode)<hr>Method to convert continuous state spaces into discrete ones for computation.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Method to convert continuous state spaces into discrete ones for computation.", "node_type": "subnode"}, {"id": "Value function approximation", "label": "Value function\napproximation", "title": "<b>Value function approximation</b> (subnode)<hr>Techniques for estimating value functions in large or continuous state spaces.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Techniques for estimating value functions in large or continuous state spaces.", "node_type": "subnode"}, {"id": "Connections between Policy and Value Iteration (Optional)", "label": "Connections\nbetween Policy\nand Value\nIteration\n(Optional)", "title": "<b>Connections between Policy and Value Iteration (Optional)</b> (subnode)<hr>Theoretical insights into the relationship between policy and value iteration methods.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Theoretical insights into the relationship between policy and value iteration methods.", "node_type": "subnode"}, {"id": "LQR, DDP and LQG", "label": "LQR, DDP and\nLQG", "title": "<b>LQR, DDP and LQG</b> (major)<hr>Control theory concepts applied to reinforcement learning problems.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Control theory concepts applied to reinforcement learning problems.", "node_type": "major"}, {"id": "Finite-horizon MDPs", "label": "Finite-horizon\nMDPs", "title": "<b>Finite-horizon MDPs</b> (subnode)<hr>Analysis of Markov decision processes with a fixed time horizon.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Analysis of Markov decision processes with a fixed time horizon.", "node_type": "subnode"}, {"id": "Linear Quadratic Regulation (LQR)", "label": "Linear\nQuadratic\nRegulation\n(LQR)", "title": "<b>Linear Quadratic Regulation (LQR)</b> (subnode)<hr>Optimal control problem for linear systems with quadratic cost functions", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Optimal control problem for linear systems with quadratic cost functions", "node_type": "subnode"}, {"id": "From non-linear dynamics to LQR", "label": "From non-linear\ndynamics to LQR", "title": "<b>From non-linear dynamics to LQR</b> (subnode)<hr>Approaches to apply LQR to nonlinear systems", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Approaches to apply LQR to nonlinear systems", "node_type": "subnode"}, {"id": "Linearization of dynamics", "label": "Linearization\nof dynamics", "title": "<b>Linearization of dynamics</b> (subnode)<hr>Approximating nonlinear dynamics with linear models", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Approximating nonlinear dynamics with linear models", "node_type": "subnode"}, {"id": "Differential Dynamic Programming (DDP)", "label": "Differential\nDynamic\nProgramming\n(DDP)", "title": "<b>Differential Dynamic Programming (DDP)</b> (subnode)<hr>Optimization technique for nonlinear systems using differential dynamic programming", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Optimization technique for nonlinear systems using differential dynamic programming", "node_type": "subnode"}, {"id": "Linear Quadratic Gaussian (LQG)", "label": "Linear\nQuadratic\nGaussian (LQG)", "title": "<b>Linear Quadratic Gaussian (LQG)</b> (subnode)<hr>Combination of LQR with stochastic dynamics and noisy measurements", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Combination of LQR with stochastic dynamics and noisy measurements", "node_type": "subnode"}, {"id": "Policy Gradient (REINFORCE)", "label": "Policy Gradient\n(REINFORCE)", "title": "<b>Policy Gradient (REINFORCE)</b> (major)<hr>Method for learning policies in reinforcement learning using gradients", "shape": "star", "size": 25, "color": "#FF6347", "description": "Method for learning policies in reinforcement learning using gradients", "node_type": "major"}, {"id": "Supervised Learning Overview", "label": "Supervised\nLearning\nOverview", "title": "<b>Supervised Learning Overview</b> (major)<hr>Introduction to supervised learning problems and concepts", "shape": "star", "size": 25, "color": "#FF6347", "description": "Introduction to supervised learning problems and concepts", "node_type": "major"}, {"id": "Supervised Learning Problem", "label": "Supervised\nLearning\nProblem", "title": "<b>Supervised Learning Problem</b> (major)<hr>Formal description of the goal in supervised learning to predict y from x using a hypothesis h.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Formal description of the goal in supervised learning to predict y from x using a hypothesis h.", "node_type": "major"}, {"id": "Regression", "label": "Regression", "title": "<b>Regression</b> (subnode)<hr>Type of supervised learning where target variable is continuous and needs to be predicted accurately.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Type of supervised learning where target variable is continuous and needs to be predicted accurately.", "node_type": "subnode"}, {"id": "Classification", "label": "Classification", "title": "<b>Classification</b> (subnode)<hr>Type of supervised learning where target variable takes on discrete values, predicting one from several categories.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Type of supervised learning where target variable takes on discrete values, predicting one from several categories.", "node_type": "subnode"}, {"id": "Linear Regression", "label": "Linear\nRegression", "title": "<b>Linear Regression</b> (major)<hr>Technique in machine learning for modeling the relationship between a scalar response and one or more explanatory variables using a linear predictor function.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Technique in machine learning for modeling the relationship between a scalar response and one or more explanatory variables using a linear predictor function.", "node_type": "major"}, {"id": "Housing Example Dataset", "label": "Housing Example\nDataset", "title": "<b>Housing Example Dataset</b> (subnode)<hr>Example dataset including living area, number of bedrooms, and price to illustrate regression problems.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Example dataset including living area, number of bedrooms, and price to illustrate regression problems.", "node_type": "subnode"}, {"id": "Features Selection", "label": "Features\nSelection", "title": "<b>Features Selection</b> (subnode)<hr>Process of selecting which features (variables) are relevant for predicting the target variable in a model.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Process of selecting which features (variables) are relevant for predicting the target variable in a model.", "node_type": "subnode"}, {"id": "Machine_Learning_Basics", "label": "Machine_Learnin\ng_Basics", "title": "<b>Machine_Learning_Basics</b> (major)<hr>Introduction to fundamental concepts in machine learning.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Introduction to fundamental concepts in machine learning.", "node_type": "major"}, {"id": "Function_Representation", "label": "Function_Repres\nentation", "title": "<b>Function_Representation</b> (subnode)<hr>How functions and hypotheses are represented in machine learning models.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "How functions and hypotheses are represented in machine learning models.", "node_type": "subnode"}, {"id": "Linear_Functions", "label": "Linear_Function\ns", "title": "<b>Linear_Functions</b> (subnode)<hr>Approximating y as a linear function of x with parameters theta.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Approximating y as a linear function of x with parameters theta.", "node_type": "subnode"}, {"id": "Parameters_Weights", "label": "Parameters_Weig\nhts", "title": "<b>Parameters_Weights</b> (subnode)<hr>Theta values parameterizing the space of linear functions.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Theta values parameterizing the space of linear functions.", "node_type": "subnode"}, {"id": "Cost_Function", "label": "Cost_Function", "title": "<b>Cost_Function</b> (subnode)<hr>Measures how close h(x) is to y for training examples.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Measures how close h(x) is to y for training examples.", "node_type": "subnode"}, {"id": "Ordinary_Least_Squares", "label": "Ordinary_Least_\nSquares", "title": "<b>Ordinary_Least_Squares</b> (subnode)<hr>Least-squares cost function giving rise to OLS regression model.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Least-squares cost function giving rise to OLS regression model.", "node_type": "subnode"}, {"id": "LMS_Algorithm", "label": "LMS_Algorithm", "title": "<b>LMS_Algorithm</b> (major)<hr>Learning algorithm for minimizing the cost function J(theta).", "shape": "star", "size": 25, "color": "#FF6347", "description": "Learning algorithm for minimizing the cost function J(theta).", "node_type": "major"}, {"id": "GradientDescentAlgorithm", "label": "GradientDescent\nAlgorithm", "title": "<b>GradientDescentAlgorithm</b> (major)<hr>Optimization algorithm that iteratively adjusts parameters to minimize a cost function.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Optimization algorithm that iteratively adjusts parameters to minimize a cost function.", "node_type": "major"}, {"id": "LearningRate", "label": "LearningRate", "title": "<b>LearningRate</b> (subnode)<hr>Hyperparameter controlling the step size in gradient descent.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Hyperparameter controlling the step size in gradient descent.", "node_type": "subnode"}, {"id": "CostFunctionJ", "label": "CostFunctionJ", "title": "<b>CostFunctionJ</b> (subnode)<hr>Function that measures the error between predicted and actual values, to be minimized.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Function that measures the error between predicted and actual values, to be minimized.", "node_type": "subnode"}, {"id": "PartialDerivativeCalculation", "label": "PartialDerivati\nveCalculation", "title": "<b>PartialDerivativeCalculation</b> (subnode)<hr>Process of calculating partial derivatives for updating parameters in gradient descent.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Process of calculating partial derivatives for updating parameters in gradient descent.", "node_type": "subnode"}, {"id": "SingleTrainingExampleCase", "label": "SingleTrainingE\nxampleCase", "title": "<b>SingleTrainingExampleCase</b> (subnode)<hr>Specific case where the cost function is calculated for a single training example.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Specific case where the cost function is calculated for a single training example.", "node_type": "subnode"}, {"id": "LMSUpdateRule", "label": "LMSUpdateRule", "title": "<b>LMSUpdateRule</b> (subnode)<hr>Update rule derived from least mean squares method, also known as Widrow-Hoff learning rule.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Update rule derived from least mean squares method, also known as Widrow-Hoff learning rule.", "node_type": "subnode"}, {"id": "LMS_Update_Rule", "label": "LMS_Update_Rule", "title": "<b>LMS_Update_Rule</b> (major)<hr>Update rule for adjusting parameters in machine learning models.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Update rule for adjusting parameters in machine learning models.", "node_type": "major"}, {"id": "Widrow_Hoff_Learning_Rule", "label": "Widrow_Hoff_Lea\nrning_Rule", "title": "<b>Widrow_Hoff_Learning_Rule</b> (subnode)<hr>Alternative name for LMS update rule.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Alternative name for LMS update rule.", "node_type": "subnode"}, {"id": "Error_Term", "label": "Error_Term", "title": "<b>Error_Term</b> (subnode)<hr>Difference between actual and predicted values used to adjust parameters.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Difference between actual and predicted values used to adjust parameters.", "node_type": "subnode"}, {"id": "Single_Training_Example", "label": "Single_Training\n_Example", "title": "<b>Single_Training_Example</b> (subnode)<hr>Derivation of LMS rule for a single training example.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Derivation of LMS rule for a single training example.", "node_type": "subnode"}, {"id": "Batch_Gradient_Descent", "label": "Batch_Gradient_\nDescent", "title": "<b>Batch_Gradient_Descent</b> (major)<hr>Method that considers all examples in the dataset at once to update parameters.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Method that considers all examples in the dataset at once to update parameters.", "node_type": "major"}, {"id": "Gradient_Descent", "label": "Gradient_Descen\nt", "title": "<b>Gradient_Descent</b> (subnode)<hr>Optimization algorithm that minimizes cost function by moving towards the steepest descent direction.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Optimization algorithm that minimizes cost function by moving towards the steepest descent direction.", "node_type": "subnode"}, {"id": "LinearRegressionOptimization", "label": "LinearRegressio\nnOptimization", "title": "<b>LinearRegressionOptimization</b> (major)<hr>Discusses the optimization problem in linear regression.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Discusses the optimization problem in linear regression.", "node_type": "major"}, {"id": "GradientDescentConvergence", "label": "GradientDescent\nConvergence", "title": "<b>GradientDescentConvergence</b> (subnode)<hr>Explains how gradient descent converges to a global minimum for convex functions.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Explains how gradient descent converges to a global minimum for convex functions.", "node_type": "subnode"}, {"id": "BatchGradientDescentExample", "label": "BatchGradientDe\nscentExample", "title": "<b>BatchGradientDescentExample</b> (subnode)<hr>Provides an example of batch gradient descent with housing price prediction.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Provides an example of batch gradient descent with housing price prediction.", "node_type": "subnode"}, {"id": "StochasticGradientDescent", "label": "StochasticGradi\nentDescent", "title": "<b>StochasticGradientDescent</b> (major)<hr>Explanation and implementation of stochastic gradient descent for minimizing loss functions.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Explanation and implementation of stochastic gradient descent for minimizing loss functions.", "node_type": "major"}, {"id": "SGDAlgorithm", "label": "SGDAlgorithm", "title": "<b>SGDAlgorithm</b> (subnode)<hr>Presents the algorithm for updating parameters in stochastic gradient descent.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Presents the algorithm for updating parameters in stochastic gradient descent.", "node_type": "subnode"}, {"id": "Machine_Learning_Optimization", "label": "Machine_Learnin\ng_Optimization", "title": "<b>Machine_Learning_Optimization</b> (major)<hr>Optimization techniques in machine learning including coordinate ascent and SVMs.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Optimization techniques in machine learning including coordinate ascent and SVMs.", "node_type": "major"}, {"id": "Stochastic_Gradient_Descent", "label": "Stochastic_Grad\nient_Descent", "title": "<b>Stochastic_Gradient_Descent</b> (subnode)<hr>Algorithm for updating parameters based on single training example gradients", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Algorithm for updating parameters based on single training example gradients", "node_type": "subnode"}, {"id": "Learning_Rate_Decay", "label": "Learning_Rate_D\necay", "title": "<b>Learning_Rate_Decay</b> (subnode)<hr>Technique to ensure convergence by gradually reducing learning rate", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Technique to ensure convergence by gradually reducing learning rate", "node_type": "subnode"}, {"id": "Normal_Equations_Method", "label": "Normal_Equation\ns_Method", "title": "<b>Normal_Equations_Method</b> (major)<hr>Alternative method for minimizing cost function without iterative algorithms", "shape": "star", "size": 25, "color": "#FF6347", "description": "Alternative method for minimizing cost function without iterative algorithms", "node_type": "major"}, {"id": "Matrix_Derivatives", "label": "Matrix_Derivati\nves", "title": "<b>Matrix_Derivatives</b> (subnode)<hr>Notation and rules for differentiating functions of matrices", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Notation and rules for differentiating functions of matrices", "node_type": "subnode"}, {"id": "Matrix Derivatives", "label": "Matrix\nDerivatives", "title": "<b>Matrix Derivatives</b> (major)<hr>Derivation of matrix derivatives for functions mapping matrices to real numbers.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Derivation of matrix derivatives for functions mapping matrices to real numbers.", "node_type": "major"}, {"id": "Gradient Calculation", "label": "Gradient\nCalculation", "title": "<b>Gradient Calculation</b> (subnode)<hr>Calculation of gradients for a specific function involving matrix elements.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Calculation of gradients for a specific function involving matrix elements.", "node_type": "subnode"}, {"id": "Least Squares Revisited", "label": "Least Squares\nRevisited", "title": "<b>Least Squares Revisited</b> (major)<hr>Revisiting least squares problems using matrix derivatives and design matrices.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Revisiting least squares problems using matrix derivatives and design matrices.", "node_type": "major"}, {"id": "Design Matrix", "label": "Design Matrix", "title": "<b>Design Matrix</b> (subnode)<hr>Matrix containing training examples' input values in its rows.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Matrix containing training examples' input values in its rows.", "node_type": "subnode"}, {"id": "Target Vector", "label": "Target Vector", "title": "<b>Target Vector</b> (subnode)<hr>Vector containing target values from the training set.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Vector containing target values from the training set.", "node_type": "subnode"}, {"id": "MachineLearningOverview", "label": "MachineLearning\nOverview", "title": "<b>MachineLearningOverview</b> (major)<hr>General overview of machine learning concepts and architectures.", "shape": "star", "size": 25, "color": "#FF6347", "description": "General overview of machine learning concepts and architectures.", "node_type": "major"}, {"id": "LinearRegression", "label": "LinearRegressio\nn", "title": "<b>LinearRegression</b> (subnode)<hr>Algorithm for modeling the relationship between a scalar dependent variable y and one or more explanatory variables (or independent variables) denoted X.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Algorithm for modeling the relationship between a scalar dependent variable y and one or more explanatory variables (or independent variables) denoted X.", "node_type": "subnode"}, {"id": "HypothesisFunction", "label": "HypothesisFunct\nion", "title": "<b>HypothesisFunction</b> (subnode)<hr>Formulation of the hypothesis function for classification using a sigmoid function.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Formulation of the hypothesis function for classification using a sigmoid function.", "node_type": "subnode"}, {"id": "CostFunction", "label": "CostFunction", "title": "<b>CostFunction</b> (subnode)<hr>A measure of how well a hypothesis fits the data, aiming for minimization.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "A measure of how well a hypothesis fits the data, aiming for minimization.", "node_type": "subnode"}, {"id": "GradientDescent", "label": "GradientDescent", "title": "<b>GradientDescent</b> (subnode)<hr>An optimization algorithm to minimize cost functions by iteratively moving towards a minimum.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "An optimization algorithm to minimize cost functions by iteratively moving towards a minimum.", "node_type": "subnode"}, {"id": "NormalEquations", "label": "NormalEquations", "title": "<b>NormalEquations</b> (subnode)<hr>A direct method for finding the parameters that minimize the cost function without iteration.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "A direct method for finding the parameters that minimize the cost function without iteration.", "node_type": "subnode"}, {"id": "Probabilistic Interpretation of Linear Regression", "label": "Probabilistic\nInterpretation\nof Linear\nRegression", "title": "<b>Probabilistic Interpretation of Linear Regression</b> (major)<hr>Explains the probabilistic assumptions behind least-squares regression.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Explains the probabilistic assumptions behind least-squares regression.", "node_type": "major"}, {"id": "Regression Problem", "label": "Regression\nProblem", "title": "<b>Regression Problem</b> (subnode)<hr>Context for why linear regression is used in certain scenarios.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Context for why linear regression is used in certain scenarios.", "node_type": "subnode"}, {"id": "Least-Squares Cost Function J", "label": "Least-Squares\nCost Function J", "title": "<b>Least-Squares Cost Function J</b> (subnode)<hr>Derivation and justification of the least-squares cost function.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Derivation and justification of the least-squares cost function.", "node_type": "subnode"}, {"id": "Target Variables and Inputs Relation", "label": "Target\nVariables and\nInputs Relation", "title": "<b>Target Variables and Inputs Relation</b> (subnode)<hr>Equation describing relationship between target variables and inputs with error term.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Equation describing relationship between target variables and inputs with error term.", "node_type": "subnode"}, {"id": "Error Term Distribution", "label": "Error Term\nDistribution", "title": "<b>Error Term Distribution</b> (subnode)<hr>Assumption that the error terms are normally distributed with mean zero and variance sigma squared.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Assumption that the error terms are normally distributed with mean zero and variance sigma squared.", "node_type": "subnode"}, {"id": "Conditional Probability of y given x", "label": "Conditional\nProbability of\ny given x", "title": "<b>Conditional Probability of y given x</b> (subnode)<hr>Expression for the conditional probability density function of y given x and theta.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Expression for the conditional probability density function of y given x and theta.", "node_type": "subnode"}, {"id": "ProbabilisticModeling", "label": "ProbabilisticMo\ndeling", "title": "<b>ProbabilisticModeling</b> (subnode)<hr>Models that use probability distributions to describe data generation processes.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Models that use probability distributions to describe data generation processes.", "node_type": "subnode"}, {"id": "ConditionalProbabilityDistribution", "label": "ConditionalProb\nabilityDistribu\ntion", "title": "<b>ConditionalProbabilityDistribution</b> (subnode)<hr>Describes the distribution of one variable given another in a probabilistic model.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Describes the distribution of one variable given another in a probabilistic model.", "node_type": "subnode"}, {"id": "LikelihoodFunction", "label": "LikelihoodFunct\nion", "title": "<b>LikelihoodFunction</b> (subnode)<hr>Mathematical function that measures how likely the data is given specific parameters.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Mathematical function that measures how likely the data is given specific parameters.", "node_type": "subnode"}, {"id": "MaximumLikelihoodEstimation", "label": "MaximumLikeliho\nodEstimation", "title": "<b>MaximumLikelihoodEstimation</b> (subnode)<hr>Method to estimate parameters by maximizing the likelihood of observing given data.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Method to estimate parameters by maximizing the likelihood of observing given data.", "node_type": "subnode"}, {"id": "LogLikelihoodFunction", "label": "LogLikelihoodFu\nnction", "title": "<b>LogLikelihoodFunction</b> (subnode)<hr>The natural logarithm of the likelihood function, often used for computational convenience.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "The natural logarithm of the likelihood function, often used for computational convenience.", "node_type": "subnode"}, {"id": "MachineLearningConcepts", "label": "MachineLearning\nConcepts", "title": "<b>MachineLearningConcepts</b> (major)<hr>Overview of key concepts in machine learning including optimization methods and models.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Overview of key concepts in machine learning including optimization methods and models.", "node_type": "major"}, {"id": "LeastSquaresRegression", "label": "LeastSquaresReg\nression", "title": "<b>LeastSquaresRegression</b> (subnode)<hr>Method for fitting a linear model to data by minimizing squared error.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Method for fitting a linear model to data by minimizing squared error.", "node_type": "subnode"}, {"id": "LocallyWeightedLinearRegression", "label": "LocallyWeighted\nLinearRegressio\nn", "title": "<b>LocallyWeightedLinearRegression</b> (major)<hr>Variant of linear regression that provides higher weight to training examples close to the query point x.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Variant of linear regression that provides higher weight to training examples close to the query point x.", "node_type": "major"}, {"id": "Underfitting", "label": "Underfitting", "title": "<b>Underfitting</b> (subnode)<hr>Explains the scenario where a model is too simple to capture underlying patterns in data.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Explains the scenario where a model is too simple to capture underlying patterns in data.", "node_type": "subnode"}, {"id": "Overfitting", "label": "Overfitting", "title": "<b>Overfitting</b> (subnode)<hr>Model captures noise and details in training data too well, harming generalization.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Model captures noise and details in training data too well, harming generalization.", "node_type": "subnode"}, {"id": "FeatureSelection", "label": "FeatureSelectio\nn", "title": "<b>FeatureSelection</b> (subnode)<hr>Choosing relevant features to improve model performance.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Choosing relevant features to improve model performance.", "node_type": "subnode"}, {"id": "FittingTheta", "label": "FittingTheta", "title": "<b>FittingTheta</b> (subnode)<hr>Process of determining model parameters (theta) by minimizing error functions.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Process of determining model parameters (theta) by minimizing error functions.", "node_type": "subnode"}, {"id": "WeightsInLWLR", "label": "WeightsInLWLR", "title": "<b>WeightsInLWLR</b> (subnode)<hr>Non-negative weights used in locally weighted linear regression to emphasize certain training examples over others based on proximity to the query point x.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Non-negative weights used in locally weighted linear regression to emphasize certain training examples over others based on proximity to the query point x.", "node_type": "subnode"}, {"id": "BandwidthParameter", "label": "BandwidthParame\nter", "title": "<b>BandwidthParameter</b> (subnode)<hr>Controls how quickly the weight of a training example falls off with distance from the query point; denoted by tau.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Controls how quickly the weight of a training example falls off with distance from the query point; denoted by tau.", "node_type": "subnode"}, {"id": "Machine Learning Overview", "label": "Machine\nLearning\nOverview", "title": "<b>Machine Learning Overview</b> (major)<hr>Introduction to machine learning concepts and algorithms.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Introduction to machine learning concepts and algorithms.", "node_type": "major"}, {"id": "Locally Weighted Linear Regression", "label": "Locally\nWeighted Linear\nRegression", "title": "<b>Locally Weighted Linear Regression</b> (subnode)<hr>A non-parametric algorithm that uses weighted linear regression for predictions.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "A non-parametric algorithm that uses weighted linear regression for predictions.", "node_type": "subnode"}, {"id": "Non-Parametric Algorithms", "label": "Non-Parametric\nAlgorithms", "title": "<b>Non-Parametric Algorithms</b> (subnode)<hr>Algorithms where the amount of data needed to represent hypotheses grows with training set size.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Algorithms where the amount of data needed to represent hypotheses grows with training set size.", "node_type": "subnode"}, {"id": "Parametric Algorithms", "label": "Parametric\nAlgorithms", "title": "<b>Parametric Algorithms</b> (subnode)<hr>Algorithms that have a fixed number of parameters independent of training set size.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Algorithms that have a fixed number of parameters independent of training set size.", "node_type": "subnode"}, {"id": "Classification Problem", "label": "Classification\nProblem", "title": "<b>Classification Problem</b> (major)<hr>A machine learning task where the output variable is categorical and discrete-valued.", "shape": "star", "size": 25, "color": "#FF6347", "description": "A machine learning task where the output variable is categorical and discrete-valued.", "node_type": "major"}, {"id": "Binary Classification", "label": "Binary\nClassification", "title": "<b>Binary Classification</b> (subnode)<hr>A classification problem with two possible outcomes (0 or 1).", "shape": "dot", "size": 15, "color": "#4682B4", "description": "A classification problem with two possible outcomes (0 or 1).", "node_type": "subnode"}, {"id": "Logistic Regression", "label": "Logistic\nRegression", "title": "<b>Logistic Regression</b> (major)<hr>Application of generalized Newton's method in logistic regression for multidimensional settings.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Application of generalized Newton's method in logistic regression for multidimensional settings.", "node_type": "major"}, {"id": "Spam Classification Example", "label": "Spam\nClassification\nExample", "title": "<b>Spam Classification Example</b> (subnode)<hr>Example of using logistic regression to classify emails as spam or not spam.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Example of using logistic regression to classify emails as spam or not spam.", "node_type": "subnode"}, {"id": "MachineLearning", "label": "MachineLearning", "title": "<b>MachineLearning</b> (major)<hr>Field of study focusing on algorithms that learn from and make predictions on data.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Field of study focusing on algorithms that learn from and make predictions on data.", "node_type": "major"}, {"id": "ClassificationProblem", "label": "ClassificationP\nroblem", "title": "<b>ClassificationProblem</b> (subnode)<hr>Explanation of classification problems including binary and multi-class scenarios.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Explanation of classification problems including binary and multi-class scenarios.", "node_type": "subnode"}, {"id": "LogisticRegression", "label": "LogisticRegress\nion", "title": "<b>LogisticRegression</b> (subnode)<hr>A discriminative algorithm used to model the probability of a binary outcome given input variables.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "A discriminative algorithm used to model the probability of a binary outcome given input variables.", "node_type": "subnode"}, {"id": "LogisticFunction", "label": "LogisticFunctio\nn", "title": "<b>LogisticFunction</b> (subnode)<hr>Use of logistic function to convert logits into probabilities for binary classification.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Use of logistic function to convert logits into probabilities for binary classification.", "node_type": "subnode"}, {"id": "DerivativeOfSigmoid", "label": "DerivativeOfSig\nmoid", "title": "<b>DerivativeOfSigmoid</b> (subnode)<hr>Mathematical expression describing the rate of change of the logistic function.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Mathematical expression describing the rate of change of the logistic function.", "node_type": "subnode"}, {"id": "MachineLearningModels", "label": "MachineLearning\nModels", "title": "<b>MachineLearningModels</b> (major)<hr>Overview of models used in machine learning including regression and classification.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Overview of models used in machine learning including regression and classification.", "node_type": "major"}, {"id": "ClassificationModelAssumptions", "label": "ClassificationM\nodelAssumptions", "title": "<b>ClassificationModelAssumptions</b> (subnode)<hr>Probabilistic assumptions made for a binary classification model.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Probabilistic assumptions made for a binary classification model.", "node_type": "subnode"}, {"id": "LogLikelihood", "label": "LogLikelihood", "title": "<b>LogLikelihood</b> (subnode)<hr>Derivation and use of log-likelihood for easier optimization.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Derivation and use of log-likelihood for easier optimization.", "node_type": "subnode"}, {"id": "GradientAscent", "label": "GradientAscent", "title": "<b>GradientAscent</b> (subnode)<hr>Method to maximize the likelihood using gradient ascent updates.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Method to maximize the likelihood using gradient ascent updates.", "node_type": "subnode"}, {"id": "GradientAscentRule", "label": "GradientAscentR\nule", "title": "<b>GradientAscentRule</b> (subnode)<hr>Derivation of the gradient ascent rule for updating parameters in logistic regression.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Derivation of the gradient ascent rule for updating parameters in logistic regression.", "node_type": "subnode"}, {"id": "StochasticGradientAscent", "label": "StochasticGradi\nentAscent", "title": "<b>StochasticGradientAscent</b> (subnode)<hr>Update rule using stochastic gradient ascent method.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Update rule using stochastic gradient ascent method.", "node_type": "subnode"}, {"id": "LogisticLossFunction", "label": "LogisticLossFun\nction", "title": "<b>LogisticLossFunction</b> (subnode)<hr>Definition and properties of the logistic loss function used in logistic regression.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Definition and properties of the logistic loss function used in logistic regression.", "node_type": "subnode"}, {"id": "NegativeLogLikelihood", "label": "NegativeLogLike\nlihood", "title": "<b>NegativeLogLikelihood</b> (subnode)<hr>Expression for negative log-likelihood in terms of logistic loss function.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Expression for negative log-likelihood in terms of logistic loss function.", "node_type": "subnode"}, {"id": "Machine_Learning_Concepts", "label": "Machine_Learnin\ng_Concepts", "title": "<b>Machine_Learning_Concepts</b> (major)<hr>Overview of key concepts in machine learning including loss functions and optimization.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Overview of key concepts in machine learning including loss functions and optimization.", "node_type": "major"}, {"id": "Logistic_Regression_Gradient_Descent", "label": "Logistic_Regres\nsion_Gradient_D\nescent", "title": "<b>Logistic_Regression_Gradient_Descent</b> (subnode)<hr>Derivation and application of gradient descent for logistic regression.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Derivation and application of gradient descent for logistic regression.", "node_type": "subnode"}, {"id": "Perceptron_Algorithm", "label": "Perceptron_Algo\nrithm", "title": "<b>Perceptron_Algorithm</b> (subnode)<hr>Historical learning algorithm that outputs binary values.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Historical learning algorithm that outputs binary values.", "node_type": "subnode"}, {"id": "Multi_Class_Classification", "label": "Multi_Class_Cla\nssification", "title": "<b>Multi_Class_Classification</b> (subnode)<hr>Classification problems where the response variable can take on multiple discrete values.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Classification problems where the response variable can take on multiple discrete values.", "node_type": "subnode"}, {"id": "Multi-classClassification", "label": "Multi-classClas\nsification", "title": "<b>Multi-classClassification</b> (subnode)<hr>Extension of binary classification to more than two classes.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Extension of binary classification to more than two classes.", "node_type": "subnode"}, {"id": "ResponseVariable", "label": "ResponseVariabl\ne", "title": "<b>ResponseVariable</b> (subnode)<hr>Discrete variable that can take on any one of k values.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Discrete variable that can take on any one of k values.", "node_type": "subnode"}, {"id": "MultinomialDistribution", "label": "MultinomialDist\nribution", "title": "<b>MultinomialDistribution</b> (subnode)<hr>Probability distribution over k possible outcomes.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Probability distribution over k possible outcomes.", "node_type": "subnode"}, {"id": "ParameterizedModel", "label": "ParameterizedMo\ndel", "title": "<b>ParameterizedModel</b> (subnode)<hr>Models that output probabilities for each class given input x.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Models that output probabilities for each class given input x.", "node_type": "subnode"}, {"id": "SoftmaxFunction", "label": "SoftmaxFunction", "title": "<b>SoftmaxFunction</b> (subnode)<hr>Transformation of logits into a probability distribution over multiple classes.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Transformation of logits into a probability distribution over multiple classes.", "node_type": "subnode"}, {"id": "Softmax_Function", "label": "Softmax_Functio\nn", "title": "<b>Softmax_Function</b> (subnode)<hr>Converts logits into probabilities for each possible value.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Converts logits into probabilities for each possible value.", "node_type": "subnode"}, {"id": "Logits", "label": "Logits", "title": "<b>Logits</b> (subnode)<hr>Inputs to the softmax function, often represented as a vector of real numbers.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Inputs to the softmax function, often represented as a vector of real numbers.", "node_type": "subnode"}, {"id": "Probability_Vector_Output", "label": "Probability_Vec\ntor_Output", "title": "<b>Probability_Vector_Output</b> (subnode)<hr>Output probabilities that sum up to 1.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Output probabilities that sum up to 1.", "node_type": "subnode"}, {"id": "Probabilistic_Model", "label": "Probabilistic_M\nodel", "title": "<b>Probabilistic_Model</b> (subnode)<hr>Model using softmax output as conditional probabilities.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Model using softmax output as conditional probabilities.", "node_type": "subnode"}, {"id": "Negative_Log_Likelihood", "label": "Negative_Log_Li\nkelihood", "title": "<b>Negative_Log_Likelihood</b> (subnode)<hr>Loss function for evaluating model performance based on log-likelihood.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Loss function for evaluating model performance based on log-likelihood.", "node_type": "subnode"}, {"id": "Cross_Entropy_Loss", "label": "Cross_Entropy_L\noss", "title": "<b>Cross_Entropy_Loss</b> (subnode)<hr>A loss function that measures the dissimilarity between two probability distributions.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "A loss function that measures the dissimilarity between two probability distributions.", "node_type": "subnode"}, {"id": "Loss_Functions", "label": "Loss_Functions", "title": "<b>Loss_Functions</b> (subnode)<hr>Functions used to evaluate the performance of a model during training.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Functions used to evaluate the performance of a model during training.", "node_type": "subnode"}, {"id": "Softmax_Cross_Entropy_Loss", "label": "Softmax_Cross_E\nntropy_Loss", "title": "<b>Softmax_Cross_Entropy_Loss</b> (subnode)<hr>Specific form of cross-entropy loss used in multi-class classification problems.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Specific form of cross-entropy loss used in multi-class classification problems.", "node_type": "subnode"}, {"id": "Gradient_Computation", "label": "Gradient_Comput\nation", "title": "<b>Gradient_Computation</b> (subnode)<hr>Computation of gradients with respect to intermediate variables and parameters.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Computation of gradients with respect to intermediate variables and parameters.", "node_type": "subnode"}, {"id": "LossFunction", "label": "LossFunction", "title": "<b>LossFunction</b> (subnode)<hr>Mathematical function used to measure the performance of a model and guide its training.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Mathematical function used to measure the performance of a model and guide its training.", "node_type": "subnode"}, {"id": "CrossEntropyLoss", "label": "CrossEntropyLos\ns", "title": "<b>CrossEntropyLoss</b> (subnode)<hr>Explanation of cross-entropy loss function used for classification tasks.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Explanation of cross-entropy loss function used for classification tasks.", "node_type": "subnode"}, {"id": "GradientCalculation", "label": "GradientCalcula\ntion", "title": "<b>GradientCalculation</b> (subnode)<hr>Process of calculating gradients with respect to parameters using the chain rule.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Process of calculating gradients with respect to parameters using the chain rule.", "node_type": "subnode"}, {"id": "NewtonMethod", "label": "NewtonMethod", "title": "<b>NewtonMethod</b> (subnode)<hr>Explanation of Newton's method for finding zeros of a function and its use in optimization problems.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Explanation of Newton's method for finding zeros of a function and its use in optimization problems.", "node_type": "subnode"}, {"id": "Newton's Method", "label": "Newton's Method", "title": "<b>Newton's Method</b> (major)<hr>An iterative optimization algorithm for finding roots of a real-valued function.", "shape": "star", "size": 25, "color": "#FF6347", "description": "An iterative optimization algorithm for finding roots of a real-valued function.", "node_type": "major"}, {"id": "Finding Roots", "label": "Finding Roots", "title": "<b>Finding Roots</b> (subnode)<hr>The process of using Newton's method to find the value of \u03b8 where f(\u03b8) = 0.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "The process of using Newton's method to find the value of \u03b8 where f(\u03b8) = 0.", "node_type": "subnode"}, {"id": "Maximizing Functions", "label": "Maximizing\nFunctions", "title": "<b>Maximizing Functions</b> (subnode)<hr>Using Newton's method to maximize a function by finding points where its first derivative is zero.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Using Newton's method to maximize a function by finding points where its first derivative is zero.", "node_type": "subnode"}, {"id": "Hessian Matrix", "label": "Hessian Matrix", "title": "<b>Hessian Matrix</b> (subnode)<hr>A matrix used in the Newton-Raphson method, containing second derivatives of \u039b(\u03b8) with respect to \u03b8_i and \u03b8_j.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "A matrix used in the Newton-Raphson method, containing second derivatives of \u039b(\u03b8) with respect to \u03b8_i and \u03b8_j.", "node_type": "subnode"}, {"id": "Gradient Descent", "label": "Gradient\nDescent", "title": "<b>Gradient Descent</b> (subnode)<hr>An optimization algorithm that uses the gradient of a function to find its minimum or maximum values.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "An optimization algorithm that uses the gradient of a function to find its minimum or maximum values.", "node_type": "subnode"}, {"id": "OptimizationMethods", "label": "OptimizationMet\nhods", "title": "<b>OptimizationMethods</b> (subnode)<hr>Techniques used to optimize the parameters of a model.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Techniques used to optimize the parameters of a model.", "node_type": "subnode"}, {"id": "FisherScoring", "label": "FisherScoring", "title": "<b>FisherScoring</b> (subnode)<hr>Application of Newton's method to logistic regression likelihood function maximization.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Application of Newton's method to logistic regression likelihood function maximization.", "node_type": "subnode"}, {"id": "GeneralizedLinearModels", "label": "GeneralizedLine\narModels", "title": "<b>GeneralizedLinearModels</b> (major)<hr>Introduction to GLMs for solving problems involving exponential family distributions.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Introduction to GLMs for solving problems involving exponential family distributions.", "node_type": "major"}, {"id": "ExponentialFamilyDistributions", "label": "ExponentialFami\nlyDistributions", "title": "<b>ExponentialFamilyDistributions</b> (subnode)<hr>Overview of distributions that belong to the exponential family, including Poisson and Gamma distributions.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Overview of distributions that belong to the exponential family, including Poisson and Gamma distributions.", "node_type": "subnode"}, {"id": "NaturalParameter", "label": "NaturalParamete\nr", "title": "<b>NaturalParameter</b> (subnode)<hr>The parameter \u03b7 that defines an exponential family distribution.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "The parameter \u03b7 that defines an exponential family distribution.", "node_type": "subnode"}, {"id": "SufficientStatistic", "label": "SufficientStati\nstic", "title": "<b>SufficientStatistic</b> (subnode)<hr>A statistic T(y) that captures all the information about a parameter in a given sample.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "A statistic T(y) that captures all the information about a parameter in a given sample.", "node_type": "subnode"}, {"id": "LogPartitionFunction", "label": "LogPartitionFun\nction", "title": "<b>LogPartitionFunction</b> (subnode)<hr>Ensures the distribution sums/integrates to 1 over y.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Ensures the distribution sums/integrates to 1 over y.", "node_type": "subnode"}, {"id": "BernoulliDistribution", "label": "BernoulliDistri\nbution", "title": "<b>BernoulliDistribution</b> (major)<hr>Formulation of Bernoulli distribution within the exponential family framework.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Formulation of Bernoulli distribution within the exponential family framework.", "node_type": "major"}, {"id": "NaturalParameterForBernoulli", "label": "NaturalParamete\nrForBernoulli", "title": "<b>NaturalParameterForBernoulli</b> (subnode)<hr>Defined as log(\u03c6/(1-\u03c6)).", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Defined as log(\u03c6/(1-\u03c6)).", "node_type": "subnode"}, {"id": "SufficientStatisticForBernoulli", "label": "SufficientStati\nsticForBernoull\ni", "title": "<b>SufficientStatisticForBernoulli</b> (subnode)<hr>T(y) = y, the outcome itself.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "T(y) = y, the outcome itself.", "node_type": "subnode"}, {"id": "LogPartitionFunctionForBernoulli", "label": "LogPartitionFun\nctionForBernoul\nli", "title": "<b>LogPartitionFunctionForBernoulli</b> (subnode)<hr>a(\u03b7) = log(1+e^\u03b7).", "shape": "dot", "size": 15, "color": "#4682B4", "description": "a(\u03b7) = log(1+e^\u03b7).", "node_type": "subnode"}, {"id": "SigmoidFunction", "label": "SigmoidFunction", "title": "<b>SigmoidFunction</b> (major)<hr>Use of sigmoid function as a default cdf for ICA due to its monotonic increase from 0 to 1.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Use of sigmoid function as a default cdf for ICA due to its monotonic increase from 0 to 1.", "node_type": "major"}, {"id": "LogisticRegressionAsGLM", "label": "LogisticRegress\nionAsGLM", "title": "<b>LogisticRegressionAsGLM</b> (subnode)<hr>Explanation of logistic regression as a Generalized Linear Model (GLM).", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Explanation of logistic regression as a Generalized Linear Model (GLM).", "node_type": "subnode"}, {"id": "GaussianDistribution", "label": "GaussianDistrib\nution", "title": "<b>GaussianDistribution</b> (subnode)<hr>Explanation of Gaussian distribution as a member of the exponential family used in linear regression.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Explanation of Gaussian distribution as a member of the exponential family used in linear regression.", "node_type": "subnode"}, {"id": "GLMConstruction", "label": "GLMConstruction", "title": "<b>GLMConstruction</b> (subnode)<hr>General method for constructing GLMs using various distributions.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "General method for constructing GLMs using various distributions.", "node_type": "subnode"}, {"id": "PoissonDistributionModeling", "label": "PoissonDistribu\ntionModeling", "title": "<b>PoissonDistributionModeling</b> (subnode)<hr>Using Poisson distribution for modeling website visitor counts based on features like promotions and weather.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Using Poisson distribution for modeling website visitor counts based on features like promotions and weather.", "node_type": "subnode"}, {"id": "GLMAssumptions", "label": "GLMAssumptions", "title": "<b>GLMAssumptions</b> (subnode)<hr>Three key assumptions/design choices for constructing GLM models.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Three key assumptions/design choices for constructing GLM models.", "node_type": "subnode"}, {"id": "OrdinaryLeastSquares", "label": "OrdinaryLeastSq\nuares", "title": "<b>OrdinaryLeastSquares</b> (subnode)<hr>Regression method for continuous target variables modeled as Gaussian distributions.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Regression method for continuous target variables modeled as Gaussian distributions.", "node_type": "subnode"}, {"id": "Assumption1", "label": "Assumption1", "title": "<b>Assumption1</b> (subnode)<hr>First assumption in GLM formulation, relating mean and natural parameter.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "First assumption in GLM formulation, relating mean and natural parameter.", "node_type": "subnode"}, {"id": "Assumption2", "label": "Assumption2", "title": "<b>Assumption2</b> (subnode)<hr>Second assumption about the expected value of y given x.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Second assumption about the expected value of y given x.", "node_type": "subnode"}, {"id": "Assumption3", "label": "Assumption3", "title": "<b>Assumption3</b> (subnode)<hr>Third assumption in GLM formulation, relating parameters to input features.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Third assumption in GLM formulation, relating parameters to input features.", "node_type": "subnode"}, {"id": "ExponentialFamilyDistribution", "label": "ExponentialFami\nlyDistribution", "title": "<b>ExponentialFamilyDistribution</b> (subnode)<hr>A family of probability distributions that includes Gaussian and Bernoulli distributions.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "A family of probability distributions that includes Gaussian and Bernoulli distributions.", "node_type": "subnode"}, {"id": "Conditional_Distribution_Modeling", "label": "Conditional_Dis\ntribution_Model\ning", "title": "<b>Conditional_Distribution_Modeling</b> (subnode)<hr>Techniques for modeling the conditional distribution p(y|x;\u03b8).", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Techniques for modeling the conditional distribution p(y|x;\u03b8).", "node_type": "subnode"}, {"id": "Bernoulli_Distribution", "label": "Bernoulli_Distr\nibution", "title": "<b>Bernoulli_Distribution</b> (subnode)<hr>Binary-valued random variable distribution used in logistic regression.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Binary-valued random variable distribution used in logistic regression.", "node_type": "subnode"}, {"id": "Exponential_Family_Distributions", "label": "Exponential_Fam\nily_Distributio\nns", "title": "<b>Exponential_Family_Distributions</b> (subnode)<hr>Family of distributions including Bernoulli and Gaussian, characterized by natural parameters.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Family of distributions including Bernoulli and Gaussian, characterized by natural parameters.", "node_type": "subnode"}, {"id": "Hypothesis_Functions", "label": "Hypothesis_Func\ntions", "title": "<b>Hypothesis_Functions</b> (subnode)<hr>Functions used to predict the expected value of y given x in machine learning models.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Functions used to predict the expected value of y given x in machine learning models.", "node_type": "subnode"}, {"id": "Logistic_Function", "label": "Logistic_Functi\non", "title": "<b>Logistic_Function</b> (subnode)<hr>Sigmoid function derived from Bernoulli distribution for predicting probabilities.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Sigmoid function derived from Bernoulli distribution for predicting probabilities.", "node_type": "subnode"}, {"id": "Canonical_Response_Function", "label": "Canonical_Respo\nnse_Function", "title": "<b>Canonical_Response_Function</b> (subnode)<hr>Function mapping natural parameters to the expected value of a random variable.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Function mapping natural parameters to the expected value of a random variable.", "node_type": "subnode"}, {"id": "Canonical_Link_Function", "label": "Canonical_Link_\nFunction", "title": "<b>Canonical_Link_Function</b> (subnode)<hr>Inverse function of canonical response, relating observed data to model parameters.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Inverse function of canonical response, relating observed data to model parameters.", "node_type": "subnode"}, {"id": "MachineLearningAlgorithms", "label": "MachineLearning\nAlgorithms", "title": "<b>MachineLearningAlgorithms</b> (major)<hr>Overview of machine learning algorithms as optimization search in model space.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Overview of machine learning algorithms as optimization search in model space.", "node_type": "major"}, {"id": "DiscriminativeLearning", "label": "DiscriminativeL\nearning", "title": "<b>DiscriminativeLearning</b> (subnode)<hr>Algorithms that learn p(y|x) directly or map inputs to labels.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Algorithms that learn p(y|x) directly or map inputs to labels.", "node_type": "subnode"}, {"id": "GenerativeLearning", "label": "GenerativeLearn\ning", "title": "<b>GenerativeLearning</b> (subnode)<hr>Algorithms that model p(x|y) and p(y) to derive p(y|x).", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Algorithms that model p(x|y) and p(y) to derive p(y|x).", "node_type": "subnode"}, {"id": "ConditionalDistribution", "label": "ConditionalDist\nribution", "title": "<b>ConditionalDistribution</b> (subnode)<hr>p(y|x;\u03b8), the conditional distribution of y given x.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "p(y|x;\u03b8), the conditional distribution of y given x.", "node_type": "subnode"}, {"id": "PerceptronAlgorithm", "label": "PerceptronAlgor\nithm", "title": "<b>PerceptronAlgorithm</b> (subnode)<hr>Attempts to find a decision boundary between classes.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Attempts to find a decision boundary between classes.", "node_type": "subnode"}, {"id": "DecisionBoundary", "label": "DecisionBoundar\ny", "title": "<b>DecisionBoundary</b> (subnode)<hr>Line that separates regions where different classes are predicted as most likely.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Line that separates regions where different classes are predicted as most likely.", "node_type": "subnode"}, {"id": "ClassPriors", "label": "ClassPriors", "title": "<b>ClassPriors</b> (subnode)<hr>p(y), the prior probability of each class before observing data.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "p(y), the prior probability of each class before observing data.", "node_type": "subnode"}, {"id": "ConditionalProbabilityModel", "label": "ConditionalProb\nabilityModel", "title": "<b>ConditionalProbabilityModel</b> (subnode)<hr>Models p(x|y) for each class to understand feature distributions.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Models p(x|y) for each class to understand feature distributions.", "node_type": "subnode"}, {"id": "BayesRule", "label": "BayesRule", "title": "<b>BayesRule</b> (subnode)<hr>Uses prior probabilities and conditional probabilities to calculate posterior distribution p(y|x).", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Uses prior probabilities and conditional probabilities to calculate posterior distribution p(y|x).", "node_type": "subnode"}, {"id": "Bayesian Classification", "label": "Bayesian\nClassification", "title": "<b>Bayesian Classification</b> (major)<hr>Classification using Bayes' theorem to calculate posterior probabilities.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Classification using Bayes' theorem to calculate posterior probabilities.", "node_type": "major"}, {"id": "Class Priors", "label": "Class Priors", "title": "<b>Class Priors</b> (subnode)<hr>Prior probability of each class before observing data.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Prior probability of each class before observing data.", "node_type": "subnode"}, {"id": "Conditional Probability p(x|y)", "label": "Conditional\nProbability\np(x|y)", "title": "<b>Conditional Probability p(x|y)</b> (subnode)<hr>Probability distribution of features given the class label.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Probability distribution of features given the class label.", "node_type": "subnode"}, {"id": "Posterior Distribution p(y|x)", "label": "Posterior\nDistribution\np(y|x)", "title": "<b>Posterior Distribution p(y|x)</b> (subnode)<hr>Derived using Bayes' rule to predict class labels based on observed data.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Derived using Bayes' rule to predict class labels based on observed data.", "node_type": "subnode"}, {"id": "Gaussian Discriminant Analysis (GDA)", "label": "Gaussian\nDiscriminant\nAnalysis (GDA)", "title": "<b>Gaussian Discriminant Analysis (GDA)</b> (major)<hr>Generative learning algorithm assuming multivariate normal distribution for p(x|y).", "shape": "star", "size": 25, "color": "#FF6347", "description": "Generative learning algorithm assuming multivariate normal distribution for p(x|y).", "node_type": "major"}, {"id": "Multivariate Normal Distribution", "label": "Multivariate\nNormal\nDistribution", "title": "<b>Multivariate Normal Distribution</b> (subnode)<hr>Distribution parameterized by mean vector and covariance matrix.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Distribution parameterized by mean vector and covariance matrix.", "node_type": "subnode"}, {"id": "Mean Vector", "label": "Mean Vector", "title": "<b>Mean Vector</b> (subnode)<hr>Vector representing the expected value of a multivariate normal distribution.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Vector representing the expected value of a multivariate normal distribution.", "node_type": "subnode"}, {"id": "Covariance Matrix", "label": "Covariance\nMatrix", "title": "<b>Covariance Matrix</b> (subnode)<hr>Matrix describing the variance and covariance between variables in a multivariate normal distribution.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Matrix describing the variance and covariance between variables in a multivariate normal distribution.", "node_type": "subnode"}, {"id": "MachineLearningBasics", "label": "MachineLearning\nBasics", "title": "<b>MachineLearningBasics</b> (major)<hr>Fundamental concepts in machine learning including probability distributions and random variables.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Fundamental concepts in machine learning including probability distributions and random variables.", "node_type": "major"}, {"id": "MeanOfGaussian", "label": "MeanOfGaussian", "title": "<b>MeanOfGaussian</b> (subnode)<hr>The expected value or mean of a Gaussian distribution is given by \u03bc.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "The expected value or mean of a Gaussian distribution is given by \u03bc.", "node_type": "subnode"}, {"id": "CovarianceMatrix", "label": "CovarianceMatri\nx", "title": "<b>CovarianceMatrix</b> (subnode)<hr>A matrix that generalizes the notion of variance to multiple dimensions for random vectors.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "A matrix that generalizes the notion of variance to multiple dimensions for random vectors.", "node_type": "subnode"}, {"id": "StandardNormalDistribution", "label": "StandardNormalD\nistribution", "title": "<b>StandardNormalDistribution</b> (subnode)<hr>A Gaussian distribution with zero mean and identity covariance matrix, representing a standard case.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "A Gaussian distribution with zero mean and identity covariance matrix, representing a standard case.", "node_type": "subnode"}, {"id": "DensityExamples", "label": "DensityExamples", "title": "<b>DensityExamples</b> (subnode)<hr>Illustrative examples of how changes in the covariance matrix affect the shape of the Gaussian density function.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Illustrative examples of how changes in the covariance matrix affect the shape of the Gaussian density function.", "node_type": "subnode"}, {"id": "MultivariateNormalDistribution", "label": "MultivariateNor\nmalDistribution", "title": "<b>MultivariateNormalDistribution</b> (subnode)<hr>Exploration of multivariate normal distribution properties and examples.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Exploration of multivariate normal distribution properties and examples.", "node_type": "subnode"}, {"id": "CovarianceMatrixImpact", "label": "CovarianceMatri\nxImpact", "title": "<b>CovarianceMatrixImpact</b> (subnode)<hr>Effect of covariance matrix on the density contours.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Effect of covariance matrix on the density contours.", "node_type": "subnode"}, {"id": "MeanVectorMovement", "label": "MeanVectorMovem\nent", "title": "<b>MeanVectorMovement</b> (subnode)<hr>Impact of mean vector changes when covariance is fixed.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Impact of mean vector changes when covariance is fixed.", "node_type": "subnode"}, {"id": "GaussianDiscriminantAnalysisModel", "label": "GaussianDiscrim\ninantAnalysisMo\ndel", "title": "<b>GaussianDiscriminantAnalysisModel</b> (major)<hr>Introduction to the Gaussian Discriminant Analysis model for classification problems.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Introduction to the Gaussian Discriminant Analysis model for classification problems.", "node_type": "major"}, {"id": "MultivariateNormalForClasses", "label": "MultivariateNor\nmalForClasses", "title": "<b>MultivariateNormalForClasses</b> (subnode)<hr>Using multivariate normal distributions to model class-conditional probabilities.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Using multivariate normal distributions to model class-conditional probabilities.", "node_type": "subnode"}, {"id": "GaussianDiscriminantAnalysis(GDA)", "label": "GaussianDiscrim\ninantAnalysis(G\nDA)", "title": "<b>GaussianDiscriminantAnalysis(GDA)</b> (subnode)<hr>Model that uses Gaussian distributions to classify data points.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Model that uses Gaussian distributions to classify data points.", "node_type": "subnode"}, {"id": "ParametersEstimation", "label": "ParametersEstim\nation", "title": "<b>ParametersEstimation</b> (subnode)<hr>Process of finding values for model parameters to maximize likelihood.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Process of finding values for model parameters to maximize likelihood.", "node_type": "subnode"}, {"id": "GaussianDiscriminantAnalysis", "label": "GaussianDiscrim\ninantAnalysis", "title": "<b>GaussianDiscriminantAnalysis</b> (subnode)<hr>A probabilistic model for classification tasks based on the assumption that the data is generated from a Gaussian distribution.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "A probabilistic model for classification tasks based on the assumption that the data is generated from a Gaussian distribution.", "node_type": "subnode"}, {"id": "DecisionBoundaries", "label": "DecisionBoundar\nies", "title": "<b>DecisionBoundaries</b> (subnode)<hr>The boundary that separates different classes in classification models.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "The boundary that separates different classes in classification models.", "node_type": "subnode"}, {"id": "ModelAssumptions", "label": "ModelAssumption\ns", "title": "<b>ModelAssumptions</b> (subnode)<hr>The underlying assumptions made by machine learning models about the data distribution.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "The underlying assumptions made by machine learning models about the data distribution.", "node_type": "subnode"}, {"id": "AsymptoticEfficiency", "label": "AsymptoticEffic\niency", "title": "<b>AsymptoticEfficiency</b> (subnode)<hr>Property of GDA that ensures optimal performance in large datasets when model assumptions are correct.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Property of GDA that ensures optimal performance in large datasets when model assumptions are correct.", "node_type": "subnode"}, {"id": "GDA", "label": "GDA", "title": "<b>GDA</b> (subnode)<hr>Generative Discriminative Algorithm with strong modeling assumptions.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Generative Discriminative Algorithm with strong modeling assumptions.", "node_type": "subnode"}, {"id": "Robustness", "label": "Robustness", "title": "<b>Robustness</b> (subnode)<hr>Strength of logistic regression in handling non-Gaussian data.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Strength of logistic regression in handling non-Gaussian data.", "node_type": "subnode"}, {"id": "NaiveBayes", "label": "NaiveBayes", "title": "<b>NaiveBayes</b> (major)<hr>A generative learning algorithm that works well for many classification problems, including text classification with modifications.", "shape": "star", "size": 25, "color": "#FF6347", "description": "A generative learning algorithm that works well for many classification problems, including text classification with modifications.", "node_type": "major"}, {"id": "DiscreteFeatures", "label": "DiscreteFeature\ns", "title": "<b>DiscreteFeatures</b> (subnode)<hr>Handling of discrete-valued feature vectors in Naive Bayes algorithm.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Handling of discrete-valued feature vectors in Naive Bayes algorithm.", "node_type": "subnode"}, {"id": "Machine_Learning", "label": "Machine_Learnin\ng", "title": "<b>Machine_Learning</b> (major)<hr>Field of study focusing on algorithms that learn from and make predictions on data.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Field of study focusing on algorithms that learn from and make predictions on data.", "node_type": "major"}, {"id": "Text_Classification", "label": "Text_Classifica\ntion", "title": "<b>Text_Classification</b> (subnode)<hr>Techniques for classifying text into predefined categories.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Techniques for classifying text into predefined categories.", "node_type": "subnode"}, {"id": "Spam_Filtering", "label": "Spam_Filtering", "title": "<b>Spam_Filtering</b> (subnode)<hr>Application of machine learning to classify emails as spam or non-spam.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Application of machine learning to classify emails as spam or non-spam.", "node_type": "subnode"}, {"id": "Training_Set", "label": "Training_Set", "title": "<b>Training_Set</b> (subnode)<hr>Set of labeled examples used for training a model.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Set of labeled examples used for training a model.", "node_type": "subnode"}, {"id": "Feature_Vector", "label": "Feature_Vector", "title": "<b>Feature_Vector</b> (subnode)<hr>Vector representation of an email based on its content words.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Vector representation of an email based on its content words.", "node_type": "subnode"}, {"id": "Vocabulary", "label": "Vocabulary", "title": "<b>Vocabulary</b> (subnode)<hr>Set of unique words used to construct the feature vector.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Set of unique words used to construct the feature vector.", "node_type": "subnode"}, {"id": "Stop_Words", "label": "Stop_Words", "title": "<b>Stop_Words</b> (subnode)<hr>Commonly excluded high-frequency words in text processing.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Commonly excluded high-frequency words in text processing.", "node_type": "subnode"}, {"id": "Machine_Learning_Topic", "label": "Machine_Learnin\ng_Topic", "title": "<b>Machine_Learning_Topic</b> (major)<hr>Overview of machine learning concepts and algorithms.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Overview of machine learning concepts and algorithms.", "node_type": "major"}, {"id": "Feature_Vector_Selection", "label": "Feature_Vector_\nSelection", "title": "<b>Feature_Vector_Selection</b> (subnode)<hr>Choosing relevant features from the text data, excluding stop words.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Choosing relevant features from the text data, excluding stop words.", "node_type": "subnode"}, {"id": "Generative_Modeling", "label": "Generative_Mode\nling", "title": "<b>Generative_Modeling</b> (subnode)<hr>Building models that generate probabilities for feature vectors given a class label.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Building models that generate probabilities for feature vectors given a class label.", "node_type": "subnode"}, {"id": "Naive_Bayes_Assumption", "label": "Naive_Bayes_Ass\numption", "title": "<b>Naive_Bayes_Assumption</b> (subnode)<hr>Assumption of conditional independence among features given the class label.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Assumption of conditional independence among features given the class label.", "node_type": "subnode"}, {"id": "Naive_Bayes_Classifier", "label": "Naive_Bayes_Cla\nssifier", "title": "<b>Naive_Bayes_Classifier</b> (subnode)<hr>Algorithm for classification based on Bayes' theorem with strong independence assumptions.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Algorithm for classification based on Bayes' theorem with strong independence assumptions.", "node_type": "subnode"}, {"id": "NaiveBayesAlgorithm", "label": "NaiveBayesAlgor\nithm", "title": "<b>NaiveBayesAlgorithm</b> (major)<hr>A probabilistic classifier based on applying Bayes' theorem with strong independence assumptions between the features.", "shape": "star", "size": 25, "color": "#FF6347", "description": "A probabilistic classifier based on applying Bayes' theorem with strong independence assumptions between the features.", "node_type": "major"}, {"id": "ConditionalProbability", "label": "ConditionalProb\nability", "title": "<b>ConditionalProbability</b> (subnode)<hr>The probability of a token given the preceding tokens in a sequence.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "The probability of a token given the preceding tokens in a sequence.", "node_type": "subnode"}, {"id": "JointLikelihood", "label": "JointLikelihood", "title": "<b>JointLikelihood</b> (subnode)<hr>Probability of observing the data given model parameters.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Probability of observing the data given model parameters.", "node_type": "subnode"}, {"id": "BinaryFeatures", "label": "BinaryFeatures", "title": "<b>BinaryFeatures</b> (subnode)<hr>Features are binary-valued in the context of spam email classification.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Features are binary-valued in the context of spam email classification.", "node_type": "subnode"}, {"id": "ParameterEstimation", "label": "ParameterEstima\ntion", "title": "<b>ParameterEstimation</b> (subnode)<hr>Process of estimating parameters from training data in the context of the multinomial model", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Process of estimating parameters from training data in the context of the multinomial model", "node_type": "subnode"}, {"id": "Machine Learning Algorithms", "label": "Machine\nLearning\nAlgorithms", "title": "<b>Machine Learning Algorithms</b> (major)<hr>Collection of algorithms used in machine learning for model training and optimization.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Collection of algorithms used in machine learning for model training and optimization.", "node_type": "major"}, {"id": "Naive Bayes Algorithm", "label": "Naive Bayes\nAlgorithm", "title": "<b>Naive Bayes Algorithm</b> (subnode)<hr>Probabilistic classifier based on applying Bayes' theorem with strong independence assumptions between the features", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Probabilistic classifier based on applying Bayes' theorem with strong independence assumptions between the features", "node_type": "subnode"}, {"id": "Binary Features", "label": "Binary Features", "title": "<b>Binary Features</b> (subnode)<hr>Features that can take only two values, typically 0 or 1", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Features that can take only two values, typically 0 or 1", "node_type": "subnode"}, {"id": "Multinomial Distribution", "label": "Multinomial\nDistribution", "title": "<b>Multinomial Distribution</b> (subnode)<hr>Probability distribution of the outcomes of a fixed number of independent trials with different possible outcomes for each trial", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Probability distribution of the outcomes of a fixed number of independent trials with different possible outcomes for each trial", "node_type": "subnode"}, {"id": "Feature Representation", "label": "Feature\nRepresentation", "title": "<b>Feature Representation</b> (subnode)<hr>Method for representing features in a dataset, such as using intervals for continuous variables", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Method for representing features in a dataset, such as using intervals for continuous variables", "node_type": "subnode"}, {"id": "Laplace Smoothing", "label": "Laplace\nSmoothing", "title": "<b>Laplace Smoothing</b> (subnode)<hr>Technique to prevent zero probability estimates by adding a small constant to the counts of each feature value", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Technique to prevent zero probability estimates by adding a small constant to the counts of each feature value", "node_type": "subnode"}, {"id": "MachineLearningConferences", "label": "MachineLearning\nConferences", "title": "<b>MachineLearningConferences</b> (major)<hr>Top machine learning conferences including NeurIPS.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Top machine learning conferences including NeurIPS.", "node_type": "major"}, {"id": "NeurIPS20xxSubmission", "label": "NeurIPS20xxSubm\nission", "title": "<b>NeurIPS20xxSubmission</b> (subnode)<hr>Submitting work to the NeurIPS conference in May 20xx.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Submitting work to the NeurIPS conference in May 20xx.", "node_type": "subnode"}, {"id": "NaiveBayesFilter", "label": "NaiveBayesFilte\nr", "title": "<b>NaiveBayesFilter</b> (major)<hr>Description of a Naive Bayes spam filter and its limitations.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Description of a Naive Bayes spam filter and its limitations.", "node_type": "major"}, {"id": "NewWordDetection", "label": "NewWordDetectio\nn", "title": "<b>NewWordDetection</b> (subnode)<hr>Issues with detecting new words like 'neurips' in the training set.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Issues with detecting new words like 'neurips' in the training set.", "node_type": "subnode"}, {"id": "Probability_Estimation", "label": "Probability_Est\nimation", "title": "<b>Probability_Estimation</b> (subnode)<hr>Techniques for estimating probabilities from data samples.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Techniques for estimating probabilities from data samples.", "node_type": "subnode"}, {"id": "Maximum_Likelihood_Estimates", "label": "Maximum_Likelih\nood_Estimates", "title": "<b>Maximum_Likelihood_Estimates</b> (subnode)<hr>Estimating parameters based on observed frequencies in a dataset.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Estimating parameters based on observed frequencies in a dataset.", "node_type": "subnode"}, {"id": "Laplace_Smoothing", "label": "Laplace_Smoothi\nng", "title": "<b>Laplace_Smoothing</b> (subnode)<hr>Adjusting estimates to avoid zero probabilities by adding small constants.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Adjusting estimates to avoid zero probabilities by adding small constants.", "node_type": "subnode"}, {"id": "Event_Models_Text_Classification", "label": "Event_Models_Te\nxt_Classificati\non", "title": "<b>Event_Models_Text_Classification</b> (subnode)<hr>Models used in text classification tasks, incorporating Laplace smoothing.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Models used in text classification tasks, incorporating Laplace smoothing.", "node_type": "subnode"}, {"id": "EventModelsForTextClassification", "label": "EventModelsForT\nextClassificati\non", "title": "<b>EventModelsForTextClassification</b> (major)<hr>Discussion of models specifically for text classification in machine learning.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Discussion of models specifically for text classification in machine learning.", "node_type": "major"}, {"id": "BernoulliEventModel", "label": "BernoulliEventM\nodel", "title": "<b>BernoulliEventModel</b> (subnode)<hr>A model where each word is included independently according to probabilities based on class priors.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "A model where each word is included independently according to probabilities based on class priors.", "node_type": "subnode"}, {"id": "MultinomialEventModel", "label": "MultinomialEven\ntModel", "title": "<b>MultinomialEventModel</b> (subnode)<hr>A model where each word in a document is generated independently from the same multinomial distribution based on class label", "shape": "dot", "size": 15, "color": "#4682B4", "description": "A model where each word in a document is generated independently from the same multinomial distribution based on class label", "node_type": "subnode"}, {"id": "NaiveBayesClassifier", "label": "NaiveBayesClass\nifier", "title": "<b>NaiveBayesClassifier</b> (subnode)<hr>A probabilistic classifier based on applying Bayes' theorem with strong (naive) independence assumptions between the features.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "A probabilistic classifier based on applying Bayes' theorem with strong (naive) independence assumptions between the features.", "node_type": "subnode"}, {"id": "LaplaceSmoothing", "label": "LaplaceSmoothin\ng", "title": "<b>LaplaceSmoothing</b> (subnode)<hr>Technique to prevent zero probabilities in categorical data estimation.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Technique to prevent zero probabilities in categorical data estimation.", "node_type": "subnode"}, {"id": "KernelMethods", "label": "KernelMethods", "title": "<b>KernelMethods</b> (major)<hr>Techniques that allow algorithms to work in high-dimensional feature spaces without explicit computation of the space.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Techniques that allow algorithms to work in high-dimensional feature spaces without explicit computation of the space.", "node_type": "major"}, {"id": "FeatureMaps", "label": "FeatureMaps", "title": "<b>FeatureMaps</b> (subnode)<hr>Transformation of input data to higher-dimensional space for better model fitting.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Transformation of input data to higher-dimensional space for better model fitting.", "node_type": "subnode"}, {"id": "Machine Learning Concepts", "label": "Machine\nLearning\nConcepts", "title": "<b>Machine Learning Concepts</b> (major)<hr>Overview of key concepts in machine learning including model complexity and regularization.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Overview of key concepts in machine learning including model complexity and regularization.", "node_type": "major"}, {"id": "Feature Mapping", "label": "Feature Mapping", "title": "<b>Feature Mapping</b> (subnode)<hr>Alternative method of testing if a function is a valid kernel through feature mapping.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Alternative method of testing if a function is a valid kernel through feature mapping.", "node_type": "subnode"}, {"id": "Linear Function Over Features", "label": "Linear Function\nOver Features", "title": "<b>Linear Function Over Features</b> (subnode)<hr>Rewriting a cubic function as a linear combination of features.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Rewriting a cubic function as a linear combination of features.", "node_type": "subnode"}, {"id": "Cubic Function Representation", "label": "Cubic Function\nRepresentation", "title": "<b>Cubic Function Representation</b> (subnode)<hr>Expressing a cubic polynomial using feature map \u03c6(x).", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Expressing a cubic polynomial using feature map \u03c6(x).", "node_type": "subnode"}, {"id": "Feature Map Definition", "label": "Feature Map\nDefinition", "title": "<b>Feature Map Definition</b> (subnode)<hr>Definition of the function \u03c6 that maps attributes to features.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Definition of the function \u03c6 that maps attributes to features.", "node_type": "subnode"}, {"id": "LMS with Features", "label": "LMS with\nFeatures", "title": "<b>LMS with Features</b> (major)<hr>Derivation and application of least mean squares algorithm using feature variables.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Derivation and application of least mean squares algorithm using feature variables.", "node_type": "major"}, {"id": "Gradient Descent Update", "label": "Gradient\nDescent Update", "title": "<b>Gradient Descent Update</b> (subnode)<hr>Update rule for fitting the model \u03b8\u2091\u03c6(x) using gradient descent.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Update rule for fitting the model \u03b8\u2091\u03c6(x) using gradient descent.", "node_type": "subnode"}, {"id": "FeatureMapping", "label": "FeatureMapping", "title": "<b>FeatureMapping</b> (subnode)<hr>Transformation that maps original feature vectors into a new space where linear algorithms can be applied more effectively.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Transformation that maps original feature vectors into a new space where linear algorithms can be applied more effectively.", "node_type": "subnode"}, {"id": "HighDimensionalFeatures", "label": "HighDimensional\nFeatures", "title": "<b>HighDimensionalFeatures</b> (subnode)<hr>Complex feature mappings leading to high-dimensional vectors in machine learning problems.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Complex feature mappings leading to high-dimensional vectors in machine learning problems.", "node_type": "subnode"}, {"id": "KernelTrick", "label": "KernelTrick", "title": "<b>KernelTrick</b> (subnode)<hr>Technique for efficiently computing inner products of high-dimensional feature space without explicitly mapping the data.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Technique for efficiently computing inner products of high-dimensional feature space without explicitly mapping the data.", "node_type": "subnode"}, {"id": "FeatureMappingPhiX", "label": "FeatureMappingP\nhiX", "title": "<b>FeatureMappingPhiX</b> (subnode)<hr>Description of the feature mapping function \u03c6(x) in machine learning models.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Description of the feature mapping function \u03c6(x) in machine learning models.", "node_type": "subnode"}, {"id": "RuntimeAndMemoryEfficiency", "label": "RuntimeAndMemor\nyEfficiency", "title": "<b>RuntimeAndMemoryEfficiency</b> (subnode)<hr>Discussion on improving runtime and memory efficiency using techniques like the kernel trick.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Discussion on improving runtime and memory efficiency using techniques like the kernel trick.", "node_type": "subnode"}, {"id": "InitializationThetaZero", "label": "InitializationT\nhetaZero", "title": "<b>InitializationThetaZero</b> (subnode)<hr>Initial setup where \u03b8 is set to zero for simplification.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Initial setup where \u03b8 is set to zero for simplification.", "node_type": "subnode"}, {"id": "IterativeUpdateRule", "label": "IterativeUpdate\nRule", "title": "<b>IterativeUpdateRule</b> (subnode)<hr>Explanation of iterative update rules in machine learning models.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Explanation of iterative update rules in machine learning models.", "node_type": "subnode"}, {"id": "LinearCombinationRepresentation", "label": "LinearCombinati\nonRepresentatio\nn", "title": "<b>LinearCombinationRepresentation</b> (subnode)<hr>Concept of representing \u03b8 as a linear combination of feature mappings \u03c6(x).", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Concept of representing \u03b8 as a linear combination of feature mappings \u03c6(x).", "node_type": "subnode"}, {"id": "CoefficientUpdateRule", "label": "CoefficientUpda\nteRule", "title": "<b>CoefficientUpdateRule</b> (subnode)<hr>Derivation and explanation of the update rule for coefficients \u03b2 in machine learning models.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Derivation and explanation of the update rule for coefficients \u03b2 in machine learning models.", "node_type": "subnode"}, {"id": "MachineLearningAlgorithm", "label": "MachineLearning\nAlgorithm", "title": "<b>MachineLearningAlgorithm</b> (major)<hr>Overview of machine learning algorithms and their properties.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Overview of machine learning algorithms and their properties.", "node_type": "major"}, {"id": "BatchGradientDescent", "label": "BatchGradientDe\nscent", "title": "<b>BatchGradientDescent</b> (subnode)<hr>A method for minimizing loss functions in machine learning models.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "A method for minimizing loss functions in machine learning models.", "node_type": "subnode"}, {"id": "BetaUpdateEquation", "label": "BetaUpdateEquat\nion", "title": "<b>BetaUpdateEquation</b> (subnode)<hr>The equation used to update the beta values iteratively during gradient descent.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "The equation used to update the beta values iteratively during gradient descent.", "node_type": "subnode"}, {"id": "FeatureMapPhi", "label": "FeatureMapPhi", "title": "<b>FeatureMapPhi</b> (subnode)<hr>A mapping function that transforms input data into a higher-dimensional space for better separation.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "A mapping function that transforms input data into a higher-dimensional space for better separation.", "node_type": "subnode"}, {"id": "InnerProductEfficiency", "label": "InnerProductEff\niciency", "title": "<b>InnerProductEfficiency</b> (subnode)<hr>Techniques to efficiently compute inner products in high-dimensional spaces without explicit feature map computation.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Techniques to efficiently compute inner products in high-dimensional spaces without explicit feature map computation.", "node_type": "subnode"}, {"id": "FeatureMapsAndKernels", "label": "FeatureMapsAndK\nernels", "title": "<b>FeatureMapsAndKernels</b> (subnode)<hr>Discussion on feature maps and their corresponding kernel functions.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Discussion on feature maps and their corresponding kernel functions.", "node_type": "subnode"}, {"id": "KernelFunctionDefinition", "label": "KernelFunctionD\nefinition", "title": "<b>KernelFunctionDefinition</b> (subnode)<hr>Definition of the kernel function in relation to feature maps.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Definition of the kernel function in relation to feature maps.", "node_type": "subnode"}, {"id": "InnerProductCalculation", "label": "InnerProductCal\nculation", "title": "<b>InnerProductCalculation</b> (subnode)<hr>Method for calculating inner products using kernels.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Method for calculating inner products using kernels.", "node_type": "subnode"}, {"id": "AlgorithmEfficiency", "label": "AlgorithmEffici\nency", "title": "<b>AlgorithmEfficiency</b> (subnode)<hr>Explanation of the efficiency of algorithms based on kernel functions.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Explanation of the efficiency of algorithms based on kernel functions.", "node_type": "subnode"}, {"id": "PropertiesOfKernels", "label": "PropertiesOfKer\nnels", "title": "<b>PropertiesOfKernels</b> (major)<hr>Exploration of properties and characteristics of kernels in machine learning.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Exploration of properties and characteristics of kernels in machine learning.", "node_type": "major"}, {"id": "Kernels_in_Machine_Learning", "label": "Kernels_in_Mach\nine_Learning", "title": "<b>Kernels_in_Machine_Learning</b> (subnode)<hr>Discussion on kernel functions in the context of machine learning algorithms.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Discussion on kernel functions in the context of machine learning algorithms.", "node_type": "subnode"}, {"id": "Feature_Map_Phi", "label": "Feature_Map_Phi", "title": "<b>Feature_Map_Phi</b> (subnode)<hr>Explicitly defined feature map phi and its role in inducing kernel function K.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Explicitly defined feature map phi and its role in inducing kernel function K.", "node_type": "subnode"}, {"id": "Kernel_Function_K", "label": "Kernel_Function\n_K", "title": "<b>Kernel_Function_K</b> (subnode)<hr>Definition of the kernel function K(x,z) and its intrinsic properties.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Definition of the kernel function K(x,z) and its intrinsic properties.", "node_type": "subnode"}, {"id": "Algorithm_5.11", "label": "Algorithm_5.11", "title": "<b>Algorithm_5.11</b> (subnode)<hr>Description of algorithm 5.11 which operates based on kernel functions without explicit feature maps.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Description of algorithm 5.11 which operates based on kernel functions without explicit feature maps.", "node_type": "subnode"}, {"id": "Characterization_of_Kernels", "label": "Characterizatio\nn_of_Kernels", "title": "<b>Characterization_of_Kernels</b> (subnode)<hr>Conditions for a function to be considered as a valid kernel function K(x,z).", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Conditions for a function to be considered as a valid kernel function K(x,z).", "node_type": "subnode"}, {"id": "Example_Kernel_Functions", "label": "Example_Kernel_\nFunctions", "title": "<b>Example_Kernel_Functions</b> (subnode)<hr>Concrete examples of kernel functions, such as polynomial kernels.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Concrete examples of kernel functions, such as polynomial kernels.", "node_type": "subnode"}, {"id": "KernelFunctions", "label": "KernelFunctions", "title": "<b>KernelFunctions</b> (subnode)<hr>Functions that compute the dot product of feature mappings in a high-dimensional space.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Functions that compute the dot product of feature mappings in a high-dimensional space.", "node_type": "subnode"}, {"id": "PolynomialKernels", "label": "PolynomialKerne\nls", "title": "<b>PolynomialKernels</b> (subnode)<hr>A type of kernel function used to map input data into higher dimensional spaces.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "A type of kernel function used to map input data into higher dimensional spaces.", "node_type": "subnode"}, {"id": "ComputationalEfficiency", "label": "ComputationalEf\nficiency", "title": "<b>ComputationalEfficiency</b> (subnode)<hr>Discussion on the efficiency of computing kernel functions compared to direct computation in high-dimensional spaces.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Discussion on the efficiency of computing kernel functions compared to direct computation in high-dimensional spaces.", "node_type": "subnode"}, {"id": "KernelsAsSimilarityMetrics", "label": "KernelsAsSimila\nrityMetrics", "title": "<b>KernelsAsSimilarityMetrics</b> (subnode)<hr>Use of kernels as a measure of similarity between data points.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Use of kernels as a measure of similarity between data points.", "node_type": "subnode"}, {"id": "GaussianKernel", "label": "GaussianKernel", "title": "<b>GaussianKernel</b> (subnode)<hr>Specific kernel function that measures the similarity based on distance in input space.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Specific kernel function that measures the similarity based on distance in input space.", "node_type": "subnode"}, {"id": "Kernel_Functions", "label": "Kernel_Function\ns", "title": "<b>Kernel_Functions</b> (major)<hr>Properties and conditions for a function to be a valid kernel.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Properties and conditions for a function to be a valid kernel.", "node_type": "major"}, {"id": "Necessary_Conditions", "label": "Necessary_Condi\ntions", "title": "<b>Necessary_Conditions</b> (subnode)<hr>Conditions that must be satisfied by a valid kernel function.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Conditions that must be satisfied by a valid kernel function.", "node_type": "subnode"}, {"id": "Symmetry_Property", "label": "Symmetry_Proper\nty", "title": "<b>Symmetry_Property</b> (subnode)<hr>A valid kernel matrix is symmetric.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "A valid kernel matrix is symmetric.", "node_type": "subnode"}, {"id": "Positive_Semi_Definite", "label": "Positive_Semi_D\nefinite", "title": "<b>Positive_Semi_Definite</b> (subnode)<hr>A valid kernel matrix must be positive semi-definite.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "A valid kernel matrix must be positive semi-definite.", "node_type": "subnode"}, {"id": "Kernel_Matrix", "label": "Kernel_Matrix", "title": "<b>Kernel_Matrix</b> (subnode)<hr>Matrix representation of the kernel function for a set of points.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Matrix representation of the kernel function for a set of points.", "node_type": "subnode"}, {"id": "Sufficient_Conditions", "label": "Sufficient_Cond\nitions", "title": "<b>Sufficient_Conditions</b> (subnode)<hr>Conditions that are both necessary and sufficient for a valid kernel function.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Conditions that are both necessary and sufficient for a valid kernel function.", "node_type": "subnode"}, {"id": "Kernel Matrix Properties", "label": "Kernel Matrix\nProperties", "title": "<b>Kernel Matrix Properties</b> (major)<hr>Properties of the kernel matrix including symmetry and positive semidefiniteness.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Properties of the kernel matrix including symmetry and positive semidefiniteness.", "node_type": "major"}, {"id": "Sufficient Conditions for Kernels", "label": "Sufficient\nConditions for\nKernels", "title": "<b>Sufficient Conditions for Kernels</b> (subnode)<hr>Conditions under which a function can be considered a valid Mercer kernel.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Conditions under which a function can be considered a valid Mercer kernel.", "node_type": "subnode"}, {"id": "Mercer's Theorem", "label": "Mercer's\nTheorem", "title": "<b>Mercer's Theorem</b> (subnode)<hr>Theorem stating necessary and sufficient conditions for a function to be a valid kernel.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Theorem stating necessary and sufficient conditions for a function to be a valid kernel.", "node_type": "subnode"}, {"id": "Kernel Examples", "label": "Kernel Examples", "title": "<b>Kernel Examples</b> (major)<hr>Examples of kernels used in machine learning problems such as SVMs.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Examples of kernels used in machine learning problems such as SVMs.", "node_type": "major"}, {"id": "Digit Recognition Problem", "label": "Digit\nRecognition\nProblem", "title": "<b>Digit Recognition Problem</b> (subnode)<hr>Use of polynomial and Gaussian kernels for recognizing handwritten digits.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Use of polynomial and Gaussian kernels for recognizing handwritten digits.", "node_type": "subnode"}, {"id": "String Classification Example", "label": "String\nClassification\nExample", "title": "<b>String Classification Example</b> (subnode)<hr>Example involving classification of strings, such as amino acid sequences.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Example involving classification of strings, such as amino acid sequences.", "node_type": "subnode"}, {"id": "FeatureExtraction", "label": "FeatureExtracti\non", "title": "<b>FeatureExtraction</b> (subnode)<hr>Techniques for converting raw data into features suitable for machine learning models.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Techniques for converting raw data into features suitable for machine learning models.", "node_type": "subnode"}, {"id": "StringFeatureExtraction", "label": "StringFeatureEx\ntraction", "title": "<b>StringFeatureExtraction</b> (subnode)<hr>Method for extracting features from strings by counting occurrences of substrings.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Method for extracting features from strings by counting occurrences of substrings.", "node_type": "subnode"}, {"id": "SupportVectorMachines", "label": "SupportVectorMa\nchines", "title": "<b>SupportVectorMachines</b> (major)<hr>Supervised learning model for classification and regression analysis.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Supervised learning model for classification and regression analysis.", "node_type": "major"}, {"id": "Machine_Learning_Algorithms", "label": "Machine_Learnin\ng_Algorithms", "title": "<b>Machine_Learning_Algorithms</b> (major)<hr>Overview of algorithms in machine learning including kernel trick and SVMs.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Overview of algorithms in machine learning including kernel trick and SVMs.", "node_type": "major"}, {"id": "Kernel_Trick", "label": "Kernel_Trick", "title": "<b>Kernel_Trick</b> (subnode)<hr>Method to derive algorithms like the kernel perceptron algorithm.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Method to derive algorithms like the kernel perceptron algorithm.", "node_type": "subnode"}, {"id": "Support_Vector_Machines", "label": "Support_Vector_\nMachines", "title": "<b>Support_Vector_Machines</b> (subnode)<hr>Supervised learning algorithm known for its effectiveness in classification tasks.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Supervised learning algorithm known for its effectiveness in classification tasks.", "node_type": "subnode"}, {"id": "Margins_Intuition", "label": "Margins_Intuiti\non", "title": "<b>Margins_Intuition</b> (subnode)<hr>Introduction to the concept of margins and confidence in predictions.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Introduction to the concept of margins and confidence in predictions.", "node_type": "subnode"}, {"id": "Optimal_Margin_Classifier", "label": "Optimal_Margin_\nClassifier", "title": "<b>Optimal_Margin_Classifier</b> (subnode)<hr>Classifier that maximizes the margin between classes, leading into Lagrange duality discussion.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Classifier that maximizes the margin between classes, leading into Lagrange duality discussion.", "node_type": "subnode"}, {"id": "Lagrange_Duality", "label": "Lagrange_Dualit\ny", "title": "<b>Lagrange_Duality</b> (subnode)<hr>Theory for converting constrained optimization problems into dual forms.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Theory for converting constrained optimization problems into dual forms.", "node_type": "subnode"}, {"id": "Kernels_in_SVMs", "label": "Kernels_in_SVMs", "title": "<b>Kernels_in_SVMs</b> (subnode)<hr>Technique allowing efficient application of SVMs in high-dimensional spaces.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Technique allowing efficient application of SVMs in high-dimensional spaces.", "node_type": "subnode"}, {"id": "SMO_Algorithm", "label": "SMO_Algorithm", "title": "<b>SMO_Algorithm</b> (subnode)<hr>Algorithm that updates two Lagrange multipliers simultaneously to optimize the dual problem.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Algorithm that updates two Lagrange multipliers simultaneously to optimize the dual problem.", "node_type": "subnode"}, {"id": "Functional Margins", "label": "Functional\nMargins", "title": "<b>Functional Margins</b> (subnode)<hr>Concept used to formalize confident classifications based on dot product of parameters and input.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Concept used to formalize confident classifications based on dot product of parameters and input.", "node_type": "subnode"}, {"id": "Geometric Margins", "label": "Geometric\nMargins", "title": "<b>Geometric Margins</b> (subnode)<hr>Distance-based measure for confidence in classification predictions far from decision boundary.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Distance-based measure for confidence in classification predictions far from decision boundary.", "node_type": "subnode"}, {"id": "Decision Boundary", "label": "Decision\nBoundary", "title": "<b>Decision Boundary</b> (subnode)<hr>Line or hyperplane separating different classes in a feature space.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Line or hyperplane separating different classes in a feature space.", "node_type": "subnode"}, {"id": "Separating Hyperplane", "label": "Separating\nHyperplane", "title": "<b>Separating Hyperplane</b> (subnode)<hr>Specific term for decision boundary used in SVMs and linear classification problems.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Specific term for decision boundary used in SVMs and linear classification problems.", "node_type": "subnode"}, {"id": "Confidence in Predictions", "label": "Confidence in\nPredictions", "title": "<b>Confidence in Predictions</b> (subnode)<hr>Measure of certainty about predictions based on distance from the decision boundary.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Measure of certainty about predictions based on distance from the decision boundary.", "node_type": "subnode"}, {"id": "Support Vector Machines (SVMs)", "label": "Support Vector\nMachines (SVMs)", "title": "<b>Support Vector Machines (SVMs)</b> (subnode)<hr>Binary classification algorithm that maximizes the margin between classes.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Binary classification algorithm that maximizes the margin between classes.", "node_type": "subnode"}, {"id": "Notation for SVMs", "label": "Notation for\nSVMs", "title": "<b>Notation for SVMs</b> (subnode)<hr>Introduction to notation used in discussing SVMs, including parameters w and b.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Introduction to notation used in discussing SVMs, including parameters w and b.", "node_type": "subnode"}, {"id": "Functional Margin", "label": "Functional\nMargin", "title": "<b>Functional Margin</b> (subnode)<hr>Definition of the functional margin for a training example with respect to classifier parameters.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Definition of the functional margin for a training example with respect to classifier parameters.", "node_type": "subnode"}, {"id": "Geometric Margin", "label": "Geometric\nMargin", "title": "<b>Geometric Margin</b> (subnode)<hr>Conceptual understanding of geometric margins in SVM context, not fully detailed here.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Conceptual understanding of geometric margins in SVM context, not fully detailed here.", "node_type": "subnode"}, {"id": "Functional_Margin", "label": "Functional_Marg\nin", "title": "<b>Functional_Margin</b> (subnode)<hr>Definition and properties of functional margin for a linear classifier.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Definition and properties of functional margin for a linear classifier.", "node_type": "subnode"}, {"id": "Confidence_and_Prediction", "label": "Confidence_and_\nPrediction", "title": "<b>Confidence_and_Prediction</b> (subnode)<hr>Relationship between functional margin and prediction accuracy.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Relationship between functional margin and prediction accuracy.", "node_type": "subnode"}, {"id": "Scaling_Issue", "label": "Scaling_Issue", "title": "<b>Scaling_Issue</b> (subnode)<hr>Problem with scaling the parameters of a linear classifier affecting the functional margin.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Problem with scaling the parameters of a linear classifier affecting the functional margin.", "node_type": "subnode"}, {"id": "Function_Margin_of_S", "label": "Function_Margin\n_of_S", "title": "<b>Function_Margin_of_S</b> (subnode)<hr>Definition of function margin for a set of training examples.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Definition of function margin for a set of training examples.", "node_type": "subnode"}, {"id": "Geometric_Margins", "label": "Geometric_Margi\nns", "title": "<b>Geometric_Margins</b> (subnode)<hr>Introduction to geometric margins in machine learning context.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Introduction to geometric margins in machine learning context.", "node_type": "subnode"}, {"id": "VectorW", "label": "VectorW", "title": "<b>VectorW</b> (subnode)<hr>A vector orthogonal to the decision boundary and pointing towards positive class.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "A vector orthogonal to the decision boundary and pointing towards positive class.", "node_type": "subnode"}, {"id": "DistanceToBoundary", "label": "DistanceToBound\nary", "title": "<b>DistanceToBoundary</b> (subnode)<hr>The shortest distance from a point to the decision boundary.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "The shortest distance from a point to the decision boundary.", "node_type": "subnode"}, {"id": "UnitVectorW", "label": "UnitVectorW", "title": "<b>UnitVectorW</b> (subnode)<hr>A unit vector in the direction of W, used for calculating distances.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "A unit vector in the direction of W, used for calculating distances.", "node_type": "subnode"}, {"id": "GeometricMargin", "label": "GeometricMargin", "title": "<b>GeometricMargin</b> (major)<hr>The perpendicular distance from a data point to the decision boundary multiplied by the class label.", "shape": "star", "size": 25, "color": "#FF6347", "description": "The perpendicular distance from a data point to the decision boundary multiplied by the class label.", "node_type": "major"}, {"id": "FunctionalMargin", "label": "FunctionalMargi\nn", "title": "<b>FunctionalMargin</b> (subnode)<hr>A measure of how confidently a model can classify a training example.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "A measure of how confidently a model can classify a training example.", "node_type": "subnode"}, {"id": "Scaling_Parameters", "label": "Scaling_Paramet\ners", "title": "<b>Scaling_Parameters</b> (subnode)<hr>Discussion on the scaling of parameters w and b.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Discussion on the scaling of parameters w and b.", "node_type": "subnode"}, {"id": "Geometric_Margin", "label": "Geometric_Margi\nn", "title": "<b>Geometric_Margin</b> (subnode)<hr>Definition and importance of geometric margin in training sets.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Definition and importance of geometric margin in training sets.", "node_type": "subnode"}, {"id": "Maximize_Geometric_Margin", "label": "Maximize_Geomet\nric_Margin", "title": "<b>Maximize_Geometric_Margin</b> (subnode)<hr>Formulation of optimization problem to maximize geometric margin.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Formulation of optimization problem to maximize geometric margin.", "node_type": "subnode"}, {"id": "Support Vector Machine (SVM)", "label": "Support Vector\nMachine (SVM)", "title": "<b>Support Vector Machine (SVM)</b> (major)<hr>A supervised learning model for classification and regression analysis.", "shape": "star", "size": 25, "color": "#FF6347", "description": "A supervised learning model for classification and regression analysis.", "node_type": "major"}, {"id": "Optimization Problem in SVM", "label": "Optimization\nProblem in SVM", "title": "<b>Optimization Problem in SVM</b> (subnode)<hr>The core mathematical problem to find the optimal hyperplane.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "The core mathematical problem to find the optimal hyperplane.", "node_type": "subnode"}, {"id": "Non-Convex Constraint", "label": "Non-Convex\nConstraint", "title": "<b>Non-Convex Constraint</b> (subnode)<hr>Constraint that complicates optimization due to its non-convex nature.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Constraint that complicates optimization due to its non-convex nature.", "node_type": "subnode"}, {"id": "Scaling Constraint", "label": "Scaling\nConstraint", "title": "<b>Scaling Constraint</b> (subnode)<hr>A constraint used to simplify the original problem by setting functional margin to 1.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "A constraint used to simplify the original problem by setting functional margin to 1.", "node_type": "subnode"}, {"id": "Support_Vector_Machines_SVM", "label": "Support_Vector_\nMachines_SVM", "title": "<b>Support_Vector_Machines_SVM</b> (subnode)<hr>Technique to find optimal margin classifiers through optimization.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Technique to find optimal margin classifiers through optimization.", "node_type": "subnode"}, {"id": "Convex_Quadratic_Objective", "label": "Convex_Quadrati\nc_Objective", "title": "<b>Convex_Quadratic_Objective</b> (subnode)<hr>Objective function in SVMs that is a convex quadratic form.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Objective function in SVMs that is a convex quadratic form.", "node_type": "subnode"}, {"id": "Linear_Constraints", "label": "Linear_Constrai\nnts", "title": "<b>Linear_Constraints</b> (subnode)<hr>Constraints in the optimization problem are linear.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Constraints in the optimization problem are linear.", "node_type": "subnode"}, {"id": "Dual_Form_Optimization", "label": "Dual_Form_Optim\nization", "title": "<b>Dual_Form_Optimization</b> (subnode)<hr>Alternative form of the original problem that can be easier to solve.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Alternative form of the original problem that can be easier to solve.", "node_type": "subnode"}, {"id": "Kernels_High_Dimensional_Spaces", "label": "Kernels_High_Di\nmensional_Space\ns", "title": "<b>Kernels_High_Dimensional_Spaces</b> (subnode)<hr>Use kernels in SVMs for efficient computation in high dimensions.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Use kernels in SVMs for efficient computation in high dimensions.", "node_type": "subnode"}, {"id": "ConstrainedOptimization", "label": "ConstrainedOpti\nmization", "title": "<b>ConstrainedOptimization</b> (major)<hr>Generalization of Lagrange multipliers to include inequality constraints.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Generalization of Lagrange multipliers to include inequality constraints.", "node_type": "major"}, {"id": "LagrangeMultipliers", "label": "LagrangeMultipl\niers", "title": "<b>LagrangeMultipliers</b> (subnode)<hr>Coefficients used in the generalized Lagrangian for optimization problems.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Coefficients used in the generalized Lagrangian for optimization problems.", "node_type": "subnode"}, {"id": "PrimalProblem", "label": "PrimalProblem", "title": "<b>PrimalProblem</b> (subnode)<hr>Minimizing a function subject to inequality and equality constraints.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Minimizing a function subject to inequality and equality constraints.", "node_type": "subnode"}, {"id": "GeneralizedLagrangian", "label": "GeneralizedLagr\nangian", "title": "<b>GeneralizedLagrangian</b> (subnode)<hr>Combination of objective function with Lagrange multipliers for constraints.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Combination of objective function with Lagrange multipliers for constraints.", "node_type": "subnode"}, {"id": "ThetaP", "label": "ThetaP", "title": "<b>ThetaP</b> (subnode)<hr>Function representing the maximum value of the generalized Lagrangian under primal constraints.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Function representing the maximum value of the generalized Lagrangian under primal constraints.", "node_type": "subnode"}, {"id": "Primal Problem", "label": "Primal Problem", "title": "<b>Primal Problem</b> (major)<hr>Original optimization problem in machine learning.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Original optimization problem in machine learning.", "node_type": "major"}, {"id": "Objective Function Primal", "label": "Objective\nFunction Primal", "title": "<b>Objective Function Primal</b> (subnode)<hr>Function to be minimized or maximized in the primal problem.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Function to be minimized or maximized in the primal problem.", "node_type": "subnode"}, {"id": "Theta P", "label": "Theta P", "title": "<b>Theta P</b> (subnode)<hr>Indicator function for the primal constraints satisfaction.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Indicator function for the primal constraints satisfaction.", "node_type": "subnode"}, {"id": "Dual Problem", "label": "Dual Problem", "title": "<b>Dual Problem</b> (major)<hr>Optimization problem derived from the primal by exchanging max and min operations.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Optimization problem derived from the primal by exchanging max and min operations.", "node_type": "major"}, {"id": "Objective Function Dual", "label": "Objective\nFunction Dual", "title": "<b>Objective Function Dual</b> (subnode)<hr>Function to be maximized in the dual problem.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Function to be maximized in the dual problem.", "node_type": "subnode"}, {"id": "Theta D", "label": "Theta D", "title": "<b>Theta D</b> (subnode)<hr>Indicator function for the dual constraints satisfaction.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Indicator function for the dual constraints satisfaction.", "node_type": "subnode"}, {"id": "Relationship Between Primal and Dual", "label": "Relationship\nBetween Primal\nand Dual", "title": "<b>Relationship Between Primal and Dual</b> (major)<hr>Theoretical relationship between primal and dual problems in terms of their optimal values.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Theoretical relationship between primal and dual problems in terms of their optimal values.", "node_type": "major"}, {"id": "Optimization_Problems", "label": "Optimization_Pr\noblems", "title": "<b>Optimization_Problems</b> (subnode)<hr>Problems involving finding the best solution in a set of possible solutions.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Problems involving finding the best solution in a set of possible solutions.", "node_type": "subnode"}, {"id": "Primal_Dual_Pairing", "label": "Primal_Dual_Pai\nring", "title": "<b>Primal_Dual_Pairing</b> (subnode)<hr>Relationship between primal and dual optimization problems.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Relationship between primal and dual optimization problems.", "node_type": "subnode"}, {"id": "Dual_Problem_Solution", "label": "Dual_Problem_So\nlution", "title": "<b>Dual_Problem_Solution</b> (subnode)<hr>Conditions under which the solution to the dual problem equals that of the primal.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Conditions under which the solution to the dual problem equals that of the primal.", "node_type": "subnode"}, {"id": "Convexity_Conditions", "label": "Convexity_Condi\ntions", "title": "<b>Convexity_Conditions</b> (subnode)<hr>Requirements for functions f, g_i and h_i to ensure d* = p*.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Requirements for functions f, g_i and h_i to ensure d* = p*.", "node_type": "subnode"}, {"id": "Feasibility_Constraints", "label": "Feasibility_Con\nstraints", "title": "<b>Feasibility_Constraints</b> (subnode)<hr>Conditions ensuring the constraints are strictly feasible.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Conditions ensuring the constraints are strictly feasible.", "node_type": "subnode"}, {"id": "KKT_Conditions", "label": "KKT_Conditions", "title": "<b>KKT_Conditions</b> (subnode)<hr>Set of necessary conditions for a solution to be optimal in constrained optimization problems.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Set of necessary conditions for a solution to be optimal in constrained optimization problems.", "node_type": "subnode"}, {"id": "Dual_Complementarity", "label": "Dual_Complement\narity", "title": "<b>Dual_Complementarity</b> (subnode)<hr>Condition indicating active constraints in the dual form of SVMs.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Condition indicating active constraints in the dual form of SVMs.", "node_type": "subnode"}, {"id": "Primal_Dual_Equivalence", "label": "Primal_Dual_Equ\nivalence", "title": "<b>Primal_Dual_Equivalence</b> (subnode)<hr>Equivalence between primal and dual optimization problems in SVM context.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Equivalence between primal and dual optimization problems in SVM context.", "node_type": "subnode"}, {"id": "Support_Vectors", "label": "Support_Vectors", "title": "<b>Support_Vectors</b> (subnode)<hr>Training examples that influence the optimal solution due to active constraints.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Training examples that influence the optimal solution due to active constraints.", "node_type": "subnode"}, {"id": "SupportVectorsConcept", "label": "SupportVectorsC\noncept", "title": "<b>SupportVectorsConcept</b> (major)<hr>Explanation of support vectors in machine learning problems.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Explanation of support vectors in machine learning problems.", "node_type": "major"}, {"id": "KernelTrickIntroduction", "label": "KernelTrickIntr\noduction", "title": "<b>KernelTrickIntroduction</b> (major)<hr>Preview of the kernel trick application in algorithms.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Preview of the kernel trick application in algorithms.", "node_type": "major"}, {"id": "LagrangianFormulation", "label": "LagrangianFormu\nlation", "title": "<b>LagrangianFormulation</b> (major)<hr>Description and formulation of Lagrangian for optimization problem.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Description and formulation of Lagrangian for optimization problem.", "node_type": "major"}, {"id": "DualProblemDerivation", "label": "DualProblemDeri\nvation", "title": "<b>DualProblemDerivation</b> (subnode)<hr>Steps to derive the dual form from the Lagrangian.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Steps to derive the dual form from the Lagrangian.", "node_type": "subnode"}, {"id": "Lagrangian Function", "label": "Lagrangian\nFunction", "title": "<b>Lagrangian Function</b> (subnode)<hr>Used to incorporate constraints into the optimization problem.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Used to incorporate constraints into the optimization problem.", "node_type": "subnode"}, {"id": "Dual Optimization Problem", "label": "Dual\nOptimization\nProblem", "title": "<b>Dual Optimization Problem</b> (subnode)<hr>Formulated by transforming the primal problem to a dual form for easier solving.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Formulated by transforming the primal problem to a dual form for easier solving.", "node_type": "subnode"}, {"id": "KKT Conditions", "label": "KKT Conditions", "title": "<b>KKT Conditions</b> (subnode)<hr>Conditions that must be satisfied at an optimal solution in constrained optimization problems.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Conditions that must be satisfied at an optimal solution in constrained optimization problems.", "node_type": "subnode"}, {"id": "Optimal Parameters w and b", "label": "Optimal\nParameters w\nand b", "title": "<b>Optimal Parameters w and b</b> (subnode)<hr>Parameters derived from solving the dual problem to define the decision boundary.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Parameters derived from solving the dual problem to define the decision boundary.", "node_type": "subnode"}, {"id": "Machine Learning Models", "label": "Machine\nLearning Models", "title": "<b>Machine Learning Models</b> (major)<hr>Overview of various machine learning models and their applications.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Overview of various machine learning models and their applications.", "node_type": "major"}, {"id": "Support Vector Machines (SVM)", "label": "Support Vector\nMachines (SVM)", "title": "<b>Support Vector Machines (SVM)</b> (subnode)<hr>A model that efficiently learns in high-dimensional spaces using kernels.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "A model that efficiently learns in high-dimensional spaces using kernels.", "node_type": "subnode"}, {"id": "Optimal Parameters Calculation", "label": "Optimal\nParameters\nCalculation", "title": "<b>Optimal Parameters Calculation</b> (subnode)<hr>Calculation of optimal parameters w and b for SVM.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Calculation of optimal parameters w and b for SVM.", "node_type": "subnode"}, {"id": "Dual Form Optimization", "label": "Dual Form\nOptimization", "title": "<b>Dual Form Optimization</b> (subnode)<hr>Derivation using dual form to find optimal alpha values.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Derivation using dual form to find optimal alpha values.", "node_type": "subnode"}, {"id": "Prediction with Inner Products", "label": "Prediction with\nInner Products", "title": "<b>Prediction with Inner Products</b> (subnode)<hr>Using inner products for prediction based on support vectors.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Using inner products for prediction based on support vectors.", "node_type": "subnode"}, {"id": "Regularization and Non-Separable Data", "label": "Regularization\nand Non-\nSeparable Data", "title": "<b>Regularization and Non-Separable Data</b> (subnode)<hr>Handling non-separable data with regularization techniques.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Handling non-separable data with regularization techniques.", "node_type": "subnode"}, {"id": "Support Vector Machines", "label": "Support Vector\nMachines", "title": "<b>Support Vector Machines</b> (major)<hr>Algorithm for learning in high-dimensional spaces.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Algorithm for learning in high-dimensional spaces.", "node_type": "major"}, {"id": "Regularization and Non-Separable Case", "label": "Regularization\nand Non-\nSeparable Case", "title": "<b>Regularization and Non-Separable Case</b> (subnode)<hr>Handling non-linearly separable datasets with regularization.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Handling non-linearly separable datasets with regularization.", "node_type": "subnode"}, {"id": "Linear Separability Assumption", "label": "Linear\nSeparability\nAssumption", "title": "<b>Linear Separability Assumption</b> (subnode)<hr>Initial SVM derivation assumes linear separability of data.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Initial SVM derivation assumes linear separability of data.", "node_type": "subnode"}, {"id": "Outlier Sensitivity", "label": "Outlier\nSensitivity", "title": "<b>Outlier Sensitivity</b> (subnode)<hr>SVMs can be sensitive to outliers in the dataset.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "SVMs can be sensitive to outliers in the dataset.", "node_type": "subnode"}, {"id": "L1 Regularization", "label": "L1\nRegularization", "title": "<b>L1 Regularization</b> (subnode)<hr>Introduces L1 regularization to handle non-linear separability and reduce outlier impact.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Introduces L1 regularization to handle non-linear separability and reduce outlier impact.", "node_type": "subnode"}, {"id": "Optimization Problem", "label": "Optimization\nProblem", "title": "<b>Optimization Problem</b> (subnode)<hr>Formulates the optimization problem with L1 regularization term.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Formulates the optimization problem with L1 regularization term.", "node_type": "subnode"}, {"id": "Objective Function", "label": "Objective\nFunction", "title": "<b>Objective Function</b> (subnode)<hr>Minimizes a function that includes both norm of w and sum of slack variables weighted by C.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Minimizes a function that includes both norm of w and sum of slack variables weighted by C.", "node_type": "subnode"}, {"id": "Slack Variables", "label": "Slack Variables", "title": "<b>Slack Variables</b> (subnode)<hr>Introduces slack variables to allow for some data points to be within the margin.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Introduces slack variables to allow for some data points to be within the margin.", "node_type": "subnode"}, {"id": "Parameter C", "label": "Parameter C", "title": "<b>Parameter C</b> (subnode)<hr>Controls trade-off between maximizing margin and minimizing classification errors.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Controls trade-off between maximizing margin and minimizing classification errors.", "node_type": "subnode"}, {"id": "Lagrangian Formulation", "label": "Lagrangian\nFormulation", "title": "<b>Lagrangian Formulation</b> (subnode)<hr>Derives the Lagrangian to solve the optimization problem with constraints.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Derives the Lagrangian to solve the optimization problem with constraints.", "node_type": "subnode"}, {"id": "Dual_Problem_Formulation", "label": "Dual_Problem_Fo\nrmulation", "title": "<b>Dual_Problem_Formulation</b> (subnode)<hr>Formulating the optimization problem in SVMs.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Formulating the optimization problem in SVMs.", "node_type": "subnode"}, {"id": "Lagrange_Multipliers", "label": "Lagrange_Multip\nliers", "title": "<b>Lagrange_Multipliers</b> (subnode)<hr>Use of Lagrange multipliers to solve constrained optimization problems.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Use of Lagrange multipliers to solve constrained optimization problems.", "node_type": "subnode"}, {"id": "Sequential_Minimal_Optimization_SMO", "label": "Sequential_Mini\nmal_Optimizatio\nn_SMO", "title": "<b>Sequential_Minimal_Optimization_SMO</b> (subnode)<hr>Efficient algorithm for solving SVM's dual problem.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Efficient algorithm for solving SVM's dual problem.", "node_type": "subnode"}, {"id": "Coordinate_Ascend_Algorithm", "label": "Coordinate_Asce\nnd_Algorithm", "title": "<b>Coordinate_Ascend_Algorithm</b> (major)<hr>Optimization technique used in various machine learning contexts.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Optimization technique used in various machine learning contexts.", "node_type": "major"}, {"id": "Unconstrained_Optimization_Problem", "label": "Unconstrained_O\nptimization_Pro\nblem", "title": "<b>Unconstrained_Optimization_Problem</b> (subnode)<hr>Maximizing a function over multiple parameters without constraints.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Maximizing a function over multiple parameters without constraints.", "node_type": "subnode"}, {"id": "Gradient_Ascend_Newtons_Method", "label": "Gradient_Ascend\n_Newtons_Method", "title": "<b>Gradient_Ascend_Newtons_Method</b> (subnode)<hr>Alternative optimization methods compared to coordinate ascent.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Alternative optimization methods compared to coordinate ascent.", "node_type": "subnode"}, {"id": "Coordinate_Ascend_Method", "label": "Coordinate_Asce\nnd_Method", "title": "<b>Coordinate_Ascend_Method</b> (subnode)<hr>A method for optimizing functions by moving along one axis at a time.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "A method for optimizing functions by moving along one axis at a time.", "node_type": "subnode"}, {"id": "Support_Vector_Machines_SVMs", "label": "Support_Vector_\nMachines_SVMs", "title": "<b>Support_Vector_Machines_SVMs</b> (subnode)<hr>Models that use support vectors to classify data in high-dimensional spaces.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Models that use support vectors to classify data in high-dimensional spaces.", "node_type": "subnode"}, {"id": "SVM_Dual_Problem", "label": "SVM_Dual_Proble\nm", "title": "<b>SVM_Dual_Problem</b> (subnode)<hr>The dual form of the optimization problem for SVMs.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "The dual form of the optimization problem for SVMs.", "node_type": "subnode"}, {"id": "Heuristic_Selection", "label": "Heuristic_Selec\ntion", "title": "<b>Heuristic_Selection</b> (subnode)<hr>Process of selecting alpha pairs to optimize efficiency.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Process of selecting alpha pairs to optimize efficiency.", "node_type": "subnode"}, {"id": "Efficient_Update", "label": "Efficient_Updat\ne", "title": "<b>Efficient_Update</b> (subnode)<hr>Derivation and explanation of efficient update mechanism.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Derivation and explanation of efficient update mechanism.", "node_type": "subnode"}, {"id": "Constraints_Satisfaction", "label": "Constraints_Sat\nisfaction", "title": "<b>Constraints_Satisfaction</b> (subnode)<hr>Explanation of constraints satisfaction for alpha values.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Explanation of constraints satisfaction for alpha values.", "node_type": "subnode"}, {"id": "Alpha_Parameters", "label": "Alpha_Parameter\ns", "title": "<b>Alpha_Parameters</b> (subnode)<hr>Parameters \u03b11 and \u03b12 with constraints.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Parameters \u03b11 and \u03b12 with constraints.", "node_type": "subnode"}, {"id": "Constraint_Equation", "label": "Constraint_Equa\ntion", "title": "<b>Constraint_Equation</b> (subnode)<hr>Equation \u03b11y^(1) + \u03b12y^(2) = \u03c7.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Equation \u03b11y^(1) + \u03b12y^(2) = \u03c7.", "node_type": "subnode"}, {"id": "Bounds_L_H", "label": "Bounds_L_H", "title": "<b>Bounds_L_H</b> (subnode)<hr>Lower-bound L and upper-bound H for permissible values of \u03b12.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Lower-bound L and upper-bound H for permissible values of \u03b12.", "node_type": "subnode"}, {"id": "Objective_Function_W", "label": "Objective_Funct\nion_W", "title": "<b>Objective_Function_W</b> (subnode)<hr>Objective function W(\u03b1) expressed in terms of \u03b12 and other parameters.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Objective function W(\u03b1) expressed in terms of \u03b12 and other parameters.", "node_type": "subnode"}, {"id": "Quadratic_Formulation", "label": "Quadratic_Formu\nlation", "title": "<b>Quadratic_Formulation</b> (subnode)<hr>W as a quadratic function in \u03b12: a\u03b12^2 + b\u03b12 + c.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "W as a quadratic function in \u03b12: a\u03b12^2 + b\u03b12 + c.", "node_type": "subnode"}, {"id": "Maximization_Process", "label": "Maximization_Pr\nocess", "title": "<b>Maximization_Process</b> (subnode)<hr>Process of maximizing W by setting derivative to zero and solving.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Process of maximizing W by setting derivative to zero and solving.", "node_type": "subnode"}, {"id": "Sequential Minimal Optimization (SMO) Algorithm", "label": "Sequential\nMinimal\nOptimization\n(SMO) Algorithm", "title": "<b>Sequential Minimal Optimization (SMO) Algorithm</b> (subnode)<hr>Efficient algorithm for solving the optimization problem in SVM.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Efficient algorithm for solving the optimization problem in SVM.", "node_type": "subnode"}, {"id": "Alpha Updates", "label": "Alpha Updates", "title": "<b>Alpha Updates</b> (subnode)<hr>Process of updating alpha values within SMO.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Process of updating alpha values within SMO.", "node_type": "subnode"}, {"id": "Deep Learning Introduction", "label": "Deep Learning\nIntroduction", "title": "<b>Deep Learning Introduction</b> (major)<hr>Introduction to deep learning concepts and neural networks.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Introduction to deep learning concepts and neural networks.", "node_type": "major"}, {"id": "Supervised Learning with Non-Linear Models", "label": "Supervised\nLearning with\nNon-Linear\nModels", "title": "<b>Supervised Learning with Non-Linear Models</b> (subnode)<hr>Exploration of non-linear models in supervised learning context.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Exploration of non-linear models in supervised learning context.", "node_type": "subnode"}, {"id": "NonLinearModel", "label": "NonLinearModel", "title": "<b>NonLinearModel</b> (subnode)<hr>Abstract non-linear model used in machine learning problems.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Abstract non-linear model used in machine learning problems.", "node_type": "subnode"}, {"id": "TrainingExamples", "label": "TrainingExample\ns", "title": "<b>TrainingExamples</b> (subnode)<hr>Set of training examples used to define the cost function.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Set of training examples used to define the cost function.", "node_type": "subnode"}, {"id": "RegressionProblems", "label": "RegressionProbl\nems", "title": "<b>RegressionProblems</b> (subnode)<hr>Introduction to regression problems where output is a real number.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Introduction to regression problems where output is a real number.", "node_type": "subnode"}, {"id": "LeastSquareCostFunction", "label": "LeastSquareCost\nFunction", "title": "<b>LeastSquareCostFunction</b> (subnode)<hr>Definition of the least square cost function for individual examples.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Definition of the least square cost function for individual examples.", "node_type": "subnode"}, {"id": "MeanSquaredLoss", "label": "MeanSquaredLoss", "title": "<b>MeanSquaredLoss</b> (subnode)<hr>Definition and properties of the mean squared loss function over a dataset.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Definition and properties of the mean squared loss function over a dataset.", "node_type": "subnode"}, {"id": "BinaryClassification", "label": "BinaryClassific\nation", "title": "<b>BinaryClassification</b> (subnode)<hr>Introduction to binary classification problems in machine learning.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Introduction to binary classification problems in machine learning.", "node_type": "subnode"}, {"id": "LogitFunction", "label": "LogitFunction", "title": "<b>LogitFunction</b> (subnode)<hr>Linear combination of input features and weights before applying the logistic function.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Linear combination of input features and weights before applying the logistic function.", "node_type": "subnode"}, {"id": "ProbabilityPrediction", "label": "ProbabilityPred\niction", "title": "<b>ProbabilityPrediction</b> (subnode)<hr>Transformation of logit to a probability using the sigmoid function.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Transformation of logit to a probability using the sigmoid function.", "node_type": "subnode"}, {"id": "NegativeLikelihoodLoss", "label": "NegativeLikelih\noodLoss", "title": "<b>NegativeLikelihoodLoss</b> (subnode)<hr>Loss function used for logistic regression, based on negative log-likelihood.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Loss function used for logistic regression, based on negative log-likelihood.", "node_type": "subnode"}, {"id": "TotalLossFunction", "label": "TotalLossFuncti\non", "title": "<b>TotalLossFunction</b> (subnode)<hr>Average of individual training example losses over the entire dataset.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Average of individual training example losses over the entire dataset.", "node_type": "subnode"}, {"id": "MultiClassClassification", "label": "MultiClassClass\nification", "title": "<b>MultiClassClassification</b> (major)<hr>Extension of binary classification to multiple classes using softmax function.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Extension of binary classification to multiple classes using softmax function.", "node_type": "major"}, {"id": "NegativeLogLikelihoodLossMulticlass", "label": "NegativeLogLike\nlihoodLossMulti\nclass", "title": "<b>NegativeLogLikelihoodLossMulticlass</b> (subnode)<hr>Loss function used in multi-class classification based on negative log-likelihood.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Loss function used in multi-class classification based on negative log-likelihood.", "node_type": "subnode"}, {"id": "Loss Function", "label": "Loss Function", "title": "<b>Loss Function</b> (major)<hr>Function measuring model performance for a single training example or average over all examples.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Function measuring model performance for a single training example or average over all examples.", "node_type": "major"}, {"id": "Negative Log-Likelihood", "label": "Negative Log-\nLikelihood", "title": "<b>Negative Log-Likelihood</b> (subnode)<hr>Specific form of loss function used in probabilistic models.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Specific form of loss function used in probabilistic models.", "node_type": "subnode"}, {"id": "Cross-Entropy Loss", "label": "Cross-Entropy\nLoss", "title": "<b>Cross-Entropy Loss</b> (subnode)<hr>Simplified notation for negative log-likelihood loss function.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Simplified notation for negative log-likelihood loss function.", "node_type": "subnode"}, {"id": "Average Loss Function", "label": "Average Loss\nFunction", "title": "<b>Average Loss Function</b> (subnode)<hr>Total loss averaged over all training examples.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Total loss averaged over all training examples.", "node_type": "subnode"}, {"id": "Conditional Probabilistic Models", "label": "Conditional\nProbabilistic\nModels", "title": "<b>Conditional Probabilistic Models</b> (major)<hr>Models where the distribution of output depends on input features.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Models where the distribution of output depends on input features.", "node_type": "major"}, {"id": "Exponential Family Distribution", "label": "Exponential\nFamily\nDistribution", "title": "<b>Exponential Family Distribution</b> (subnode)<hr>Distribution family for conditional probabilistic models with exponential form.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Distribution family for conditional probabilistic models with exponential form.", "node_type": "subnode"}, {"id": "Optimizers", "label": "Optimizers", "title": "<b>Optimizers</b> (major)<hr>Discussion on the impact of optimizers on model generalization.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Discussion on the impact of optimizers on model generalization.", "node_type": "major"}, {"id": "Gradient Descent (GD)", "label": "Gradient\nDescent (GD)", "title": "<b>Gradient Descent (GD)</b> (subnode)<hr>Algorithm that updates parameters in the direction of steepest descent of loss function.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Algorithm that updates parameters in the direction of steepest descent of loss function.", "node_type": "subnode"}, {"id": "Stochastic Gradient Descent (SGD)", "label": "Stochastic\nGradient\nDescent (SGD)", "title": "<b>Stochastic Gradient Descent (SGD)</b> (subnode)<hr>Variant of GD using a single training example for each update to speed up convergence and reduce computational cost.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Variant of GD using a single training example for each update to speed up convergence and reduce computational cost.", "node_type": "subnode"}, {"id": "Mini-batch Stochastic Gradient Descent", "label": "Mini-batch\nStochastic\nGradient\nDescent", "title": "<b>Mini-batch Stochastic Gradient Descent</b> (subnode)<hr>Variant of SGD where gradients are computed over small batches of data for efficiency.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Variant of SGD where gradients are computed over small batches of data for efficiency.", "node_type": "subnode"}, {"id": "Hyperparameters", "label": "Hyperparameters", "title": "<b>Hyperparameters</b> (subnode)<hr>Parameters like learning rate and number of iterations that control the optimization process.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Parameters like learning rate and number of iterations that control the optimization process.", "node_type": "subnode"}, {"id": "Initialization", "label": "Initialization", "title": "<b>Initialization</b> (subnode)<hr>Different initializations can lead to different generalization performance.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Different initializations can lead to different generalization performance.", "node_type": "subnode"}, {"id": "Backpropagation Algorithm", "label": "Backpropagation\nAlgorithm", "title": "<b>Backpropagation Algorithm</b> (subnode)<hr>Method for efficiently computing gradients of loss functions in neural networks.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Method for efficiently computing gradients of loss functions in neural networks.", "node_type": "subnode"}, {"id": "NeuralNetworks", "label": "NeuralNetworks", "title": "<b>NeuralNetworks</b> (subnode)<hr>Description and formal representation of neural networks used for prediction.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Description and formal representation of neural networks used for prediction.", "node_type": "subnode"}, {"id": "RegressionProblem", "label": "RegressionProbl\nem", "title": "<b>RegressionProblem</b> (subnode)<hr>Explanation of regression problems in the context of neural networks.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Explanation of regression problems in the context of neural networks.", "node_type": "subnode"}, {"id": "SingleNeuronNN", "label": "SingleNeuronNN", "title": "<b>SingleNeuronNN</b> (subnode)<hr>Introduction to neural networks with a single neuron focusing on parametrization functions.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Introduction to neural networks with a single neuron focusing on parametrization functions.", "node_type": "subnode"}, {"id": "HousingPricePrediction", "label": "HousingPricePre\ndiction", "title": "<b>HousingPricePrediction</b> (subnode)<hr>Example of using a single neuron network for predicting housing prices based on size.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Example of using a single neuron network for predicting housing prices based on size.", "node_type": "subnode"}, {"id": "ReLUFunction", "label": "ReLUFunction", "title": "<b>ReLUFunction</b> (subnode)<hr>Introduction to the ReLU function used in neural networks to prevent negative outputs.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Introduction to the ReLU function used in neural networks to prevent negative outputs.", "node_type": "subnode"}, {"id": "Neural_Networks", "label": "Neural_Networks", "title": "<b>Neural_Networks</b> (subnode)<hr>System of algorithms designed to recognize patterns and extract features from complex data sets.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "System of algorithms designed to recognize patterns and extract features from complex data sets.", "node_type": "subnode"}, {"id": "Activation_Functions", "label": "Activation_Func\ntions", "title": "<b>Activation_Functions</b> (subnode)<hr>Functions that introduce non-linearity in the model, such as ReLU.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Functions that introduce non-linearity in the model, such as ReLU.", "node_type": "subnode"}, {"id": "ReLU", "label": "ReLU", "title": "<b>ReLU</b> (subnode)<hr>Rectified Linear Unit function used to activate neurons.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Rectified Linear Unit function used to activate neurons.", "node_type": "subnode"}, {"id": "Single_Neuron_Model", "label": "Single_Neuron_M\nodel", "title": "<b>Single_Neuron_Model</b> (subnode)<hr>A model with a single neuron and its mathematical representation.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "A model with a single neuron and its mathematical representation.", "node_type": "subnode"}, {"id": "Bias_and_Weights", "label": "Bias_and_Weight\ns", "title": "<b>Bias_and_Weights</b> (subnode)<hr>Explanation of bias term and weight vector in the context of neural networks.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Explanation of bias term and weight vector in the context of neural networks.", "node_type": "subnode"}, {"id": "Stacking_Neurons", "label": "Stacking_Neuron\ns", "title": "<b>Stacking_Neurons</b> (subnode)<hr>Process of combining multiple neurons to form a more complex network.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Process of combining multiple neurons to form a more complex network.", "node_type": "subnode"}, {"id": "Complex_Neural_Networks", "label": "Complex_Neural_\nNetworks", "title": "<b>Complex_Neural_Networks</b> (subnode)<hr>Building neural networks with multiple layers and neurons.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Building neural networks with multiple layers and neurons.", "node_type": "subnode"}, {"id": "DerivedFeatures", "label": "DerivedFeatures", "title": "<b>DerivedFeatures</b> (subnode)<hr>Description of family size, walkability, and school quality as derived features.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Description of family size, walkability, and school quality as derived features.", "node_type": "subnode"}, {"id": "FamilySize", "label": "FamilySize", "title": "<b>FamilySize</b> (subnode)<hr>Feature indicating the maximum number of people a house can accommodate.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Feature indicating the maximum number of people a house can accommodate.", "node_type": "subnode"}, {"id": "WalkableNeighborhood", "label": "WalkableNeighbo\nrhood", "title": "<b>WalkableNeighborhood</b> (subnode)<hr>Measure of how easily one can walk to local amenities such as grocery stores.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Measure of how easily one can walk to local amenities such as grocery stores.", "node_type": "subnode"}, {"id": "SchoolQuality", "label": "SchoolQuality", "title": "<b>SchoolQuality</b> (subnode)<hr>Indicator of the quality of the elementary school in a neighborhood.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Indicator of the quality of the elementary school in a neighborhood.", "node_type": "subnode"}, {"id": "InputFeatures", "label": "InputFeatures", "title": "<b>InputFeatures</b> (subnode)<hr>Set of input features (x1, x2, x3, x4) to the neural network.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Set of input features (x1, x2, x3, x4) to the neural network.", "node_type": "subnode"}, {"id": "HiddenUnits", "label": "HiddenUnits", "title": "<b>HiddenUnits</b> (subnode)<hr>Intermediate variables a1, a2, a3 representing derived features as hidden units in the neural network.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Intermediate variables a1, a2, a3 representing derived features as hidden units in the neural network.", "node_type": "subnode"}, {"id": "ReLUActivation", "label": "ReLUActivation", "title": "<b>ReLUActivation</b> (subnode)<hr>Rectified Linear Unit (ReLU) activation function used in neural networks.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Rectified Linear Unit (ReLU) activation function used in neural networks.", "node_type": "subnode"}, {"id": "NeuralNetworksInspiration", "label": "NeuralNetworksI\nnspiration", "title": "<b>NeuralNetworksInspiration</b> (subnode)<hr>Explanation of how artificial neural networks are inspired by biological ones.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Explanation of how artificial neural networks are inspired by biological ones.", "node_type": "subnode"}, {"id": "TwoLayerNN", "label": "TwoLayerNN", "title": "<b>TwoLayerNN</b> (subnode)<hr>A simple model consisting of an input layer, a hidden layer, and an output layer.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "A simple model consisting of an input layer, a hidden layer, and an output layer.", "node_type": "subnode"}, {"id": "ParametersTheta", "label": "ParametersTheta", "title": "<b>ParametersTheta</b> (subnode)<hr>Details on the parameters \u03b8 used in neural networks.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Details on the parameters \u03b8 used in neural networks.", "node_type": "subnode"}, {"id": "BiologicalSimilarity", "label": "BiologicalSimil\narity", "title": "<b>BiologicalSimilarity</b> (subnode)<hr>Discussion on similarities and differences between artificial and biological neural networks.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Discussion on similarities and differences between artificial and biological neural networks.", "node_type": "subnode"}, {"id": "PriorKnowledge", "label": "PriorKnowledge", "title": "<b>PriorKnowledge</b> (subnode)<hr>Explanation of the role of prior knowledge in constructing neural network models.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Explanation of the role of prior knowledge in constructing neural network models.", "node_type": "subnode"}, {"id": "FullyConnectedNN", "label": "FullyConnectedN\nN", "title": "<b>FullyConnectedNN</b> (subnode)<hr>A type of neural network where each neuron is connected to every neuron in the previous layer.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "A type of neural network where each neuron is connected to every neuron in the previous layer.", "node_type": "subnode"}, {"id": "IntermediateVariables", "label": "IntermediateVar\niables", "title": "<b>IntermediateVariables</b> (subnode)<hr>Variables like a1, a2, and a3 that are functions of input variables x1-x4.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Variables like a1, a2, and a3 that are functions of input variables x1-x4.", "node_type": "subnode"}, {"id": "Parameterization", "label": "Parameterizatio\nn", "title": "<b>Parameterization</b> (subnode)<hr>Generic parameterization using weights w and biases b for each neuron.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Generic parameterization using weights w and biases b for each neuron.", "node_type": "subnode"}, {"id": "Vectorization", "label": "Vectorization", "title": "<b>Vectorization</b> (subnode)<hr>Use of matrix and vector notations to simplify expressions for neural networks and improve computational efficiency.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Use of matrix and vector notations to simplify expressions for neural networks and improve computational efficiency.", "node_type": "subnode"}, {"id": "VectorizationInNN", "label": "VectorizationIn\nNN", "title": "<b>VectorizationInNN</b> (major)<hr>Overview of vectorization in neural networks for efficiency.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Overview of vectorization in neural networks for efficiency.", "node_type": "major"}, {"id": "SpeedPerspective", "label": "SpeedPerspectiv\ne", "title": "<b>SpeedPerspective</b> (subnode)<hr>Discussion on the importance of speed in implementing neural networks.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Discussion on the importance of speed in implementing neural networks.", "node_type": "subnode"}, {"id": "ParallelismGPUs", "label": "ParallelismGPUs", "title": "<b>ParallelismGPUs</b> (subnode)<hr>Role of parallel processing in GPUs for deep learning efficiency.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Role of parallel processing in GPUs for deep learning efficiency.", "node_type": "subnode"}, {"id": "MatrixAlgebra", "label": "MatrixAlgebra", "title": "<b>MatrixAlgebra</b> (subnode)<hr>Use of matrix algebra and optimized numerical packages in vectorization.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Use of matrix algebra and optimized numerical packages in vectorization.", "node_type": "subnode"}, {"id": "TwoLayerNetwork", "label": "TwoLayerNetwork", "title": "<b>TwoLayerNetwork</b> (subnode)<hr>Definition and vectorization of a two-layer fully-connected neural network.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Definition and vectorization of a two-layer fully-connected neural network.", "node_type": "subnode"}, {"id": "WeightMatrices", "label": "WeightMatrices", "title": "<b>WeightMatrices</b> (subnode)<hr>Matrix representation of weights in a neural network layer.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Matrix representation of weights in a neural network layer.", "node_type": "subnode"}, {"id": "BiasVectors", "label": "BiasVectors", "title": "<b>BiasVectors</b> (subnode)<hr>Vector representation of biases in a neural network layer.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Vector representation of biases in a neural network layer.", "node_type": "subnode"}, {"id": "ActivationFunctions", "label": "ActivationFunct\nions", "title": "<b>ActivationFunctions</b> (subnode)<hr>Non-linear functions applied element-wise to the output of a neuron or layer.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Non-linear functions applied element-wise to the output of a neuron or layer.", "node_type": "subnode"}, {"id": "LayerArchitecture", "label": "LayerArchitectu\nre", "title": "<b>LayerArchitecture</b> (subnode)<hr>Structure of layers including input, hidden, and output layers in a neural network.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Structure of layers including input, hidden, and output layers in a neural network.", "node_type": "subnode"}, {"id": "Multi-layer Neural Networks", "label": "Multi-layer\nNeural Networks", "title": "<b>Multi-layer Neural Networks</b> (major)<hr>A neural network with multiple layers of neurons connected fully to each other.", "shape": "star", "size": 25, "color": "#FF6347", "description": "A neural network with multiple layers of neurons connected fully to each other.", "node_type": "major"}, {"id": "Weight Matrices and Biases", "label": "Weight Matrices\nand Biases", "title": "<b>Weight Matrices and Biases</b> (subnode)<hr>Matrices and vectors that define the connections and biases in a multi-layer network.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Matrices and vectors that define the connections and biases in a multi-layer network.", "node_type": "subnode"}, {"id": "ReLU Activation Function", "label": "ReLU Activation\nFunction", "title": "<b>ReLU Activation Function</b> (subnode)<hr>A non-linear activation function used to introduce non-linearity into the model.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "A non-linear activation function used to introduce non-linearity into the model.", "node_type": "subnode"}, {"id": "Total Number of Neurons", "label": "Total Number of\nNeurons", "title": "<b>Total Number of Neurons</b> (subnode)<hr>The sum of neurons across all layers in a multi-layer network.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "The sum of neurons across all layers in a multi-layer network.", "node_type": "subnode"}, {"id": "Total Parameters Count", "label": "Total\nParameters\nCount", "title": "<b>Total Parameters Count</b> (subnode)<hr>Sum of weights and biases across all layers.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Sum of weights and biases across all layers.", "node_type": "subnode"}, {"id": "Notational Consistency", "label": "Notational\nConsistency", "title": "<b>Notational Consistency</b> (subnode)<hr>Consistent notation for inputs and outputs in multi-layer networks.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Consistent notation for inputs and outputs in multi-layer networks.", "node_type": "subnode"}, {"id": "Other Activation Functions", "label": "Other\nActivation\nFunctions", "title": "<b>Other Activation Functions</b> (major)<hr>Alternative non-linear functions used instead of ReLU in neural networks.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Alternative non-linear functions used instead of ReLU in neural networks.", "node_type": "major"}, {"id": "TanhFunction", "label": "TanhFunction", "title": "<b>TanhFunction</b> (subnode)<hr>Hyperbolic tangent function, similar to sigmoid but ranges from -1 to 1.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Hyperbolic tangent function, similar to sigmoid but ranges from -1 to 1.", "node_type": "subnode"}, {"id": "LeakyReLUFunction", "label": "LeakyReLUFuncti\non", "title": "<b>LeakyReLUFunction</b> (subnode)<hr>Variant of ReLU with a small slope for negative inputs.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Variant of ReLU with a small slope for negative inputs.", "node_type": "subnode"}, {"id": "GELUFunction", "label": "GELUFunction", "title": "<b>GELUFunction</b> (subnode)<hr>Gaussian Error Linear Unit, smooth non-linear function used in NLP models.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Gaussian Error Linear Unit, smooth non-linear function used in NLP models.", "node_type": "subnode"}, {"id": "SoftplusFunction", "label": "SoftplusFunctio\nn", "title": "<b>SoftplusFunction</b> (subnode)<hr>Smoothed ReLU variant with a proper second-order derivative.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Smoothed ReLU variant with a proper second-order derivative.", "node_type": "subnode"}, {"id": "IdentityFunction", "label": "IdentityFunctio\nn", "title": "<b>IdentityFunction</b> (subnode)<hr>Linear function where output is equal to input, not commonly used in neural networks.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Linear function where output is equal to input, not commonly used in neural networks.", "node_type": "subnode"}, {"id": "Feature_Engineering", "label": "Feature_Enginee\nring", "title": "<b>Feature_Engineering</b> (subnode)<hr>Process of selecting and transforming raw data into features for use in machine learning models.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Process of selecting and transforming raw data into features for use in machine learning models.", "node_type": "subnode"}, {"id": "Deep_Learning", "label": "Deep_Learning", "title": "<b>Deep_Learning</b> (subnode)<hr>Subfield of machine learning that uses neural networks to learn representations from data.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Subfield of machine learning that uses neural networks to learn representations from data.", "node_type": "subnode"}, {"id": "Feature_Maps", "label": "Feature_Maps", "title": "<b>Feature_Maps</b> (subnode)<hr>Functions used by deep learning models to transform input into a useful representation.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Functions used by deep learning models to transform input into a useful representation.", "node_type": "subnode"}, {"id": "Linear_Model", "label": "Linear_Model", "title": "<b>Linear_Model</b> (subnode)<hr>Model used to predict outcomes based on features generated by a neural network.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Model used to predict outcomes based on features generated by a neural network.", "node_type": "subnode"}, {"id": "Deep Learning Representations", "label": "Deep Learning\nRepresentations", "title": "<b>Deep Learning Representations</b> (major)<hr>Discusses how neural networks discover features for prediction.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Discusses how neural networks discover features for prediction.", "node_type": "major"}, {"id": "House Price Prediction Example", "label": "House Price\nPrediction\nExample", "title": "<b>House Price Prediction Example</b> (subnode)<hr>Illustrates use of fully-connected network in predicting house prices.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Illustrates use of fully-connected network in predicting house prices.", "node_type": "subnode"}, {"id": "Feature Discovery", "label": "Feature\nDiscovery", "title": "<b>Feature Discovery</b> (subnode)<hr>Explains how neural networks automatically discover useful features.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Explains how neural networks automatically discover useful features.", "node_type": "subnode"}, {"id": "Black Box Nature", "label": "Black Box\nNature", "title": "<b>Black Box Nature</b> (subnode)<hr>Highlights difficulty in interpreting the discovered features by humans.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Highlights difficulty in interpreting the discovered features by humans.", "node_type": "subnode"}, {"id": "Modern Neural Network Modules", "label": "Modern Neural\nNetwork Modules", "title": "<b>Modern Neural Network Modules</b> (major)<hr>Introduces various building blocks of modern neural networks.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Introduces various building blocks of modern neural networks.", "node_type": "major"}, {"id": "Matrix Multiplication Module", "label": "Matrix\nMultiplication\nModule", "title": "<b>Matrix Multiplication Module</b> (subnode)<hr>Describes the basic operation with parameters W and b.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Describes the basic operation with parameters W and b.", "node_type": "subnode"}, {"id": "Nonlinear Activation Module", "label": "Nonlinear\nActivation\nModule", "title": "<b>Nonlinear Activation Module</b> (subnode)<hr>Explains role of nonlinear activation in neural network architecture.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Explains role of nonlinear activation in neural network architecture.", "node_type": "subnode"}, {"id": "MLP Composition", "label": "MLP Composition", "title": "<b>MLP Composition</b> (subnode)<hr>Describes MLP as a composition of matrix multiplication and activation modules.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Describes MLP as a composition of matrix multiplication and activation modules.", "node_type": "subnode"}, {"id": "MLPArchitecture", "label": "MLPArchitecture", "title": "<b>MLPArchitecture</b> (subnode)<hr>Description of the architecture of a Multi-Layer Perceptron (MLP).", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Description of the architecture of a Multi-Layer Perceptron (MLP).", "node_type": "subnode"}, {"id": "MatrixMultiplicationModule", "label": "MatrixMultiplic\nationModule", "title": "<b>MatrixMultiplicationModule</b> (subnode)<hr>Component of MLP that involves matrix multiplication and parameter sets.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Component of MLP that involves matrix multiplication and parameter sets.", "node_type": "subnode"}, {"id": "NonlinearActivationModule", "label": "NonlinearActiva\ntionModule", "title": "<b>NonlinearActivationModule</b> (subnode)<hr>Component of MLP involving nonlinear activation functions such as ReLU, sigmoid, etc.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Component of MLP involving nonlinear activation functions such as ReLU, sigmoid, etc.", "node_type": "subnode"}, {"id": "ResNetOverview", "label": "ResNetOverview", "title": "<b>ResNetOverview</b> (major)<hr>Introduction to Residual Networks (ResNets) and their architecture.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Introduction to Residual Networks (ResNets) and their architecture.", "node_type": "major"}, {"id": "ResidualBlockDefinition", "label": "ResidualBlockDe\nfinition", "title": "<b>ResidualBlockDefinition</b> (subnode)<hr>Simplified definition of a residual block in ResNet using matrix multiplication and activation functions.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Simplified definition of a residual block in ResNet using matrix multiplication and activation functions.", "node_type": "subnode"}, {"id": "ResNetComposition", "label": "ResNetCompositi\non", "title": "<b>ResNetComposition</b> (subnode)<hr>Description of how ResNets are composed from multiple residual blocks followed by matrix multiplication.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Description of how ResNets are composed from multiple residual blocks followed by matrix multiplication.", "node_type": "subnode"}, {"id": "ResNetArchitecture", "label": "ResNetArchitect\nure", "title": "<b>ResNetArchitecture</b> (subnode)<hr>Introduction to Residual Network architecture.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Introduction to Residual Network architecture.", "node_type": "subnode"}, {"id": "ConvolutionalLayers", "label": "ConvolutionalLa\nyers", "title": "<b>ConvolutionalLayers</b> (subnode)<hr>Discussion on convolution layers used in ResNet.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Discussion on convolution layers used in ResNet.", "node_type": "subnode"}, {"id": "BatchNormalization", "label": "BatchNormalizat\nion", "title": "<b>BatchNormalization</b> (subnode)<hr>Explanation of batch normalization technique.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Explanation of batch normalization technique.", "node_type": "subnode"}, {"id": "LayerNormalization", "label": "LayerNormalizat\nion", "title": "<b>LayerNormalization</b> (major)<hr>Technique for normalizing the layer inputs in neural networks.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Technique for normalizing the layer inputs in neural networks.", "node_type": "major"}, {"id": "LN-SModule", "label": "LN-SModule", "title": "<b>LN-SModule</b> (subnode)<hr>Definition and formula for the sub-module LN-S of layer normalization.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Definition and formula for the sub-module LN-S of layer normalization.", "node_type": "subnode"}, {"id": "TransformerArchitecture", "label": "TransformerArch\nitecture", "title": "<b>TransformerArchitecture</b> (major)<hr>Overview of Transformer architecture including ResNet-S and layer normalization.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Overview of Transformer architecture including ResNet-S and layer normalization.", "node_type": "major"}, {"id": "LN-S", "label": "LN-S", "title": "<b>LN-S</b> (subnode)<hr>Standardized version of layer normalization before affine transformation.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Standardized version of layer normalization before affine transformation.", "node_type": "subnode"}, {"id": "AffineTransformation", "label": "AffineTransform\nation", "title": "<b>AffineTransformation</b> (subnode)<hr>Transforms the output to have desired mean and standard deviation using learnable parameters.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Transforms the output to have desired mean and standard deviation using learnable parameters.", "node_type": "subnode"}, {"id": "LearnableParameters", "label": "LearnableParame\nters", "title": "<b>LearnableParameters</b> (subnode)<hr>Scalars beta and gamma that are learned during training.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Scalars beta and gamma that are learned during training.", "node_type": "subnode"}, {"id": "ScalingInvariantProperty", "label": "ScalingInvarian\ntProperty", "title": "<b>ScalingInvariantProperty</b> (major)<hr>Property of layer normalization making the model invariant to parameter scaling.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Property of layer normalization making the model invariant to parameter scaling.", "node_type": "major"}, {"id": "MM_Wb", "label": "MM_Wb", "title": "<b>MM_Wb</b> (subnode)<hr>Matrix multiplication with weights W and bias b used in the proof of scaling invariance.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Matrix multiplication with weights W and bias b used in the proof of scaling invariance.", "node_type": "subnode"}, {"id": "Normalization Techniques", "label": "Normalization\nTechniques", "title": "<b>Normalization Techniques</b> (major)<hr>Techniques for normalizing data in machine learning.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Techniques for normalizing data in machine learning.", "node_type": "major"}, {"id": "Layer Normalization (LN)", "label": "Layer\nNormalization\n(LN)", "title": "<b>Layer Normalization (LN)</b> (subnode)<hr>Normalizes the inputs of each layer for stable training.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Normalizes the inputs of each layer for stable training.", "node_type": "subnode"}, {"id": "Equation 7.43", "label": "Equation 7.43", "title": "<b>Equation 7.43</b> (subnode)<hr>Mathematical representation of LN-S transformation.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Mathematical representation of LN-S transformation.", "node_type": "subnode"}, {"id": "Equation 7.44-7.47", "label": "Equation\n7.44-7.47", "title": "<b>Equation 7.44-7.47</b> (subnode)<hr>Series of equations showing properties and transformations in layer normalization.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Series of equations showing properties and transformations in layer normalization.", "node_type": "subnode"}, {"id": "Scale-Invariant Property", "label": "Scale-Invariant\nProperty", "title": "<b>Scale-Invariant Property</b> (subnode)<hr>Property indicating network stability under weight scaling except for the last layer.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Property indicating network stability under weight scaling except for the last layer.", "node_type": "subnode"}, {"id": "Batch Normalization (BN)", "label": "Batch\nNormalization\n(BN)", "title": "<b>Batch Normalization (BN)</b> (subnode)<hr>Normalizes intermediate layers, commonly used in computer vision.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Normalizes intermediate layers, commonly used in computer vision.", "node_type": "subnode"}, {"id": "Group Normalization", "label": "Group\nNormalization", "title": "<b>Group Normalization</b> (subnode)<hr>Alternative normalization method for fixed and controllable scaling.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Alternative normalization method for fixed and controllable scaling.", "node_type": "subnode"}, {"id": "Convolutional Layers", "label": "Convolutional\nLayers", "title": "<b>Convolutional Layers</b> (major)<hr>Layers in neural networks designed to process data with a grid-like topology.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Layers in neural networks designed to process data with a grid-like topology.", "node_type": "major"}, {"id": "1-D Convolution (Conv1D)", "label": "1-D Convolution\n(Conv1D)", "title": "<b>1-D Convolution (Conv1D)</b> (subnode)<hr>Simplified version of convolution layer for sequential data processing.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Simplified version of convolution layer for sequential data processing.", "node_type": "subnode"}, {"id": "Convolutional_Neural_Networks", "label": "Convolutional_N\neural_Networks", "title": "<b>Convolutional_Neural_Networks</b> (subnode)<hr>Introduction to convolutional neural networks in machine learning.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Introduction to convolutional neural networks in machine learning.", "node_type": "subnode"}, {"id": "1D_Convolution", "label": "1D_Convolution", "title": "<b>1D_Convolution</b> (subnode)<hr>Explanation of 1-dimensional convolution layers used in NLP.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Explanation of 1-dimensional convolution layers used in NLP.", "node_type": "subnode"}, {"id": "Simplified_1D_Convolution", "label": "Simplified_1D_C\nonvolution", "title": "<b>Simplified_1D_Convolution</b> (subnode)<hr>Description of a simplified version of the 1-D convolution layer, Conv1D-S.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Description of a simplified version of the 1-D convolution layer, Conv1D-S.", "node_type": "subnode"}, {"id": "Filter_Vector", "label": "Filter_Vector", "title": "<b>Filter_Vector</b> (subnode)<hr>Definition and properties of the filter vector used in Conv1D-S.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Definition and properties of the filter vector used in Conv1D-S.", "node_type": "subnode"}, {"id": "Bias_Scalar", "label": "Bias_Scalar", "title": "<b>Bias_Scalar</b> (subnode)<hr>Explanation of the bias scalar parameter in Conv1D-S.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Explanation of the bias scalar parameter in Conv1D-S.", "node_type": "subnode"}, {"id": "Matrix_Multiplication", "label": "Matrix_Multipli\ncation", "title": "<b>Matrix_Multiplication</b> (subnode)<hr>Representation of Conv1D-S as a matrix multiplication with shared parameters Qz.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Representation of Conv1D-S as a matrix multiplication with shared parameters Qz.", "node_type": "subnode"}, {"id": "Convolutional_Layers", "label": "Convolutional_L\nayers", "title": "<b>Convolutional_Layers</b> (subnode)<hr>Discussion on convolutional layers in neural networks.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Discussion on convolutional layers in neural networks.", "node_type": "subnode"}, {"id": "Parameter_Sharing", "label": "Parameter_Shari\nng", "title": "<b>Parameter_Sharing</b> (subnode)<hr>Explanation of parameter sharing in convolutional layers.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Explanation of parameter sharing in convolutional layers.", "node_type": "subnode"}, {"id": "Efficiency_of_Convolution", "label": "Efficiency_of_C\nonvolution", "title": "<b>Efficiency_of_Convolution</b> (subnode)<hr>Comparison of efficiency between convolution and generic matrix multiplication.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Comparison of efficiency between convolution and generic matrix multiplication.", "node_type": "subnode"}, {"id": "Conv1D_Channel_Variants", "label": "Conv1D_Channel_\nVariants", "title": "<b>Conv1D_Channel_Variants</b> (subnode)<hr>Discussion on the variants of Conv1D layers with multiple channels.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Discussion on the variants of Conv1D layers with multiple channels.", "node_type": "subnode"}, {"id": "Conv1D-S", "label": "Conv1D-S", "title": "<b>Conv1D-S</b> (subnode)<hr>One-dimensional convolutional module with distinct parameters for each channel.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "One-dimensional convolutional module with distinct parameters for each channel.", "node_type": "subnode"}, {"id": "TotalParametersConv1D", "label": "TotalParameters\nConv1D", "title": "<b>TotalParametersConv1D</b> (subnode)<hr>Calculation of total number of parameters in Conv1D model.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Calculation of total number of parameters in Conv1D model.", "node_type": "subnode"}, {"id": "LinearMappingComparison", "label": "LinearMappingCo\nmparison", "title": "<b>LinearMappingComparison</b> (subnode)<hr>Comparison with generic linear mapping parameter count.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Comparison with generic linear mapping parameter count.", "node_type": "subnode"}, {"id": "ParameterTensorRepresentation", "label": "ParameterTensor\nRepresentation", "title": "<b>ParameterTensorRepresentation</b> (subnode)<hr>Three-dimensional tensor representation of parameters.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Three-dimensional tensor representation of parameters.", "node_type": "subnode"}, {"id": "Conv2D-S", "label": "Conv2D-S", "title": "<b>Conv2D-S</b> (subnode)<hr>Two-dimensional convolutional module with distinct parameters for each channel.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Two-dimensional convolutional module with distinct parameters for each channel.", "node_type": "subnode"}, {"id": "TotalParametersConv2D", "label": "TotalParameters\nConv2D", "title": "<b>TotalParametersConv2D</b> (subnode)<hr>Calculation of total number of parameters in Conv2D model.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Calculation of total number of parameters in Conv2D model.", "node_type": "subnode"}, {"id": "Differentiable Circuit", "label": "Differentiable\nCircuit", "title": "<b>Differentiable Circuit</b> (subnode)<hr>Composition of arithmetic operations and elementary functions that can compute a real-valued function efficiently.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Composition of arithmetic operations and elementary functions that can compute a real-valued function efficiently.", "node_type": "subnode"}, {"id": "Gradient Computation", "label": "Gradient\nComputation", "title": "<b>Gradient Computation</b> (subnode)<hr>Efficient computation of gradients for differentiable circuits in O(N) time.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Efficient computation of gradients for differentiable circuits in O(N) time.", "node_type": "subnode"}, {"id": "Theorem 7.4.1", "label": "Theorem 7.4.1", "title": "<b>Theorem 7.4.1</b> (subnode)<hr>Informal statement of the theorem regarding gradient computation efficiency.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Informal statement of the theorem regarding gradient computation efficiency.", "node_type": "subnode"}, {"id": "Chain Rule", "label": "Chain Rule", "title": "<b>Chain Rule</b> (subnode)<hr>Mathematical rule for computing derivatives of composite functions.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Mathematical rule for computing derivatives of composite functions.", "node_type": "subnode"}, {"id": "Auto-differentiation", "label": "Auto-\ndifferentiation", "title": "<b>Auto-differentiation</b> (subnode)<hr>Automatic computation of gradients in deep learning frameworks.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Automatic computation of gradients in deep learning frameworks.", "node_type": "subnode"}, {"id": "Deep Learning Packages", "label": "Deep Learning\nPackages", "title": "<b>Deep Learning Packages</b> (subnode)<hr>Software libraries and frameworks used for implementing deep learning models.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Software libraries and frameworks used for implementing deep learning models.", "node_type": "subnode"}, {"id": "MLPs (Multilayer Perceptrons)", "label": "MLPs\n(Multilayer\nPerceptrons)", "title": "<b>MLPs (Multilayer Perceptrons)</b> (subnode)<hr>Feedforward neural networks with multiple layers of perceptrons.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Feedforward neural networks with multiple layers of perceptrons.", "node_type": "subnode"}, {"id": "Partial_Derivatives", "label": "Partial_Derivat\nives", "title": "<b>Partial_Derivatives</b> (subnode)<hr>Discussion on partial derivatives and their complexities.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Discussion on partial derivatives and their complexities.", "node_type": "subnode"}, {"id": "Chain_Rule", "label": "Chain_Rule", "title": "<b>Chain_Rule</b> (subnode)<hr>Explanation of the chain rule in calculus for auto-differentiation.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Explanation of the chain rule in calculus for auto-differentiation.", "node_type": "subnode"}, {"id": "Scalar_Variable_J", "label": "Scalar_Variable\n_J", "title": "<b>Scalar_Variable_J</b> (subnode)<hr>Description of scalar variable J as a composition of functions f and g on z.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Description of scalar variable J as a composition of functions f and g on z.", "node_type": "subnode"}, {"id": "Vectorized_Notation", "label": "Vectorized_Nota\ntion", "title": "<b>Vectorized_Notation</b> (subnode)<hr>Explanation of the chain rule in vectorized notation for vectors z and u.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Explanation of the chain rule in vectorized notation for vectors z and u.", "node_type": "subnode"}, {"id": "Machine_Learning_Backward_Propagation", "label": "Machine_Learnin\ng_Backward_Prop\nagation", "title": "<b>Machine_Learning_Backward_Propagation</b> (major)<hr>Overview of backward propagation in machine learning", "shape": "star", "size": 25, "color": "#FF6347", "description": "Overview of backward propagation in machine learning", "node_type": "major"}, {"id": "Backward_Function_Linear_Map", "label": "Backward_Functi\non_Linear_Map", "title": "<b>Backward_Function_Linear_Map</b> (subnode)<hr>Explanation of the backward function as a linear map", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Explanation of the backward function as a linear map", "node_type": "subnode"}, {"id": "Jacobian_Matrix_Transpose", "label": "Jacobian_Matrix\n_Transpose", "title": "<b>Jacobian_Matrix_Transpose</b> (subnode)<hr>The matrix in equation (7.54) is the transpose of Jacobian matrix", "shape": "dot", "size": 15, "color": "#4682B4", "description": "The matrix in equation (7.54) is the transpose of Jacobian matrix", "node_type": "subnode"}, {"id": "Complexity_of_Jacobian_Matrices", "label": "Complexity_of_J\nacobian_Matrice\ns", "title": "<b>Complexity_of_Jacobian_Matrices</b> (subnode)<hr>Avoiding complications with Jacobian matrices for tensors", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Avoiding complications with Jacobian matrices for tensors", "node_type": "subnode"}, {"id": "Equation_7.53_Usefulness", "label": "Equation_7.53_U\nsefulness", "title": "<b>Equation_7.53_Usefulness</b> (subnode)<hr>Explanation of the convenience and effectiveness of equation (7.53)", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Explanation of the convenience and effectiveness of equation (7.53)", "node_type": "subnode"}, {"id": "Derivations_Section_7.4.3", "label": "Derivations_Sec\ntion_7.4.3", "title": "<b>Derivations_Section_7.4.3</b> (subnode)<hr>Use of equation (7.53) in derivations", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Use of equation (7.53) in derivations", "node_type": "subnode"}, {"id": "Chain_Rule_Interpretation", "label": "Chain_Rule_Inte\nrpretation", "title": "<b>Chain_Rule_Interpretation</b> (subnode)<hr>Interpreting the chain rule for computing partial derivatives", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Interpreting the chain rule for computing partial derivatives", "node_type": "subnode"}, {"id": "Loss Function Composition", "label": "Loss Function\nComposition", "title": "<b>Loss Function Composition</b> (subnode)<hr>Abstract representation of loss functions as compositions of modules.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Abstract representation of loss functions as compositions of modules.", "node_type": "subnode"}, {"id": "Modules", "label": "Modules", "title": "<b>Modules</b> (subnode)<hr>Building blocks such as MM, \u03c3, Conv2D, LN used in neural networks.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Building blocks such as MM, \u03c3, Conv2D, LN used in neural networks.", "node_type": "subnode"}, {"id": "BinaryClassificationProblem", "label": "BinaryClassific\nationProblem", "title": "<b>BinaryClassificationProblem</b> (subnode)<hr>A specific problem involving binary classification using a neural network model.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "A specific problem involving binary classification using a neural network model.", "node_type": "subnode"}, {"id": "MLPModelDefinition", "label": "MLPModelDefinit\nion", "title": "<b>MLPModelDefinition</b> (subnode)<hr>Definition of the MLP model used in the example, including parameters and layers.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Definition of the MLP model used in the example, including parameters and layers.", "node_type": "subnode"}, {"id": "LossFunctionFormulation", "label": "LossFunctionFor\nmulation", "title": "<b>LossFunctionFormulation</b> (subnode)<hr>Description of how the loss function is formulated for a binary classification problem.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Description of how the loss function is formulated for a binary classification problem.", "node_type": "subnode"}, {"id": "ModulesInMLP", "label": "ModulesInMLP", "title": "<b>ModulesInMLP</b> (subnode)<hr>Explanation of different modules in an MLP, including linear and activation layers.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Explanation of different modules in an MLP, including linear and activation layers.", "node_type": "subnode"}, {"id": "ParameterizationOfModules", "label": "Parameterizatio\nnOfModules", "title": "<b>ParameterizationOfModules</b> (subnode)<hr>Discussion on how each module is parameterized or may involve fixed operations.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Discussion on how each module is parameterized or may involve fixed operations.", "node_type": "subnode"}, {"id": "ForwardPass", "label": "ForwardPass", "title": "<b>ForwardPass</b> (subnode)<hr>Description of the forward pass, where intermediate variables are computed and saved.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Description of the forward pass, where intermediate variables are computed and saved.", "node_type": "subnode"}, {"id": "BackwardPass", "label": "BackwardPass", "title": "<b>BackwardPass</b> (subnode)<hr>Explanation of the backward pass, involving computation of derivatives w.r.t. parameters and intermediate variables.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Explanation of the backward pass, involving computation of derivatives w.r.t. parameters and intermediate variables.", "node_type": "subnode"}, {"id": "Machine_Learning_Backpropagation", "label": "Machine_Learnin\ng_Backpropagati\non", "title": "<b>Machine_Learning_Backpropagation</b> (major)<hr>Overview of backpropagation in machine learning.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Overview of backpropagation in machine learning.", "node_type": "major"}, {"id": "Chain_Rule_Application", "label": "Chain_Rule_Appl\nication", "title": "<b>Chain_Rule_Application</b> (subnode)<hr>Application of the chain rule to compute gradients efficiently.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Application of the chain rule to compute gradients efficiently.", "node_type": "subnode"}, {"id": "Efficient_Backward_Propagation", "label": "Efficient_Backw\nard_Propagation", "title": "<b>Efficient_Backward_Propagation</b> (subnode)<hr>Discussion on the efficiency of backward propagation in neural networks.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Discussion on the efficiency of backward propagation in neural networks.", "node_type": "subnode"}, {"id": "NeuralNetworksComposition", "label": "NeuralNetworksC\nomposition", "title": "<b>NeuralNetworksComposition</b> (subnode)<hr>Viewing neural networks as compositions of atomic operations.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Viewing neural networks as compositions of atomic operations.", "node_type": "subnode"}, {"id": "BackpropagationDiscussion", "label": "Backpropagation\nDiscussion", "title": "<b>BackpropagationDiscussion</b> (subnode)<hr>Discussion on backpropagation and its role in computing gradients.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Discussion on backpropagation and its role in computing gradients.", "node_type": "subnode"}, {"id": "ModulesInPractice", "label": "ModulesInPracti\nce", "title": "<b>ModulesInPractice</b> (subnode)<hr>Practical modularization of neural networks using basic modules like matrix multiplication.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Practical modularization of neural networks using basic modules like matrix multiplication.", "node_type": "subnode"}, {"id": "BackwardFunctionsBasics", "label": "BackwardFunctio\nnsBasics", "title": "<b>BackwardFunctionsBasics</b> (major)<hr>Introduction to computing backward functions for basic modules in machine learning models.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Introduction to computing backward functions for basic modules in machine learning models.", "node_type": "major"}, {"id": "LossFunctionBackward", "label": "LossFunctionBac\nkward", "title": "<b>LossFunctionBackward</b> (subnode)<hr>Details on the backward function computation for loss functions.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Details on the backward function computation for loss functions.", "node_type": "subnode"}, {"id": "BackwardFunctionWb", "label": "BackwardFunctio\nnWb", "title": "<b>BackwardFunctionWb</b> (subnode)<hr>Explanation of the backward function for parameters W and b.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Explanation of the backward function for parameters W and b.", "node_type": "subnode"}, {"id": "VectorizedNotation", "label": "VectorizedNotat\nion", "title": "<b>VectorizedNotation</b> (subnode)<hr>Expression in vectorized form for clarity.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Expression in vectorized form for clarity.", "node_type": "subnode"}, {"id": "EfficiencyConsiderations", "label": "EfficiencyConsi\nderations", "title": "<b>EfficiencyConsiderations</b> (subnode)<hr>Discussion on computational efficiency of the backward function.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Discussion on computational efficiency of the backward function.", "node_type": "subnode"}, {"id": "BackwardFunctionActivations", "label": "BackwardFunctio\nnActivations", "title": "<b>BackwardFunctionActivations</b> (subnode)<hr>Explanation of the backward function for element-wise activations.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Explanation of the backward function for element-wise activations.", "node_type": "subnode"}, {"id": "Machine_Learning_Backward_Functions", "label": "Machine_Learnin\ng_Backward_Func\ntions", "title": "<b>Machine_Learning_Backward_Functions</b> (major)<hr>Overview of backward functions in machine learning modules", "shape": "star", "size": 25, "color": "#FF6347", "description": "Overview of backward functions in machine learning modules", "node_type": "major"}, {"id": "Efficiency_of_Backward_Pass", "label": "Efficiency_of_B\nackward_Pass", "title": "<b>Efficiency_of_Backward_Pass</b> (subnode)<hr>Discussion on the computational efficiency of the backward pass", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Discussion on the computational efficiency of the backward pass", "node_type": "subnode"}, {"id": "Vectorized_Notation_Backward_Func", "label": "Vectorized_Nota\ntion_Backward_F\nunc", "title": "<b>Vectorized_Notation_Backward_Func</b> (subnode)<hr>Explanation of vectorized notation for backward functions", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Explanation of vectorized notation for backward functions", "node_type": "subnode"}, {"id": "Squared_Loss_Backward", "label": "Squared_Loss_Ba\nckward", "title": "<b>Squared_Loss_Backward</b> (subnode)<hr>Derivation and explanation of the backward function for squared loss", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Derivation and explanation of the backward function for squared loss", "node_type": "subnode"}, {"id": "Logistic_Loss_Backward", "label": "Logistic_Loss_B\nackward", "title": "<b>Logistic_Loss_Backward</b> (subnode)<hr>Explanation of the backward function for logistic loss", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Explanation of the backward function for logistic loss", "node_type": "subnode"}, {"id": "Cross_Entropy_Loss_Backward", "label": "Cross_Entropy_L\noss_Backward", "title": "<b>Cross_Entropy_Loss_Backward</b> (subnode)<hr>Explanation of the backward function for cross-entropy loss", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Explanation of the backward function for cross-entropy loss", "node_type": "subnode"}, {"id": "Loss Functions", "label": "Loss Functions", "title": "<b>Loss Functions</b> (subnode)<hr>Different loss functions used in machine learning models.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Different loss functions used in machine learning models.", "node_type": "subnode"}, {"id": "Logistic Loss", "label": "Logistic Loss", "title": "<b>Logistic Loss</b> (subnode)<hr>The logistic loss function and its gradient calculation.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "The logistic loss function and its gradient calculation.", "node_type": "subnode"}, {"id": "Backpropagation for MLPs", "label": "Backpropagation\nfor MLPs", "title": "<b>Backpropagation for MLPs</b> (major)<hr>Explanation of backpropagation in multi-layer perceptrons (MLPs).", "shape": "star", "size": 25, "color": "#FF6347", "description": "Explanation of backpropagation in multi-layer perceptrons (MLPs).", "node_type": "major"}, {"id": "Forward Pass", "label": "Forward Pass", "title": "<b>Forward Pass</b> (subnode)<hr>Sequence of operations during the forward pass through an r-layer MLP.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Sequence of operations during the forward pass through an r-layer MLP.", "node_type": "subnode"}, {"id": "Intermediate Values Storage", "label": "Intermediate\nValues Storage", "title": "<b>Intermediate Values Storage</b> (subnode)<hr>Storing intermediate values for gradient computation in the backward pass.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Storing intermediate values for gradient computation in the backward pass.", "node_type": "subnode"}, {"id": "Vectorization Over Training Examples", "label": "Vectorization\nOver Training\nExamples", "title": "<b>Vectorization Over Training Examples</b> (major)<hr>Techniques to vectorize neural network computations over multiple training examples.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Techniques to vectorize neural network computations over multiple training examples.", "node_type": "major"}, {"id": "TrainingSetExamples", "label": "TrainingSetExam\nples", "title": "<b>TrainingSetExamples</b> (subnode)<hr>Explanation of training set examples and their representation.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Explanation of training set examples and their representation.", "node_type": "subnode"}, {"id": "MatrixNotation", "label": "MatrixNotation", "title": "<b>MatrixNotation</b> (subnode)<hr>Use of matrix notation in representing multiple training examples.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Use of matrix notation in representing multiple training examples.", "node_type": "subnode"}, {"id": "LayerActivations", "label": "LayerActivation\ns", "title": "<b>LayerActivations</b> (subnode)<hr>First-layer activations for each example using matrix operations.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "First-layer activations for each example using matrix operations.", "node_type": "subnode"}, {"id": "VectorizationTechniques", "label": "VectorizationTe\nchniques", "title": "<b>VectorizationTechniques</b> (subnode)<hr>Techniques to vectorize operations in machine learning models.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Techniques to vectorize operations in machine learning models.", "node_type": "subnode"}, {"id": "Broadcasting", "label": "Broadcasting", "title": "<b>Broadcasting</b> (subnode)<hr>Explanation of broadcasting technique for adding bias terms.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Explanation of broadcasting technique for adding bias terms.", "node_type": "subnode"}, {"id": "LayerGeneralization", "label": "LayerGeneraliza\ntion", "title": "<b>LayerGeneralization</b> (subnode)<hr>Discussion on generalizing matricization approach to multiple layers.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Discussion on generalizing matricization approach to multiple layers.", "node_type": "subnode"}, {"id": "Machine Learning", "label": "Machine\nLearning", "title": "<b>Machine Learning</b> (major)<hr>Field of study focusing on algorithms that learn from and make predictions on data.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Field of study focusing on algorithms that learn from and make predictions on data.", "node_type": "major"}, {"id": "Matricization Approach", "label": "Matricization\nApproach", "title": "<b>Matricization Approach</b> (subnode)<hr>Technique for organizing data in matrix form to facilitate multi-layer neural network implementation.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Technique for organizing data in matrix form to facilitate multi-layer neural network implementation.", "node_type": "subnode"}, {"id": "Data Matrix Representation", "label": "Data Matrix\nRepresentation", "title": "<b>Data Matrix Representation</b> (subnode)<hr>Representation of data points as rows in a matrix, differing from theoretical notation.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Representation of data points as rows in a matrix, differing from theoretical notation.", "node_type": "subnode"}, {"id": "Conversion Between Representations", "label": "Conversion\nBetween\nRepresentations", "title": "<b>Conversion Between Representations</b> (subnode)<hr>Process to convert between row-major and column-major representations in deep learning implementations.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Process to convert between row-major and column-major representations in deep learning implementations.", "node_type": "subnode"}, {"id": "Training Loss Function", "label": "Training Loss\nFunction", "title": "<b>Training Loss Function</b> (subnode)<hr>Function used during training to minimize error between predicted and actual outcomes.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Function used during training to minimize error between predicted and actual outcomes.", "node_type": "subnode"}, {"id": "Training_Loss", "label": "Training_Loss", "title": "<b>Training_Loss</b> (subnode)<hr>Evaluation metric for training data, often mean squared error.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Evaluation metric for training data, often mean squared error.", "node_type": "subnode"}, {"id": "Test_Error", "label": "Test_Error", "title": "<b>Test_Error</b> (subnode)<hr>Most important evaluation metric on unseen test examples.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Most important evaluation metric on unseen test examples.", "node_type": "subnode"}, {"id": "Empirical_Distribution", "label": "Empirical_Distr\nibution", "title": "<b>Empirical_Distribution</b> (subnode)<hr>Distribution based on the training dataset, denoted by \u02dc\u0394.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Distribution based on the training dataset, denoted by \u02dc\u0394.", "node_type": "subnode"}, {"id": "Population_Distribution", "label": "Population_Dist\nribution", "title": "<b>Population_Distribution</b> (subnode)<hr>True distribution of data from which test examples are drawn, denoted by D.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "True distribution of data from which test examples are drawn, denoted by D.", "node_type": "subnode"}, {"id": "Training_Data_Set", "label": "Training_Data_S\net", "title": "<b>Training_Data_Set</b> (subnode)<hr>Data used to train the model, seen during training.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Data used to train the model, seen during training.", "node_type": "subnode"}, {"id": "Test_Data_Set", "label": "Test_Data_Set", "title": "<b>Test_Data_Set</b> (subnode)<hr>Unseen data used for evaluating model performance.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Unseen data used for evaluating model performance.", "node_type": "subnode"}, {"id": "Learning_Settings", "label": "Learning_Settin\ngs", "title": "<b>Learning_Settings</b> (subnode)<hr>Settings under which a model learns from data.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Settings under which a model learns from data.", "node_type": "subnode"}, {"id": "Training_Distribution", "label": "Training_Distri\nbution", "title": "<b>Training_Distribution</b> (subnode)<hr>The distribution of training examples in machine learning.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "The distribution of training examples in machine learning.", "node_type": "subnode"}, {"id": "Test_Distribution", "label": "Test_Distributi\non", "title": "<b>Test_Distribution</b> (subnode)<hr>The distribution of test examples used to evaluate model performance.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "The distribution of test examples used to evaluate model performance.", "node_type": "subnode"}, {"id": "Domain_Shift", "label": "Domain_Shift", "title": "<b>Domain_Shift</b> (subnode)<hr>Situation where training and test distributions differ.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Situation where training and test distributions differ.", "node_type": "subnode"}, {"id": "Overfitting_Underfitting", "label": "Overfitting_Und\nerfitting", "title": "<b>Overfitting_Underfitting</b> (subnode)<hr>Concepts describing model performance on unseen data.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Concepts describing model performance on unseen data.", "node_type": "subnode"}, {"id": "Test_Error_Training_Error", "label": "Test_Error_Trai\nning_Error", "title": "<b>Test_Error_Training_Error</b> (subnode)<hr>Difference between errors measured on training and test datasets.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Difference between errors measured on training and test datasets.", "node_type": "subnode"}, {"id": "Generalization_Gap", "label": "Generalization_\nGap", "title": "<b>Generalization_Gap</b> (subnode)<hr>The difference between the test error and training error.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "The difference between the test error and training error.", "node_type": "subnode"}, {"id": "Bias_Variance_Tradoff", "label": "Bias_Variance_T\nradoff", "title": "<b>Bias_Variance_Tradoff</b> (major)<hr>Analysis of model performance in terms of bias and variance.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Analysis of model performance in terms of bias and variance.", "node_type": "major"}, {"id": "Model_Parameterizations", "label": "Model_Parameter\nizations", "title": "<b>Model_Parameterizations</b> (subnode)<hr>Impact of parameter choices on test error decomposition.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Impact of parameter choices on test error decomposition.", "node_type": "subnode"}, {"id": "Bias-Variance Tradeoff", "label": "Bias-Variance\nTradeoff", "title": "<b>Bias-Variance Tradeoff</b> (subnode)<hr>Formalization and discussion of bias-variance tradeoff in machine learning models.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Formalization and discussion of bias-variance tradeoff in machine learning models.", "node_type": "subnode"}, {"id": "Double Descent Phenomenon", "label": "Double Descent\nPhenomenon", "title": "<b>Double Descent Phenomenon</b> (subnode)<hr>Describes the unexpected decrease in test error after an initial increase with model complexity or data size.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Describes the unexpected decrease in test error after an initial increase with model complexity or data size.", "node_type": "subnode"}, {"id": "Training and Test Datasets", "label": "Training and\nTest Datasets", "title": "<b>Training and Test Datasets</b> (subnode)<hr>Illustrates the use of training and test datasets to evaluate model performance.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Illustrates the use of training and test datasets to evaluate model performance.", "node_type": "subnode"}, {"id": "Linear Regression Example", "label": "Linear\nRegression\nExample", "title": "<b>Linear Regression Example</b> (subnode)<hr>Provides an example using linear regression models on a quadratic function dataset.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Provides an example using linear regression models on a quadratic function dataset.", "node_type": "subnode"}, {"id": "Model Complexity", "label": "Model\nComplexity", "title": "<b>Model Complexity</b> (subnode)<hr>Discusses the impact of model complexity on underfitting and overfitting.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Discusses the impact of model complexity on underfitting and overfitting.", "node_type": "subnode"}, {"id": "Machine_Learning_Bias_Variance_Tradeoff", "label": "Machine_Learnin\ng_Bias_Variance\n_Tradeoff", "title": "<b>Machine_Learning_Bias_Variance_Tradeoff</b> (major)<hr>Exploration of bias and variance in machine learning hypothesis classes.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Exploration of bias and variance in machine learning hypothesis classes.", "node_type": "major"}, {"id": "Linear_Model_Failure", "label": "Linear_Model_Fa\nilure", "title": "<b>Linear_Model_Failure</b> (subnode)<hr>Failure of linear models to capture data structure despite large training sets.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Failure of linear models to capture data structure despite large training sets.", "node_type": "subnode"}, {"id": "Bias_Definition", "label": "Bias_Definition", "title": "<b>Bias_Definition</b> (subnode)<hr>Definition and implications of model bias in the context of fitting errors.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Definition and implications of model bias in the context of fitting errors.", "node_type": "subnode"}, {"id": "5th_Degree_Polynomial_Failure", "label": "5th_Degree_Poly\nnomial_Failure", "title": "<b>5th_Degree_Polynomial_Failure</b> (subnode)<hr>Failure of 5th-degree polynomials to generalize despite capturing training data well.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Failure of 5th-degree polynomials to generalize despite capturing training data well.", "node_type": "subnode"}, {"id": "Generalization_Error", "label": "Generalization_\nError", "title": "<b>Generalization_Error</b> (subnode)<hr>Probability that a hypothesis will misclassify new data drawn from the same distribution.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Probability that a hypothesis will misclassify new data drawn from the same distribution.", "node_type": "subnode"}, {"id": "PolynomialFitting", "label": "PolynomialFitti\nng", "title": "<b>PolynomialFitting</b> (subnode)<hr>Discussion on fitting polynomials to data sets.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Discussion on fitting polynomials to data sets.", "node_type": "subnode"}, {"id": "Variance", "label": "Variance", "title": "<b>Variance</b> (subnode)<hr>Description of variance in model fitting procedures.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Description of variance in model fitting procedures.", "node_type": "subnode"}, {"id": "BiasVsVarianceTradeoff", "label": "BiasVsVarianceT\nradeoff", "title": "<b>BiasVsVarianceTradeoff</b> (subnode)<hr>Discussion on the trade-off between bias and variance in models.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Discussion on the trade-off between bias and variance in models.", "node_type": "subnode"}, {"id": "Test Error Decomposition", "label": "Test Error\nDecomposition", "title": "<b>Test Error Decomposition</b> (subnode)<hr>Decomposes test error into bias and variance components.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Decomposes test error into bias and variance components.", "node_type": "subnode"}, {"id": "Mathematical Decomposition for Regression", "label": "Mathematical\nDecomposition\nfor Regression", "title": "<b>Mathematical Decomposition for Regression</b> (major)<hr>Formal mathematical description of bias-variance tradeoff in regression problems.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Formal mathematical description of bias-variance tradeoff in regression problems.", "node_type": "major"}, {"id": "BiasVarianceTradeoff", "label": "BiasVarianceTra\ndeoff", "title": "<b>BiasVarianceTradeoff</b> (subnode)<hr>Exploration of the trade-off between model complexity and error types.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Exploration of the trade-off between model complexity and error types.", "node_type": "subnode"}, {"id": "TrainingDataset", "label": "TrainingDataset", "title": "<b>TrainingDataset</b> (subnode)<hr>Description of a training dataset for regression problems.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Description of a training dataset for regression problems.", "node_type": "subnode"}, {"id": "TestExample", "label": "TestExample", "title": "<b>TestExample</b> (subnode)<hr>Explanation of how to use a test example in the context of model evaluation.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Explanation of how to use a test example in the context of model evaluation.", "node_type": "subnode"}, {"id": "MSE", "label": "MSE", "title": "<b>MSE</b> (subnode)<hr>Definition and calculation of Mean Squared Error (MSE) for evaluating models.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Definition and calculation of Mean Squared Error (MSE) for evaluating models.", "node_type": "subnode"}, {"id": "Claim8.1.1", "label": "Claim8.1.1", "title": "<b>Claim8.1.1</b> (subnode)<hr>Mathematical claim used to decompose MSE into bias and variance terms.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Mathematical claim used to decompose MSE into bias and variance terms.", "node_type": "subnode"}, {"id": "MSEDecomposition", "label": "MSEDecompositio\nn", "title": "<b>MSEDecomposition</b> (subnode)<hr>Breakdown of Mean Squared Error into bias and variance components.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Breakdown of Mean Squared Error into bias and variance components.", "node_type": "subnode"}, {"id": "AverageModel", "label": "AverageModel", "title": "<b>AverageModel</b> (subnode)<hr>Definition and properties of the average model in machine learning theory.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Definition and properties of the average model in machine learning theory.", "node_type": "subnode"}, {"id": "BiasTerm", "label": "BiasTerm", "title": "<b>BiasTerm</b> (subnode)<hr>Explanation of bias as a component of error due to model limitations.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Explanation of bias as a component of error due to model limitations.", "node_type": "subnode"}, {"id": "VarianceTerm", "label": "VarianceTerm", "title": "<b>VarianceTerm</b> (subnode)<hr>Explanation of variance as the variability in predictions across different datasets.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Explanation of variance as the variability in predictions across different datasets.", "node_type": "subnode"}, {"id": "Machine_Learning_Bias_Variance", "label": "Machine_Learnin\ng_Bias_Variance", "title": "<b>Machine_Learning_Bias_Variance</b> (major)<hr>Overview of bias and variance in machine learning models.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Overview of bias and variance in machine learning models.", "node_type": "major"}, {"id": "Bias_Term", "label": "Bias_Term", "title": "<b>Bias_Term</b> (subnode)<hr>Explains the concept of bias term in model approximation.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Explains the concept of bias term in model approximation.", "node_type": "subnode"}, {"id": "Variance_Term", "label": "Variance_Term", "title": "<b>Variance_Term</b> (subnode)<hr>Describes how variance captures errors due to dataset randomness.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Describes how variance captures errors due to dataset randomness.", "node_type": "subnode"}, {"id": "Noise_Prediction_Impact", "label": "Noise_Predictio\nn_Impact", "title": "<b>Noise_Prediction_Impact</b> (subnode)<hr>Discusses the impact of noise prediction limitations.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Discusses the impact of noise prediction limitations.", "node_type": "subnode"}, {"id": "Bias_Variance_Classification", "label": "Bias_Variance_C\nlassification", "title": "<b>Bias_Variance_Classification</b> (subnode)<hr>Notes on bias-variance decomposition for classification problems.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Notes on bias-variance decomposition for classification problems.", "node_type": "subnode"}, {"id": "Double_Descent_Phenomenon", "label": "Double_Descent_\nPhenomenon", "title": "<b>Double_Descent_Phenomenon</b> (major)<hr>Introduction to the double descent phenomenon in machine learning models.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Introduction to the double descent phenomenon in machine learning models.", "node_type": "major"}, {"id": "Model_Wise_Double_Descent", "label": "Model_Wise_Doub\nle_Descent", "title": "<b>Model_Wise_Double_Descent</b> (subnode)<hr>Explains model-wise double descent observed in various ML models.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Explains model-wise double descent observed in various ML models.", "node_type": "subnode"}, {"id": "Model-wise Double Descent", "label": "Model-wise\nDouble Descent", "title": "<b>Model-wise Double Descent</b> (subnode)<hr>Test error decreases, increases, then decreases again as model parameters exceed training data size.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Test error decreases, increases, then decreases again as model parameters exceed training data size.", "node_type": "subnode"}, {"id": "Sample-wise Double Descent", "label": "Sample-wise\nDouble Descent", "title": "<b>Sample-wise Double Descent</b> (subnode)<hr>Similar to model-wise but considers the effect of increasing sample size on test error.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Similar to model-wise but considers the effect of increasing sample size on test error.", "node_type": "subnode"}, {"id": "Overparameterized Models", "label": "Overparameteriz\ned Models", "title": "<b>Overparameterized Models</b> (subnode)<hr>Models with more parameters than necessary, often used in deep learning.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Models with more parameters than necessary, often used in deep learning.", "node_type": "subnode"}, {"id": "Historical Context", "label": "Historical\nContext", "title": "<b>Historical Context</b> (subnode)<hr>Early work by Opper and recent popularization by Belkin et al., Hastie et al.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Early work by Opper and recent popularization by Belkin et al., Hastie et al.", "node_type": "subnode"}, {"id": "Optimal Regularization", "label": "Optimal\nRegularization", "title": "<b>Optimal Regularization</b> (subnode)<hr>Improves performance by tuning regularization parameters effectively.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Improves performance by tuning regularization parameters effectively.", "node_type": "subnode"}, {"id": "Implicit Regularization", "label": "Implicit\nRegularization", "title": "<b>Implicit Regularization</b> (subnode)<hr>Effect of optimizers like gradient descent in overparameterized models.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Effect of optimizers like gradient descent in overparameterized models.", "node_type": "subnode"}, {"id": "Regularization Techniques", "label": "Regularization\nTechniques", "title": "<b>Regularization Techniques</b> (subnode)<hr>Techniques to prevent overfitting such as L2 regularization.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Techniques to prevent overfitting such as L2 regularization.", "node_type": "subnode"}, {"id": "Parameter Count vs. Model Norm", "label": "Parameter Count\nvs. Model Norm", "title": "<b>Parameter Count vs. Model Norm</b> (subnode)<hr>Comparison between using number of parameters and norm as measures of model complexity.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Comparison between using number of parameters and norm as measures of model complexity.", "node_type": "subnode"}, {"id": "Linear Regression Setup", "label": "Linear\nRegression\nSetup", "title": "<b>Linear Regression Setup</b> (subnode)<hr>Setup for linear regression with specific dataset and input/output configurations.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Setup for linear regression with specific dataset and input/output configurations.", "node_type": "subnode"}, {"id": "Sample Complexity Bounds", "label": "Sample\nComplexity\nBounds", "title": "<b>Sample Complexity Bounds</b> (major)<hr>Theoretical bounds on the number of samples required for learning algorithms to generalize well.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Theoretical bounds on the number of samples required for learning algorithms to generalize well.", "node_type": "major"}, {"id": "Model Selection Methods", "label": "Model Selection\nMethods", "title": "<b>Model Selection Methods</b> (subnode)<hr>Methods for selecting the appropriate model complexity based on training data performance.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Methods for selecting the appropriate model complexity based on training data performance.", "node_type": "subnode"}, {"id": "Generalization Error", "label": "Generalization\nError", "title": "<b>Generalization Error</b> (subnode)<hr>Discussion on how well a model performs on unseen data compared to training set accuracy.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Discussion on how well a model performs on unseen data compared to training set accuracy.", "node_type": "subnode"}, {"id": "Learning Theory Proofs", "label": "Learning Theory\nProofs", "title": "<b>Learning Theory Proofs</b> (major)<hr>Conditions and lemmas to prove learning algorithms work well.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Conditions and lemmas to prove learning algorithms work well.", "node_type": "major"}, {"id": "Union Bound Lemma", "label": "Union Bound\nLemma", "title": "<b>Union Bound Lemma</b> (subnode)<hr>Probability bound for union of events.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Probability bound for union of events.", "node_type": "subnode"}, {"id": "Hoeffding Inequality", "label": "Hoeffding\nInequality", "title": "<b>Hoeffding Inequality</b> (subnode)<hr>Bound on deviation between sample mean and true probability.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Bound on deviation between sample mean and true probability.", "node_type": "subnode"}, {"id": "Training Set", "label": "Training Set", "title": "<b>Training Set</b> (subnode)<hr>Set of input-output pairs used for training the model.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Set of input-output pairs used for training the model.", "node_type": "subnode"}, {"id": "Binary_Classification", "label": "Binary_Classifi\ncation", "title": "<b>Binary_Classification</b> (subnode)<hr>Classification where labels are binary (0 or 1).", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Classification where labels are binary (0 or 1).", "node_type": "subnode"}, {"id": "Hypothesis", "label": "Hypothesis", "title": "<b>Hypothesis</b> (subnode)<hr>A function that maps input data to predicted labels.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "A function that maps input data to predicted labels.", "node_type": "subnode"}, {"id": "Training_Error", "label": "Training_Error", "title": "<b>Training_Error</b> (subnode)<hr>Error calculated on the training dataset used to optimize model parameters.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Error calculated on the training dataset used to optimize model parameters.", "node_type": "subnode"}, {"id": "PAC_Assumptions", "label": "PAC_Assumptions", "title": "<b>PAC_Assumptions</b> (subnode)<hr>Probably approximately correct assumptions in learning theory.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Probably approximately correct assumptions in learning theory.", "node_type": "subnode"}, {"id": "Linear_Classification", "label": "Linear_Classifi\ncation", "title": "<b>Linear_Classification</b> (subnode)<hr>Classification using linear functions to separate data into classes.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Classification using linear functions to separate data into classes.", "node_type": "subnode"}, {"id": "Empirical_Risk_Minimization", "label": "Empirical_Risk_\nMinimization", "title": "<b>Empirical_Risk_Minimization</b> (subnode)<hr>Process of selecting a hypothesis with the smallest training error from a set of hypotheses.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Process of selecting a hypothesis with the smallest training error from a set of hypotheses.", "node_type": "subnode"}, {"id": "Hypothesis_Class", "label": "Hypothesis_Clas\ns", "title": "<b>Hypothesis_Class</b> (subnode)<hr>Set of all classifiers considered by a learning algorithm, abstracting from specific parameterization.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Set of all classifiers considered by a learning algorithm, abstracting from specific parameterization.", "node_type": "subnode"}, {"id": "Finite_Hypothesis_Class", "label": "Finite_Hypothes\nis_Class", "title": "<b>Finite_Hypothesis_Class</b> (subnode)<hr>Case where hypothesis class consists of finite number of hypotheses for easier analysis.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Case where hypothesis class consists of finite number of hypotheses for easier analysis.", "node_type": "subnode"}, {"id": "Hoeffding_Inequality", "label": "Hoeffding_Inequ\nality", "title": "<b>Hoeffding_Inequality</b> (subnode)<hr>Statistical tool used to bound the probability that the observed error deviates from true error by more than a specified amount.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Statistical tool used to bound the probability that the observed error deviates from true error by more than a specified amount.", "node_type": "subnode"}, {"id": "Bernoulli_Random_Variables", "label": "Bernoulli_Rando\nm_Variables", "title": "<b>Bernoulli_Random_Variables</b> (subnode)<hr>Random variables indicating whether a hypothesis misclassifies an example drawn from distribution D.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Random variables indicating whether a hypothesis misclassifies an example drawn from distribution D.", "node_type": "subnode"}, {"id": "Uniform_Convergence", "label": "Uniform_Converg\nence", "title": "<b>Uniform_Convergence</b> (major)<hr>Property ensuring that training error closely approximates generalization error for all hypotheses in the hypothesis space.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Property ensuring that training error closely approximates generalization error for all hypotheses in the hypothesis space.", "node_type": "major"}, {"id": "Training_Error_Generalization_Error_Difference", "label": "Training_Error_\nGeneralization_\nError_Differenc\ne", "title": "<b>Training_Error_Generalization_Error_Difference</b> (subnode)<hr>Difference between training and generalization errors for a hypothesis set", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Difference between training and generalization errors for a hypothesis set", "node_type": "subnode"}, {"id": "Union_Bound_Application", "label": "Union_Bound_App\nlication", "title": "<b>Union_Bound_Application</b> (subnode)<hr>Application of the union bound to extend individual hypothesis error bounds to all hypotheses", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Application of the union bound to extend individual hypothesis error bounds to all hypotheses", "node_type": "subnode"}, {"id": "Probability_Error_Bounds", "label": "Probability_Err\nor_Bounds", "title": "<b>Probability_Error_Bounds</b> (subnode)<hr>Bounds on the probability that training error differs from generalization error by more than gamma", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Bounds on the probability that training error differs from generalization error by more than gamma", "node_type": "subnode"}, {"id": "Quantities_of_Interest", "label": "Quantities_of_I\nnterest", "title": "<b>Quantities_of_Interest</b> (major)<hr>Three key quantities in machine learning: n (sample size), \u03b3 (error margin), and \u03b4 (probability of error)", "shape": "star", "size": 25, "color": "#FF6347", "description": "Three key quantities in machine learning: n (sample size), \u03b3 (error margin), and \u03b4 (probability of error)", "node_type": "major"}, {"id": "Sample_Size_Calculation", "label": "Sample_Size_Cal\nculation", "title": "<b>Sample_Size_Calculation</b> (subnode)<hr>Calculation to determine necessary sample size for a given probability of error bound", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Calculation to determine necessary sample size for a given probability of error bound", "node_type": "subnode"}, {"id": "Error_Margin_Determination", "label": "Error_Margin_De\ntermination", "title": "<b>Error_Margin_Determination</b> (subnode)<hr>Determining the margin \u03b3 based on desired confidence level and sample size", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Determining the margin \u03b3 based on desired confidence level and sample size", "node_type": "subnode"}, {"id": "Machine_Learning_Theory", "label": "Machine_Learnin\ng_Theory", "title": "<b>Machine_Learning_Theory</b> (major)<hr>Theoretical foundations of machine learning including bounds and complexity.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Theoretical foundations of machine learning including bounds and complexity.", "node_type": "major"}, {"id": "Generalization_Error_Bound", "label": "Generalization_\nError_Bound", "title": "<b>Generalization_Error_Bound</b> (subnode)<hr>Bound on how much worse \u02c6h can be compared to h* in terms of generalization error.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Bound on how much worse \u02c6h can be compared to h* in terms of generalization error.", "node_type": "subnode"}, {"id": "Sample_Complexity", "label": "Sample_Complexi\nty", "title": "<b>Sample_Complexity</b> (subnode)<hr>Number of samples needed to achieve a certain level of performance with high probability.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Number of samples needed to achieve a certain level of performance with high probability.", "node_type": "subnode"}, {"id": "Hypothesis_Space_Size", "label": "Hypothesis_Spac\ne_Size", "title": "<b>Hypothesis_Space_Size</b> (subnode)<hr>The number of hypotheses impacts the sample complexity logarithmically.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "The number of hypotheses impacts the sample complexity logarithmically.", "node_type": "subnode"}, {"id": "Best_Hypothesis", "label": "Best_Hypothesis", "title": "<b>Best_Hypothesis</b> (subnode)<hr>The hypothesis with the lowest generalization error in the hypothesis space.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "The hypothesis with the lowest generalization error in the hypothesis space.", "node_type": "subnode"}, {"id": "Machine_Learning_Theorem", "label": "Machine_Learnin\ng_Theorem", "title": "<b>Machine_Learning_Theorem</b> (major)<hr>A theorem relating uniform convergence and generalization error in machine learning.", "shape": "star", "size": 25, "color": "#FF6347", "description": "A theorem relating uniform convergence and generalization error in machine learning.", "node_type": "major"}, {"id": "Uniform_Convergence_Assumption", "label": "Uniform_Converg\nence_Assumption", "title": "<b>Uniform_Convergence_Assumption</b> (subnode)<hr>Assumption that the empirical risk is close to true risk with high probability.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Assumption that the empirical risk is close to true risk with high probability.", "node_type": "subnode"}, {"id": "Bias_Variance_Tradeoff", "label": "Bias_Variance_T\nradeoff", "title": "<b>Bias_Variance_Tradeoff</b> (subnode)<hr>Discusses the reconciliation of modern ML practice with classical bias-variance trade-off concepts.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Discusses the reconciliation of modern ML practice with classical bias-variance trade-off concepts.", "node_type": "subnode"}, {"id": "Hypothesis_Class_Switching", "label": "Hypothesis_Clas\ns_Switching", "title": "<b>Hypothesis_Class_Switching</b> (subnode)<hr>Discussion on switching from \u03a9 to a larger hypothesis class \u03a9'.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Discussion on switching from \u03a9 to a larger hypothesis class \u03a9'.", "node_type": "subnode"}, {"id": "Bias_Decrease", "label": "Bias_Decrease", "title": "<b>Bias_Decrease</b> (subnode)<hr>Explanation of how bias decreases when moving to a larger hypothesis class.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Explanation of how bias decreases when moving to a larger hypothesis class.", "node_type": "subnode"}, {"id": "Variance_Increase", "label": "Variance_Increa\nse", "title": "<b>Variance_Increase</b> (subnode)<hr>Discussion on the increase in variance with a larger hypothesis class.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Discussion on the increase in variance with a larger hypothesis class.", "node_type": "subnode"}, {"id": "Sample_Complexity_Bound", "label": "Sample_Complexi\nty_Bound", "title": "<b>Sample_Complexity_Bound</b> (subnode)<hr>Derivation of sample complexity bound for finite \u03a9 classes.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Derivation of sample complexity bound for finite \u03a9 classes.", "node_type": "subnode"}, {"id": "Infinite_Hypothesis_Classes", "label": "Infinite_Hypoth\nesis_Classes", "title": "<b>Infinite_Hypothesis_Classes</b> (subnode)<hr>Consideration of hypothesis classes parameterized by real numbers and their implications.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Consideration of hypothesis classes parameterized by real numbers and their implications.", "node_type": "subnode"}, {"id": "Hypothesis_Class_Size", "label": "Hypothesis_Clas\ns_Size", "title": "<b>Hypothesis_Class_Size</b> (subnode)<hr>Size of the hypothesis class in terms of parameters and bits used.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Size of the hypothesis class in terms of parameters and bits used.", "node_type": "subnode"}, {"id": "Parameterization_Impact", "label": "Parameterizatio\nn_Impact", "title": "<b>Parameterization_Impact</b> (subnode)<hr>Effect of model parameterization on sample complexity and learning guarantees.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Effect of model parameterization on sample complexity and learning guarantees.", "node_type": "subnode"}, {"id": "HypothesisClassParameterization", "label": "HypothesisClass\nParameterizatio\nn", "title": "<b>HypothesisClassParameterization</b> (subnode)<hr>Exploration of how different parameterizations can represent the same hypothesis class.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Exploration of how different parameterizations can represent the same hypothesis class.", "node_type": "subnode"}, {"id": "LinearClassifierDefinition", "label": "LinearClassifie\nrDefinition", "title": "<b>LinearClassifierDefinition</b> (subnode)<hr>Definition and representation of linear classifiers with varying parameters.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Definition and representation of linear classifiers with varying parameters.", "node_type": "subnode"}, {"id": "ShatteringConcept", "label": "ShatteringConce\npt", "title": "<b>ShatteringConcept</b> (subnode)<hr>Explanation of the concept of shattering in hypothesis classes.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Explanation of the concept of shattering in hypothesis classes.", "node_type": "subnode"}, {"id": "VCDimensionIntroduction", "label": "VCDimensionIntr\noduction", "title": "<b>VCDimensionIntroduction</b> (subnode)<hr>Definition and importance of Vapnik-Chervonenkis dimension for a hypothesis class.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Definition and importance of Vapnik-Chervonenkis dimension for a hypothesis class.", "node_type": "subnode"}, {"id": "VC Dimension", "label": "VC Dimension", "title": "<b>VC Dimension</b> (major)<hr>Measure of the capacity of a statistical classification algorithm", "shape": "star", "size": 25, "color": "#FF6347", "description": "Measure of the capacity of a statistical classification algorithm", "node_type": "major"}, {"id": "Shattering", "label": "Shattering", "title": "<b>Shattering</b> (subnode)<hr>A set is shattered if every possible labeling can be achieved by some hypothesis in H", "shape": "dot", "size": 15, "color": "#4682B4", "description": "A set is shattered if every possible labeling can be achieved by some hypothesis in H", "node_type": "subnode"}, {"id": "Vapnik's Theorem", "label": "Vapnik's\nTheorem", "title": "<b>Vapnik's Theorem</b> (major)<hr>Theorem linking VC dimension to uniform convergence with high probability", "shape": "star", "size": 25, "color": "#FF6347", "description": "Theorem linking VC dimension to uniform convergence with high probability", "node_type": "major"}, {"id": "Uniform Convergence", "label": "Uniform\nConvergence", "title": "<b>Uniform Convergence</b> (subnode)<hr>Asymptotic property ensuring that empirical risk approximates true risk", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Asymptotic property ensuring that empirical risk approximates true risk", "node_type": "subnode"}, {"id": "Corollary of Vapnik's Theorem", "label": "Corollary of\nVapnik's\nTheorem", "title": "<b>Corollary of Vapnik's Theorem</b> (major)<hr>Number of training examples needed for learning well is linear in VC dimension", "shape": "star", "size": 25, "color": "#FF6347", "description": "Number of training examples needed for learning well is linear in VC dimension", "node_type": "major"}, {"id": "Chapter9", "label": "Chapter9", "title": "<b>Chapter9</b> (major)<hr>Regularization and model selection in machine learning", "shape": "star", "size": 25, "color": "#FF6347", "description": "Regularization and model selection in machine learning", "node_type": "major"}, {"id": "ModelComplexity", "label": "ModelComplexity", "title": "<b>ModelComplexity</b> (subnode)<hr>Measured by number of parameters or function of parameters", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Measured by number of parameters or function of parameters", "node_type": "subnode"}, {"id": "RegularizerFunction", "label": "RegularizerFunc\ntion", "title": "<b>RegularizerFunction</b> (subnode)<hr>Additional term added to training loss to control complexity", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Additional term added to training loss to control complexity", "node_type": "subnode"}, {"id": "TrainingLoss", "label": "TrainingLoss", "title": "<b>TrainingLoss</b> (subnode)<hr>Cost function that includes the regularizer term", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Cost function that includes the regularizer term", "node_type": "subnode"}, {"id": "RegularizationParameter", "label": "RegularizationP\narameter", "title": "<b>RegularizationParameter</b> (subnode)<hr>Controls the impact of regularization on model complexity", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Controls the impact of regularization on model complexity", "node_type": "subnode"}, {"id": "Regularized Loss Function", "label": "Regularized\nLoss Function", "title": "<b>Regularized Loss Function</b> (major)<hr>Combination of loss and regularizer to balance model fit and complexity.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Combination of loss and regularizer to balance model fit and complexity.", "node_type": "major"}, {"id": "Loss J(\u03b8)", "label": "Loss J(\u03b8)", "title": "<b>Loss J(\u03b8)</b> (subnode)<hr>Measures how well the model fits the training data.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Measures how well the model fits the training data.", "node_type": "subnode"}, {"id": "Regularizer R(\u03b8)", "label": "Regularizer\nR(\u03b8)", "title": "<b>Regularizer R(\u03b8)</b> (subnode)<hr>Penalizes model complexity to prevent overfitting.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Penalizes model complexity to prevent overfitting.", "node_type": "subnode"}, {"id": "Regularization Parameter \u03bb", "label": "Regularization\nParameter \u03bb", "title": "<b>Regularization Parameter \u03bb</b> (subnode)<hr>Controls the trade-off between loss and regularizer.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Controls the trade-off between loss and regularizer.", "node_type": "subnode"}, {"id": "L2 Regularization", "label": "L2\nRegularization", "title": "<b>L2 Regularization</b> (subnode)<hr>Encourages small model weights to reduce complexity.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Encourages small model weights to reduce complexity.", "node_type": "subnode"}, {"id": "Weight Decay", "label": "Weight Decay", "title": "<b>Weight Decay</b> (subnode)<hr>In deep learning, L2 regularization is equivalent to decaying the weights during training.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "In deep learning, L2 regularization is equivalent to decaying the weights during training.", "node_type": "subnode"}, {"id": "Sparsity Inducing Regularization", "label": "Sparsity\nInducing\nRegularization", "title": "<b>Sparsity Inducing Regularization</b> (subnode)<hr>Promotes models with fewer non-zero parameters based on prior belief of sparsity.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Promotes models with fewer non-zero parameters based on prior belief of sparsity.", "node_type": "subnode"}, {"id": "Regularization in Machine Learning", "label": "Regularization\nin Machine\nLearning", "title": "<b>Regularization in Machine Learning</b> (major)<hr>Techniques to prevent overfitting by adding constraints on model parameters.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Techniques to prevent overfitting by adding constraints on model parameters.", "node_type": "major"}, {"id": "Sparsity Regularization", "label": "Sparsity\nRegularization", "title": "<b>Sparsity Regularization</b> (subnode)<hr>Encourages models to have fewer non-zero parameter values.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Encourages models to have fewer non-zero parameter values.", "node_type": "subnode"}, {"id": "L1 Norm (LASSO)", "label": "L1 Norm (LASSO)", "title": "<b>L1 Norm (LASSO)</b> (subnode)<hr>Promotes sparsity by penalizing the absolute value of parameters.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Promotes sparsity by penalizing the absolute value of parameters.", "node_type": "subnode"}, {"id": "L2 Norm Regularization", "label": "L2 Norm\nRegularization", "title": "<b>L2 Norm Regularization</b> (subnode)<hr>Penalizes the square of parameter values to reduce model complexity.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Penalizes the square of parameter values to reduce model complexity.", "node_type": "subnode"}, {"id": "Gradient Descent Incompatibility", "label": "Gradient\nDescent\nIncompatibility", "title": "<b>Gradient Descent Incompatibility</b> (subnode)<hr>L1 norm is not differentiable, making it incompatible with gradient descent methods.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "L1 norm is not differentiable, making it incompatible with gradient descent methods.", "node_type": "subnode"}, {"id": "Kernel Methods Compatibility", "label": "Kernel Methods\nCompatibility", "title": "<b>Kernel Methods Compatibility</b> (subnode)<hr>L2 regularization works well with kernel methods due to compatibility issues with L1.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "L2 regularization works well with kernel methods due to compatibility issues with L1.", "node_type": "subnode"}, {"id": "Deep Learning Regularization Techniques", "label": "Deep Learning\nRegularization\nTechniques", "title": "<b>Deep Learning Regularization Techniques</b> (subnode)<hr>Includes weight decay, dropout, data augmentation, and more.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Includes weight decay, dropout, data augmentation, and more.", "node_type": "subnode"}, {"id": "Regularization in Deep Learning", "label": "Regularization\nin Deep\nLearning", "title": "<b>Regularization in Deep Learning</b> (major)<hr>Overview of regularization techniques and concepts in deep learning.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Overview of regularization techniques and concepts in deep learning.", "node_type": "major"}, {"id": "Explicit Regularization Techniques", "label": "Explicit\nRegularization\nTechniques", "title": "<b>Explicit Regularization Techniques</b> (subnode)<hr>Techniques such as weight decay, dropout, data augmentation, spectral norm regularization, and Lipschitz regularization.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Techniques such as weight decay, dropout, data augmentation, spectral norm regularization, and Lipschitz regularization.", "node_type": "subnode"}, {"id": "Implicit Regularization Effect", "label": "Implicit\nRegularization\nEffect", "title": "<b>Implicit Regularization Effect</b> (subnode)<hr>The impact of optimizers on model parameters beyond explicit regularization.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "The impact of optimizers on model parameters beyond explicit regularization.", "node_type": "subnode"}, {"id": "Global Minima Diversity", "label": "Global Minima\nDiversity", "title": "<b>Global Minima Diversity</b> (subnode)<hr>Different optimizers converge to different global minima with varying generalization performance.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Different optimizers converge to different global minima with varying generalization performance.", "node_type": "subnode"}, {"id": "Optimizer Impact", "label": "Optimizer\nImpact", "title": "<b>Optimizer Impact</b> (subnode)<hr>Optimizers can bias towards certain types of global minima, affecting model generalization.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Optimizers can bias towards certain types of global minima, affecting model generalization.", "node_type": "subnode"}, {"id": "Global Minima", "label": "Global Minima", "title": "<b>Global Minima</b> (subnode)<hr>Different global minima can have varying test performance.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Different global minima can have varying test performance.", "node_type": "subnode"}, {"id": "Learning Rate Schedules", "label": "Learning Rate\nSchedules", "title": "<b>Learning Rate Schedules</b> (subnode)<hr>Impact of learning rate schedules on model generalization.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Impact of learning rate schedules on model generalization.", "node_type": "subnode"}, {"id": "Model Selection via Cross Validation", "label": "Model Selection\nvia Cross\nValidation", "title": "<b>Model Selection via Cross Validation</b> (major)<hr>Process of selecting among different models using cross validation.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Process of selecting among different models using cross validation.", "node_type": "major"}, {"id": "Model Selection", "label": "Model Selection", "title": "<b>Model Selection</b> (major)<hr>Process of choosing the best model for a given task.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Process of choosing the best model for a given task.", "node_type": "major"}, {"id": "Cross Validation", "label": "Cross\nValidation", "title": "<b>Cross Validation</b> (subnode)<hr>Techniques for selecting and evaluating models in machine learning.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Techniques for selecting and evaluating models in machine learning.", "node_type": "subnode"}, {"id": "Polynomial Regression Model", "label": "Polynomial\nRegression\nModel", "title": "<b>Polynomial Regression Model</b> (subnode)<hr>Regression model using polynomial functions of the input variables.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Regression model using polynomial functions of the input variables.", "node_type": "subnode"}, {"id": "Bias and Variance Tradeoff", "label": "Bias and\nVariance\nTradeoff", "title": "<b>Bias and Variance Tradeoff</b> (subnode)<hr>Balancing underfitting (high bias) and overfitting (high variance).", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Balancing underfitting (high bias) and overfitting (high variance).", "node_type": "subnode"}, {"id": "Model Set M", "label": "Model Set M", "title": "<b>Model Set M</b> (subnode)<hr>Finite set of models to choose from.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Finite set of models to choose from.", "node_type": "subnode"}, {"id": "SVM", "label": "SVM", "title": "<b>SVM</b> (subnode)<hr>Support Vector Machine for classification and regression analysis.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Support Vector Machine for classification and regression analysis.", "node_type": "subnode"}, {"id": "Neural Network", "label": "Neural Network", "title": "<b>Neural Network</b> (subnode)<hr>Artificial neural network for machine learning tasks.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Artificial neural network for machine learning tasks.", "node_type": "subnode"}, {"id": "EmpiricalRiskMinimization", "label": "EmpiricalRiskMi\nnimization", "title": "<b>EmpiricalRiskMinimization</b> (subnode)<hr>Process of minimizing empirical risk for model selection.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Process of minimizing empirical risk for model selection.", "node_type": "subnode"}, {"id": "CrossValidation", "label": "CrossValidation", "title": "<b>CrossValidation</b> (major)<hr>Technique to estimate the generalization error of models.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Technique to estimate the generalization error of models.", "node_type": "major"}, {"id": "HoldOutCrossValidation", "label": "HoldOutCrossVal\nidation", "title": "<b>HoldOutCrossValidation</b> (subnode)<hr>Method involving splitting data into training and validation sets for model selection.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Method involving splitting data into training and validation sets for model selection.", "node_type": "subnode"}, {"id": "TrainingSetS", "label": "TrainingSetS", "title": "<b>TrainingSetS</b> (subnode)<hr>Dataset used to train models in empirical risk minimization.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Dataset used to train models in empirical risk minimization.", "node_type": "subnode"}, {"id": "HypothesesHi", "label": "HypothesesHi", "title": "<b>HypothesesHi</b> (subnode)<hr>Set of hypotheses generated from training different models on the dataset.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Set of hypotheses generated from training different models on the dataset.", "node_type": "subnode"}, {"id": "TrainingError", "label": "TrainingError", "title": "<b>TrainingError</b> (subnode)<hr>Measure of error based on the training set used for model selection.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Measure of error based on the training set used for model selection.", "node_type": "subnode"}, {"id": "DegreeOfPolynomial", "label": "DegreeOfPolynom\nial", "title": "<b>DegreeOfPolynomial</b> (subnode)<hr>Example parameter affecting model complexity and generalization ability.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Example parameter affecting model complexity and generalization ability.", "node_type": "subnode"}, {"id": "S_train", "label": "S_train", "title": "<b>S_train</b> (subnode)<hr>Subset of the training data used for training models in cross validation.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Subset of the training data used for training models in cross validation.", "node_type": "subnode"}, {"id": "S_cv", "label": "S_cv", "title": "<b>S_cv</b> (subnode)<hr>Subset of the training data used to validate model performance.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Subset of the training data used to validate model performance.", "node_type": "subnode"}, {"id": "ValidationError", "label": "ValidationError", "title": "<b>ValidationError</b> (subnode)<hr>Measure of error based on validation set, estimating generalization ability of models.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Measure of error based on validation set, estimating generalization ability of models.", "node_type": "subnode"}, {"id": "Machine_Learning_Techniques", "label": "Machine_Learnin\ng_Techniques", "title": "<b>Machine_Learning_Techniques</b> (major)<hr>Techniques used in machine learning for model evaluation and selection.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Techniques used in machine learning for model evaluation and selection.", "node_type": "major"}, {"id": "Model_Selection", "label": "Model_Selection", "title": "<b>Model_Selection</b> (subnode)<hr>Process of choosing the best model based on validation set performance.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Process of choosing the best model based on validation set performance.", "node_type": "subnode"}, {"id": "Validation_Set_Size", "label": "Validation_Set_\nSize", "title": "<b>Validation_Set_Size</b> (subnode)<hr>Determining an appropriate size for the validation dataset.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Determining an appropriate size for the validation dataset.", "node_type": "subnode"}, {"id": "Cross_Validation", "label": "Cross_Validatio\nn", "title": "<b>Cross_Validation</b> (subnode)<hr>Technique to evaluate models by partitioning data into subsets.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Technique to evaluate models by partitioning data into subsets.", "node_type": "subnode"}, {"id": "Hold_Out_Cross_Validation", "label": "Hold_Out_Cross_\nValidation", "title": "<b>Hold_Out_Cross_Validation</b> (subnode)<hr>Method where a portion of the dataset is held out for validation purposes.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Method where a portion of the dataset is held out for validation purposes.", "node_type": "subnode"}, {"id": "k_Fold_Cross_Validation", "label": "k_Fold_Cross_Va\nlidation", "title": "<b>k_Fold_Cross_Validation</b> (subnode)<hr>Technique that splits data into k partitions and iterates over these subsets for training and validation.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Technique that splits data into k partitions and iterates over these subsets for training and validation.", "node_type": "subnode"}, {"id": "Machine_Learning_Challenges", "label": "Machine_Learnin\ng_Challenges", "title": "<b>Machine_Learning_Challenges</b> (major)<hr>Challenges in machine learning when data is scarce.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Challenges in machine learning when data is scarce.", "node_type": "major"}, {"id": "Leave_One_Out_Cross_Validation", "label": "Leave_One_Out_C\nross_Validation", "title": "<b>Leave_One_Out_Cross_Validation</b> (subnode)<hr>Special case of cross validation where one data point is left out for testing.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Special case of cross validation where one data point is left out for testing.", "node_type": "subnode"}, {"id": "Training_and_Testing_Process", "label": "Training_and_Te\nsting_Process", "title": "<b>Training_and_Testing_Process</b> (subnode)<hr>Process of training models on subsets and testing on remaining data.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Process of training models on subsets and testing on remaining data.", "node_type": "subnode"}, {"id": "Leave-One-Out CV", "label": "Leave-One-Out\nCV", "title": "<b>Leave-One-Out CV</b> (subnode)<hr>A method of cross validation where one training example is held out at a time.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "A method of cross validation where one training example is held out at a time.", "node_type": "subnode"}, {"id": "Bayesian Statistics", "label": "Bayesian\nStatistics", "title": "<b>Bayesian Statistics</b> (major)<hr>An approach to statistics that treats parameters as random variables with prior distributions.", "shape": "star", "size": 25, "color": "#FF6347", "description": "An approach to statistics that treats parameters as random variables with prior distributions.", "node_type": "major"}, {"id": "MLE", "label": "MLE", "title": "<b>MLE</b> (subnode)<hr>Maximum likelihood estimation for parameter fitting without considering \u03b8 as a random variable.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Maximum likelihood estimation for parameter fitting without considering \u03b8 as a random variable.", "node_type": "subnode"}, {"id": "Prior Distribution", "label": "Prior\nDistribution", "title": "<b>Prior Distribution</b> (subnode)<hr>A probability distribution that represents prior beliefs about the parameters before observing data.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "A probability distribution that represents prior beliefs about the parameters before observing data.", "node_type": "subnode"}, {"id": "Bayesian Machine Learning", "label": "Bayesian\nMachine\nLearning", "title": "<b>Bayesian Machine Learning</b> (major)<hr>Predictions made using posterior distributions on parameters.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Predictions made using posterior distributions on parameters.", "node_type": "major"}, {"id": "Posterior Distribution", "label": "Posterior\nDistribution", "title": "<b>Posterior Distribution</b> (subnode)<hr>Probability distribution over parameters given the data.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Probability distribution over parameters given the data.", "node_type": "subnode"}, {"id": "Bayes' Theorem", "label": "Bayes' Theorem", "title": "<b>Bayes' Theorem</b> (subnode)<hr>Formula for calculating posterior from likelihood and prior.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Formula for calculating posterior from likelihood and prior.", "node_type": "subnode"}, {"id": "Likelihood Function", "label": "Likelihood\nFunction", "title": "<b>Likelihood Function</b> (subnode)<hr>Function to maximize for parameter estimation", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Function to maximize for parameter estimation", "node_type": "subnode"}, {"id": "Prediction on New Data", "label": "Prediction on\nNew Data", "title": "<b>Prediction on New Data</b> (subnode)<hr>Making predictions using posterior distribution for new inputs.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Making predictions using posterior distribution for new inputs.", "node_type": "subnode"}, {"id": "Expected Value Prediction", "label": "Expected Value\nPrediction", "title": "<b>Expected Value Prediction</b> (subnode)<hr>Predicting the expected value of y given x and S.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Predicting the expected value of y given x and S.", "node_type": "subnode"}, {"id": "Fully Bayesian Prediction", "label": "Fully Bayesian\nPrediction", "title": "<b>Fully Bayesian Prediction</b> (subnode)<hr>Averaging predictions over posterior distribution p(\u03b8|S).", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Averaging predictions over posterior distribution p(\u03b8|S).", "node_type": "subnode"}, {"id": "Computational Challenges", "label": "Computational\nChallenges", "title": "<b>Computational Challenges</b> (subnode)<hr>Difficulty in computing high-dimensional integrals for the posterior.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Difficulty in computing high-dimensional integrals for the posterior.", "node_type": "subnode"}, {"id": "BayesianInference", "label": "BayesianInferen\nce", "title": "<b>BayesianInference</b> (subnode)<hr>Techniques for estimating parameters using prior knowledge and data.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Techniques for estimating parameters using prior knowledge and data.", "node_type": "subnode"}, {"id": "PosteriorApproximation", "label": "PosteriorApprox\nimation", "title": "<b>PosteriorApproximation</b> (subnode)<hr>Methods to approximate the posterior distribution when exact computation is infeasible.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Methods to approximate the posterior distribution when exact computation is infeasible.", "node_type": "subnode"}, {"id": "MAPEstimate", "label": "MAPEstimate", "title": "<b>MAPEstimate</b> (subnode)<hr>Finding a point estimate for parameters that maximizes the posterior probability.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Finding a point estimate for parameters that maximizes the posterior probability.", "node_type": "subnode"}, {"id": "MLEvsMAP", "label": "MLEvsMAP", "title": "<b>MLEvsMAP</b> (subnode)<hr>Comparison between maximum likelihood and MAP estimates, highlighting the role of prior distributions.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Comparison between maximum likelihood and MAP estimates, highlighting the role of prior distributions.", "node_type": "subnode"}, {"id": "PriorDistributions", "label": "PriorDistributi\nons", "title": "<b>PriorDistributions</b> (subnode)<hr>Selection of appropriate priors for parameter estimation, such as normal distribution with mean 0.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Selection of appropriate priors for parameter estimation, such as normal distribution with mean 0.", "node_type": "subnode"}, {"id": "UnsupervisedLearning", "label": "UnsupervisedLea\nrning", "title": "<b>UnsupervisedLearning</b> (major)<hr>Techniques that learn from data without labeled responses.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Techniques that learn from data without labeled responses.", "node_type": "major"}, {"id": "Clustering", "label": "Clustering", "title": "<b>Clustering</b> (subnode)<hr>Grouping a set of objects in such a way that objects in the same group are more similar to each other than to those in other groups.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Grouping a set of objects in such a way that objects in the same group are more similar to each other than to those in other groups.", "node_type": "subnode"}, {"id": "KMeansAlgorithm", "label": "KMeansAlgorithm", "title": "<b>KMeansAlgorithm</b> (subnode)<hr>A method for partitioning data into clusters based on minimizing the sum of squared distances from points to cluster centers.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "A method for partitioning data into clusters based on minimizing the sum of squared distances from points to cluster centers.", "node_type": "subnode"}, {"id": "k-means_algorithm", "label": "k-\nmeans_algorithm", "title": "<b>k-means_algorithm</b> (major)<hr>Clustering algorithm that partitions data into k clusters.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Clustering algorithm that partitions data into k clusters.", "node_type": "major"}, {"id": "distortion_function", "label": "distortion_func\ntion", "title": "<b>distortion_function</b> (subnode)<hr>Measures sum of squared distances between examples and cluster centroids.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Measures sum of squared distances between examples and cluster centroids.", "node_type": "subnode"}, {"id": "centroid_initialization", "label": "centroid_initia\nlization", "title": "<b>centroid_initialization</b> (subnode)<hr>Randomly selects k training examples as initial centroids.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Randomly selects k training examples as initial centroids.", "node_type": "subnode"}, {"id": "inner_loop_steps", "label": "inner_loop_step\ns", "title": "<b>inner_loop_steps</b> (subnode)<hr>Assigns each example to closest centroid and updates centroid positions.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Assigns each example to closest centroid and updates centroid positions.", "node_type": "subnode"}, {"id": "coordinate_descent", "label": "coordinate_desc\nent", "title": "<b>coordinate_descent</b> (subnode)<hr>Optimization technique used in k-means for minimizing distortion function.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Optimization technique used in k-means for minimizing distortion function.", "node_type": "subnode"}, {"id": "k-means Algorithm", "label": "k-means\nAlgorithm", "title": "<b>k-means Algorithm</b> (subnode)<hr>Clustering algorithm that partitions data into k clusters.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Clustering algorithm that partitions data into k clusters.", "node_type": "subnode"}, {"id": "Distortion Function J", "label": "Distortion\nFunction J", "title": "<b>Distortion Function J</b> (subnode)<hr>Function measuring the quality of clustering; aims to minimize this value.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Function measuring the quality of clustering; aims to minimize this value.", "node_type": "subnode"}, {"id": "Convergence in k-means", "label": "Convergence in\nk-means", "title": "<b>Convergence in k-means</b> (subnode)<hr>Process by which cluster centroids and assignments stabilize over iterations.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Process by which cluster centroids and assignments stabilize over iterations.", "node_type": "subnode"}, {"id": "EM Algorithms", "label": "EM Algorithms", "title": "<b>EM Algorithms</b> (major)<hr>Techniques for density estimation using iterative expectation-maximization process.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Techniques for density estimation using iterative expectation-maximization process.", "node_type": "major"}, {"id": "Mixture of Gaussians", "label": "Mixture of\nGaussians", "title": "<b>Mixture of Gaussians</b> (subnode)<hr>Modeling data with a combination of Gaussian distributions to capture complex patterns.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Modeling data with a combination of Gaussian distributions to capture complex patterns.", "node_type": "subnode"}, {"id": "Unsupervised Learning", "label": "Unsupervised\nLearning", "title": "<b>Unsupervised Learning</b> (major)<hr>Learning from data without labels", "shape": "star", "size": 25, "color": "#FF6347", "description": "Learning from data without labels", "node_type": "major"}, {"id": "Mixture of Gaussians Model", "label": "Mixture of\nGaussians Model", "title": "<b>Mixture of Gaussians Model</b> (subnode)<hr>Model using multiple Gaussian distributions for clustering", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Model using multiple Gaussian distributions for clustering", "node_type": "subnode"}, {"id": "Joint Distribution", "label": "Joint\nDistribution", "title": "<b>Joint Distribution</b> (subnode)<hr>Distribution combining latent and observed variables", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Distribution combining latent and observed variables", "node_type": "subnode"}, {"id": "Latent Variables", "label": "Latent\nVariables", "title": "<b>Latent Variables</b> (subnode)<hr>Hidden random variables influencing data generation", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Hidden random variables influencing data generation", "node_type": "subnode"}, {"id": "Parameter Estimation", "label": "Parameter\nEstimation", "title": "<b>Parameter Estimation</b> (subnode)<hr>Estimating parameters \u03c6, \u03bc, and \u03a3 from data likelihood", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Estimating parameters \u03c6, \u03bc, and \u03a3 from data likelihood", "node_type": "subnode"}, {"id": "Closed Form Solution", "label": "Closed Form\nSolution", "title": "<b>Closed Form Solution</b> (subnode)<hr>Infeasibility of deriving parameters directly from likelihood", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Infeasibility of deriving parameters directly from likelihood", "node_type": "subnode"}, {"id": "DensityEstimation", "label": "DensityEstimati\non", "title": "<b>DensityEstimation</b> (subnode)<hr>Techniques for estimating probability density functions from data.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Techniques for estimating probability density functions from data.", "node_type": "subnode"}, {"id": "GaussianMixtureModel", "label": "GaussianMixture\nModel", "title": "<b>GaussianMixtureModel</b> (subnode)<hr>A probabilistic model that assumes all the data points are generated from a mixture of several Gaussian distributions with unknown parameters.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "A probabilistic model that assumes all the data points are generated from a mixture of several Gaussian distributions with unknown parameters.", "node_type": "subnode"}, {"id": "EMAlgorithm", "label": "EMAlgorithm", "title": "<b>EMAlgorithm</b> (subnode)<hr>Iterative method for finding maximum likelihood or maximum a posteriori estimates of parameters in statistical models, where the model depends on unobserved latent variables.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Iterative method for finding maximum likelihood or maximum a posteriori estimates of parameters in statistical models, where the model depends on unobserved latent variables.", "node_type": "subnode"}, {"id": "EM_Algorithm", "label": "EM_Algorithm", "title": "<b>EM_Algorithm</b> (major)<hr>Iterative method for finding maximum likelihood or maximum a posteriori estimates in statistical models with latent variables.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Iterative method for finding maximum likelihood or maximum a posteriori estimates in statistical models with latent variables.", "node_type": "major"}, {"id": "E_Step", "label": "E_Step", "title": "<b>E_Step</b> (subnode)<hr>Calculates posterior probabilities of latent variables given data and current parameter estimates.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Calculates posterior probabilities of latent variables given data and current parameter estimates.", "node_type": "subnode"}, {"id": "M_Step", "label": "M_Step", "title": "<b>M_Step</b> (subnode)<hr>Updates parameters to maximize the expected log-likelihood found in E-step.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Updates parameters to maximize the expected log-likelihood found in E-step.", "node_type": "subnode"}, {"id": "Gaussian_Mixture_Models", "label": "Gaussian_Mixtur\ne_Models", "title": "<b>Gaussian_Mixture_Models</b> (subnode)<hr>Model that uses Gaussian distributions to represent subpopulations within an overall population.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Model that uses Gaussian distributions to represent subpopulations within an overall population.", "node_type": "subnode"}, {"id": "Soft_Assignments", "label": "Soft_Assignment\ns", "title": "<b>Soft_Assignments</b> (subnode)<hr>Assign probabilities to each latent variable rather than hard assignments.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Assign probabilities to each latent variable rather than hard assignments.", "node_type": "subnode"}, {"id": "K_Means_Clustering", "label": "K_Means_Cluster\ning", "title": "<b>K_Means_Clustering</b> (subnode)<hr>Algorithm for partitioning data into clusters, similar in concept but uses hard assignments.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Algorithm for partitioning data into clusters, similar in concept but uses hard assignments.", "node_type": "subnode"}, {"id": "Convergence_Issues", "label": "Convergence_Iss\nues", "title": "<b>Convergence_Issues</b> (subnode)<hr>Discussion on convergence properties of fitted value iteration compared to traditional value iteration.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Discussion on convergence properties of fitted value iteration compared to traditional value iteration.", "node_type": "subnode"}, {"id": "EM_Algorithm_Generalization", "label": "EM_Algorithm_Ge\nneralization", "title": "<b>EM_Algorithm_Generalization</b> (subnode)<hr>General view of the EM algorithm for estimation problems with latent variables.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "General view of the EM algorithm for estimation problems with latent variables.", "node_type": "subnode"}, {"id": "Jensens_Inequality", "label": "Jensens_Inequal\nity", "title": "<b>Jensens_Inequality</b> (major)<hr>A fundamental inequality used in convex analysis and probability theory.", "shape": "star", "size": 25, "color": "#FF6347", "description": "A fundamental inequality used in convex analysis and probability theory.", "node_type": "major"}, {"id": "Convex_Functions", "label": "Convex_Function\ns", "title": "<b>Convex_Functions</b> (subnode)<hr>Functions where the second derivative is non-negative or positive definite for vector inputs.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Functions where the second derivative is non-negative or positive definite for vector inputs.", "node_type": "subnode"}, {"id": "Theorem_Jensens_Inequality", "label": "Theorem_Jensens\n_Inequality", "title": "<b>Theorem_Jensens_Inequality</b> (subnode)<hr>Statement of Jensen's inequality and its implications on expectations and random variables.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Statement of Jensen's inequality and its implications on expectations and random variables.", "node_type": "subnode"}, {"id": "Jensen's Inequality", "label": "Jensen's\nInequality", "title": "<b>Jensen's Inequality</b> (major)<hr>Inequality relating expected values of convex functions and their arguments.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Inequality relating expected values of convex functions and their arguments.", "node_type": "major"}, {"id": "Convex Function", "label": "Convex Function", "title": "<b>Convex Function</b> (subnode)<hr>A function where the line segment between any two points on the graph lies above or on the graph.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "A function where the line segment between any two points on the graph lies above or on the graph.", "node_type": "subnode"}, {"id": "Concave Function", "label": "Concave\nFunction", "title": "<b>Concave Function</b> (subnode)<hr>Opposite of a convex function, lying below the line segments connecting its points.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Opposite of a convex function, lying below the line segments connecting its points.", "node_type": "subnode"}, {"id": "E[f(X)] vs f(E[X])", "label": "E[f(X)] vs\nf(E[X])", "title": "<b>E[f(X)] vs f(E[X])</b> (subnode)<hr>Relationship between expected value of a function and function of an expected value for convex/concave functions.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Relationship between expected value of a function and function of an expected value for convex/concave functions.", "node_type": "subnode"}, {"id": "EM Algorithm", "label": "EM Algorithm", "title": "<b>EM Algorithm</b> (major)<hr>Iterative method to find maximum likelihood estimates in probabilistic models with latent variables.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Iterative method to find maximum likelihood estimates in probabilistic models with latent variables.", "node_type": "major"}, {"id": "OptimizationChallenges", "label": "OptimizationCha\nllenges", "title": "<b>OptimizationChallenges</b> (subnode)<hr>Difficulties in optimizing parameters due to non-convex problems.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Difficulties in optimizing parameters due to non-convex problems.", "node_type": "subnode"}, {"id": "EMAlgorithmIntroduction", "label": "EMAlgorithmIntr\noduction", "title": "<b>EMAlgorithmIntroduction</b> (subnode)<hr>Efficient method for maximum likelihood estimation using E-step and M-step.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Efficient method for maximum likelihood estimation using E-step and M-step.", "node_type": "subnode"}, {"id": "LatentVariables", "label": "LatentVariables", "title": "<b>LatentVariables</b> (subnode)<hr>Use of latent variables to simplify optimization problems.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Use of latent variables to simplify optimization problems.", "node_type": "subnode"}, {"id": "EStepMStepProcess", "label": "EStepMStepProce\nss", "title": "<b>EStepMStepProcess</b> (subnode)<hr>Iterative process of constructing a lower-bound and optimizing it.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Iterative process of constructing a lower-bound and optimizing it.", "node_type": "subnode"}, {"id": "SingleExampleOptimization", "label": "SingleExampleOp\ntimization", "title": "<b>SingleExampleOptimization</b> (subnode)<hr>Simplifying the problem by considering optimization for a single example first.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Simplifying the problem by considering optimization for a single example first.", "node_type": "subnode"}, {"id": "SummationNotEssential", "label": "SummationNotEss\nential", "title": "<b>SummationNotEssential</b> (subnode)<hr>Explanation that summation is not essential and can be added later.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Explanation that summation is not essential and can be added later.", "node_type": "subnode"}, {"id": "ProbabilityDistributions", "label": "ProbabilityDist\nributions", "title": "<b>ProbabilityDistributions</b> (subnode)<hr>Discussion on probability distributions used in ML models.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Discussion on probability distributions used in ML models.", "node_type": "subnode"}, {"id": "JensensInequality", "label": "JensensInequali\nty", "title": "<b>JensensInequality</b> (subnode)<hr>Explanation of Jensen's inequality and its application in deriving lower bounds.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Explanation of Jensen's inequality and its application in deriving lower bounds.", "node_type": "subnode"}, {"id": "LogLikelihoodBound", "label": "LogLikelihoodBo\nund", "title": "<b>LogLikelihoodBound</b> (subnode)<hr>Derivation of a lower bound for log likelihood using specific distributions Q.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Derivation of a lower bound for log likelihood using specific distributions Q.", "node_type": "subnode"}, {"id": "EvidenceLowerBoundELBO", "label": "EvidenceLowerBo\nundELBO", "title": "<b>EvidenceLowerBoundELBO</b> (subnode)<hr>Definition and importance of ELBO in machine learning models.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Definition and importance of ELBO in machine learning models.", "node_type": "subnode"}, {"id": "Log_Likelihood_Optimization", "label": "Log_Likelihood_\nOptimization", "title": "<b>Log_Likelihood_Optimization</b> (subnode)<hr>Process of maximizing the log-likelihood function to estimate parameters in probabilistic models.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Process of maximizing the log-likelihood function to estimate parameters in probabilistic models.", "node_type": "subnode"}, {"id": "Evidence_Lower_Bound_(ELBO)", "label": "Evidence_Lower_\nBound_(ELBO)", "title": "<b>Evidence_Lower_Bound_(ELBO)</b> (subnode)<hr>Objective function used in variational inference and EM algorithm to approximate posterior distributions.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Objective function used in variational inference and EM algorithm to approximate posterior distributions.", "node_type": "subnode"}, {"id": "Multiple_Examples_Consideration", "label": "Multiple_Exampl\nes_Consideratio\nn", "title": "<b>Multiple_Examples_Consideration</b> (subnode)<hr>Extension of ELBO for multiple training examples, summing over individual example bounds.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Extension of ELBO for multiple training examples, summing over individual example bounds.", "node_type": "subnode"}, {"id": "ExpectationMaximizationAlgorithm", "label": "ExpectationMaxi\nmizationAlgorit\nhm", "title": "<b>ExpectationMaximizationAlgorithm</b> (subnode)<hr>Iterative method for finding maximum likelihood or maximum a posteriori estimates of parameters in statistical models.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Iterative method for finding maximum likelihood or maximum a posteriori estimates of parameters in statistical models.", "node_type": "subnode"}, {"id": "EStep", "label": "EStep", "title": "<b>EStep</b> (subnode)<hr>Calculates the expected value of the log-likelihood, with respect to the current estimate of the hidden variables, given data and parameters.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Calculates the expected value of the log-likelihood, with respect to the current estimate of the hidden variables, given data and parameters.", "node_type": "subnode"}, {"id": "MStep", "label": "MStep", "title": "<b>MStep</b> (subnode)<hr>The M-step maximizes the expected log-likelihood found in the E-step with respect to model parameters.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "The M-step maximizes the expected log-likelihood found in the E-step with respect to model parameters.", "node_type": "subnode"}, {"id": "LogLikelihoodImprovement", "label": "LogLikelihoodIm\nprovement", "title": "<b>LogLikelihoodImprovement</b> (subnode)<hr>Explanation of how EM monotonically improves the log-likelihood at each iteration.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Explanation of how EM monotonically improves the log-likelihood at each iteration.", "node_type": "subnode"}, {"id": "ELBO", "label": "ELBO", "title": "<b>ELBO</b> (subnode)<hr>Evidence Lower Bound (ELBO) is used in the E-step to approximate the posterior distribution.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Evidence Lower Bound (ELBO) is used in the E-step to approximate the posterior distribution.", "node_type": "subnode"}, {"id": "ELBOExplanation", "label": "ELBOExplanation", "title": "<b>ELBOExplanation</b> (subnode)<hr>Detailed explanation of Evidence Lower Bound (ELBO) definition and its various forms.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Detailed explanation of Evidence Lower Bound (ELBO) definition and its various forms.", "node_type": "subnode"}, {"id": "AlternativeFormulationsOfELBO", "label": "AlternativeForm\nulationsOfELBO", "title": "<b>AlternativeFormulationsOfELBO</b> (subnode)<hr>Different mathematical formulations of ELBO including expectations and KL divergence.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Different mathematical formulations of ELBO including expectations and KL divergence.", "node_type": "subnode"}, {"id": "MarginalDistributionIndependence", "label": "MarginalDistrib\nutionIndependen\nce", "title": "<b>MarginalDistributionIndependence</b> (subnode)<hr>Discussion on the independence of marginal distribution from parameter theta in ELBO maximization.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Discussion on the independence of marginal distribution from parameter theta in ELBO maximization.", "node_type": "subnode"}, {"id": "ConditionalLikelihoodSimplification", "label": "ConditionalLike\nlihoodSimplific\nation", "title": "<b>ConditionalLikelihoodSimplification</b> (subnode)<hr>Explanation that maximizing conditional likelihood simplifies optimization problems compared to joint likelihood.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Explanation that maximizing conditional likelihood simplifies optimization problems compared to joint likelihood.", "node_type": "subnode"}, {"id": "EMAlgorithmOverview", "label": "EMAlgorithmOver\nview", "title": "<b>EMAlgorithmOverview</b> (subnode)<hr>Introduction and overview of the Expectation-Maximization (EM) algorithm.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Introduction and overview of the Expectation-Maximization (EM) algorithm.", "node_type": "subnode"}, {"id": "MixtureOfGaussiansExample", "label": "MixtureOfGaussi\nansExample", "title": "<b>MixtureOfGaussiansExample</b> (subnode)<hr>Application of EM to fitting parameters in a mixture of Gaussian distributions example.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Application of EM to fitting parameters in a mixture of Gaussian distributions example.", "node_type": "subnode"}, {"id": "PhiParameterUpdate", "label": "PhiParameterUpd\nate", "title": "<b>PhiParameterUpdate</b> (subnode)<hr>Specific derivation of the update rule for parameters \u03c6_j in the M-step.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Specific derivation of the update rule for parameters \u03c6_j in the M-step.", "node_type": "subnode"}, {"id": "MuParameterUpdate", "label": "MuParameterUpda\nte", "title": "<b>MuParameterUpdate</b> (subnode)<hr>Update rule for \u03bc_l parameter during M-step iteration.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Update rule for \u03bc_l parameter during M-step iteration.", "node_type": "subnode"}, {"id": "SigmaParameterUpdate", "label": "SigmaParameterU\npdate", "title": "<b>SigmaParameterUpdate</b> (subnode)<hr>Exercise left to reader: update rule for \u03a3_j parameter during M-step iteration.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Exercise left to reader: update rule for \u03a3_j parameter during M-step iteration.", "node_type": "subnode"}, {"id": "MStepUpdateRule", "label": "MStepUpdateRule", "title": "<b>MStepUpdateRule</b> (subnode)<hr>Derivation and update rule for the M-step in EM algorithm, focusing on parameter updates.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Derivation and update rule for the M-step in EM algorithm, focusing on parameter updates.", "node_type": "subnode"}, {"id": "LagrangianConstruction", "label": "LagrangianConst\nruction", "title": "<b>LagrangianConstruction</b> (subnode)<hr>Use of Lagrangian to handle constraints during parameter updates.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Use of Lagrangian to handle constraints during parameter updates.", "node_type": "subnode"}, {"id": "VariationalInference", "label": "VariationalInfe\nrence", "title": "<b>VariationalInference</b> (major)<hr>Techniques for approximating probability distributions in complex models using variational methods.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Techniques for approximating probability distributions in complex models using variational methods.", "node_type": "major"}, {"id": "VariationalAutoEncoder", "label": "VariationalAuto\nEncoder", "title": "<b>VariationalAutoEncoder</b> (subnode)<hr>Family of algorithms extending EM to more complex models parameterized by neural networks.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Family of algorithms extending EM to more complex models parameterized by neural networks.", "node_type": "subnode"}, {"id": "Variational_Autoencoder", "label": "Variational_Aut\noencoder", "title": "<b>Variational_Autoencoder</b> (subnode)<hr>A type of neural network architecture used to learn latent variable models.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "A type of neural network architecture used to learn latent variable models.", "node_type": "subnode"}, {"id": "EM_Algorithms", "label": "EM_Algorithms", "title": "<b>EM_Algorithms</b> (subnode)<hr>Expectation-Maximization algorithm for parameter estimation in statistical models.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Expectation-Maximization algorithm for parameter estimation in statistical models.", "node_type": "subnode"}, {"id": "Variational_Inference", "label": "Variational_Inf\nerence", "title": "<b>Variational_Inference</b> (subnode)<hr>Technique to approximate posterior distributions in Bayesian inference.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Technique to approximate posterior distributions in Bayesian inference.", "node_type": "subnode"}, {"id": "Reparametrization_Trick", "label": "Reparametrizati\non_Trick", "title": "<b>Reparametrization_Trick</b> (subnode)<hr>Method for sampling from a distribution using differentiable transformations.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Method for sampling from a distribution using differentiable transformations.", "node_type": "subnode"}, {"id": "Posterior_Distribution", "label": "Posterior_Distr\nibution", "title": "<b>Posterior_Distribution</b> (subnode)<hr>Probability distribution of an unobserved variable given observed data.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Probability distribution of an unobserved variable given observed data.", "node_type": "subnode"}, {"id": "Variational Inference", "label": "Variational\nInference", "title": "<b>Variational Inference</b> (major)<hr>Approximating true posterior distribution using a family of Q distributions.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Approximating true posterior distribution using a family of Q distributions.", "node_type": "major"}, {"id": "ELBO Lower Bound", "label": "ELBO Lower\nBound", "title": "<b>ELBO Lower Bound</b> (subnode)<hr>Expected lower bound on log-likelihood used in variational inference optimization.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Expected lower bound on log-likelihood used in variational inference optimization.", "node_type": "subnode"}, {"id": "Mean Field Assumption", "label": "Mean Field\nAssumption", "title": "<b>Mean Field Assumption</b> (subnode)<hr>Assumption that Q distribution can be decomposed into independent coordinates for discrete latent variables.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Assumption that Q distribution can be decomposed into independent coordinates for discrete latent variables.", "node_type": "subnode"}, {"id": "Continuous Latent Variables", "label": "Continuous\nLatent\nVariables", "title": "<b>Continuous Latent Variables</b> (subnode)<hr>Handling continuous latent variables requires additional techniques beyond mean field assumptions.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Handling continuous latent variables requires additional techniques beyond mean field assumptions.", "node_type": "subnode"}, {"id": "Latent_Variables", "label": "Latent_Variable\ns", "title": "<b>Latent_Variables</b> (subnode)<hr>Continuous latent variables used in probabilistic models.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Continuous latent variables used in probabilistic models.", "node_type": "subnode"}, {"id": "Gaussian_Distribution_Qi", "label": "Gaussian_Distri\nbution_Qi", "title": "<b>Gaussian_Distribution_Qi</b> (subnode)<hr>Distribution Qi modeled as a Gaussian with mean and variance functions of input x.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Distribution Qi modeled as a Gaussian with mean and variance functions of input x.", "node_type": "subnode"}, {"id": "Mean_and_Variance_Functions", "label": "Mean_and_Varian\nce_Functions", "title": "<b>Mean_and_Variance_Functions</b> (subnode)<hr>Functions q(x;phi) and v(x;psi) that determine the mean and diagonal covariance matrix for Qi.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Functions q(x;phi) and v(x;psi) that determine the mean and diagonal covariance matrix for Qi.", "node_type": "subnode"}, {"id": "Encoder_Decoder_Networks", "label": "Encoder_Decoder\n_Networks", "title": "<b>Encoder_Decoder_Networks</b> (subnode)<hr>Components q and v (encoder) and g(z;theta) (decoder) in variational autoencoders.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Components q and v (encoder) and g(z;theta) (decoder) in variational autoencoders.", "node_type": "subnode"}, {"id": "ELBO_Optimization", "label": "ELBO_Optimizati\non", "title": "<b>ELBO_Optimization</b> (subnode)<hr>Process of optimizing the evidence lower bound for probabilistic models.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Process of optimizing the evidence lower bound for probabilistic models.", "node_type": "subnode"}, {"id": "ELBOOptimization", "label": "ELBOOptimizatio\nn", "title": "<b>ELBOOptimization</b> (subnode)<hr>Details on optimizing the Evidence Lower Bound (ELBO) in variational inference.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Details on optimizing the Evidence Lower Bound (ELBO) in variational inference.", "node_type": "subnode"}, {"id": "QFormRequirements", "label": "QFormRequiremen\nts", "title": "<b>QFormRequirements</b> (subnode)<hr>Conditions and requirements for the form of Q_i in ELBO optimization.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Conditions and requirements for the form of Q_i in ELBO optimization.", "node_type": "subnode"}, {"id": "EfficientEvaluationOfELBO", "label": "EfficientEvalua\ntionOfELBO", "title": "<b>EfficientEvaluationOfELBO</b> (subnode)<hr>Methods to efficiently evaluate the value of ELBO given fixed Q and theta.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Methods to efficiently evaluate the value of ELBO given fixed Q and theta.", "node_type": "subnode"}, {"id": "GaussianDistributionQ_i", "label": "GaussianDistrib\nutionQ_i", "title": "<b>GaussianDistributionQ_i</b> (subnode)<hr>Properties of Gaussian distribution for Q_i in efficient evaluation.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Properties of Gaussian distribution for Q_i in efficient evaluation.", "node_type": "subnode"}, {"id": "GradientAscentOptimization", "label": "GradientAscentO\nptimization", "title": "<b>GradientAscentOptimization</b> (subnode)<hr>Using gradient ascent to optimize the ELBO parameters phi, psi, and theta.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Using gradient ascent to optimize the ELBO parameters phi, psi, and theta.", "node_type": "subnode"}, {"id": "ELBO_Gradient_Computation", "label": "ELBO_Gradient_C\nomputation", "title": "<b>ELBO_Gradient_Computation</b> (subnode)<hr>Computation of the gradient of the ELBO with respect to parameters.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Computation of the gradient of the ELBO with respect to parameters.", "node_type": "subnode"}, {"id": "Gradient_Simple_Case", "label": "Gradient_Simple\n_Case", "title": "<b>Gradient_Simple_Case</b> (subnode)<hr>Simple case where computing gradients is straightforward.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Simple case where computing gradients is straightforward.", "node_type": "subnode"}, {"id": "Complexity_in_Computing_Gradients", "label": "Complexity_in_C\nomputing_Gradie\nnts", "title": "<b>Complexity_in_Computing_Gradients</b> (subnode)<hr>Challenges in computing gradients over parameters phi and psi due to dependency on sampling distribution Q_i.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Challenges in computing gradients over parameters phi and psi due to dependency on sampling distribution Q_i.", "node_type": "subnode"}, {"id": "Reparameterization_Trick", "label": "Reparameterizat\nion_Trick", "title": "<b>Reparameterization_Trick</b> (subnode)<hr>Technique used to simplify gradient computation by re-parameterizing the random variable z^{(i)}.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Technique used to simplify gradient computation by re-parameterizing the random variable z^{(i)}.", "node_type": "subnode"}, {"id": "Mathematical_Formulation_Reparametrization", "label": "Mathematical_Fo\nrmulation_Repar\nametrization", "title": "<b>Mathematical_Formulation_Reparametrization</b> (subnode)<hr>Detailed mathematical formulation of the re-parametrization trick for simplifying gradient calculations.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Detailed mathematical formulation of the re-parametrization trick for simplifying gradient calculations.", "node_type": "subnode"}, {"id": "Gradient_Estimation", "label": "Gradient_Estima\ntion", "title": "<b>Gradient_Estimation</b> (subnode)<hr>Estimating the gradient of a policy's log probability with respect to parameters.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Estimating the gradient of a policy's log probability with respect to parameters.", "node_type": "subnode"}, {"id": "PCA_Method", "label": "PCA_Method", "title": "<b>PCA_Method</b> (major)<hr>Principal Components Analysis for dimensionality reduction in datasets.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Principal Components Analysis for dimensionality reduction in datasets.", "node_type": "major"}, {"id": "Dataset_Analysis", "label": "Dataset_Analysi\ns", "title": "<b>Dataset_Analysis</b> (subnode)<hr>Analysis of dataset attributes using PCA.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Analysis of dataset attributes using PCA.", "node_type": "subnode"}, {"id": "RedundancyDetection", "label": "RedundancyDetec\ntion", "title": "<b>RedundancyDetection</b> (major)<hr>Identifying and removing redundant data attributes in machine learning.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Identifying and removing redundant data attributes in machine learning.", "node_type": "major"}, {"id": "CarAttributesExample", "label": "CarAttributesEx\nample", "title": "<b>CarAttributesExample</b> (subnode)<hr>Illustration of redundancy with car speed measurements in mph and kph.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Illustration of redundancy with car speed measurements in mph and kph.", "node_type": "subnode"}, {"id": "PilotSurveyExample", "label": "PilotSurveyExam\nple", "title": "<b>PilotSurveyExample</b> (subnode)<hr>Illustration of correlation between piloting skill and enjoyment among RC helicopter pilots.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Illustration of correlation between piloting skill and enjoyment among RC helicopter pilots.", "node_type": "subnode"}, {"id": "PCAAlgorithm", "label": "PCAAlgorithm", "title": "<b>PCAAlgorithm</b> (major)<hr>Principal Component Analysis for dimensionality reduction in data sets.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Principal Component Analysis for dimensionality reduction in data sets.", "node_type": "major"}, {"id": "DataNormalization", "label": "DataNormalizati\non", "title": "<b>DataNormalization</b> (subnode)<hr>Preprocessing step to normalize features before PCA application.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Preprocessing step to normalize features before PCA application.", "node_type": "subnode"}, {"id": "Mean Normalization", "label": "Mean\nNormalization", "title": "<b>Mean Normalization</b> (subnode)<hr>Subtracting the mean from each feature to center the data around zero.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Subtracting the mean from each feature to center the data around zero.", "node_type": "subnode"}, {"id": "Variance Scaling", "label": "Variance\nScaling", "title": "<b>Variance Scaling</b> (subnode)<hr>Dividing by standard deviation to ensure unit variance for comparable attributes.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Dividing by standard deviation to ensure unit variance for comparable attributes.", "node_type": "subnode"}, {"id": "Data Rescaling", "label": "Data Rescaling", "title": "<b>Data Rescaling</b> (subnode)<hr>Adjusting data scale when features are already on the same level.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Adjusting data scale when features are already on the same level.", "node_type": "subnode"}, {"id": "Major Axis of Variation", "label": "Major Axis of\nVariation", "title": "<b>Major Axis of Variation</b> (major)<hr>Finding the direction that maximizes variance for projected data.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Finding the direction that maximizes variance for projected data.", "node_type": "major"}, {"id": "Projection Direction", "label": "Projection\nDirection", "title": "<b>Projection Direction</b> (subnode)<hr>Selecting a unit vector to project data onto maximizing retained variance.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Selecting a unit vector to project data onto maximizing retained variance.", "node_type": "subnode"}, {"id": "PrincipalComponentAnalysis", "label": "PrincipalCompon\nentAnalysis", "title": "<b>PrincipalComponentAnalysis</b> (major)<hr>Technique for reducing dimensionality while retaining variance.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Technique for reducing dimensionality while retaining variance.", "node_type": "major"}, {"id": "ProjectionOntoDirectionU", "label": "ProjectionOntoD\nirectionU", "title": "<b>ProjectionOntoDirectionU</b> (subnode)<hr>Process of projecting data points onto a unit vector u to maximize variance.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Process of projecting data points onto a unit vector u to maximize variance.", "node_type": "subnode"}, {"id": "VarianceMaximization", "label": "VarianceMaximiz\nation", "title": "<b>VarianceMaximization</b> (subnode)<hr>Objective is to find direction u that maximizes the variance of projections.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Objective is to find direction u that maximizes the variance of projections.", "node_type": "subnode"}, {"id": "EmpiricalCovarianceMatrix", "label": "EmpiricalCovari\nanceMatrix", "title": "<b>EmpiricalCovarianceMatrix</b> (subnode)<hr>Matrix representing data covariance, used in PCA calculations.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Matrix representing data covariance, used in PCA calculations.", "node_type": "subnode"}, {"id": "LagrangeMultipliersMethod", "label": "LagrangeMultipl\niersMethod", "title": "<b>LagrangeMultipliersMethod</b> (subnode)<hr>Mathematical method to solve optimization problems with constraints.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Mathematical method to solve optimization problems with constraints.", "node_type": "subnode"}, {"id": "PrincipalEigenvector", "label": "PrincipalEigenv\nector", "title": "<b>PrincipalEigenvector</b> (subnode)<hr>Direction u that maximizes variance, found as eigenvector of covariance matrix.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Direction u that maximizes variance, found as eigenvector of covariance matrix.", "node_type": "subnode"}, {"id": "kDimensionalSubspace", "label": "kDimensionalSub\nspace", "title": "<b>kDimensionalSubspace</b> (subnode)<hr>Generalization to projecting data into k-dimensional subspace using top k eigenvectors.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Generalization to projecting data into k-dimensional subspace using top k eigenvectors.", "node_type": "subnode"}, {"id": "Principal Component Analysis (PCA)", "label": "Principal\nComponent\nAnalysis (PCA)", "title": "<b>Principal Component Analysis (PCA)</b> (major)<hr>A statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components.", "shape": "star", "size": 25, "color": "#FF6347", "description": "A statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components.", "node_type": "major"}, {"id": "Dimensionality Reduction", "label": "Dimensionality\nReduction", "title": "<b>Dimensionality Reduction</b> (subnode)<hr>Process of converting high-dimensional data into a lower-dimensional space.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Process of converting high-dimensional data into a lower-dimensional space.", "node_type": "subnode"}, {"id": "Eigenvectors and Eigenvalues", "label": "Eigenvectors\nand Eigenvalues", "title": "<b>Eigenvectors and Eigenvalues</b> (subnode)<hr>Key concepts in PCA for finding principal components.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Key concepts in PCA for finding principal components.", "node_type": "subnode"}, {"id": "Orthogonal Basis", "label": "Orthogonal\nBasis", "title": "<b>Orthogonal Basis</b> (subnode)<hr>Set of orthogonal vectors used to represent data in a lower-dimensional space.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Set of orthogonal vectors used to represent data in a lower-dimensional space.", "node_type": "subnode"}, {"id": "Approximation Error Minimization", "label": "Approximation\nError\nMinimization", "title": "<b>Approximation Error Minimization</b> (subnode)<hr>Method for deriving PCA by minimizing projection errors onto k-dimensional subspace.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Method for deriving PCA by minimizing projection errors onto k-dimensional subspace.", "node_type": "subnode"}, {"id": "Data Visualization", "label": "Data\nVisualization", "title": "<b>Data Visualization</b> (subnode)<hr>Application of PCA to visualize high-dimensional data in 2D or 3D.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Application of PCA to visualize high-dimensional data in 2D or 3D.", "node_type": "subnode"}, {"id": "Compression", "label": "Compression", "title": "<b>Compression</b> (subnode)<hr>Use of PCA for compressing high-dimensional data into lower dimensions.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Use of PCA for compressing high-dimensional data into lower dimensions.", "node_type": "subnode"}, {"id": "Dimensionality Reduction Techniques", "label": "Dimensionality\nReduction\nTechniques", "title": "<b>Dimensionality Reduction Techniques</b> (major)<hr>Techniques to reduce the number of random variables under consideration.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Techniques to reduce the number of random variables under consideration.", "node_type": "major"}, {"id": "Plotting Similarity", "label": "Plotting\nSimilarity", "title": "<b>Plotting Similarity</b> (subnode)<hr>Visualizing data points in PCA space to identify similar and clustered groups.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Visualizing data points in PCA space to identify similar and clustered groups.", "node_type": "subnode"}, {"id": "Dimension Reduction Before Supervised Learning", "label": "Dimension\nReduction\nBefore\nSupervised\nLearning", "title": "<b>Dimension Reduction Before Supervised Learning</b> (subnode)<hr>Preprocessing step that reduces dimensionality before applying supervised learning algorithms.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Preprocessing step that reduces dimensionality before applying supervised learning algorithms.", "node_type": "subnode"}, {"id": "Noise Reduction", "label": "Noise Reduction", "title": "<b>Noise Reduction</b> (subnode)<hr>Using PCA to estimate intrinsic features from noisy data, such as estimating piloting skill from noisy measurements.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Using PCA to estimate intrinsic features from noisy data, such as estimating piloting skill from noisy measurements.", "node_type": "subnode"}, {"id": "Eigenfaces Method", "label": "Eigenfaces\nMethod", "title": "<b>Eigenfaces Method</b> (subnode)<hr>Application of PCA in face recognition by reducing the dimensionality of image vectors.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Application of PCA in face recognition by reducing the dimensionality of image vectors.", "node_type": "subnode"}, {"id": "Independent Components Analysis (ICA)", "label": "Independent\nComponents\nAnalysis (ICA)", "title": "<b>Independent Components Analysis (ICA)</b> (major)<hr>A computational method for separating a multivariate signal into independent, non-Gaussian components assuming that the recorded signals are linear mixtures of some unknown latent variables.", "shape": "star", "size": 25, "color": "#FF6347", "description": "A computational method for separating a multivariate signal into independent, non-Gaussian components assuming that the recorded signals are linear mixtures of some unknown latent variables.", "node_type": "major"}, {"id": "Machine_Learning_Topics", "label": "Machine_Learnin\ng_Topics", "title": "<b>Machine_Learning_Topics</b> (major)<hr>Various topics in machine learning.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Various topics in machine learning.", "node_type": "major"}, {"id": "ICA", "label": "ICA", "title": "<b>ICA</b> (subnode)<hr>Independent Component Analysis (ICA) for separating mixed signals.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Independent Component Analysis (ICA) for separating mixed signals.", "node_type": "subnode"}, {"id": "Cocktail_Party_Problem", "label": "Cocktail_Party_\nProblem", "title": "<b>Cocktail_Party_Problem</b> (subnode)<hr>Example problem of ICA where multiple speakers' voices are separated from microphone recordings.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Example problem of ICA where multiple speakers' voices are separated from microphone recordings.", "node_type": "subnode"}, {"id": "Mixing_Matrix_A", "label": "Mixing_Matrix_A", "title": "<b>Mixing_Matrix_A</b> (subnode)<hr>Matrix A that mixes independent sources into observed data.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Matrix A that mixes independent sources into observed data.", "node_type": "subnode"}, {"id": "Unmixing_Matrix_W", "label": "Unmixing_Matrix\n_W", "title": "<b>Unmixing_Matrix_W</b> (subnode)<hr>Inverse of mixing matrix used to recover original sources from mixed signals.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Inverse of mixing matrix used to recover original sources from mixed signals.", "node_type": "subnode"}, {"id": "ICA_Ambiguities", "label": "ICA_Ambiguities", "title": "<b>ICA_Ambiguities</b> (subnode)<hr>Discussion on the ambiguities in recovering the unmixing matrix W without prior knowledge.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Discussion on the ambiguities in recovering the unmixing matrix W without prior knowledge.", "node_type": "subnode"}, {"id": "ICA Ambiguities", "label": "ICA Ambiguities", "title": "<b>ICA Ambiguities</b> (major)<hr>Discusses inherent ambiguities in ICA recovery", "shape": "star", "size": 25, "color": "#FF6347", "description": "Discusses inherent ambiguities in ICA recovery", "node_type": "major"}, {"id": "Permutation Matrix", "label": "Permutation\nMatrix", "title": "<b>Permutation Matrix</b> (subnode)<hr>Matrix that permutes the coordinates of a vector", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Matrix that permutes the coordinates of a vector", "node_type": "subnode"}, {"id": "Scaling Ambiguity", "label": "Scaling\nAmbiguity", "title": "<b>Scaling Ambiguity</b> (subnode)<hr>Ambiguity in determining correct scaling factors for sources", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Ambiguity in determining correct scaling factors for sources", "node_type": "subnode"}, {"id": "Volume Adjustment", "label": "Volume\nAdjustment", "title": "<b>Volume Adjustment</b> (subnode)<hr>Adjusting volume does not affect the identification of sources", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Adjusting volume does not affect the identification of sources", "node_type": "subnode"}, {"id": "Scaling_Impact", "label": "Scaling_Impact", "title": "<b>Scaling_Impact</b> (subnode)<hr>Explains the effect of scaling a speaker's speech signal.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Explains the effect of scaling a speaker's speech signal.", "node_type": "subnode"}, {"id": "Sign_Ignorance", "label": "Sign_Ignorance", "title": "<b>Sign_Ignorance</b> (subnode)<hr>Notes that sign changes in signals are irrelevant.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Notes that sign changes in signals are irrelevant.", "node_type": "subnode"}, {"id": "Non_Gaussian_Sources", "label": "Non_Gaussian_So\nurces", "title": "<b>Non_Gaussian_Sources</b> (subnode)<hr>States that non-Gaussian sources resolve ambiguities.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "States that non-Gaussian sources resolve ambiguities.", "node_type": "subnode"}, {"id": "Gaussian_Data_Issue", "label": "Gaussian_Data_I\nssue", "title": "<b>Gaussian_Data_Issue</b> (subnode)<hr>Describes the problem with Gaussian data in ICA.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Describes the problem with Gaussian data in ICA.", "node_type": "subnode"}, {"id": "Mixing_Matrix_Rotation", "label": "Mixing_Matrix_R\notation", "title": "<b>Mixing_Matrix_Rotation</b> (subnode)<hr>Explains how rotation of mixing matrix affects Gaussian data.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Explains how rotation of mixing matrix affects Gaussian data.", "node_type": "subnode"}, {"id": "ICAOnGaussianData", "label": "ICAOnGaussianDa\nta", "title": "<b>ICAOnGaussianData</b> (subnode)<hr>Discussion on the limitations of Independent Component Analysis (ICA) when applied to Gaussian data due to rotational symmetry.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Discussion on the limitations of Independent Component Analysis (ICA) when applied to Gaussian data due to rotational symmetry.", "node_type": "subnode"}, {"id": "RotationalSymmetry", "label": "RotationalSymme\ntry", "title": "<b>RotationalSymmetry</b> (subnode)<hr>Explanation that multivariate standard normal distribution is rotationally symmetric, making ICA impossible on Gaussian data.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Explanation that multivariate standard normal distribution is rotationally symmetric, making ICA impossible on Gaussian data.", "node_type": "subnode"}, {"id": "NonGaussianDataRecovery", "label": "NonGaussianData\nRecovery", "title": "<b>NonGaussianDataRecovery</b> (subnode)<hr>Discussion on the possibility of recovering independent sources from non-Gaussian data using sufficient data.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Discussion on the possibility of recovering independent sources from non-Gaussian data using sufficient data.", "node_type": "subnode"}, {"id": "LinearTransformationsEffect", "label": "LinearTransform\nationsEffect", "title": "<b>LinearTransformationsEffect</b> (subnode)<hr>Explanation of how linear transformations affect densities in random variables.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Explanation of how linear transformations affect densities in random variables.", "node_type": "subnode"}, {"id": "Density Transformation", "label": "Density\nTransformation", "title": "<b>Density Transformation</b> (major)<hr>Transformation of density functions under linear transformations.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Transformation of density functions under linear transformations.", "node_type": "major"}, {"id": "1D Example", "label": "1D Example", "title": "<b>1D Example</b> (subnode)<hr>Example in one dimension illustrating the transformation formula.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Example in one dimension illustrating the transformation formula.", "node_type": "subnode"}, {"id": "General Case", "label": "General Case", "title": "<b>General Case</b> (subnode)<hr>Generalization to vector-valued distributions and higher dimensions.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Generalization to vector-valued distributions and higher dimensions.", "node_type": "subnode"}, {"id": "Volume Calculation", "label": "Volume\nCalculation", "title": "<b>Volume Calculation</b> (subnode)<hr>Calculation of volume changes under linear transformations.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Calculation of volume changes under linear transformations.", "node_type": "subnode"}, {"id": "ICA Algorithm", "label": "ICA Algorithm", "title": "<b>ICA Algorithm</b> (major)<hr>Derivation and interpretation of an ICA algorithm based on maximum likelihood estimation.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Derivation and interpretation of an ICA algorithm based on maximum likelihood estimation.", "node_type": "major"}, {"id": "Bell and Sejnowski's Method", "label": "Bell and\nSejnowski's\nMethod", "title": "<b>Bell and Sejnowski's Method</b> (subnode)<hr>Description of the ICA method by Bell and Sejnowski.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Description of the ICA method by Bell and Sejnowski.", "node_type": "subnode"}, {"id": "ICAConcepts", "label": "ICAConcepts", "title": "<b>ICAConcepts</b> (subnode)<hr>Introduction to Independent Component Analysis (ICA) principles and applications.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Introduction to Independent Component Analysis (ICA) principles and applications.", "node_type": "subnode"}, {"id": "JointDistributionModeling", "label": "JointDistributi\nonModeling", "title": "<b>JointDistributionModeling</b> (subnode)<hr>Description of modeling joint distribution as a product of marginals for independent sources.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Description of modeling joint distribution as a product of marginals for independent sources.", "node_type": "subnode"}, {"id": "CumulativeDistributionFunction", "label": "CumulativeDistr\nibutionFunction", "title": "<b>CumulativeDistributionFunction</b> (subnode)<hr>Definition and properties of cumulative distribution function (CDF).", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Definition and properties of cumulative distribution function (CDF).", "node_type": "subnode"}, {"id": "DataPreprocessing", "label": "DataPreprocessi\nng", "title": "<b>DataPreprocessing</b> (subnode)<hr>Process of preparing data for analysis by normalizing or transforming it.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Process of preparing data for analysis by normalizing or transforming it.", "node_type": "subnode"}, {"id": "LogisticFunctionDerivative", "label": "LogisticFunctio\nnDerivative", "title": "<b>LogisticFunctionDerivative</b> (subnode)<hr>The derivative of the logistic function and its properties.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "The derivative of the logistic function and its properties.", "node_type": "subnode"}, {"id": "ModelParameters", "label": "ModelParameters", "title": "<b>ModelParameters</b> (subnode)<hr>Description of model parameters such as matrix W.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Description of model parameters such as matrix W.", "node_type": "subnode"}, {"id": "LogLikelihoodCalculation", "label": "LogLikelihoodCa\nlculation", "title": "<b>LogLikelihoodCalculation</b> (subnode)<hr>Computation of log likelihood for a given set of data and parameters.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Computation of log likelihood for a given set of data and parameters.", "node_type": "subnode"}, {"id": "ConvergenceAndSourceRecovery", "label": "ConvergenceAndS\nourceRecovery", "title": "<b>ConvergenceAndSourceRecovery</b> (subnode)<hr>Process after convergence to recover original sources from data.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Process after convergence to recover original sources from data.", "node_type": "subnode"}, {"id": "IndependenceAssumption", "label": "IndependenceAss\numption", "title": "<b>IndependenceAssumption</b> (subnode)<hr>Discussion on the independence assumption of training examples and its implications.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Discussion on the independence assumption of training examples and its implications.", "node_type": "subnode"}, {"id": "Stochastic Gradient Ascent", "label": "Stochastic\nGradient Ascent", "title": "<b>Stochastic Gradient Ascent</b> (subnode)<hr>Optimization technique used for minimizing loss functions in machine learning models.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Optimization technique used for minimizing loss functions in machine learning models.", "node_type": "subnode"}, {"id": "Self-supervised Learning", "label": "Self-supervised\nLearning", "title": "<b>Self-supervised Learning</b> (major)<hr>Learning paradigm where the model learns from unlabelled data using implicit supervision from the input itself.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Learning paradigm where the model learns from unlabelled data using implicit supervision from the input itself.", "node_type": "major"}, {"id": "Foundation Models", "label": "Foundation\nModels", "title": "<b>Foundation Models</b> (subnode)<hr>Models pre-trained on large datasets that can be adapted to various downstream tasks with limited labeled data.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Models pre-trained on large datasets that can be adapted to various downstream tasks with limited labeled data.", "node_type": "subnode"}, {"id": "Pretraining and Adaptation", "label": "Pretraining and\nAdaptation", "title": "<b>Pretraining and Adaptation</b> (subnode)<hr>Two-phase process involving training a model on unlabeled data followed by adapting it for specific tasks.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Two-phase process involving training a model on unlabeled data followed by adapting it for specific tasks.", "node_type": "subnode"}, {"id": "Transfer_Learning", "label": "Transfer_Learni\nng", "title": "<b>Transfer_Learning</b> (subnode)<hr>Technique where a pre-trained model is adapted to new tasks.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Technique where a pre-trained model is adapted to new tasks.", "node_type": "subnode"}, {"id": "Pretraining_Phase", "label": "Pretraining_Pha\nse", "title": "<b>Pretraining_Phase</b> (subnode)<hr>Initial training phase on large, unlabeled datasets.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Initial training phase on large, unlabeled datasets.", "node_type": "subnode"}, {"id": "Adaptation_Phase", "label": "Adaptation_Phas\ne", "title": "<b>Adaptation_Phase</b> (subnode)<hr>Customizing the pre-trained model for specific tasks.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Customizing the pre-trained model for specific tasks.", "node_type": "subnode"}, {"id": "Unlabeled_Dataset", "label": "Unlabeled_Datas\net", "title": "<b>Unlabeled_Dataset</b> (subnode)<hr>Dataset used in initial training without labels.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Dataset used in initial training without labels.", "node_type": "subnode"}, {"id": "Labeled_Task_Dataset", "label": "Labeled_Task_Da\ntaset", "title": "<b>Labeled_Task_Dataset</b> (subnode)<hr>Dataset with labeled data for specific tasks.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Dataset with labeled data for specific tasks.", "node_type": "subnode"}, {"id": "Model_Parameter_Theta", "label": "Model_Parameter\n_Theta", "title": "<b>Model_Parameter_Theta</b> (subnode)<hr>Parameters of the model during pretraining.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Parameters of the model during pretraining.", "node_type": "subnode"}, {"id": "Embedding_Features", "label": "Embedding_Featu\nres", "title": "<b>Embedding_Features</b> (subnode)<hr>Representations learned by the model from data.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Representations learned by the model from data.", "node_type": "subnode"}, {"id": "Self_Supervised_Loss", "label": "Self_Supervised\n_Loss", "title": "<b>Self_Supervised_Loss</b> (subnode)<hr>Loss function using data point itself as supervision.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Loss function using data point itself as supervision.", "node_type": "subnode"}, {"id": "Machine_Learning_Adaptation", "label": "Machine_Learnin\ng_Adaptation", "title": "<b>Machine_Learning_Adaptation</b> (major)<hr>Overview of machine learning adaptation techniques.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Overview of machine learning adaptation techniques.", "node_type": "major"}, {"id": "Downstream_Task_Dataset", "label": "Downstream_Task\n_Dataset", "title": "<b>Downstream_Task_Dataset</b> (subnode)<hr>Labeled dataset for a specific task with varying sizes.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Labeled dataset for a specific task with varying sizes.", "node_type": "subnode"}, {"id": "Zero_Shot_Learning", "label": "Zero_Shot_Learn\ning", "title": "<b>Zero_Shot_Learning</b> (subnode)<hr>Scenario where no labeled data is available for the downstream task.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Scenario where no labeled data is available for the downstream task.", "node_type": "subnode"}, {"id": "Few_Shot_Learning", "label": "Few_Shot_Learni\nng", "title": "<b>Few_Shot_Learning</b> (subnode)<hr>Situation with a small number of labeled examples, typically between 1 and 50.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Situation with a small number of labeled examples, typically between 1 and 50.", "node_type": "subnode"}, {"id": "Adaptation_Algorithm", "label": "Adaptation_Algo\nrithm", "title": "<b>Adaptation_Algorithm</b> (subnode)<hr>Process that modifies a pretrained model to fit the downstream task.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Process that modifies a pretrained model to fit the downstream task.", "node_type": "subnode"}, {"id": "Linear_Probe_Method", "label": "Linear_Probe_Me\nthod", "title": "<b>Linear_Probe_Method</b> (subnode)<hr>Uses a linear head on top of fixed pretrained model representations for prediction.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Uses a linear head on top of fixed pretrained model representations for prediction.", "node_type": "subnode"}, {"id": "Finetuning_Method", "label": "Finetuning_Meth\nod", "title": "<b>Finetuning_Method</b> (subnode)<hr>Modifies both the downstream task parameters and the pretrained model parameters.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Modifies both the downstream task parameters and the pretrained model parameters.", "node_type": "subnode"}, {"id": "Language_Problems_Adaptation", "label": "Language_Proble\nms_Adaptation", "title": "<b>Language_Problems_Adaptation</b> (subnode)<hr>Specific adaptation methods for language tasks discussed in 14.3.2.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Specific adaptation methods for language tasks discussed in 14.3.2.", "node_type": "subnode"}, {"id": "Machine_Learning_Adaptation_Methods", "label": "Machine_Learnin\ng_Adaptation_Me\nthods", "title": "<b>Machine_Learning_Adaptation_Methods</b> (major)<hr>Methods for adapting pre-trained models to new tasks.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Methods for adapting pre-trained models to new tasks.", "node_type": "major"}, {"id": "Finetuning_Pretrained_Models", "label": "Finetuning_Pret\nrained_Models", "title": "<b>Finetuning_Pretrained_Models</b> (subnode)<hr>Adjusting both weights and pretrained model parameters during downstream task training.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Adjusting both weights and pretrained model parameters during downstream task training.", "node_type": "subnode"}, {"id": "Linear_Head_Initiation", "label": "Linear_Head_Ini\ntiation", "title": "<b>Linear_Head_Initiation</b> (subnode)<hr>Random initialization of the linear head weight vector w.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Random initialization of the linear head weight vector w.", "node_type": "subnode"}, {"id": "Optimization_Objective", "label": "Optimization_Ob\njective", "title": "<b>Optimization_Objective</b> (subnode)<hr>Objective function to minimize loss for downstream tasks.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Objective function to minimize loss for downstream tasks.", "node_type": "subnode"}, {"id": "Pretraining_Methods_Computer_Vision", "label": "Pretraining_Met\nhods_Computer_V\nision", "title": "<b>Pretraining_Methods_Computer_Vision</b> (major)<hr>Techniques used in computer vision for pre-training models.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Techniques used in computer vision for pre-training models.", "node_type": "major"}, {"id": "Supervised_Pretraining", "label": "Supervised_Pret\nraining", "title": "<b>Supervised_Pretraining</b> (subnode)<hr>Training with large labeled datasets to learn representations.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Training with large labeled datasets to learn representations.", "node_type": "subnode"}, {"id": "Contrastive_Learning", "label": "Contrastive_Lea\nrning", "title": "<b>Contrastive_Learning</b> (subnode)<hr>Self-supervised method using unlabeled data for representation learning.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Self-supervised method using unlabeled data for representation learning.", "node_type": "subnode"}, {"id": "Self-Supervised Learning", "label": "Self-Supervised\nLearning", "title": "<b>Self-Supervised Learning</b> (major)<hr>Method using unlabeled data for pretraining.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Method using unlabeled data for pretraining.", "node_type": "major"}, {"id": "Representation Function", "label": "Representation\nFunction", "title": "<b>Representation Function</b> (subnode)<hr>Function \u03c6\u03b8(\u2218) maps images to representations.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Function \u03c6\u03b8(\u2218) maps images to representations.", "node_type": "subnode"}, {"id": "Positive Pair", "label": "Positive Pair", "title": "<b>Positive Pair</b> (subnode)<hr>Augmentations of the same image, semantically related.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Augmentations of the same image, semantically related.", "node_type": "subnode"}, {"id": "Negative Pair", "label": "Negative Pair", "title": "<b>Negative Pair</b> (subnode)<hr>Randomly selected augmentations from different images.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Randomly selected augmentations from different images.", "node_type": "subnode"}, {"id": "Data Augmentation", "label": "Data\nAugmentation", "title": "<b>Data Augmentation</b> (subnode)<hr>Technique to generate variations of an image for training.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Technique to generate variations of an image for training.", "node_type": "subnode"}, {"id": "Supervised Contrastive Algorithms", "label": "Supervised\nContrastive\nAlgorithms", "title": "<b>Supervised Contrastive Algorithms</b> (subnode)<hr>Use labeled data and class similarity for pretraining.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Use labeled data and class similarity for pretraining.", "node_type": "subnode"}, {"id": "ContrastiveLearning", "label": "ContrastiveLear\nning", "title": "<b>ContrastiveLearning</b> (subnode)<hr>Technique used to learn representations by contrasting positive and negative pairs.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Technique used to learn representations by contrasting positive and negative pairs.", "node_type": "subnode"}, {"id": "SIMCLRAlgorithm", "label": "SIMCLRAlgorithm", "title": "<b>SIMCLRAlgorithm</b> (subnode)<hr>Specific algorithm based on contrastive learning for unsupervised representation learning.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Specific algorithm based on contrastive learning for unsupervised representation learning.", "node_type": "subnode"}, {"id": "AugmentationTechniques", "label": "AugmentationTec\nhniques", "title": "<b>AugmentationTechniques</b> (subnode)<hr>Methods for creating variations of input data to improve model robustness.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Methods for creating variations of input data to improve model robustness.", "node_type": "subnode"}, {"id": "PositivePairs", "label": "PositivePairs", "title": "<b>PositivePairs</b> (subnode)<hr>Data pairs that should be close in the learned representation space.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Data pairs that should be close in the learned representation space.", "node_type": "subnode"}, {"id": "NegativePairs", "label": "NegativePairs", "title": "<b>NegativePairs</b> (subnode)<hr>Data pairs that should be far apart in the learned representation space.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Data pairs that should be far apart in the learned representation space.", "node_type": "subnode"}, {"id": "Pretrained_Models", "label": "Pretrained_Mode\nls", "title": "<b>Pretrained_Models</b> (major)<hr>Overview of pretrained models in machine learning, particularly focusing on language processing.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Overview of pretrained models in machine learning, particularly focusing on language processing.", "node_type": "major"}, {"id": "Natural_Language_Processing", "label": "Natural_Languag\ne_Processing", "title": "<b>Natural_Language_Processing</b> (subnode)<hr>Application of pretraining techniques to natural language processing tasks.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Application of pretraining techniques to natural language processing tasks.", "node_type": "subnode"}, {"id": "Language_Models", "label": "Language_Models", "title": "<b>Language_Models</b> (subnode)<hr>Probabilistic models representing the probability distribution over sequences of words in a document.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Probabilistic models representing the probability distribution over sequences of words in a document.", "node_type": "subnode"}, {"id": "ConditionalProbabilityModeling", "label": "ConditionalProb\nabilityModeling", "title": "<b>ConditionalProbabilityModeling</b> (subnode)<hr>Modeling conditional probabilities in sequence prediction tasks.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Modeling conditional probabilities in sequence prediction tasks.", "node_type": "subnode"}, {"id": "ParameterizedFunction", "label": "ParameterizedFu\nnction", "title": "<b>ParameterizedFunction</b> (subnode)<hr>Using parameterized functions to model conditional probabilities.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Using parameterized functions to model conditional probabilities.", "node_type": "subnode"}, {"id": "Embeddings", "label": "Embeddings", "title": "<b>Embeddings</b> (subnode)<hr>Introduction of word embeddings for numerical input handling.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Introduction of word embeddings for numerical input handling.", "node_type": "subnode"}, {"id": "TransformerModel", "label": "TransformerMode\nl", "title": "<b>TransformerModel</b> (subnode)<hr>Overview and use of the Transformer model in sequence prediction tasks.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Overview and use of the Transformer model in sequence prediction tasks.", "node_type": "subnode"}, {"id": "InputOutputInterface", "label": "InputOutputInte\nrface", "title": "<b>InputOutputInterface</b> (subnode)<hr>Description of how input sequences are transformed into output logits using a Transformer model.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Description of how input sequences are transformed into output logits using a Transformer model.", "node_type": "subnode"}, {"id": "Transformer_Models", "label": "Transformer_Mod\nels", "title": "<b>Transformer_Models</b> (major)<hr>Models that map inputs to outputs using a blackbox function.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Models that map inputs to outputs using a blackbox function.", "node_type": "major"}, {"id": "Conditional_Probability", "label": "Conditional_Pro\nbability", "title": "<b>Conditional_Probability</b> (subnode)<hr>Probability of the next input given previous inputs.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Probability of the next input given previous inputs.", "node_type": "subnode"}, {"id": "Training_Transformer", "label": "Training_Transf\normer", "title": "<b>Training_Transformer</b> (subnode)<hr>Minimizing negative log-likelihood to train the model parameters.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Minimizing negative log-likelihood to train the model parameters.", "node_type": "subnode"}, {"id": "Autoregressive_Decoding", "label": "Autoregressive_\nDecoding", "title": "<b>Autoregressive_Decoding</b> (subnode)<hr>Generating text sequentially using the conditional distribution from previous tokens.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Generating text sequentially using the conditional distribution from previous tokens.", "node_type": "subnode"}, {"id": "LanguageModels", "label": "LanguageModels", "title": "<b>LanguageModels</b> (subnode)<hr>Models that generate text based on learned patterns from large datasets.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Models that generate text based on learned patterns from large datasets.", "node_type": "subnode"}, {"id": "TemperatureParameter", "label": "TemperaturePara\nmeter", "title": "<b>TemperatureParameter</b> (subnode)<hr>A parameter that adjusts the randomness or determinism of generated text.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "A parameter that adjusts the randomness or determinism of generated text.", "node_type": "subnode"}, {"id": "TextGeneration", "label": "TextGeneration", "title": "<b>TextGeneration</b> (subnode)<hr>The process of generating sequences of tokens based on learned probabilities.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "The process of generating sequences of tokens based on learned probabilities.", "node_type": "subnode"}, {"id": "AdaptiveSampling", "label": "AdaptiveSamplin\ng", "title": "<b>AdaptiveSampling</b> (subnode)<hr>Adjusting the sampling method to control randomness in text generation.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Adjusting the sampling method to control randomness in text generation.", "node_type": "subnode"}, {"id": "ModelAdaptation", "label": "ModelAdaptation", "title": "<b>ModelAdaptation</b> (subnode)<hr>Techniques for adapting pretrained models to new tasks without fine-tuning.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Techniques for adapting pretrained models to new tasks without fine-tuning.", "node_type": "subnode"}, {"id": "Finetuning", "label": "Finetuning", "title": "<b>Finetuning</b> (subnode)<hr>Adjusting model parameters based on a specific task's dataset.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Adjusting model parameters based on a specific task's dataset.", "node_type": "subnode"}, {"id": "ZeroShotLearning", "label": "ZeroShotLearnin\ng", "title": "<b>ZeroShotLearning</b> (subnode)<hr>Using pretrained models for tasks without additional training data.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Using pretrained models for tasks without additional training data.", "node_type": "subnode"}, {"id": "InContextLearning", "label": "InContextLearni\nng", "title": "<b>InContextLearning</b> (subnode)<hr>A method where the model learns from a small number of examples provided during inference.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "A method where the model learns from a small number of examples provided during inference.", "node_type": "subnode"}, {"id": "Machine_Learning_Adaptation_Techniques", "label": "Machine_Learnin\ng_Adaptation_Te\nchniques", "title": "<b>Machine_Learning_Adaptation_Techniques</b> (major)<hr>Techniques for adapting machine learning models to new tasks without additional training data.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Techniques for adapting machine learning models to new tasks without additional training data.", "node_type": "major"}, {"id": "Zero-Shot_Adaptation", "label": "Zero-\nShot_Adaptation", "title": "<b>Zero-Shot_Adaptation</b> (subnode)<hr>Method where no input-output pairs from downstream tasks are available.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Method where no input-output pairs from downstream tasks are available.", "node_type": "subnode"}, {"id": "In-Context_Learning", "label": "In-Context_Lear\nning", "title": "<b>In-Context_Learning</b> (subnode)<hr>Approach for few-shot settings using a small number of labeled examples to guide model predictions.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Approach for few-shot settings using a small number of labeled examples to guide model predictions.", "node_type": "subnode"}, {"id": "Task_Formulation", "label": "Task_Formulatio\nn", "title": "<b>Task_Formulation</b> (subnode)<hr>Formulating tasks as questions or cloze tests for language problems.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Formulating tasks as questions or cloze tests for language problems.", "node_type": "subnode"}, {"id": "Prompting_Strategy", "label": "Prompting_Strat\negy", "title": "<b>Prompting_Strategy</b> (subnode)<hr>Strategy of using labeled examples to construct prompts for guiding model predictions.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Strategy of using labeled examples to construct prompts for guiding model predictions.", "node_type": "subnode"}, {"id": "Model_Optimization", "label": "Model_Optimizat\nion", "title": "<b>Model_Optimization</b> (subnode)<hr>Optimizing parameters in a pretrained model for new tasks.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Optimizing parameters in a pretrained model for new tasks.", "node_type": "subnode"}, {"id": "Reinforcement Learning", "label": "Reinforcement\nLearning", "title": "<b>Reinforcement Learning</b> (major)<hr>Type of machine learning where an agent learns to make decisions based on rewards and punishments.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Type of machine learning where an agent learns to make decisions based on rewards and punishments.", "node_type": "major"}, {"id": "Sequential Decision Making", "label": "Sequential\nDecision Making", "title": "<b>Sequential Decision Making</b> (subnode)<hr>Decision making in sequences without explicit supervision.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Decision making in sequences without explicit supervision.", "node_type": "subnode"}, {"id": "Markov Decision Processes (MDP)", "label": "Markov Decision\nProcesses (MDP)", "title": "<b>Markov Decision Processes (MDP)</b> (subnode)<hr>Formal framework for modeling decision-making situations in reinforcement learning.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Formal framework for modeling decision-making situations in reinforcement learning.", "node_type": "subnode"}, {"id": "States", "label": "States", "title": "<b>States</b> (subnode)<hr>Set of all possible conditions or configurations in an environment.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Set of all possible conditions or configurations in an environment.", "node_type": "subnode"}, {"id": "Actions", "label": "Actions", "title": "<b>Actions</b> (subnode)<hr>Set of all possible actions that can be taken in a given state.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Set of all possible actions that can be taken in a given state.", "node_type": "subnode"}, {"id": "State Transition Probabilities", "label": "State\nTransition\nProbabilities", "title": "<b>State Transition Probabilities</b> (subnode)<hr>Probabilities associated with moving from one state to another based on an action.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Probabilities associated with moving from one state to another based on an action.", "node_type": "subnode"}, {"id": "Discount Factor", "label": "Discount Factor", "title": "<b>Discount Factor</b> (subnode)<hr>Parameter that determines the importance of future rewards relative to immediate ones.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Parameter that determines the importance of future rewards relative to immediate ones.", "node_type": "subnode"}, {"id": "Reward Function", "label": "Reward Function", "title": "<b>Reward Function</b> (subnode)<hr>Function mapping state-action pairs or states to real numbers representing rewards.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Function mapping state-action pairs or states to real numbers representing rewards.", "node_type": "subnode"}, {"id": "Markov Decision Process (MDP)", "label": "Markov Decision\nProcess (MDP)", "title": "<b>Markov Decision Process (MDP)</b> (major)<hr>Framework for modeling decision-making in situations where outcomes are partly random and partly under the control of a decision maker.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Framework for modeling decision-making in situations where outcomes are partly random and partly under the control of a decision maker.", "node_type": "major"}, {"id": "State Transition", "label": "State\nTransition", "title": "<b>State Transition</b> (subnode)<hr>Random transition from one state to another based on action taken.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Random transition from one state to another based on action taken.", "node_type": "subnode"}, {"id": "Action Selection", "label": "Action\nSelection", "title": "<b>Action Selection</b> (subnode)<hr>Choosing actions in each state according to a policy.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Choosing actions in each state according to a policy.", "node_type": "subnode"}, {"id": "Total Payoff", "label": "Total Payoff", "title": "<b>Total Payoff</b> (subnode)<hr>Cumulative reward over time, discounted by \u03b3^t.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Cumulative reward over time, discounted by \u03b3^t.", "node_type": "subnode"}, {"id": "Discount Factor (\u03b3)", "label": "Discount Factor\n(\u03b3)", "title": "<b>Discount Factor (\u03b3)</b> (subnode)<hr>Factor that discounts future rewards based on their temporal distance from the present.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Factor that discounts future rewards based on their temporal distance from the present.", "node_type": "subnode"}, {"id": "Policy", "label": "Policy", "title": "<b>Policy</b> (major)<hr>Function mapping states to actions in reinforcement learning.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Function mapping states to actions in reinforcement learning.", "node_type": "major"}, {"id": "Value Function", "label": "Value Function", "title": "<b>Value Function</b> (subnode)<hr>Expected sum of discounted rewards for starting in state s and following policy \u03c0.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Expected sum of discounted rewards for starting in state s and following policy \u03c0.", "node_type": "subnode"}, {"id": "Policy Execution", "label": "Policy\nExecution", "title": "<b>Policy Execution</b> (major)<hr>Process of selecting actions based on a policy in given states.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Process of selecting actions based on a policy in given states.", "node_type": "major"}, {"id": "Bellman Equations", "label": "Bellman\nEquations", "title": "<b>Bellman Equations</b> (subnode)<hr>Set of equations used to solve for value function V^\u03c0(s) in finite-state MDPs.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Set of equations used to solve for value function V^\u03c0(s) in finite-state MDPs.", "node_type": "subnode"}, {"id": "Immediate Reward", "label": "Immediate\nReward", "title": "<b>Immediate Reward</b> (subnode)<hr>Reward received immediately upon entering a state s.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Reward received immediately upon entering a state s.", "node_type": "subnode"}, {"id": "Future Discounted Rewards", "label": "Future\nDiscounted\nRewards", "title": "<b>Future Discounted Rewards</b> (subnode)<hr>Expected sum of discounted rewards after the first step in an MDP from state s.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Expected sum of discounted rewards after the first step in an MDP from state s.", "node_type": "subnode"}, {"id": "Policy Evaluation", "label": "Policy\nEvaluation", "title": "<b>Policy Evaluation</b> (subnode)<hr>Process of calculating value function given a fixed policy.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Process of calculating value function given a fixed policy.", "node_type": "subnode"}, {"id": "Optimal Value Function", "label": "Optimal Value\nFunction", "title": "<b>Optimal Value Function</b> (subnode)<hr>Best possible expected sum of discounted rewards using any policy.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Best possible expected sum of discounted rewards using any policy.", "node_type": "subnode"}, {"id": "Bellman's Equation", "label": "Bellman's\nEquation", "title": "<b>Bellman's Equation</b> (subnode)<hr>Equation defining the optimal value function recursively.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Equation defining the optimal value function recursively.", "node_type": "subnode"}, {"id": "Optimal Policy", "label": "Optimal Policy", "title": "<b>Optimal Policy</b> (subnode)<hr>Policy that maximizes the expected sum of discounted rewards for all states.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Policy that maximizes the expected sum of discounted rewards for all states.", "node_type": "subnode"}, {"id": "Optimal_Policy", "label": "Optimal_Policy", "title": "<b>Optimal_Policy</b> (subnode)<hr>Policy \u03c0* that maximizes the value function for all states in an MDP.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Policy \u03c0* that maximizes the value function for all states in an MDP.", "node_type": "subnode"}, {"id": "MDP_Finite_State_Space", "label": "MDP_Finite_Stat\ne_Space", "title": "<b>MDP_Finite_State_Space</b> (subnode)<hr>Markov Decision Process with a finite set of states and actions.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Markov Decision Process with a finite set of states and actions.", "node_type": "subnode"}, {"id": "Value_Iteration", "label": "Value_Iteration", "title": "<b>Value_Iteration</b> (subnode)<hr>Algorithm used to find the optimal policy in reinforcement learning through iterative application of Bellman's equation.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Algorithm used to find the optimal policy in reinforcement learning through iterative application of Bellman's equation.", "node_type": "subnode"}, {"id": "Synchronous_Update", "label": "Synchronous_Upd\nate", "title": "<b>Synchronous_Update</b> (subnode)<hr>Update method where all state values are updated simultaneously before overwriting.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Update method where all state values are updated simultaneously before overwriting.", "node_type": "subnode"}, {"id": "Asynchronous_Update", "label": "Asynchronous_Up\ndate", "title": "<b>Asynchronous_Update</b> (subnode)<hr>Update method where each state value is updated one at a time in sequence.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Update method where each state value is updated one at a time in sequence.", "node_type": "subnode"}, {"id": "ValueIterationAlgorithm", "label": "ValueIterationA\nlgorithm", "title": "<b>ValueIterationAlgorithm</b> (subnode)<hr>Iterative algorithm to find optimal value function in an MDP.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Iterative algorithm to find optimal value function in an MDP.", "node_type": "subnode"}, {"id": "PolicyIterationAlgorithm", "label": "PolicyIteration\nAlgorithm", "title": "<b>PolicyIterationAlgorithm</b> (subnode)<hr>Alternative method for finding the optimal policy by iteratively improving policies and their corresponding value functions.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Alternative method for finding the optimal policy by iteratively improving policies and their corresponding value functions.", "node_type": "subnode"}, {"id": "ConvergenceOfAlgorithms", "label": "ConvergenceOfAl\ngorithms", "title": "<b>ConvergenceOfAlgorithms</b> (subnode)<hr>Conditions under which value iteration and policy iteration converge to optimal solutions.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Conditions under which value iteration and policy iteration converge to optimal solutions.", "node_type": "subnode"}, {"id": "ComparisonValuePolicyIteration", "label": "ComparisonValue\nPolicyIteration", "title": "<b>ComparisonValuePolicyIteration</b> (subnode)<hr>Discussion on the relative merits of value iteration versus policy iteration in different contexts.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Discussion on the relative merits of value iteration versus policy iteration in different contexts.", "node_type": "subnode"}, {"id": "BellmanEquations", "label": "BellmanEquation\ns", "title": "<b>BellmanEquations</b> (subnode)<hr>Set of equations used to solve for optimal policies and values in MDPs.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Set of equations used to solve for optimal policies and values in MDPs.", "node_type": "subnode"}, {"id": "Policy_Iteration", "label": "Policy_Iteratio\nn", "title": "<b>Policy_Iteration</b> (subnode)<hr>Alternative method to find optimal policies through successive policy improvements", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Alternative method to find optimal policies through successive policy improvements", "node_type": "subnode"}, {"id": "Learning_Model_for_MDP", "label": "Learning_Model_\nfor_MDP", "title": "<b>Learning_Model_for_MDP</b> (major)<hr>Discussion on learning state transition probabilities and rewards in MDPs from data.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Discussion on learning state transition probabilities and rewards in MDPs from data.", "node_type": "major"}, {"id": "Inverted_Pendulum_Problem", "label": "Inverted_Pendul\num_Problem", "title": "<b>Inverted_Pendulum_Problem</b> (subnode)<hr>Example problem used to illustrate the process of estimating MDP parameters from experience.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Example problem used to illustrate the process of estimating MDP parameters from experience.", "node_type": "subnode"}, {"id": "State_Transition_Probabilities", "label": "State_Transitio\nn_Probabilities", "title": "<b>State_Transition_Probabilities</b> (subnode)<hr>Estimation method for state transition probabilities using maximum likelihood estimation based on observed data.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Estimation method for state transition probabilities using maximum likelihood estimation based on observed data.", "node_type": "subnode"}, {"id": "MDP_Model_Learning", "label": "MDP_Model_Learn\ning", "title": "<b>MDP_Model_Learning</b> (major)<hr>Process of learning state transition probabilities and rewards in an MDP", "shape": "star", "size": 25, "color": "#FF6347", "description": "Process of learning state transition probabilities and rewards in an MDP", "node_type": "major"}, {"id": "Expected_Immediate_Rewards", "label": "Expected_Immedi\nate_Rewards", "title": "<b>Expected_Immediate_Rewards</b> (subnode)<hr>Calculation of average rewards observed in a particular state", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Calculation of average rewards observed in a particular state", "node_type": "subnode"}, {"id": "Optimization_Techniques", "label": "Optimization_Te\nchniques", "title": "<b>Optimization_Techniques</b> (subnode)<hr>Techniques for improving the efficiency of learning algorithms in MDPs", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Techniques for improving the efficiency of learning algorithms in MDPs", "node_type": "subnode"}, {"id": "Continuous_State_MDPs", "label": "Continuous_Stat\ne_MDPs", "title": "<b>Continuous_State_MDPs</b> (subnode)<hr>Discussion on Markov Decision Processes with infinite state spaces, such as those involving continuous variables.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Discussion on Markov Decision Processes with infinite state spaces, such as those involving continuous variables.", "node_type": "subnode"}, {"id": "Discretization_Method", "label": "Discretization_\nMethod", "title": "<b>Discretization_Method</b> (subnode)<hr>Technique to convert continuous-state MDPs into discrete-state MDPs for easier computation and algorithm application.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Technique to convert continuous-state MDPs into discrete-state MDPs for easier computation and algorithm application.", "node_type": "subnode"}, {"id": "Discretization in MDPs", "label": "Discretization\nin MDPs", "title": "<b>Discretization in MDPs</b> (subnode)<hr>Process of converting continuous state space into discrete states for easier computation.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Process of converting continuous state space into discrete states for easier computation.", "node_type": "subnode"}, {"id": "Value Iteration", "label": "Value Iteration", "title": "<b>Value Iteration</b> (subnode)<hr>Algorithm to find optimal value function in a discrete state Markov Decision Process.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Algorithm to find optimal value function in a discrete state Markov Decision Process.", "node_type": "subnode"}, {"id": "Policy Iteration", "label": "Policy\nIteration", "title": "<b>Policy Iteration</b> (subnode)<hr>Algorithm to find optimal policy in a discrete state Markov Decision Process.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Algorithm to find optimal policy in a discrete state Markov Decision Process.", "node_type": "subnode"}, {"id": "Supervised Learning", "label": "Supervised\nLearning", "title": "<b>Supervised Learning</b> (major)<hr>Learning from labeled data to predict outcomes for new inputs.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Learning from labeled data to predict outcomes for new inputs.", "node_type": "major"}, {"id": "Piecewise Constant Representation", "label": "Piecewise\nConstant\nRepresentation", "title": "<b>Piecewise Constant Representation</b> (subnode)<hr>Representation that assumes constant value within each discrete state interval.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Representation that assumes constant value within each discrete state interval.", "node_type": "subnode"}, {"id": "Curse of Dimensionality", "label": "Curse of\nDimensionality", "title": "<b>Curse of Dimensionality</b> (subnode)<hr>Exponential increase in volume required to represent the state space as dimensionality increases.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Exponential increase in volume required to represent the state space as dimensionality increases.", "node_type": "subnode"}, {"id": "StateRepresentation", "label": "StateRepresenta\ntion", "title": "<b>StateRepresentation</b> (subnode)<hr>Methods for representing states in machine learning models.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Methods for representing states in machine learning models.", "node_type": "subnode"}, {"id": "GridCellRepresentation", "label": "GridCellReprese\nntation", "title": "<b>GridCellRepresentation</b> (subnode)<hr>Using grid cells to represent continuous state spaces, requiring fine discretization.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Using grid cells to represent continuous state spaces, requiring fine discretization.", "node_type": "subnode"}, {"id": "CurseOfDimensionality", "label": "CurseOfDimensio\nnality", "title": "<b>CurseOfDimensionality</b> (subnode)<hr>Exponential increase in discrete states with dimensionality, limiting scalability of discretization methods.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Exponential increase in discrete states with dimensionality, limiting scalability of discretization methods.", "node_type": "subnode"}, {"id": "ValueFunctionApproximation", "label": "ValueFunctionAp\nproximation", "title": "<b>ValueFunctionApproximation</b> (major)<hr>Techniques for approximating the value function in reinforcement learning using supervised learning methods.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Techniques for approximating the value function in reinforcement learning using supervised learning methods.", "node_type": "major"}, {"id": "ModelOrSimulator", "label": "ModelOrSimulato\nr", "title": "<b>ModelOrSimulator</b> (subnode)<hr>Use of models or simulators to approximate value functions in continuous-state MDPs.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Use of models or simulators to approximate value functions in continuous-state MDPs.", "node_type": "subnode"}, {"id": "Model Creation Methods", "label": "Model Creation\nMethods", "title": "<b>Model Creation Methods</b> (major)<hr>Different methods to create a model for state transitions in an MDP.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Different methods to create a model for state transitions in an MDP.", "node_type": "major"}, {"id": "Physics Simulation", "label": "Physics\nSimulation", "title": "<b>Physics Simulation</b> (subnode)<hr>Using physical laws or software to simulate system behavior.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Using physical laws or software to simulate system behavior.", "node_type": "subnode"}, {"id": "Off-the-Shelf Software", "label": "Off-the-Shelf\nSoftware", "title": "<b>Off-the-Shelf Software</b> (subnode)<hr>Utilizing existing physics simulation tools like Open Dynamics Engine.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Utilizing existing physics simulation tools like Open Dynamics Engine.", "node_type": "subnode"}, {"id": "Learning from Data", "label": "Learning from\nData", "title": "<b>Learning from Data</b> (subnode)<hr>Inferring state transition probabilities from collected data in an MDP.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Inferring state transition probabilities from collected data in an MDP.", "node_type": "subnode"}, {"id": "Data Collection Process", "label": "Data Collection\nProcess", "title": "<b>Data Collection Process</b> (subnode)<hr>Executing trials and observing state sequences to learn model parameters.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Executing trials and observing state sequences to learn model parameters.", "node_type": "subnode"}, {"id": "LinearModelPrediction", "label": "LinearModelPred\niction", "title": "<b>LinearModelPrediction</b> (subnode)<hr>Using linear models to predict the next state based on current state and action.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Using linear models to predict the next state based on current state and action.", "node_type": "subnode"}, {"id": "DeterministicModel", "label": "DeterministicMo\ndel", "title": "<b>DeterministicModel</b> (subnode)<hr>Predicting exact outcomes given inputs in a deterministic manner.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Predicting exact outcomes given inputs in a deterministic manner.", "node_type": "subnode"}, {"id": "StochasticModel", "label": "StochasticModel", "title": "<b>StochasticModel</b> (subnode)<hr>Introducing randomness to model predictions with noise terms.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Introducing randomness to model predictions with noise terms.", "node_type": "subnode"}, {"id": "NonLinearFeatureMapping", "label": "NonLinearFeatur\neMapping", "title": "<b>NonLinearFeatureMapping</b> (subnode)<hr>Using non-linear transformations of states and actions for more complex models.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Using non-linear transformations of states and actions for more complex models.", "node_type": "subnode"}, {"id": "LossFunctions", "label": "LossFunctions", "title": "<b>LossFunctions</b> (subnode)<hr>Different functions used to measure the error in predictions, like L2 norm.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Different functions used to measure the error in predictions, like L2 norm.", "node_type": "subnode"}, {"id": "Non-linear Feature Mappings", "label": "Non-linear\nFeature\nMappings", "title": "<b>Non-linear Feature Mappings</b> (subnode)<hr>Feature mappings that transform states and actions into non-linear representations.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Feature mappings that transform states and actions into non-linear representations.", "node_type": "subnode"}, {"id": "MDP Simulators", "label": "MDP Simulators", "title": "<b>MDP Simulators</b> (subnode)<hr>Simulations of Markov Decision Processes using learned models.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Simulations of Markov Decision Processes using learned models.", "node_type": "subnode"}, {"id": "Fitted Value Iteration", "label": "Fitted Value\nIteration", "title": "<b>Fitted Value Iteration</b> (major)<hr>Algorithm for approximating the value function in continuous state MDPs.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Algorithm for approximating the value function in continuous state MDPs.", "node_type": "major"}, {"id": "Continuous State Space", "label": "Continuous\nState Space", "title": "<b>Continuous State Space</b> (subnode)<hr>The state space is modeled as a continuous set of real numbers.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "The state space is modeled as a continuous set of real numbers.", "node_type": "subnode"}, {"id": "Discrete Action Space", "label": "Discrete Action\nSpace", "title": "<b>Discrete Action Space</b> (subnode)<hr>Action space consists of a small, finite number of discrete actions.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Action space consists of a small, finite number of discrete actions.", "node_type": "subnode"}, {"id": "Value Function Approximation", "label": "Value Function\nApproximation", "title": "<b>Value Function Approximation</b> (subnode)<hr>Approximating the value function using supervised learning algorithms like linear regression.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Approximating the value function using supervised learning algorithms like linear regression.", "node_type": "subnode"}, {"id": "FittedValueIteration", "label": "FittedValueIter\nation", "title": "<b>FittedValueIteration</b> (subnode)<hr>Algorithm that uses linear regression to approximate the value function over a finite sample of states.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Algorithm that uses linear regression to approximate the value function over a finite sample of states.", "node_type": "subnode"}, {"id": "SupervisedLearning", "label": "SupervisedLearn\ning", "title": "<b>SupervisedLearning</b> (subnode)<hr>Process of training models on labeled data to predict outcomes for new inputs.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Process of training models on labeled data to predict outcomes for new inputs.", "node_type": "subnode"}, {"id": "StateSampling", "label": "StateSampling", "title": "<b>StateSampling</b> (subnode)<hr>Random selection of states to approximate the value function over a finite sample.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Random selection of states to approximate the value function over a finite sample.", "node_type": "subnode"}, {"id": "ActionEvaluation", "label": "ActionEvaluatio\nn", "title": "<b>ActionEvaluation</b> (subnode)<hr>Process of evaluating actions in sampled states to estimate future rewards and state values.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Process of evaluating actions in sampled states to estimate future rewards and state values.", "node_type": "subnode"}, {"id": "Supervised_Learning", "label": "Supervised_Lear\nning", "title": "<b>Supervised_Learning</b> (subnode)<hr>Learning with labeled training data to predict outcomes for new inputs.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Learning with labeled training data to predict outcomes for new inputs.", "node_type": "subnode"}, {"id": "Linear_Regression", "label": "Linear_Regressi\non", "title": "<b>Linear_Regression</b> (subnode)<hr>Statistical method for modeling relationships between a dependent variable and one or more independent variables.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Statistical method for modeling relationships between a dependent variable and one or more independent variables.", "node_type": "subnode"}, {"id": "Fitted_Value_Iteration", "label": "Fitted_Value_It\neration", "title": "<b>Fitted_Value_Iteration</b> (subnode)<hr>Technique used in reinforcement learning to approximate value functions using regression methods.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Technique used in reinforcement learning to approximate value functions using regression methods.", "node_type": "subnode"}, {"id": "Deterministic_Simulator", "label": "Deterministic_S\nimulator", "title": "<b>Deterministic_Simulator</b> (subnode)<hr>Simplification when using a deterministic model for the Markov Decision Process (MDP).", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Simplification when using a deterministic model for the Markov Decision Process (MDP).", "node_type": "subnode"}, {"id": "Policy_Definition", "label": "Policy_Definiti\non", "title": "<b>Policy_Definition</b> (subnode)<hr>Definition of policy based on approximated value function in reinforcement learning context.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Definition of policy based on approximated value function in reinforcement learning context.", "node_type": "subnode"}, {"id": "Expectation_Computation", "label": "Expectation_Com\nputation", "title": "<b>Expectation_Computation</b> (subnode)<hr>Process of approximating expectations in reinforcement learning algorithms.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Process of approximating expectations in reinforcement learning algorithms.", "node_type": "subnode"}, {"id": "Gaussian_Noise_Model", "label": "Gaussian_Noise_\nModel", "title": "<b>Gaussian_Noise_Model</b> (subnode)<hr>Model involving deterministic function and Gaussian noise.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Model involving deterministic function and Gaussian noise.", "node_type": "subnode"}, {"id": "Approximation_Methods", "label": "Approximation_M\nethods", "title": "<b>Approximation_Methods</b> (subnode)<hr>Techniques for approximating expectations in value iteration.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Techniques for approximating expectations in value iteration.", "node_type": "subnode"}, {"id": "Linear_System_Solver", "label": "Linear_System_S\nolver", "title": "<b>Linear_System_Solver</b> (subnode)<hr>Method used to compute policy evaluation in policy iteration.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Method used to compute policy evaluation in policy iteration.", "node_type": "subnode"}, {"id": "Bellman_Updates", "label": "Bellman_Updates", "title": "<b>Bellman_Updates</b> (subnode)<hr>Iterative process for evaluating policies using Bellman equations.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Iterative process for evaluating policies using Bellman equations.", "node_type": "subnode"}, {"id": "Algorithm 6", "label": "Algorithm 6", "title": "<b>Algorithm 6</b> (subnode)<hr>Variant of policy iteration that uses value evaluation procedure VE.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Variant of policy iteration that uses value evaluation procedure VE.", "node_type": "subnode"}, {"id": "Procedure VE", "label": "Procedure VE", "title": "<b>Procedure VE</b> (subnode)<hr>Evaluation function used in Algorithm 6 to update state values.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Evaluation function used in Algorithm 6 to update state values.", "node_type": "subnode"}, {"id": "Option 1 and Option 2", "label": "Option 1 and\nOption 2", "title": "<b>Option 1 and Option 2</b> (subnode)<hr>Two initialization options for the value evaluation procedure VE.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Two initialization options for the value evaluation procedure VE.", "node_type": "subnode"}, {"id": "Chapter_15_Summary", "label": "Chapter_15_Summ\nary", "title": "<b>Chapter_15_Summary</b> (major)<hr>Summary of Chapter 15 on MDPs and iterative methods.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Summary of Chapter 15 on MDPs and iterative methods.", "node_type": "major"}, {"id": "k_steps_update_frequency", "label": "k_steps_update_\nfrequency", "title": "<b>k_steps_update_frequency</b> (subnode)<hr>Discussion on optimal update frequency for k steps in iterative processes.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Discussion on optimal update frequency for k steps in iterative processes.", "node_type": "subnode"}, {"id": "Policy_Iteration_Speedup", "label": "Policy_Iteratio\nn_Speedup", "title": "<b>Policy_Iteration_Speedup</b> (subnode)<hr>Explanation of policy iteration's efficiency advantage over value iteration.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Explanation of policy iteration's efficiency advantage over value iteration.", "node_type": "subnode"}, {"id": "Value_Iteration_Preference", "label": "Value_Iteration\n_Preference", "title": "<b>Value_Iteration_Preference</b> (subnode)<hr>Conditions under which value iteration is preferred over policy iteration.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Conditions under which value iteration is preferred over policy iteration.", "node_type": "subnode"}, {"id": "Chapter_16_LQR_DDP_LQG", "label": "Chapter_16_LQR_\nDDP_LQG", "title": "<b>Chapter_16_LQR_DDP_LQG</b> (major)<hr>Introduction to Chapter 16 on Linear Quadratic Regulator (LQR), Differential Dynamic Programming (DDP) and Linear Quadratic Gaussian (LQG).", "shape": "star", "size": 25, "color": "#FF6347", "description": "Introduction to Chapter 16 on Linear Quadratic Regulator (LQR), Differential Dynamic Programming (DDP) and Linear Quadratic Gaussian (LQG).", "node_type": "major"}, {"id": "Finite_Horizon_MDPs", "label": "Finite_Horizon_\nMDPs", "title": "<b>Finite_Horizon_MDPs</b> (subnode)<hr>Introduction to finite-horizon MDPs in a general setting.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Introduction to finite-horizon MDPs in a general setting.", "node_type": "subnode"}, {"id": "Optimal_Bellman_Equation", "label": "Optimal_Bellman\n_Equation", "title": "<b>Optimal_Bellman_Equation</b> (subnode)<hr>Definition of the optimal Bellman equation for finding the optimal value function and policy.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Definition of the optimal Bellman equation for finding the optimal value function and policy.", "node_type": "subnode"}, {"id": "General_Setting_Equations", "label": "General_Setting\n_Equations", "title": "<b>General_Setting_Equations</b> (subnode)<hr>Formulation of equations that apply to both discrete and continuous state spaces.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Formulation of equations that apply to both discrete and continuous state spaces.", "node_type": "subnode"}, {"id": "ExpectationRewriting", "label": "ExpectationRewr\niting", "title": "<b>ExpectationRewriting</b> (major)<hr>Rewriting expectations for finite and continuous states.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Rewriting expectations for finite and continuous states.", "node_type": "major"}, {"id": "RewardsDependency", "label": "RewardsDependen\ncy", "title": "<b>RewardsDependency</b> (subnode)<hr>Rewards depend on both states and actions.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Rewards depend on both states and actions.", "node_type": "subnode"}, {"id": "OptimalActionComputation", "label": "OptimalActionCo\nmputation", "title": "<b>OptimalActionComputation</b> (subnode)<hr>Computing optimal action considering state-action rewards and future value expectations.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Computing optimal action considering state-action rewards and future value expectations.", "node_type": "subnode"}, {"id": "FiniteHorizonMDP", "label": "FiniteHorizonMD\nP", "title": "<b>FiniteHorizonMDP</b> (major)<hr>Definition of a finite horizon Markov Decision Process (MDP).", "shape": "star", "size": 25, "color": "#FF6347", "description": "Definition of a finite horizon Markov Decision Process (MDP).", "node_type": "major"}, {"id": "TimeHorizonTuple", "label": "TimeHorizonTupl\ne", "title": "<b>TimeHorizonTuple</b> (subnode)<hr>Defining MDP with time horizon T.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Defining MDP with time horizon T.", "node_type": "subnode"}, {"id": "PayoffDefinition", "label": "PayoffDefinitio\nn", "title": "<b>PayoffDefinition</b> (subnode)<hr>Summation of rewards over a finite number of steps without discount factor.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Summation of rewards over a finite number of steps without discount factor.", "node_type": "subnode"}, {"id": "DiscountFactorImpact", "label": "DiscountFactorI\nmpact", "title": "<b>DiscountFactorImpact</b> (subnode)<hr>Explanation for the absence of discount factor in finite horizon MDPs.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Explanation for the absence of discount factor in finite horizon MDPs.", "node_type": "subnode"}, {"id": "NonStationaryOptimalPolicy", "label": "NonStationaryOp\ntimalPolicy", "title": "<b>NonStationaryOptimalPolicy</b> (major)<hr>Optimal policy changes over time in a finite horizon setting.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Optimal policy changes over time in a finite horizon setting.", "node_type": "major"}, {"id": "Non-Stationary Policies", "label": "Non-Stationary\nPolicies", "title": "<b>Non-Stationary Policies</b> (major)<hr>Policies that change over time in finite-horizon MDPs.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Policies that change over time in finite-horizon MDPs.", "node_type": "major"}, {"id": "Finite Horizon MDP Dynamics", "label": "Finite Horizon\nMDP Dynamics", "title": "<b>Finite Horizon MDP Dynamics</b> (subnode)<hr>Dynamics of an MDP with a changing policy over time steps.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Dynamics of an MDP with a changing policy over time steps.", "node_type": "subnode"}, {"id": "Time Dependent Transition Probabilities", "label": "Time Dependent\nTransition\nProbabilities", "title": "<b>Time Dependent Transition Probabilities</b> (subnode)<hr>Transition probabilities that vary based on the current time step and state-action pair.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Transition probabilities that vary based on the current time step and state-action pair.", "node_type": "subnode"}, {"id": "Optimal Policy in Finite Horizon", "label": "Optimal Policy\nin Finite\nHorizon", "title": "<b>Optimal Policy in Finite Horizon</b> (subnode)<hr>The optimal policy varies depending on remaining steps to goal attainment.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "The optimal policy varies depending on remaining steps to goal attainment.", "node_type": "subnode"}, {"id": "Value Function for Non-Stationary MDPs", "label": "Value Function\nfor Non-\nStationary MDPs", "title": "<b>Value Function for Non-Stationary MDPs</b> (subnode)<hr>Definition of the value function considering time-varying policies and states.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Definition of the value function considering time-varying policies and states.", "node_type": "subnode"}, {"id": "Reinforcement_Learning", "label": "Reinforcement_L\nearning", "title": "<b>Reinforcement_Learning</b> (subnode)<hr>Field of machine learning concerned with how software agents ought to take actions in an environment.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Field of machine learning concerned with how software agents ought to take actions in an environment.", "node_type": "subnode"}, {"id": "Value_Functions", "label": "Value_Functions", "title": "<b>Value_Functions</b> (subnode)<hr>Function that estimates the expected cumulative reward from a given state under a policy.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Function that estimates the expected cumulative reward from a given state under a policy.", "node_type": "subnode"}, {"id": "Policy_Evaluation", "label": "Policy_Evaluati\non", "title": "<b>Policy_Evaluation</b> (subnode)<hr>Process of estimating the value function for a fixed policy in reinforcement learning.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Process of estimating the value function for a fixed policy in reinforcement learning.", "node_type": "subnode"}, {"id": "Bellman_Equation", "label": "Bellman_Equatio\nn", "title": "<b>Bellman_Equation</b> (subnode)<hr>Equation used to express the relationship between the value of a state and its successor states.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Equation used to express the relationship between the value of a state and its successor states.", "node_type": "subnode"}, {"id": "Dynamic_Programming", "label": "Dynamic_Program\nming", "title": "<b>Dynamic_Programming</b> (subnode)<hr>Technique for solving complex problems by breaking them down into simpler subproblems.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Technique for solving complex problems by breaking them down into simpler subproblems.", "node_type": "subnode"}, {"id": "Machine_Learning_Overview", "label": "Machine_Learnin\ng_Overview", "title": "<b>Machine_Learning_Overview</b> (major)<hr>General overview of machine learning concepts and techniques.", "shape": "star", "size": 25, "color": "#FF6347", "description": "General overview of machine learning concepts and techniques.", "node_type": "major"}, {"id": "Bellman_Equations", "label": "Bellman_Equatio\nns", "title": "<b>Bellman_Equations</b> (subnode)<hr>Equations used to describe the relationship between value functions at different time steps.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Equations used to describe the relationship between value functions at different time steps.", "node_type": "subnode"}, {"id": "Geometric_Convergence", "label": "Geometric_Conve\nrgence", "title": "<b>Geometric_Convergence</b> (subnode)<hr>Rate of convergence in value iteration, characterized by a geometric progression.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Rate of convergence in value iteration, characterized by a geometric progression.", "node_type": "subnode"}, {"id": "Linear_Quadratic_Regulation_(LQR)", "label": "Linear_Quadrati\nc_Regulation_(L\nQR)", "title": "<b>Linear_Quadratic_Regulation_(LQR)</b> (major)<hr>A special case of finite-horizon setting where exact solutions are tractable.", "shape": "star", "size": 25, "color": "#FF6347", "description": "A special case of finite-horizon setting where exact solutions are tractable.", "node_type": "major"}, {"id": "Continuous_Model_Assumptions", "label": "Continuous_Mode\nl_Assumptions", "title": "<b>Continuous_Model_Assumptions</b> (subnode)<hr>Assumptions about the state and action spaces in a continuous setting.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Assumptions about the state and action spaces in a continuous setting.", "node_type": "subnode"}, {"id": "Linear_Transitions", "label": "Linear_Transiti\nons", "title": "<b>Linear_Transitions</b> (subnode)<hr>Model of system dynamics with linear transitions between states.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Model of system dynamics with linear transitions between states.", "node_type": "subnode"}, {"id": "Gaussian_Noise", "label": "Gaussian_Noise", "title": "<b>Gaussian_Noise</b> (subnode)<hr>Assumption about noise in the transition model, specifically Gaussian with zero mean.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Assumption about noise in the transition model, specifically Gaussian with zero mean.", "node_type": "subnode"}, {"id": "Quadratic_Rewards", "label": "Quadratic_Rewar\nds", "title": "<b>Quadratic_Rewards</b> (subnode)<hr>Rewards are modeled as quadratic functions of state and action variables.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Rewards are modeled as quadratic functions of state and action variables.", "node_type": "subnode"}, {"id": "LQRModelAssumptions", "label": "LQRModelAssumpt\nions", "title": "<b>LQRModelAssumptions</b> (subnode)<hr>Assumptions made in the Linear Quadratic Regulator (LQR) model for optimal control problems.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Assumptions made in the Linear Quadratic Regulator (LQR) model for optimal control problems.", "node_type": "subnode"}, {"id": "LQRAlgorithmSteps", "label": "LQRAlgorithmSte\nps", "title": "<b>LQRAlgorithmSteps</b> (subnode)<hr>Two-step process of estimating and applying the LQR algorithm to find an optimal policy.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Two-step process of estimating and applying the LQR algorithm to find an optimal policy.", "node_type": "subnode"}, {"id": "Step1Estimation", "label": "Step1Estimation", "title": "<b>Step1Estimation</b> (subnode)<hr>First step involves collecting data and using linear regression to estimate model parameters.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "First step involves collecting data and using linear regression to estimate model parameters.", "node_type": "subnode"}, {"id": "Step2OptimalPolicy", "label": "Step2OptimalPol\nicy", "title": "<b>Step2OptimalPolicy</b> (subnode)<hr>Second step uses dynamic programming to derive the optimal policy given known or estimated parameters.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Second step uses dynamic programming to derive the optimal policy given known or estimated parameters.", "node_type": "subnode"}, {"id": "DynamicProgrammingApplication", "label": "DynamicProgramm\ningApplication", "title": "<b>DynamicProgrammingApplication</b> (subnode)<hr>Application of dynamic programming in solving for the optimal value function in LQR problems.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Application of dynamic programming in solving for the optimal value function in LQR problems.", "node_type": "subnode"}, {"id": "Optimal_Value_Function", "label": "Optimal_Value_F\nunction", "title": "<b>Optimal_Value_Function</b> (major)<hr>Definition and properties of the optimal value function in a quadratic form.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Definition and properties of the optimal value function in a quadratic form.", "node_type": "major"}, {"id": "Quadratic_Form", "label": "Quadratic_Form", "title": "<b>Quadratic_Form</b> (subnode)<hr>The optimal value function is expressed as a quadratic function of state.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "The optimal value function is expressed as a quadratic function of state.", "node_type": "subnode"}, {"id": "Dynamics_Model", "label": "Dynamics_Model", "title": "<b>Dynamics_Model</b> (subnode)<hr>Model dynamics and their integration into the value function calculation.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Model dynamics and their integration into the value function calculation.", "node_type": "subnode"}, {"id": "Linear_Policy_Derivation", "label": "Linear_Policy_D\nerivation", "title": "<b>Linear_Policy_Derivation</b> (subnode)<hr>Process to derive the linear form of the optimal policy from quadratic value functions.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Process to derive the linear form of the optimal policy from quadratic value functions.", "node_type": "subnode"}, {"id": "Optimal Policy in LQR", "label": "Optimal Policy\nin LQR", "title": "<b>Optimal Policy in LQR</b> (subnode)<hr>Discussion on optimal policy formulation in Linear Quadratic Regulator (LQR).", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Discussion on optimal policy formulation in Linear Quadratic Regulator (LQR).", "node_type": "subnode"}, {"id": "Discrete Ricatti Equations", "label": "Discrete\nRicatti\nEquations", "title": "<b>Discrete Ricatti Equations</b> (subnode)<hr>Equations used to update \u03a6_t and \u03a8_t iteratively.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Equations used to update \u03a6_t and \u03a8_t iteratively.", "node_type": "subnode"}, {"id": "Independence of Noise", "label": "Independence of\nNoise", "title": "<b>Independence of Noise</b> (subnode)<hr>Explanation that optimal policy does not depend on noise but cost function value does.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Explanation that optimal policy does not depend on noise but cost function value does.", "node_type": "subnode"}, {"id": "LQR Algorithm Steps", "label": "LQR Algorithm\nSteps", "title": "<b>LQR Algorithm Steps</b> (subnode)<hr>Steps involved in implementing the Linear Quadratic Regulator algorithm.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Steps involved in implementing the Linear Quadratic Regulator algorithm.", "node_type": "subnode"}, {"id": "Efficiency Improvement", "label": "Efficiency\nImprovement", "title": "<b>Efficiency Improvement</b> (subnode)<hr>Method to optimize algorithm performance by updating only \u03a6_t.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Method to optimize algorithm performance by updating only \u03a6_t.", "node_type": "subnode"}, {"id": "Non-linear Dynamics and LQR", "label": "Non-linear\nDynamics and\nLQR", "title": "<b>Non-linear Dynamics and LQR</b> (major)<hr>Exploration of how non-linear systems can be approximated using Linear Quadratic Regulator methods.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Exploration of how non-linear systems can be approximated using Linear Quadratic Regulator methods.", "node_type": "major"}, {"id": "Inverted Pendulum Example", "label": "Inverted\nPendulum\nExample", "title": "<b>Inverted Pendulum Example</b> (subnode)<hr>Illustration of applying LQR to the inverted pendulum problem.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Illustration of applying LQR to the inverted pendulum problem.", "node_type": "subnode"}, {"id": "Inverted_Pendulum_Model", "label": "Inverted_Pendul\num_Model", "title": "<b>Inverted_Pendulum_Model</b> (subnode)<hr>A model used to illustrate control theory principles.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "A model used to illustrate control theory principles.", "node_type": "subnode"}, {"id": "State_Transitions", "label": "State_Transitio\nns", "title": "<b>State_Transitions</b> (subnode)<hr>Describes how states change over time in the inverted pendulum system.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Describes how states change over time in the inverted pendulum system.", "node_type": "subnode"}, {"id": "Linearization_of_Dynamics", "label": "Linearization_o\nf_Dynamics", "title": "<b>Linearization_of_Dynamics</b> (major)<hr>Process of approximating nonlinear dynamics with linear equations around each point on the nominal trajectory.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Process of approximating nonlinear dynamics with linear equations around each point on the nominal trajectory.", "node_type": "major"}, {"id": "Taylor_Expansion_Method", "label": "Taylor_Expansio\nn_Method", "title": "<b>Taylor_Expansion_Method</b> (subnode)<hr>Uses Taylor series to approximate functions around a point.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Uses Taylor series to approximate functions around a point.", "node_type": "subnode"}, {"id": "LQR_Assumptions", "label": "LQR_Assumptions", "title": "<b>LQR_Assumptions</b> (subnode)<hr>Connection between linearized dynamics and Linear Quadratic Regulator assumptions.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Connection between linearized dynamics and Linear Quadratic Regulator assumptions.", "node_type": "subnode"}, {"id": "Differential_Dynamic_Programming", "label": "Differential_Dy\nnamic_Programmi\nng", "title": "<b>Differential_Dynamic_Programming</b> (major)<hr>Optimization technique for nonlinear systems aiming to stay near a target state.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Optimization technique for nonlinear systems aiming to stay near a target state.", "node_type": "major"}, {"id": "Optimization_Methods", "label": "Optimization_Me\nthods", "title": "<b>Optimization_Methods</b> (subnode)<hr>Techniques used to find optimal solutions in machine learning problems.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Techniques used to find optimal solutions in machine learning problems.", "node_type": "subnode"}, {"id": "Differential_Dynamic_Programming_(DDP)", "label": "Differential_Dy\nnamic_Programmi\nng_(DDP)", "title": "<b>Differential_Dynamic_Programming_(DDP)</b> (subnode)<hr>Method for trajectory optimization that discretizes the path and uses linear approximations around each point.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Method for trajectory optimization that discretizes the path and uses linear approximations around each point.", "node_type": "subnode"}, {"id": "Nominal_Trajectory", "label": "Nominal_Traject\nory", "title": "<b>Nominal_Trajectory</b> (subnode)<hr>Initial approximation of the desired trajectory using a simple controller.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Initial approximation of the desired trajectory using a simple controller.", "node_type": "subnode"}, {"id": "Rewriting_Dynamics", "label": "Rewriting_Dynam\nics", "title": "<b>Rewriting_Dynamics</b> (subnode)<hr>Expressing the system's dynamics in a linear form using matrices A and B for state and action respectively.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Expressing the system's dynamics in a linear form using matrices A and B for state and action respectively.", "node_type": "subnode"}, {"id": "Reward_Function_Approximation", "label": "Reward_Function\n_Approximation", "title": "<b>Reward_Function_Approximation</b> (subnode)<hr>Approximating the reward function around each point on the trajectory with Taylor expansion.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Approximating the reward function around each point on the trajectory with Taylor expansion.", "node_type": "subnode"}, {"id": "Optimization_Frameworks", "label": "Optimization_Fr\nameworks", "title": "<b>Optimization_Frameworks</b> (subnode)<hr>Frameworks used for optimization in machine learning problems.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Frameworks used for optimization in machine learning problems.", "node_type": "subnode"}, {"id": "Linear_Quadratic_Regulator_(LQR)", "label": "Linear_Quadrati\nc_Regulator_(LQ\nR)", "title": "<b>Linear_Quadratic_Regulator_(LQR)</b> (subnode)<hr>A control strategy that minimizes a quadratic cost function over time.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "A control strategy that minimizes a quadratic cost function over time.", "node_type": "subnode"}, {"id": "Hessian_Matrix", "label": "Hessian_Matrix", "title": "<b>Hessian_Matrix</b> (subnode)<hr>Matrix of second-order partial derivatives used in optimization problems.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Matrix of second-order partial derivatives used in optimization problems.", "node_type": "subnode"}, {"id": "LQR_Framework_Applications", "label": "LQR_Framework_A\npplications", "title": "<b>LQR_Framework_Applications</b> (subnode)<hr>Application of LQR to find optimal policies and controllers.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Application of LQR to find optimal policies and controllers.", "node_type": "subnode"}, {"id": "Trajectory_Generation", "label": "Trajectory_Gene\nration", "title": "<b>Trajectory_Generation</b> (subnode)<hr>Process of generating new trajectories using the derived policy in an iterative manner.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Process of generating new trajectories using the derived policy in an iterative manner.", "node_type": "subnode"}, {"id": "Linear_Quadratic_Gaussian_(LQG)", "label": "Linear_Quadrati\nc_Gaussian_(LQG\n)", "title": "<b>Linear_Quadratic_Gaussian_(LQG)</b> (major)<hr>Extension of LQR to handle systems with partial state observability.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Extension of LQR to handle systems with partial state observability.", "node_type": "major"}, {"id": "Observation vs State", "label": "Observation vs\nState", "title": "<b>Observation vs State</b> (subnode)<hr>Discussion on the difference between observation and state in real-world problems.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Discussion on the difference between observation and state in real-world problems.", "node_type": "subnode"}, {"id": "Partially Observable MDPs (POMDP)", "label": "Partially\nObservable MDPs\n(POMDP)", "title": "<b>Partially Observable MDPs (POMDP)</b> (subnode)<hr>Introduction to POMDP as a tool for modeling partially observable environments.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Introduction to POMDP as a tool for modeling partially observable environments.", "node_type": "subnode"}, {"id": "Belief State", "label": "Belief State", "title": "<b>Belief State</b> (subnode)<hr>Maintaining belief state based on observations in POMDP framework.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Maintaining belief state based on observations in POMDP framework.", "node_type": "subnode"}, {"id": "LQR Extension to POMDP", "label": "LQR Extension\nto POMDP", "title": "<b>LQR Extension to POMDP</b> (subnode)<hr>Extension of Linear Quadratic Regulator (LQR) to Partially Observable MDPs.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Extension of Linear Quadratic Regulator (LQR) to Partially Observable MDPs.", "node_type": "subnode"}, {"id": "Kalman Filter", "label": "Kalman Filter", "title": "<b>Kalman Filter</b> (subnode)<hr>A recursive algorithm that estimates the state of a system over time using a series of measurements.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "A recursive algorithm that estimates the state of a system over time using a series of measurements.", "node_type": "subnode"}, {"id": "Step 1", "label": "Step 1", "title": "<b>Step 1</b> (subnode)<hr>Initial step where the system dynamics are defined without action dependence.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Initial step where the system dynamics are defined without action dependence.", "node_type": "subnode"}, {"id": "Gaussian Distributions", "label": "Gaussian\nDistributions", "title": "<b>Gaussian Distributions</b> (major)<hr>Statistical distributions used in Kalman filter for modeling uncertainties.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Statistical distributions used in Kalman filter for modeling uncertainties.", "node_type": "major"}, {"id": "Predict Step", "label": "Predict Step", "title": "<b>Predict Step</b> (subnode)<hr>Computes the distribution of the next state given current observations.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Computes the distribution of the next state given current observations.", "node_type": "subnode"}, {"id": "Update Step", "label": "Update Step", "title": "<b>Update Step</b> (subnode)<hr>Step to update the predicted state with new observations.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Step to update the predicted state with new observations.", "node_type": "subnode"}, {"id": "LQR Algorithm", "label": "LQR Algorithm", "title": "<b>LQR Algorithm</b> (major)<hr>Linear Quadratic Regulator algorithm used for control problems.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Linear Quadratic Regulator algorithm used for control problems.", "node_type": "major"}, {"id": "Computational Efficiency", "label": "Computational\nEfficiency", "title": "<b>Computational Efficiency</b> (subnode)<hr>Efficient computation of state estimates over time using Kalman filter.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Efficient computation of state estimates over time using Kalman filter.", "node_type": "subnode"}, {"id": "Belief States Update", "label": "Belief States\nUpdate", "title": "<b>Belief States Update</b> (subnode)<hr>Combination of predict and update steps to refine belief states over time.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Combination of predict and update steps to refine belief states over time.", "node_type": "subnode"}, {"id": "Gaussian Distribution", "label": "Gaussian\nDistribution", "title": "<b>Gaussian Distribution</b> (subnode)<hr>The distribution used in the predict step is Gaussian with mean s_t|t and covariance Sigma_t|t.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "The distribution used in the predict step is Gaussian with mean s_t|t and covariance Sigma_t|t.", "node_type": "subnode"}, {"id": "Kalman Gain (K_t)", "label": "Kalman Gain\n(K_t)", "title": "<b>Kalman Gain (K_t)</b> (subnode)<hr>A matrix that determines how much new measurements should be trusted over previous estimates.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "A matrix that determines how much new measurements should be trusted over previous estimates.", "node_type": "subnode"}, {"id": "Backward Pass (LQR Updates)", "label": "Backward Pass\n(LQR Updates)", "title": "<b>Backward Pass (LQR Updates)</b> (subnode)<hr>Computes Psi_t, Psi_t and L_t to refine the optimal policy based on previous estimates.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Computes Psi_t, Psi_t and L_t to refine the optimal policy based on previous estimates.", "node_type": "subnode"}, {"id": "Chapter_17_Policy_Gradient_REINFORCE", "label": "Chapter_17_Poli\ncy_Gradient_REI\nNFORCE", "title": "<b>Chapter_17_Policy_Gradient_REINFORCE</b> (major)<hr>Model-free algorithm that optimizes policy parameters without value functions.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Model-free algorithm that optimizes policy parameters without value functions.", "node_type": "major"}, {"id": "Finite_Horizon_Case", "label": "Finite_Horizon_\nCase", "title": "<b>Finite_Horizon_Case</b> (subnode)<hr>Assumption for trajectory length in REINFORCE method.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Assumption for trajectory length in REINFORCE method.", "node_type": "subnode"}, {"id": "Randomized_Policy", "label": "Randomized_Poli\ncy", "title": "<b>Randomized_Policy</b> (subnode)<hr>REINFORCE applies to learning policies that output actions probabilistically.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "REINFORCE applies to learning policies that output actions probabilistically.", "node_type": "subnode"}, {"id": "Transition_Probabilities_Sampling", "label": "Transition_Prob\nabilities_Sampl\ning", "title": "<b>Transition_Probabilities_Sampling</b> (subnode)<hr>Sampling from transition probabilities is sufficient for REINFORCE, no need for analytical form.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Sampling from transition probabilities is sufficient for REINFORCE, no need for analytical form.", "node_type": "subnode"}, {"id": "Reward_Function_Querying", "label": "Reward_Function\n_Querying", "title": "<b>Reward_Function_Querying</b> (subnode)<hr>REINFORCE queries reward function at state-action pairs without needing its analytical form.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "REINFORCE queries reward function at state-action pairs without needing its analytical form.", "node_type": "subnode"}, {"id": "Expected_Total_Payoff_Optimization", "label": "Expected_Total_\nPayoff_Optimiza\ntion", "title": "<b>Expected_Total_Payoff_Optimization</b> (subnode)<hr>Optimizing expected total payoff over policy parameters in finite horizon setting.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Optimizing expected total payoff over policy parameters in finite horizon setting.", "node_type": "subnode"}, {"id": "Policy Gradient Methods", "label": "Policy Gradient\nMethods", "title": "<b>Policy Gradient Methods</b> (subnode)<hr>Techniques for optimizing policies in reinforcement learning.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Techniques for optimizing policies in reinforcement learning.", "node_type": "subnode"}, {"id": "Gradient Ascent", "label": "Gradient Ascent", "title": "<b>Gradient Ascent</b> (subnode)<hr>Optimization technique to maximize the expected reward function.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Optimization technique to maximize the expected reward function.", "node_type": "subnode"}, {"id": "Reward Function Estimation", "label": "Reward Function\nEstimation", "title": "<b>Reward Function Estimation</b> (subnode)<hr>Challenges in estimating gradients without knowing the exact form of the reward function.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Challenges in estimating gradients without knowing the exact form of the reward function.", "node_type": "subnode"}, {"id": "Variational Auto-Encoder (VAE)", "label": "Variational\nAuto-Encoder\n(VAE)", "title": "<b>Variational Auto-Encoder (VAE)</b> (subnode)<hr>Technique for learning latent variable models by optimizing variational lower bound.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Technique for learning latent variable models by optimizing variational lower bound.", "node_type": "subnode"}, {"id": "Re-parametrization Technique", "label": "Re-\nparametrization\nTechnique", "title": "<b>Re-parametrization Technique</b> (subnode)<hr>Method to compute gradients through random variables in VAEs.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Method to compute gradients through random variables in VAEs.", "node_type": "subnode"}, {"id": "REINFORCE Algorithm", "label": "REINFORCE\nAlgorithm", "title": "<b>REINFORCE Algorithm</b> (subnode)<hr>Algorithm for estimating policy gradients using likelihood ratio methods.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Algorithm for estimating policy gradients using likelihood ratio methods.", "node_type": "subnode"}, {"id": "GradientEstimation", "label": "GradientEstimat\nion", "title": "<b>GradientEstimation</b> (major)<hr>Overview of estimating gradients in machine learning.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Overview of estimating gradients in machine learning.", "node_type": "major"}, {"id": "PolicyGradients", "label": "PolicyGradients", "title": "<b>PolicyGradients</b> (subnode)<hr>Methods for computing policy gradients using samples.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Methods for computing policy gradients using samples.", "node_type": "subnode"}, {"id": "SampleBasedEstimator", "label": "SampleBasedEsti\nmator", "title": "<b>SampleBasedEstimator</b> (subnode)<hr>Using empirical samples to estimate the gradient of expected values.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Using empirical samples to estimate the gradient of expected values.", "node_type": "subnode"}, {"id": "LogProbabilityDerivative", "label": "LogProbabilityD\nerivative", "title": "<b>LogProbabilityDerivative</b> (subnode)<hr>Computing the derivative of log probability with respect to policy parameters.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Computing the derivative of log probability with respect to policy parameters.", "node_type": "subnode"}, {"id": "AnalyticalFormula", "label": "AnalyticalFormu\nla", "title": "<b>AnalyticalFormula</b> (subnode)<hr>Deriving an analytical formula for \u03c0_\u03b8(a|s).", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Deriving an analytical formula for \u03c0_\u03b8(a|s).", "node_type": "subnode"}, {"id": "AutoDifferentiation", "label": "AutoDifferentia\ntion", "title": "<b>AutoDifferentiation</b> (subnode)<hr>Using automatic differentiation to compute gradients.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Using automatic differentiation to compute gradients.", "node_type": "subnode"}, {"id": "Policy_Gradient_Theorem", "label": "Policy_Gradient\n_Theorem", "title": "<b>Policy_Gradient_Theorem</b> (major)<hr>Fundamental theorem in reinforcement learning for policy optimization.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Fundamental theorem in reinforcement learning for policy optimization.", "node_type": "major"}, {"id": "Log_Probability_Ratio", "label": "Log_Probability\n_Ratio", "title": "<b>Log_Probability_Ratio</b> (subnode)<hr>Ratio of the probabilities of an action given a state according to two different policies, often used in reinforcement learning algorithms.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Ratio of the probabilities of an action given a state according to two different policies, often used in reinforcement learning algorithms.", "node_type": "subnode"}, {"id": "Vanilla_REINFORCE_Algorithm", "label": "Vanilla_REINFOR\nCE_Algorithm", "title": "<b>Vanilla_REINFORCE_Algorithm</b> (subnode)<hr>Basic algorithm for learning policies using policy gradients.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Basic algorithm for learning policies using policy gradients.", "node_type": "subnode"}, {"id": "Empirical_Sample_Trajectories", "label": "Empirical_Sampl\ne_Trajectories", "title": "<b>Empirical_Sample_Trajectories</b> (subnode)<hr>Using sample trajectories to estimate gradients in reinforcement learning.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Using sample trajectories to estimate gradients in reinforcement learning.", "node_type": "subnode"}, {"id": "Trajectory_Probability_Change", "label": "Trajectory_Prob\nability_Change", "title": "<b>Trajectory_Probability_Change</b> (subnode)<hr>Change in trajectory probability due to policy parameter changes.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Change in trajectory probability due to policy parameter changes.", "node_type": "subnode"}, {"id": "PolicyGradientMethods", "label": "PolicyGradientM\nethods", "title": "<b>PolicyGradientMethods</b> (subnode)<hr>Techniques that optimize policies in reinforcement learning by using gradients of the performance measure with respect to policy parameters.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Techniques that optimize policies in reinforcement learning by using gradients of the performance measure with respect to policy parameters.", "node_type": "subnode"}, {"id": "TrajectoryProbability", "label": "TrajectoryProba\nbility", "title": "<b>TrajectoryProbability</b> (subnode)<hr>The probability of a trajectory given a policy and its importance in gradient calculation.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "The probability of a trajectory given a policy and its importance in gradient calculation.", "node_type": "subnode"}, {"id": "RewardFunction", "label": "RewardFunction", "title": "<b>RewardFunction</b> (subnode)<hr>A function that assigns a scalar value to each possible outcome, used to evaluate the quality of actions taken by an agent.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "A function that assigns a scalar value to each possible outcome, used to evaluate the quality of actions taken by an agent.", "node_type": "subnode"}, {"id": "ExpectationCalculation", "label": "ExpectationCalc\nulation", "title": "<b>ExpectationCalculation</b> (subnode)<hr>Calculating expectations over trajectories under a policy and their implications for gradient estimation.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Calculating expectations over trajectories under a policy and their implications for gradient estimation.", "node_type": "subnode"}, {"id": "SimplificationOfFormulas", "label": "SimplificationO\nfFormulas", "title": "<b>SimplificationOfFormulas</b> (subnode)<hr>Simplifying complex formulas to more manageable expressions based on certain assumptions or properties.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Simplifying complex formulas to more manageable expressions based on certain assumptions or properties.", "node_type": "subnode"}, {"id": "Policy_Gradient_Methods", "label": "Policy_Gradient\n_Methods", "title": "<b>Policy_Gradient_Methods</b> (subnode)<hr>Techniques for optimizing the policy directly by taking gradients of the performance measure with respect to the parameters of the policy.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Techniques for optimizing the policy directly by taking gradients of the performance measure with respect to the parameters of the policy.", "node_type": "subnode"}, {"id": "Law_of_Total_Expectation", "label": "Law_of_Total_Ex\npectation", "title": "<b>Law_of_Total_Expectation</b> (subnode)<hr>Theorem that states how to compute expectations under a probability distribution by conditioning on another random variable.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Theorem that states how to compute expectations under a probability distribution by conditioning on another random variable.", "node_type": "subnode"}, {"id": "Value_Function_Estimation", "label": "Value_Function_\nEstimation", "title": "<b>Value_Function_Estimation</b> (subnode)<hr>Estimating the expected cumulative reward from a state or action under a policy.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Estimating the expected cumulative reward from a state or action under a policy.", "node_type": "subnode"}, {"id": "Gradient Estimation", "label": "Gradient\nEstimation", "title": "<b>Gradient Estimation</b> (subnode)<hr>Estimating gradients of policy parameters using trajectories.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Estimating gradients of policy parameters using trajectories.", "node_type": "subnode"}, {"id": "Baseline Function", "label": "Baseline\nFunction", "title": "<b>Baseline Function</b> (subnode)<hr>Function used to reduce variance in gradient estimation.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Function used to reduce variance in gradient estimation.", "node_type": "subnode"}, {"id": "Algorithm 7", "label": "Algorithm 7", "title": "<b>Algorithm 7</b> (subnode)<hr>Vanilla policy gradient algorithm with baseline function.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Vanilla policy gradient algorithm with baseline function.", "node_type": "subnode"}, {"id": "Bibliography References", "label": "Bibliography\nReferences", "title": "<b>Bibliography References</b> (major)<hr>References to academic papers related to machine learning concepts.", "shape": "star", "size": 25, "color": "#FF6347", "description": "References to academic papers related to machine learning concepts.", "node_type": "major"}, {"id": "Machine_Learning_Papers", "label": "Machine_Learnin\ng_Papers", "title": "<b>Machine_Learning_Papers</b> (major)<hr>Collection of influential papers in machine learning and statistics.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Collection of influential papers in machine learning and statistics.", "node_type": "major"}, {"id": "Double_Descent_Weak_Features", "label": "Double_Descent_\nWeak_Features", "title": "<b>Double_Descent_Weak_Features</b> (subnode)<hr>Explores double descent phenomenon in machine learning models using weak features.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Explores double descent phenomenon in machine learning models using weak features.", "node_type": "subnode"}, {"id": "Variational_Inference_Review", "label": "Variational_Inf\nerence_Review", "title": "<b>Variational_Inference_Review</b> (subnode)<hr>Provides a comprehensive review of variational inference for statisticians.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Provides a comprehensive review of variational inference for statisticians.", "node_type": "subnode"}, {"id": "Foundation_Models", "label": "Foundation_Mode\nls", "title": "<b>Foundation_Models</b> (subnode)<hr>Discusses the opportunities and risks associated with foundation models in AI.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Discusses the opportunities and risks associated with foundation models in AI.", "node_type": "subnode"}, {"id": "Contrastive_Learning_Visual_Representations", "label": "Contrastive_Lea\nrning_Visual_Re\npresentations", "title": "<b>Contrastive_Learning_Visual_Representations</b> (subnode)<hr>Introduces a simple framework for contrastive learning of visual representations.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Introduces a simple framework for contrastive learning of visual representations.", "node_type": "subnode"}, {"id": "BERT_Model", "label": "BERT_Model", "title": "<b>BERT_Model</b> (subnode)<hr>Describes the pre-training of deep bidirectional transformers for language understanding.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Describes the pre-training of deep bidirectional transformers for language understanding.", "node_type": "subnode"}, {"id": "Implicit_Bias_Noise_Covariance", "label": "Implicit_Bias_N\noise_Covariance", "title": "<b>Implicit_Bias_Noise_Covariance</b> (subnode)<hr>Analyzes the implicit bias introduced by noise covariance in machine learning models.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Analyzes the implicit bias introduced by noise covariance in machine learning models.", "node_type": "subnode"}, {"id": "High_Dimensional_Statistics", "label": "High_Dimensiona\nl_Statistics", "title": "<b>High_Dimensional_Statistics</b> (subnode)<hr>Surveys unexpected phenomena in high-dimensional statistical analysis and modeling.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Surveys unexpected phenomena in high-dimensional statistical analysis and modeling.", "node_type": "subnode"}, {"id": "Machine Learning Papers", "label": "Machine\nLearning Papers", "title": "<b>Machine Learning Papers</b> (major)<hr>Collection of papers related to machine learning and statistical learning theory.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Collection of papers related to machine learning and statistical learning theory.", "node_type": "major"}, {"id": "Implicit Bias in Machine Learning", "label": "Implicit Bias\nin Machine\nLearning", "title": "<b>Implicit Bias in Machine Learning</b> (subnode)<hr>Studies on the implicit bias introduced by different methods in high-dimensional settings.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Studies on the implicit bias introduced by different methods in high-dimensional settings.", "node_type": "subnode"}, {"id": "High-Dimensional Interpolation", "label": "High-\nDimensional\nInterpolation", "title": "<b>High-Dimensional Interpolation</b> (subnode)<hr>Research on interpolation properties of machine learning models in high dimensions.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Research on interpolation properties of machine learning models in high dimensions.", "node_type": "subnode"}, {"id": "Deep Residual Learning", "label": "Deep Residual\nLearning", "title": "<b>Deep Residual Learning</b> (subnode)<hr>Work on deep residual networks for image recognition tasks.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Work on deep residual networks for image recognition tasks.", "node_type": "subnode"}, {"id": "Statistical Learning Textbook", "label": "Statistical\nLearning\nTextbook", "title": "<b>Statistical Learning Textbook</b> (subnode)<hr>Textbook covering fundamental concepts and techniques in statistical learning.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Textbook covering fundamental concepts and techniques in statistical learning.", "node_type": "subnode"}, {"id": "Optimization Methods", "label": "Optimization\nMethods", "title": "<b>Optimization Methods</b> (subnode)<hr>Methods for stochastic optimization, including Adam and other advanced algorithms.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Methods for stochastic optimization, including Adam and other advanced algorithms.", "node_type": "subnode"}, {"id": "Variational Autoencoders", "label": "Variational\nAutoencoders", "title": "<b>Variational Autoencoders</b> (subnode)<hr>Research on variational Bayesian methods applied to auto-encoding models.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Research on variational Bayesian methods applied to auto-encoding models.", "node_type": "subnode"}, {"id": "Model-Based Reinforcement Learning", "label": "Model-Based\nReinforcement\nLearning", "title": "<b>Model-Based Reinforcement Learning</b> (subnode)<hr>Framework for model-based reinforcement learning with theoretical guarantees.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Framework for model-based reinforcement learning with theoretical guarantees.", "node_type": "subnode"}, {"id": "Generalization Error Analysis", "label": "Generalization\nError Analysis", "title": "<b>Generalization Error Analysis</b> (subnode)<hr>Analysis of generalization error in random features regression and linear models.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Analysis of generalization error in random features regression and linear models.", "node_type": "subnode"}, {"id": "Learning Theory Fundamentals", "label": "Learning Theory\nFundamentals", "title": "<b>Learning Theory Fundamentals</b> (subnode)<hr>Fundamental concepts in learning theory, including statistical mechanics approaches.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Fundamental concepts in learning theory, including statistical mechanics approaches.", "node_type": "subnode"}, {"id": "double_descent", "label": "double_descent", "title": "<b>double_descent</b> (major)<hr>Phenomenon in machine learning where performance initially improves, then worsens, and finally improves again as model complexity increases.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Phenomenon in machine learning where performance initially improves, then worsens, and finally improves again as model complexity increases.", "node_type": "major"}, {"id": "statistical_mechanics_of_learning", "label": "statistical_mec\nhanics_of_learn\ning", "title": "<b>statistical_mechanics_of_learning</b> (subnode)<hr>Application of statistical mechanics principles to understand learning processes in neural networks.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Application of statistical mechanics principles to understand learning processes in neural networks.", "node_type": "subnode"}, {"id": "generalization", "label": "generalization", "title": "<b>generalization</b> (subnode)<hr>Concept within statistical mechanics of learning focusing on how well a model performs on unseen data.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Concept within statistical mechanics of learning focusing on how well a model performs on unseen data.", "node_type": "subnode"}, {"id": "learning_to_generalize", "label": "learning_to_gen\neralize", "title": "<b>learning_to_generalize</b> (subnode)<hr>Study of methods and theories for improving the generalization ability of machine learning models.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Study of methods and theories for improving the generalization ability of machine learning models.", "node_type": "subnode"}]; // Will now contain labels with '
'
        const hierarchicalEdgesData = [{"from": "I Supervised learning", "to": "1 Linear regression", "arrows": "to", "title": "Subtopic of I Supervised learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "1 Linear regression", "to": "1.1 LMS algorithm", "arrows": "to", "title": "Subtopic of 1 Linear regression", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "1 Linear regression", "to": "1.2 The normal equations", "arrows": "to", "title": "Subtopic of 1 Linear regression", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "1.2 The normal equations", "to": "1.2.1 Matrix derivatives", "arrows": "to", "title": "Subtopic of 1.2 The normal equations", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "1.2 The normal equations", "to": "1.2.2 Least squares revisited", "arrows": "to", "title": "Subtopic of 1.2 The normal equations", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "1 Linear regression", "to": "1.3 Probabilistic interpretation", "arrows": "to", "title": "Subtopic of 1 Linear regression", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "1 Linear regression", "to": "1.4 Locally weighted linear regression (optional reading)", "arrows": "to", "title": "Subtopic of 1 Linear regression", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "I Supervised learning", "to": "2 Classification and logistic regression", "arrows": "to", "title": "Subtopic of I Supervised learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "2 Classification and logistic regression", "to": "2.1 Logistic regression", "arrows": "to", "title": "Subtopic of 2 Classification and logistic regression", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "2 Classification and logistic regression", "to": "2.2 Digression: the perceptron learning algorithm", "arrows": "to", "title": "Subtopic of 2 Classification and logistic regression", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "2 Classification and logistic regression", "to": "2.3 Multi-class classification", "arrows": "to", "title": "Subtopic of 2 Classification and logistic regression", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "2 Classification and logistic regression", "to": "2.4 Another algorithm for maximizing \u03bb(\u03b8)", "arrows": "to", "title": "Subtopic of 2 Classification and logistic regression", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "I Supervised learning", "to": "3 Generalized linear models", "arrows": "to", "title": "Subtopic of I Supervised learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "3 Generalized linear models", "to": "3.1 The exponential family", "arrows": "to", "title": "Subtopic of 3 Generalized linear models", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "3 Generalized linear models", "to": "3.2 Constructing GLMs", "arrows": "to", "title": "Subtopic of 3 Generalized linear models", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "3.2 Constructing GLMs", "to": "3.2.1 Ordinary least squares", "arrows": "to", "title": "Subtopic of 3.2 Constructing GLMs", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "3.2 Constructing GLMs", "to": "3.2.2 Logistic regression", "arrows": "to", "title": "Subtopic of 3.2 Constructing GLMs", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "I Supervised learning", "to": "4 Generative learning algorithms", "arrows": "to", "title": "Subtopic of I Supervised learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "4 Generative learning algorithms", "to": "4.1 Gaussian discriminant analysis", "arrows": "to", "title": "Subtopic of 4 Generative learning algorithms", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "4.1 Gaussian discriminant analysis", "to": "4.1.1 The multivariate normal distribution", "arrows": "to", "title": "Subtopic of 4.1 Gaussian discriminant analysis", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "4.1 Gaussian discriminant analysis", "to": "4.1.2 The Gaussian discriminant analysis model", "arrows": "to", "title": "Subtopic of 4.1 Gaussian discriminant analysis", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "4.1 Gaussian discriminant analysis", "to": "4.1.3 Discussion: GDA and logistic regression", "arrows": "to", "title": "Subtopic of 4.1 Gaussian discriminant analysis", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "4 Generative learning algorithms", "to": "4.2 Naive bayes (Option Reading)", "arrows": "to", "title": "Subtopic of 4 Generative learning algorithms", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "4.2 Naive bayes (Option Reading)", "to": "4.2.1 Laplace smoothing", "arrows": "to", "title": "Subtopic of 4.2 Naive bayes (Option Reading)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "4.2 Naive bayes (Option Reading)", "to": "4.2.2 Event models for text classification", "arrows": "to", "title": "Subtopic of 4.2 Naive bayes (Option Reading)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "I Supervised learning", "to": "5 Kernel methods", "arrows": "to", "title": "Subtopic of I Supervised learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "5 Kernel methods", "to": "5.1 Feature maps", "arrows": "to", "title": "Subtopic of 5 Kernel methods", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "5 Kernel methods", "to": "5.2 LMS (least mean squares) with features", "arrows": "to", "title": "Subtopic of 5 Kernel methods", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "5 Kernel methods", "to": "5.3 LMS with the kernel trick", "arrows": "to", "title": "Subtopic of 5 Kernel methods", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "5 Kernel methods", "to": "5.4 Properties of kernels", "arrows": "to", "title": "Subtopic of 5 Kernel methods", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "I Supervised learning", "to": "6 Support vector machines", "arrows": "to", "title": "Subtopic of I Supervised learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "II Deep learning", "to": "7 Deep learning", "arrows": "to", "title": "Subtopic of II Deep learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "7 Deep learning", "to": "7.1 Supervised learning with non-linear models", "arrows": "to", "title": "Subtopic of 7 Deep learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "7 Deep learning", "to": "7.2 Neural networks", "arrows": "to", "title": "Subtopic of 7 Deep learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "7 Deep learning", "to": "7.3 Modules in Modern Neural Networks", "arrows": "to", "title": "Subtopic of 7 Deep learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "7 Deep learning", "to": "7.4 Backpropagation", "arrows": "to", "title": "Subtopic of 7 Deep learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "7.4 Backpropagation", "to": "7.4.1 Preliminaries on partial derivatives", "arrows": "to", "title": "Subtopic of 7.4 Backpropagation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "7.4 Backpropagation", "to": "7.4.2 General strategy of backpropagation", "arrows": "to", "title": "Subtopic of 7.4 Backpropagation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "7.4 Backpropagation", "to": "7.4.3 Backward functions for basic modules", "arrows": "to", "title": "Subtopic of 7.4 Backpropagation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "7.4 Backpropagation", "to": "7.4.4 Back-propagation for MLPs", "arrows": "to", "title": "Subtopic of 7.4 Backpropagation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "Neural Networks", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Neural Networks", "to": "Modules in Modern Neural Networks", "arrows": "to", "title": "Subtopic of Neural Networks", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Neural Networks", "to": "Backpropagation", "arrows": "to", "title": "Subtopic of Neural Networks", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Backpropagation", "to": "Preliminaries on partial derivatives", "arrows": "to", "title": "Subtopic of Backpropagation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Backpropagation", "to": "General strategy of backpropagation", "arrows": "to", "title": "Subtopic of Backpropagation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Backpropagation", "to": "Backward functions for basic modules", "arrows": "to", "title": "Subtopic of Backpropagation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Backpropagation", "to": "Back-propagation for MLPs", "arrows": "to", "title": "Subtopic of Backpropagation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Neural Networks", "to": "Vectorization over training examples", "arrows": "to", "title": "Subtopic of Neural Networks", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Generalization and regularization", "to": "Generalization", "arrows": "to", "title": "Subtopic of Generalization and regularization", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Generalization", "to": "Bias-variance tradeoff", "arrows": "to", "title": "Subtopic of Generalization", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Bias-variance tradeoff", "to": "A mathematical decomposition (for regression)", "arrows": "to", "title": "Subtopic of Bias-variance tradeoff", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Generalization", "to": "The double descent phenomenon", "arrows": "to", "title": "Subtopic of Generalization", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Generalization", "to": "Sample complexity bounds (optional readings)", "arrows": "to", "title": "Subtopic of Generalization", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Regularization and model selection", "to": "Regularization", "arrows": "to", "title": "Subtopic of Regularization and model selection", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Regularization and model selection", "to": "Implicit regularization effect (optional reading)", "arrows": "to", "title": "Subtopic of Regularization and model selection", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Regularization and model selection", "to": "Model selection via cross validation", "arrows": "to", "title": "Subtopic of Regularization and model selection", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Regularization and model selection", "to": "Bayesian statistics and regularization", "arrows": "to", "title": "Subtopic of Regularization and model selection", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Unsupervised learning", "to": "Clustering and the k-means algorithm", "arrows": "to", "title": "Subtopic of Unsupervised learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Unsupervised learning", "to": "EM algorithms", "arrows": "to", "title": "Subtopic of Unsupervised learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "EM algorithms", "to": "EM for mixture of Gaussians", "arrows": "to", "title": "Subtopic of EM algorithms", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "EM algorithms", "to": "Jensen's inequality", "arrows": "to", "title": "Subtopic of EM algorithms", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "EM algorithms", "to": "General EM algorithms", "arrows": "to", "title": "Subtopic of EM algorithms", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "General EM algorithms", "to": "Other interpretation of ELBO", "arrows": "to", "title": "Subtopic of General EM algorithms", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "EM algorithms", "to": "Mixture of Gaussians revisited", "arrows": "to", "title": "Subtopic of EM algorithms", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "EM algorithms", "to": "Variational inference and variational auto-encoder (optional reading)", "arrows": "to", "title": "Subtopic of EM algorithms", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Unsupervised learning", "to": "Principal components analysis", "arrows": "to", "title": "Subtopic of Unsupervised learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Unsupervised learning", "to": "Independent components analysis", "arrows": "to", "title": "Subtopic of Unsupervised learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Independent components analysis", "to": "ICA ambiguities", "arrows": "to", "title": "Subtopic of Independent components analysis", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Independent components analysis", "to": "Densities and linear transformations", "arrows": "to", "title": "Subtopic of Independent components analysis", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Independent components analysis", "to": "ICA algorithm", "arrows": "to", "title": "Subtopic of Independent components analysis", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Self-supervised learning and foundation models", "to": "Pretraining and adaptation", "arrows": "to", "title": "Subtopic of Self-supervised learning and foundation models", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Self-supervised learning and foundation models", "to": "Pretraining methods in computer vision", "arrows": "to", "title": "Subtopic of Self-supervised learning and foundation models", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Self-supervised learning and foundation models", "to": "Pretrained large language models", "arrows": "to", "title": "Subtopic of Self-supervised learning and foundation models", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Pretrained large language models", "to": "Open up the blackbox of Transformers", "arrows": "to", "title": "Subtopic of Pretrained large language models", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Pretrained large language models", "to": "Zero-shot learning and in-context learning", "arrows": "to", "title": "Subtopic of Pretrained large language models", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Reinforcement Learning and Control", "to": "Reinforcement learning", "arrows": "to", "title": "Subtopic of Reinforcement Learning and Control", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Reinforcement learning", "to": "Markov decision processes", "arrows": "to", "title": "Subtopic of Reinforcement learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Reinforcement learning", "to": "Value iteration and policy iteration", "arrows": "to", "title": "Subtopic of Reinforcement learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Reinforcement learning", "to": "Learning a model for an MDP", "arrows": "to", "title": "Subtopic of Reinforcement learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Reinforcement learning", "to": "Continuous state MDPs", "arrows": "to", "title": "Subtopic of Reinforcement learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Continuous state MDPs", "to": "Discretization", "arrows": "to", "title": "Subtopic of Continuous state MDPs", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Continuous state MDPs", "to": "Value function approximation", "arrows": "to", "title": "Subtopic of Continuous state MDPs", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Reinforcement learning", "to": "Connections between Policy and Value Iteration (Optional)", "arrows": "to", "title": "Subtopic of Reinforcement learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "LQR, DDP and LQG", "to": "Finite-horizon MDPs", "arrows": "to", "title": "Subtopic of LQR, DDP and LQG", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "LQR, DDP and LQG", "to": "Linear Quadratic Regulation (LQR)", "arrows": "to", "title": "Subtopic of LQR, DDP and LQG", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "LQR, DDP and LQG", "to": "From non-linear dynamics to LQR", "arrows": "to", "title": "Subtopic of LQR, DDP and LQG", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "From non-linear dynamics to LQR", "to": "Linearization of dynamics", "arrows": "to", "title": "Subtopic of From non-linear dynamics to LQR", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "From non-linear dynamics to LQR", "to": "Differential Dynamic Programming (DDP)", "arrows": "to", "title": "Subtopic of From non-linear dynamics to LQR", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "LQR, DDP and LQG", "to": "Linear Quadratic Gaussian (LQG)", "arrows": "to", "title": "Subtopic of LQR, DDP and LQG", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Supervised Learning Problem", "to": "Regression", "arrows": "to", "title": "Subtopic of Supervised Learning Problem", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Supervised Learning Problem", "to": "Classification", "arrows": "to", "title": "Subtopic of Supervised Learning Problem", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Supervised Learning", "to": "Linear Regression", "arrows": "to", "title": "Subtopic of Supervised Learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Linear Regression", "to": "Housing Example Dataset", "arrows": "to", "title": "Subtopic of Linear Regression", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Linear Regression", "to": "Features Selection", "arrows": "to", "title": "Subtopic of Linear Regression", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning_Basics", "to": "Function_Representation", "arrows": "to", "title": "Subtopic of Machine_Learning_Basics", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Function_Representation", "to": "Linear_Functions", "arrows": "to", "title": "Subtopic of Function_Representation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Linear_Functions", "to": "Parameters_Weights", "arrows": "to", "title": "Subtopic of Linear_Functions", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Function_Representation", "to": "Cost_Function", "arrows": "to", "title": "Subtopic of Function_Representation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Cost_Function", "to": "Ordinary_Least_Squares", "arrows": "to", "title": "Subtopic of Cost_Function", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningOverview", "to": "GradientDescentAlgorithm", "arrows": "to", "title": "Subtopic of MachineLearningOverview", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "GradientDescentAlgorithm", "to": "LearningRate", "arrows": "to", "title": "Subtopic of GradientDescentAlgorithm", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "GradientDescentAlgorithm", "to": "CostFunctionJ", "arrows": "to", "title": "Subtopic of GradientDescentAlgorithm", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "GradientDescentAlgorithm", "to": "PartialDerivativeCalculation", "arrows": "to", "title": "Subtopic of GradientDescentAlgorithm", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "PartialDerivativeCalculation", "to": "SingleTrainingExampleCase", "arrows": "to", "title": "Subtopic of PartialDerivativeCalculation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "GradientDescentAlgorithm", "to": "LMSUpdateRule", "arrows": "to", "title": "Subtopic of GradientDescentAlgorithm", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "LMS_Update_Rule", "to": "Widrow_Hoff_Learning_Rule", "arrows": "to", "title": "Subtopic of LMS_Update_Rule", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "LMS_Update_Rule", "to": "Error_Term", "arrows": "to", "title": "Subtopic of LMS_Update_Rule", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "LMS_Update_Rule", "to": "Single_Training_Example", "arrows": "to", "title": "Subtopic of LMS_Update_Rule", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning_Optimization", "to": "Batch_Gradient_Descent", "arrows": "to", "title": "Subtopic of Machine_Learning_Optimization", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Batch_Gradient_Descent", "to": "Gradient_Descent", "arrows": "to", "title": "Subtopic of Batch_Gradient_Descent", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "LinearRegressionOptimization", "to": "GradientDescentConvergence", "arrows": "to", "title": "Subtopic of LinearRegressionOptimization", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "LinearRegressionOptimization", "to": "BatchGradientDescentExample", "arrows": "to", "title": "Subtopic of LinearRegressionOptimization", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "GradientCalculation", "to": "StochasticGradientDescent", "arrows": "to", "title": "Subtopic of GradientCalculation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "StochasticGradientDescent", "to": "SGDAlgorithm", "arrows": "to", "title": "Subtopic of StochasticGradientDescent", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning_Optimization", "to": "Stochastic_Gradient_Descent", "arrows": "to", "title": "Subtopic of Machine_Learning_Optimization", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Stochastic_Gradient_Descent", "to": "Learning_Rate_Decay", "arrows": "to", "title": "Subtopic of Stochastic_Gradient_Descent", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Normal_Equations_Method", "to": "Matrix_Derivatives", "arrows": "to", "title": "Subtopic of Normal_Equations_Method", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Matrix Derivatives", "to": "Gradient Calculation", "arrows": "to", "title": "Subtopic of Matrix Derivatives", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Least Squares Revisited", "to": "Design Matrix", "arrows": "to", "title": "Subtopic of Least Squares Revisited", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Least Squares Revisited", "to": "Target Vector", "arrows": "to", "title": "Subtopic of Least Squares Revisited", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningOverview", "to": "LinearRegression", "arrows": "to", "title": "Subtopic of MachineLearningOverview", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "LinearRegression", "to": "HypothesisFunction", "arrows": "to", "title": "Subtopic of LinearRegression", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "LinearRegression", "to": "CostFunction", "arrows": "to", "title": "Subtopic of LinearRegression", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "LinearRegression", "to": "GradientDescent", "arrows": "to", "title": "Subtopic of LinearRegression", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "LinearRegression", "to": "NormalEquations", "arrows": "to", "title": "Subtopic of LinearRegression", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Probabilistic Interpretation of Linear Regression", "to": "Regression Problem", "arrows": "to", "title": "Subtopic of Probabilistic Interpretation of Linear Regression", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Probabilistic Interpretation of Linear Regression", "to": "Least-Squares Cost Function J", "arrows": "to", "title": "Subtopic of Probabilistic Interpretation of Linear Regression", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Least-Squares Cost Function J", "to": "Target Variables and Inputs Relation", "arrows": "to", "title": "Subtopic of Least-Squares Cost Function J", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Least-Squares Cost Function J", "to": "Error Term Distribution", "arrows": "to", "title": "Subtopic of Least-Squares Cost Function J", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Error Term Distribution", "to": "Conditional Probability of y given x", "arrows": "to", "title": "Subtopic of Error Term Distribution", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningOverview", "to": "ProbabilisticModeling", "arrows": "to", "title": "Subtopic of MachineLearningOverview", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "ProbabilisticModeling", "to": "ConditionalProbabilityDistribution", "arrows": "to", "title": "Subtopic of ProbabilisticModeling", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "ProbabilisticModeling", "to": "LikelihoodFunction", "arrows": "to", "title": "Subtopic of ProbabilisticModeling", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "ProbabilisticModeling", "to": "MaximumLikelihoodEstimation", "arrows": "to", "title": "Subtopic of ProbabilisticModeling", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MaximumLikelihoodEstimation", "to": "LogLikelihoodFunction", "arrows": "to", "title": "Subtopic of MaximumLikelihoodEstimation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningConcepts", "to": "LeastSquaresRegression", "arrows": "to", "title": "Subtopic of MachineLearningConcepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningOverview", "to": "LocallyWeightedLinearRegression", "arrows": "to", "title": "Subtopic of MachineLearningOverview", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "LinearRegression", "to": "Underfitting", "arrows": "to", "title": "Subtopic of LinearRegression", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "LinearRegression", "to": "Overfitting", "arrows": "to", "title": "Subtopic of LinearRegression", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "LinearRegression", "to": "FeatureSelection", "arrows": "to", "title": "Subtopic of LinearRegression", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "LinearRegression", "to": "FittingTheta", "arrows": "to", "title": "Subtopic of LinearRegression", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "LocallyWeightedLinearRegression", "to": "WeightsInLWLR", "arrows": "to", "title": "Subtopic of LocallyWeightedLinearRegression", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "WeightsInLWLR", "to": "BandwidthParameter", "arrows": "to", "title": "Subtopic of WeightsInLWLR", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Overview", "to": "Locally Weighted Linear Regression", "arrows": "to", "title": "Subtopic of Machine Learning Overview", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Overview", "to": "Non-Parametric Algorithms", "arrows": "to", "title": "Subtopic of Machine Learning Overview", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Overview", "to": "Parametric Algorithms", "arrows": "to", "title": "Subtopic of Machine Learning Overview", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Classification Problem", "to": "Binary Classification", "arrows": "to", "title": "Subtopic of Classification Problem", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Newton's Method", "to": "Logistic Regression", "arrows": "to", "title": "Subtopic of Newton's Method", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Classification Problem", "to": "Spam Classification Example", "arrows": "to", "title": "Subtopic of Classification Problem", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearning", "to": "ClassificationProblem", "arrows": "to", "title": "Subtopic of MachineLearning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "ClassificationProblem", "to": "LogisticRegression", "arrows": "to", "title": "Subtopic of ClassificationProblem", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "LogisticRegression", "to": "LogisticFunction", "arrows": "to", "title": "Subtopic of LogisticRegression", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "LogisticFunction", "to": "DerivativeOfSigmoid", "arrows": "to", "title": "Subtopic of LogisticFunction", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningModels", "to": "ClassificationModelAssumptions", "arrows": "to", "title": "Subtopic of MachineLearningModels", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "LikelihoodFunction", "to": "LogLikelihood", "arrows": "to", "title": "Subtopic of LikelihoodFunction", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "LogLikelihood", "to": "GradientAscent", "arrows": "to", "title": "Subtopic of LogLikelihood", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "LogisticRegression", "to": "GradientAscentRule", "arrows": "to", "title": "Subtopic of LogisticRegression", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "GradientAscentRule", "to": "StochasticGradientAscent", "arrows": "to", "title": "Subtopic of GradientAscentRule", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "LogisticRegression", "to": "LogisticLossFunction", "arrows": "to", "title": "Subtopic of LogisticRegression", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "LogisticLossFunction", "to": "NegativeLogLikelihood", "arrows": "to", "title": "Subtopic of LogisticLossFunction", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning_Concepts", "to": "Logistic_Regression_Gradient_Descent", "arrows": "to", "title": "Subtopic of Machine_Learning_Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning_Concepts", "to": "Perceptron_Algorithm", "arrows": "to", "title": "Subtopic of Machine_Learning_Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning_Concepts", "to": "Multi_Class_Classification", "arrows": "to", "title": "Subtopic of Machine_Learning_Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearning", "to": "Multi-classClassification", "arrows": "to", "title": "Subtopic of MachineLearning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Multi-classClassification", "to": "ResponseVariable", "arrows": "to", "title": "Subtopic of Multi-classClassification", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Multi-classClassification", "to": "MultinomialDistribution", "arrows": "to", "title": "Subtopic of Multi-classClassification", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Multi-classClassification", "to": "ParameterizedModel", "arrows": "to", "title": "Subtopic of Multi-classClassification", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MultinomialDistribution", "to": "SoftmaxFunction", "arrows": "to", "title": "Subtopic of MultinomialDistribution", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning_Concepts", "to": "Softmax_Function", "arrows": "to", "title": "Subtopic of Machine_Learning_Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Softmax_Function", "to": "Logits", "arrows": "to", "title": "Subtopic of Softmax_Function", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Softmax_Function", "to": "Probability_Vector_Output", "arrows": "to", "title": "Subtopic of Softmax_Function", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning_Concepts", "to": "Probabilistic_Model", "arrows": "to", "title": "Subtopic of Machine_Learning_Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning_Concepts", "to": "Negative_Log_Likelihood", "arrows": "to", "title": "Subtopic of Machine_Learning_Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Negative_Log_Likelihood", "to": "Cross_Entropy_Loss", "arrows": "to", "title": "Subtopic of Negative_Log_Likelihood", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning_Concepts", "to": "Loss_Functions", "arrows": "to", "title": "Subtopic of Machine_Learning_Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Cross_Entropy_Loss", "to": "Softmax_Cross_Entropy_Loss", "arrows": "to", "title": "Subtopic of Cross_Entropy_Loss", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Loss_Functions", "to": "Gradient_Computation", "arrows": "to", "title": "Subtopic of Loss_Functions", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningOverview", "to": "LossFunction", "arrows": "to", "title": "Subtopic of MachineLearningOverview", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "LossFunction", "to": "CrossEntropyLoss", "arrows": "to", "title": "Subtopic of LossFunction", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "LossFunction", "to": "GradientCalculation", "arrows": "to", "title": "Subtopic of LossFunction", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "LogisticRegression", "to": "NewtonMethod", "arrows": "to", "title": "Subtopic of LogisticRegression", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Newton's Method", "to": "Finding Roots", "arrows": "to", "title": "Subtopic of Newton's Method", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Newton's Method", "to": "Maximizing Functions", "arrows": "to", "title": "Subtopic of Newton's Method", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Logistic Regression", "to": "Hessian Matrix", "arrows": "to", "title": "Subtopic of Logistic Regression", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningConcepts", "to": "OptimizationMethods", "arrows": "to", "title": "Subtopic of MachineLearningConcepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "NewtonMethod", "to": "FisherScoring", "arrows": "to", "title": "Subtopic of NewtonMethod", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningOverview", "to": "GeneralizedLinearModels", "arrows": "to", "title": "Subtopic of MachineLearningOverview", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "GeneralizedLinearModels", "to": "ExponentialFamilyDistributions", "arrows": "to", "title": "Subtopic of GeneralizedLinearModels", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "ExponentialFamilyDistributions", "to": "NaturalParameter", "arrows": "to", "title": "Subtopic of ExponentialFamilyDistributions", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "ExponentialFamilyDistributions", "to": "SufficientStatistic", "arrows": "to", "title": "Subtopic of ExponentialFamilyDistributions", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "ExponentialFamilyDistributions", "to": "LogPartitionFunction", "arrows": "to", "title": "Subtopic of ExponentialFamilyDistributions", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "LogisticRegressionAsGLM", "to": "BernoulliDistribution", "arrows": "to", "title": "Subtopic of LogisticRegressionAsGLM", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "BernoulliDistribution", "to": "NaturalParameterForBernoulli", "arrows": "to", "title": "Subtopic of BernoulliDistribution", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "BernoulliDistribution", "to": "SufficientStatisticForBernoulli", "arrows": "to", "title": "Subtopic of BernoulliDistribution", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "BernoulliDistribution", "to": "LogPartitionFunctionForBernoulli", "arrows": "to", "title": "Subtopic of BernoulliDistribution", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "ActivationFunctions", "to": "SigmoidFunction", "arrows": "to", "title": "Subtopic of ActivationFunctions", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningModels", "to": "LogisticRegressionAsGLM", "arrows": "to", "title": "Subtopic of MachineLearningModels", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningModels", "to": "GaussianDistribution", "arrows": "to", "title": "Subtopic of MachineLearningModels", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningModels", "to": "GLMConstruction", "arrows": "to", "title": "Subtopic of MachineLearningModels", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningOverview", "to": "PoissonDistributionModeling", "arrows": "to", "title": "Subtopic of MachineLearningOverview", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "GeneralizedLinearModels", "to": "GLMAssumptions", "arrows": "to", "title": "Subtopic of GeneralizedLinearModels", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningModels", "to": "OrdinaryLeastSquares", "arrows": "to", "title": "Subtopic of MachineLearningModels", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "GeneralizedLinearModels", "to": "Assumption1", "arrows": "to", "title": "Subtopic of GeneralizedLinearModels", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "GeneralizedLinearModels", "to": "Assumption2", "arrows": "to", "title": "Subtopic of GeneralizedLinearModels", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "GeneralizedLinearModels", "to": "Assumption3", "arrows": "to", "title": "Subtopic of GeneralizedLinearModels", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "GeneralizedLinearModels", "to": "ExponentialFamilyDistribution", "arrows": "to", "title": "Subtopic of GeneralizedLinearModels", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning_Concepts", "to": "Conditional_Distribution_Modeling", "arrows": "to", "title": "Subtopic of Machine_Learning_Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Conditional_Distribution_Modeling", "to": "Bernoulli_Distribution", "arrows": "to", "title": "Subtopic of Conditional_Distribution_Modeling", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Conditional_Distribution_Modeling", "to": "Exponential_Family_Distributions", "arrows": "to", "title": "Subtopic of Conditional_Distribution_Modeling", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Conditional_Distribution_Modeling", "to": "Hypothesis_Functions", "arrows": "to", "title": "Subtopic of Conditional_Distribution_Modeling", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Exponential_Family_Distributions", "to": "Logistic_Function", "arrows": "to", "title": "Subtopic of Exponential_Family_Distributions", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Exponential_Family_Distributions", "to": "Canonical_Response_Function", "arrows": "to", "title": "Subtopic of Exponential_Family_Distributions", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Exponential_Family_Distributions", "to": "Canonical_Link_Function", "arrows": "to", "title": "Subtopic of Exponential_Family_Distributions", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningAlgorithms", "to": "DiscriminativeLearning", "arrows": "to", "title": "Subtopic of MachineLearningAlgorithms", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningAlgorithms", "to": "GenerativeLearning", "arrows": "to", "title": "Subtopic of MachineLearningAlgorithms", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "DiscriminativeLearning", "to": "ConditionalDistribution", "arrows": "to", "title": "Subtopic of DiscriminativeLearning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "DiscriminativeLearning", "to": "PerceptronAlgorithm", "arrows": "to", "title": "Subtopic of DiscriminativeLearning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "PerceptronAlgorithm", "to": "DecisionBoundary", "arrows": "to", "title": "Subtopic of PerceptronAlgorithm", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "GenerativeLearning", "to": "ClassPriors", "arrows": "to", "title": "Subtopic of GenerativeLearning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "GenerativeLearning", "to": "ConditionalProbabilityModel", "arrows": "to", "title": "Subtopic of GenerativeLearning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "GenerativeLearning", "to": "BayesRule", "arrows": "to", "title": "Subtopic of GenerativeLearning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Bayesian Classification", "to": "Class Priors", "arrows": "to", "title": "Subtopic of Bayesian Classification", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Bayesian Classification", "to": "Conditional Probability p(x|y)", "arrows": "to", "title": "Subtopic of Bayesian Classification", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Bayesian Classification", "to": "Posterior Distribution p(y|x)", "arrows": "to", "title": "Subtopic of Bayesian Classification", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Gaussian Discriminant Analysis (GDA)", "to": "Multivariate Normal Distribution", "arrows": "to", "title": "Subtopic of Gaussian Discriminant Analysis (GDA)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Multivariate Normal Distribution", "to": "Mean Vector", "arrows": "to", "title": "Subtopic of Multivariate Normal Distribution", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Multivariate Normal Distribution", "to": "Covariance Matrix", "arrows": "to", "title": "Subtopic of Multivariate Normal Distribution", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "GaussianDistribution", "to": "MeanOfGaussian", "arrows": "to", "title": "Subtopic of GaussianDistribution", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "GaussianDistribution", "to": "CovarianceMatrix", "arrows": "to", "title": "Subtopic of GaussianDistribution", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "GaussianDistribution", "to": "StandardNormalDistribution", "arrows": "to", "title": "Subtopic of GaussianDistribution", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "GaussianDistribution", "to": "DensityExamples", "arrows": "to", "title": "Subtopic of GaussianDistribution", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningConcepts", "to": "MultivariateNormalDistribution", "arrows": "to", "title": "Subtopic of MachineLearningConcepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MultivariateNormalDistribution", "to": "CovarianceMatrixImpact", "arrows": "to", "title": "Subtopic of MultivariateNormalDistribution", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MultivariateNormalDistribution", "to": "MeanVectorMovement", "arrows": "to", "title": "Subtopic of MultivariateNormalDistribution", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "GaussianDiscriminantAnalysisModel", "to": "MultivariateNormalForClasses", "arrows": "to", "title": "Subtopic of GaussianDiscriminantAnalysisModel", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningModels", "to": "GaussianDiscriminantAnalysis(GDA)", "arrows": "to", "title": "Subtopic of MachineLearningModels", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "GaussianDiscriminantAnalysis(GDA)", "to": "ParametersEstimation", "arrows": "to", "title": "Subtopic of GaussianDiscriminantAnalysis(GDA)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningModels", "to": "GaussianDiscriminantAnalysis", "arrows": "to", "title": "Subtopic of MachineLearningModels", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningModels", "to": "DecisionBoundaries", "arrows": "to", "title": "Subtopic of MachineLearningModels", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningModels", "to": "ModelAssumptions", "arrows": "to", "title": "Subtopic of MachineLearningModels", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "GaussianDiscriminantAnalysis", "to": "AsymptoticEfficiency", "arrows": "to", "title": "Subtopic of GaussianDiscriminantAnalysis", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningModels", "to": "GDA", "arrows": "to", "title": "Subtopic of MachineLearningModels", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "LogisticRegression", "to": "Robustness", "arrows": "to", "title": "Subtopic of LogisticRegression", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "BernoulliEventModel", "to": "NaiveBayes", "arrows": "to", "title": "Subtopic of BernoulliEventModel", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "NaiveBayes", "to": "DiscreteFeatures", "arrows": "to", "title": "Subtopic of NaiveBayes", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning", "to": "Text_Classification", "arrows": "to", "title": "Subtopic of Machine_Learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Text_Classification", "to": "Spam_Filtering", "arrows": "to", "title": "Subtopic of Text_Classification", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Spam_Filtering", "to": "Training_Set", "arrows": "to", "title": "Subtopic of Spam_Filtering", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Spam_Filtering", "to": "Feature_Vector", "arrows": "to", "title": "Subtopic of Spam_Filtering", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Feature_Vector", "to": "Vocabulary", "arrows": "to", "title": "Subtopic of Feature_Vector", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Feature_Vector", "to": "Stop_Words", "arrows": "to", "title": "Subtopic of Feature_Vector", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Text_Classification", "to": "Feature_Vector_Selection", "arrows": "to", "title": "Subtopic of Text_Classification", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Text_Classification", "to": "Generative_Modeling", "arrows": "to", "title": "Subtopic of Text_Classification", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Generative_Modeling", "to": "Naive_Bayes_Assumption", "arrows": "to", "title": "Subtopic of Generative_Modeling", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Generative_Modeling", "to": "Naive_Bayes_Classifier", "arrows": "to", "title": "Subtopic of Generative_Modeling", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "NaiveBayesAlgorithm", "to": "ConditionalProbability", "arrows": "to", "title": "Subtopic of NaiveBayesAlgorithm", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "NaiveBayesAlgorithm", "to": "JointLikelihood", "arrows": "to", "title": "Subtopic of NaiveBayesAlgorithm", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "NaiveBayesAlgorithm", "to": "BinaryFeatures", "arrows": "to", "title": "Subtopic of NaiveBayesAlgorithm", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "NaiveBayesAlgorithm", "to": "ParameterEstimation", "arrows": "to", "title": "Subtopic of NaiveBayesAlgorithm", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Algorithms", "to": "Naive Bayes Algorithm", "arrows": "to", "title": "Subtopic of Machine Learning Algorithms", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Naive Bayes Algorithm", "to": "Binary Features", "arrows": "to", "title": "Subtopic of Naive Bayes Algorithm", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Naive Bayes Algorithm", "to": "Multinomial Distribution", "arrows": "to", "title": "Subtopic of Naive Bayes Algorithm", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Naive Bayes Algorithm", "to": "Feature Representation", "arrows": "to", "title": "Subtopic of Naive Bayes Algorithm", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Algorithms", "to": "Laplace Smoothing", "arrows": "to", "title": "Subtopic of Machine Learning Algorithms", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningConferences", "to": "NeurIPS20xxSubmission", "arrows": "to", "title": "Subtopic of MachineLearningConferences", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "NaiveBayesFilter", "to": "NewWordDetection", "arrows": "to", "title": "Subtopic of NaiveBayesFilter", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning_Concepts", "to": "Probability_Estimation", "arrows": "to", "title": "Subtopic of Machine_Learning_Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Probability_Estimation", "to": "Maximum_Likelihood_Estimates", "arrows": "to", "title": "Subtopic of Probability_Estimation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Probability_Estimation", "to": "Laplace_Smoothing", "arrows": "to", "title": "Subtopic of Probability_Estimation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Naive_Bayes_Classifier", "to": "Event_Models_Text_Classification", "arrows": "to", "title": "Subtopic of Naive_Bayes_Classifier", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "EventModelsForTextClassification", "to": "BernoulliEventModel", "arrows": "to", "title": "Subtopic of EventModelsForTextClassification", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "EventModelsForTextClassification", "to": "MultinomialEventModel", "arrows": "to", "title": "Subtopic of EventModelsForTextClassification", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningOverview", "to": "NaiveBayesClassifier", "arrows": "to", "title": "Subtopic of MachineLearningOverview", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MaximumLikelihoodEstimation", "to": "LaplaceSmoothing", "arrows": "to", "title": "Subtopic of MaximumLikelihoodEstimation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningConcepts", "to": "KernelMethods", "arrows": "to", "title": "Subtopic of MachineLearningConcepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "KernelMethods", "to": "FeatureMaps", "arrows": "to", "title": "Subtopic of KernelMethods", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "Feature Mapping", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Feature Mapping", "to": "Linear Function Over Features", "arrows": "to", "title": "Subtopic of Feature Mapping", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Linear Function Over Features", "to": "Cubic Function Representation", "arrows": "to", "title": "Subtopic of Linear Function Over Features", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Feature Mapping", "to": "Feature Map Definition", "arrows": "to", "title": "Subtopic of Feature Mapping", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "LMS with Features", "to": "Gradient Descent Update", "arrows": "to", "title": "Subtopic of LMS with Features", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningOverview", "to": "FeatureMapping", "arrows": "to", "title": "Subtopic of MachineLearningOverview", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "FeatureMapping", "to": "HighDimensionalFeatures", "arrows": "to", "title": "Subtopic of FeatureMapping", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningOverview", "to": "KernelTrick", "arrows": "to", "title": "Subtopic of MachineLearningOverview", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningOverview", "to": "FeatureMappingPhiX", "arrows": "to", "title": "Subtopic of MachineLearningOverview", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "FeatureMappingPhiX", "to": "RuntimeAndMemoryEfficiency", "arrows": "to", "title": "Subtopic of FeatureMappingPhiX", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "RuntimeAndMemoryEfficiency", "to": "InitializationThetaZero", "arrows": "to", "title": "Subtopic of RuntimeAndMemoryEfficiency", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "InitializationThetaZero", "to": "IterativeUpdateRule", "arrows": "to", "title": "Subtopic of InitializationThetaZero", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "IterativeUpdateRule", "to": "LinearCombinationRepresentation", "arrows": "to", "title": "Subtopic of IterativeUpdateRule", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "LinearCombinationRepresentation", "to": "CoefficientUpdateRule", "arrows": "to", "title": "Subtopic of LinearCombinationRepresentation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningAlgorithm", "to": "BatchGradientDescent", "arrows": "to", "title": "Subtopic of MachineLearningAlgorithm", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "BatchGradientDescent", "to": "BetaUpdateEquation", "arrows": "to", "title": "Subtopic of BatchGradientDescent", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "BatchGradientDescent", "to": "FeatureMapPhi", "arrows": "to", "title": "Subtopic of BatchGradientDescent", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "FeatureMapPhi", "to": "InnerProductEfficiency", "arrows": "to", "title": "Subtopic of FeatureMapPhi", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningOverview", "to": "FeatureMapsAndKernels", "arrows": "to", "title": "Subtopic of MachineLearningOverview", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "FeatureMapsAndKernels", "to": "KernelFunctionDefinition", "arrows": "to", "title": "Subtopic of FeatureMapsAndKernels", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "FeatureMapsAndKernels", "to": "InnerProductCalculation", "arrows": "to", "title": "Subtopic of FeatureMapsAndKernels", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "FeatureMapsAndKernels", "to": "AlgorithmEfficiency", "arrows": "to", "title": "Subtopic of FeatureMapsAndKernels", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning_Concepts", "to": "Kernels_in_Machine_Learning", "arrows": "to", "title": "Subtopic of Machine_Learning_Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Kernels_in_Machine_Learning", "to": "Kernel_Function_K", "arrows": "to", "title": "Subtopic of Kernels_in_Machine_Learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Kernels_in_Machine_Learning", "to": "Algorithm_5.11", "arrows": "to", "title": "Subtopic of Kernels_in_Machine_Learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Kernels_in_Machine_Learning", "to": "Characterization_of_Kernels", "arrows": "to", "title": "Subtopic of Kernels_in_Machine_Learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Characterization_of_Kernels", "to": "Example_Kernel_Functions", "arrows": "to", "title": "Subtopic of Characterization_of_Kernels", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningOverview", "to": "KernelFunctions", "arrows": "to", "title": "Subtopic of MachineLearningOverview", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "KernelFunctions", "to": "PolynomialKernels", "arrows": "to", "title": "Subtopic of KernelFunctions", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "PolynomialKernels", "to": "ComputationalEfficiency", "arrows": "to", "title": "Subtopic of PolynomialKernels", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningConcepts", "to": "KernelsAsSimilarityMetrics", "arrows": "to", "title": "Subtopic of MachineLearningConcepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "KernelsAsSimilarityMetrics", "to": "GaussianKernel", "arrows": "to", "title": "Subtopic of KernelsAsSimilarityMetrics", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Kernel_Functions", "to": "Necessary_Conditions", "arrows": "to", "title": "Subtopic of Kernel_Functions", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Necessary_Conditions", "to": "Symmetry_Property", "arrows": "to", "title": "Subtopic of Necessary_Conditions", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Necessary_Conditions", "to": "Positive_Semi_Definite", "arrows": "to", "title": "Subtopic of Necessary_Conditions", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Necessary_Conditions", "to": "Kernel_Matrix", "arrows": "to", "title": "Subtopic of Necessary_Conditions", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Kernel_Functions", "to": "Sufficient_Conditions", "arrows": "to", "title": "Subtopic of Kernel_Functions", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Kernel Matrix Properties", "to": "Sufficient Conditions for Kernels", "arrows": "to", "title": "Subtopic of Kernel Matrix Properties", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Sufficient Conditions for Kernels", "to": "Mercer's Theorem", "arrows": "to", "title": "Subtopic of Sufficient Conditions for Kernels", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Kernel Examples", "to": "Digit Recognition Problem", "arrows": "to", "title": "Subtopic of Kernel Examples", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Kernel Examples", "to": "String Classification Example", "arrows": "to", "title": "Subtopic of Kernel Examples", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningConcepts", "to": "FeatureExtraction", "arrows": "to", "title": "Subtopic of MachineLearningConcepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "FeatureExtraction", "to": "StringFeatureExtraction", "arrows": "to", "title": "Subtopic of FeatureExtraction", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning_Algorithms", "to": "Kernel_Trick", "arrows": "to", "title": "Subtopic of Machine_Learning_Algorithms", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning_Algorithms", "to": "Support_Vector_Machines", "arrows": "to", "title": "Subtopic of Machine_Learning_Algorithms", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Support_Vector_Machines", "to": "Margins_Intuition", "arrows": "to", "title": "Subtopic of Support_Vector_Machines", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Support_Vector_Machines", "to": "Optimal_Margin_Classifier", "arrows": "to", "title": "Subtopic of Support_Vector_Machines", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Optimal_Margin_Classifier", "to": "Lagrange_Duality", "arrows": "to", "title": "Subtopic of Optimal_Margin_Classifier", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Support_Vector_Machines", "to": "Kernels_in_SVMs", "arrows": "to", "title": "Subtopic of Support_Vector_Machines", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Support_Vector_Machines", "to": "SMO_Algorithm", "arrows": "to", "title": "Subtopic of Support_Vector_Machines", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "Functional Margins", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "Geometric Margins", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "Decision Boundary", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Decision Boundary", "to": "Separating Hyperplane", "arrows": "to", "title": "Subtopic of Decision Boundary", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "Confidence in Predictions", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "Support Vector Machines (SVMs)", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Support Vector Machines (SVMs)", "to": "Notation for SVMs", "arrows": "to", "title": "Subtopic of Support Vector Machines (SVMs)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Support Vector Machines (SVMs)", "to": "Functional Margin", "arrows": "to", "title": "Subtopic of Support Vector Machines (SVMs)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Support Vector Machines (SVMs)", "to": "Geometric Margin", "arrows": "to", "title": "Subtopic of Support Vector Machines (SVMs)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning_Concepts", "to": "Functional_Margin", "arrows": "to", "title": "Subtopic of Machine_Learning_Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Functional_Margin", "to": "Confidence_and_Prediction", "arrows": "to", "title": "Subtopic of Functional_Margin", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Functional_Margin", "to": "Scaling_Issue", "arrows": "to", "title": "Subtopic of Functional_Margin", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Functional_Margin", "to": "Function_Margin_of_S", "arrows": "to", "title": "Subtopic of Functional_Margin", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning_Concepts", "to": "Geometric_Margins", "arrows": "to", "title": "Subtopic of Machine_Learning_Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "DecisionBoundary", "to": "VectorW", "arrows": "to", "title": "Subtopic of DecisionBoundary", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "DecisionBoundary", "to": "DistanceToBoundary", "arrows": "to", "title": "Subtopic of DecisionBoundary", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "VectorW", "to": "UnitVectorW", "arrows": "to", "title": "Subtopic of VectorW", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "GeometricMargin", "to": "FunctionalMargin", "arrows": "to", "title": "Subtopic of GeometricMargin", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning_Concepts", "to": "Scaling_Parameters", "arrows": "to", "title": "Subtopic of Machine_Learning_Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning_Concepts", "to": "Geometric_Margin", "arrows": "to", "title": "Subtopic of Machine_Learning_Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Optimal_Margin_Classifier", "to": "Maximize_Geometric_Margin", "arrows": "to", "title": "Subtopic of Optimal_Margin_Classifier", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Support Vector Machine (SVM)", "to": "Optimization Problem in SVM", "arrows": "to", "title": "Subtopic of Support Vector Machine (SVM)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Optimization Problem in SVM", "to": "Non-Convex Constraint", "arrows": "to", "title": "Subtopic of Optimization Problem in SVM", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Optimization Problem in SVM", "to": "Scaling Constraint", "arrows": "to", "title": "Subtopic of Optimization Problem in SVM", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning_Optimization", "to": "Support_Vector_Machines_SVM", "arrows": "to", "title": "Subtopic of Machine_Learning_Optimization", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Support_Vector_Machines_SVM", "to": "Convex_Quadratic_Objective", "arrows": "to", "title": "Subtopic of Support_Vector_Machines_SVM", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Lagrange_Duality", "to": "Dual_Form_Optimization", "arrows": "to", "title": "Subtopic of Lagrange_Duality", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Lagrange_Duality", "to": "Kernels_High_Dimensional_Spaces", "arrows": "to", "title": "Subtopic of Lagrange_Duality", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "ConstrainedOptimization", "to": "LagrangeMultipliers", "arrows": "to", "title": "Subtopic of ConstrainedOptimization", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "ConstrainedOptimization", "to": "PrimalProblem", "arrows": "to", "title": "Subtopic of ConstrainedOptimization", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "PrimalProblem", "to": "GeneralizedLagrangian", "arrows": "to", "title": "Subtopic of PrimalProblem", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "PrimalProblem", "to": "ThetaP", "arrows": "to", "title": "Subtopic of PrimalProblem", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Primal Problem", "to": "Objective Function Primal", "arrows": "to", "title": "Subtopic of Primal Problem", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Primal Problem", "to": "Theta P", "arrows": "to", "title": "Subtopic of Primal Problem", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Dual Problem", "to": "Objective Function Dual", "arrows": "to", "title": "Subtopic of Dual Problem", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Dual Problem", "to": "Theta D", "arrows": "to", "title": "Subtopic of Dual Problem", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning", "to": "Optimization_Problems", "arrows": "to", "title": "Subtopic of Machine_Learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Optimization_Problems", "to": "Primal_Dual_Pairing", "arrows": "to", "title": "Subtopic of Optimization_Problems", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Primal_Dual_Pairing", "to": "Dual_Problem_Solution", "arrows": "to", "title": "Subtopic of Primal_Dual_Pairing", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Dual_Problem_Solution", "to": "Convexity_Conditions", "arrows": "to", "title": "Subtopic of Dual_Problem_Solution", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Dual_Problem_Solution", "to": "Feasibility_Constraints", "arrows": "to", "title": "Subtopic of Dual_Problem_Solution", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Dual_Problem_Solution", "to": "KKT_Conditions", "arrows": "to", "title": "Subtopic of Dual_Problem_Solution", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "KKT_Conditions", "to": "Dual_Complementarity", "arrows": "to", "title": "Subtopic of KKT_Conditions", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Optimization_Problems", "to": "Primal_Dual_Equivalence", "arrows": "to", "title": "Subtopic of Optimization_Problems", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Dual_Complementarity", "to": "Support_Vectors", "arrows": "to", "title": "Subtopic of Dual_Complementarity", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "LagrangianFormulation", "to": "DualProblemDerivation", "arrows": "to", "title": "Subtopic of LagrangianFormulation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Support Vector Machine (SVM)", "to": "Lagrangian Function", "arrows": "to", "title": "Subtopic of Support Vector Machine (SVM)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Support Vector Machine (SVM)", "to": "Dual Optimization Problem", "arrows": "to", "title": "Subtopic of Support Vector Machine (SVM)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Support Vector Machine (SVM)", "to": "KKT Conditions", "arrows": "to", "title": "Subtopic of Support Vector Machine (SVM)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Support Vector Machine (SVM)", "to": "Optimal Parameters w and b", "arrows": "to", "title": "Subtopic of Support Vector Machine (SVM)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Models", "to": "Support Vector Machines (SVM)", "arrows": "to", "title": "Subtopic of Machine Learning Models", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Support Vector Machines (SVM)", "to": "Optimal Parameters Calculation", "arrows": "to", "title": "Subtopic of Support Vector Machines (SVM)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Optimal Parameters Calculation", "to": "Dual Form Optimization", "arrows": "to", "title": "Subtopic of Optimal Parameters Calculation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Optimal Parameters Calculation", "to": "Prediction with Inner Products", "arrows": "to", "title": "Subtopic of Optimal Parameters Calculation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Models", "to": "Regularization and Non-Separable Data", "arrows": "to", "title": "Subtopic of Machine Learning Models", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Support Vector Machines", "to": "Regularization and Non-Separable Case", "arrows": "to", "title": "Subtopic of Support Vector Machines", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Regularization and Non-Separable Case", "to": "Linear Separability Assumption", "arrows": "to", "title": "Subtopic of Regularization and Non-Separable Case", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Regularization and Non-Separable Case", "to": "Outlier Sensitivity", "arrows": "to", "title": "Subtopic of Regularization and Non-Separable Case", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Regularization and Non-Separable Case", "to": "L1 Regularization", "arrows": "to", "title": "Subtopic of Regularization and Non-Separable Case", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "L1 Regularization", "to": "Optimization Problem", "arrows": "to", "title": "Subtopic of L1 Regularization", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Optimization Problem", "to": "Objective Function", "arrows": "to", "title": "Subtopic of Optimization Problem", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Objective Function", "to": "Slack Variables", "arrows": "to", "title": "Subtopic of Objective Function", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Optimization Problem", "to": "Parameter C", "arrows": "to", "title": "Subtopic of Optimization Problem", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Regularization and Non-Separable Case", "to": "Lagrangian Formulation", "arrows": "to", "title": "Subtopic of Regularization and Non-Separable Case", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Support_Vector_Machines_SVM", "to": "Dual_Problem_Formulation", "arrows": "to", "title": "Subtopic of Support_Vector_Machines_SVM", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Dual_Problem_Formulation", "to": "Lagrange_Multipliers", "arrows": "to", "title": "Subtopic of Dual_Problem_Formulation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Support_Vector_Machines_SVM", "to": "Sequential_Minimal_Optimization_SMO", "arrows": "to", "title": "Subtopic of Support_Vector_Machines_SVM", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Coordinate_Ascend_Algorithm", "to": "Unconstrained_Optimization_Problem", "arrows": "to", "title": "Subtopic of Coordinate_Ascend_Algorithm", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Coordinate_Ascend_Algorithm", "to": "Gradient_Ascend_Newtons_Method", "arrows": "to", "title": "Subtopic of Coordinate_Ascend_Algorithm", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning_Optimization", "to": "Coordinate_Ascend_Method", "arrows": "to", "title": "Subtopic of Machine_Learning_Optimization", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning_Optimization", "to": "Support_Vector_Machines_SVMs", "arrows": "to", "title": "Subtopic of Machine_Learning_Optimization", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Support_Vector_Machines_SVMs", "to": "SVM_Dual_Problem", "arrows": "to", "title": "Subtopic of Support_Vector_Machines_SVMs", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "SMO_Algorithm", "to": "Heuristic_Selection", "arrows": "to", "title": "Subtopic of SMO_Algorithm", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "SMO_Algorithm", "to": "Efficient_Update", "arrows": "to", "title": "Subtopic of SMO_Algorithm", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Efficient_Update", "to": "Constraints_Satisfaction", "arrows": "to", "title": "Subtopic of Efficient_Update", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning_Concepts", "to": "Alpha_Parameters", "arrows": "to", "title": "Subtopic of Machine_Learning_Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Alpha_Parameters", "to": "Constraint_Equation", "arrows": "to", "title": "Subtopic of Alpha_Parameters", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Alpha_Parameters", "to": "Bounds_L_H", "arrows": "to", "title": "Subtopic of Alpha_Parameters", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning_Concepts", "to": "Objective_Function_W", "arrows": "to", "title": "Subtopic of Machine_Learning_Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Objective_Function_W", "to": "Quadratic_Formulation", "arrows": "to", "title": "Subtopic of Objective_Function_W", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Objective_Function_W", "to": "Maximization_Process", "arrows": "to", "title": "Subtopic of Objective_Function_W", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Support Vector Machines (SVM)", "to": "Sequential Minimal Optimization (SMO) Algorithm", "arrows": "to", "title": "Subtopic of Support Vector Machines (SVM)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Sequential Minimal Optimization (SMO) Algorithm", "to": "Alpha Updates", "arrows": "to", "title": "Subtopic of Sequential Minimal Optimization (SMO) Algorithm", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Deep Learning Introduction", "to": "Supervised Learning with Non-Linear Models", "arrows": "to", "title": "Subtopic of Deep Learning Introduction", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningModels", "to": "NonLinearModel", "arrows": "to", "title": "Subtopic of MachineLearningModels", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "NonLinearModel", "to": "TrainingExamples", "arrows": "to", "title": "Subtopic of NonLinearModel", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningModels", "to": "RegressionProblems", "arrows": "to", "title": "Subtopic of MachineLearningModels", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "RegressionProblems", "to": "LeastSquareCostFunction", "arrows": "to", "title": "Subtopic of RegressionProblems", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "RegressionProblems", "to": "MeanSquaredLoss", "arrows": "to", "title": "Subtopic of RegressionProblems", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningModels", "to": "BinaryClassification", "arrows": "to", "title": "Subtopic of MachineLearningModels", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "LogisticRegression", "to": "LogitFunction", "arrows": "to", "title": "Subtopic of LogisticRegression", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "LogisticRegression", "to": "ProbabilityPrediction", "arrows": "to", "title": "Subtopic of LogisticRegression", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "LogisticRegression", "to": "NegativeLikelihoodLoss", "arrows": "to", "title": "Subtopic of LogisticRegression", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "LogisticRegression", "to": "TotalLossFunction", "arrows": "to", "title": "Subtopic of LogisticRegression", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MultiClassClassification", "to": "NegativeLogLikelihoodLossMulticlass", "arrows": "to", "title": "Subtopic of MultiClassClassification", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "Loss Function", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Loss Function", "to": "Negative Log-Likelihood", "arrows": "to", "title": "Subtopic of Loss Function", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Negative Log-Likelihood", "to": "Cross-Entropy Loss", "arrows": "to", "title": "Subtopic of Negative Log-Likelihood", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Loss Function", "to": "Average Loss Function", "arrows": "to", "title": "Subtopic of Loss Function", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Conditional Probabilistic Models", "to": "Exponential Family Distribution", "arrows": "to", "title": "Subtopic of Conditional Probabilistic Models", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Pretraining_Phase", "to": "Optimizers", "arrows": "to", "title": "Subtopic of Pretraining_Phase", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Optimizers", "to": "Gradient Descent (GD)", "arrows": "to", "title": "Subtopic of Optimizers", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Optimizers", "to": "Stochastic Gradient Descent (SGD)", "arrows": "to", "title": "Subtopic of Optimizers", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Algorithms", "to": "Mini-batch Stochastic Gradient Descent", "arrows": "to", "title": "Subtopic of Machine Learning Algorithms", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Stochastic Gradient Descent (SGD)", "to": "Hyperparameters", "arrows": "to", "title": "Subtopic of Stochastic Gradient Descent (SGD)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Algorithms", "to": "Initialization", "arrows": "to", "title": "Subtopic of Machine Learning Algorithms", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Algorithms", "to": "Backpropagation Algorithm", "arrows": "to", "title": "Subtopic of Machine Learning Algorithms", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningOverview", "to": "NeuralNetworks", "arrows": "to", "title": "Subtopic of MachineLearningOverview", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "NeuralNetworks", "to": "RegressionProblem", "arrows": "to", "title": "Subtopic of NeuralNetworks", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "NeuralNetworks", "to": "SingleNeuronNN", "arrows": "to", "title": "Subtopic of NeuralNetworks", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "SingleNeuronNN", "to": "HousingPricePrediction", "arrows": "to", "title": "Subtopic of SingleNeuronNN", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "SingleNeuronNN", "to": "ReLUFunction", "arrows": "to", "title": "Subtopic of SingleNeuronNN", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning_Concepts", "to": "Neural_Networks", "arrows": "to", "title": "Subtopic of Machine_Learning_Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Neural_Networks", "to": "Activation_Functions", "arrows": "to", "title": "Subtopic of Neural_Networks", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Activation_Functions", "to": "ReLU", "arrows": "to", "title": "Subtopic of Activation_Functions", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Neural_Networks", "to": "Single_Neuron_Model", "arrows": "to", "title": "Subtopic of Neural_Networks", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Single_Neuron_Model", "to": "Bias_and_Weights", "arrows": "to", "title": "Subtopic of Single_Neuron_Model", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Neural_Networks", "to": "Stacking_Neurons", "arrows": "to", "title": "Subtopic of Neural_Networks", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Stacking_Neurons", "to": "Complex_Neural_Networks", "arrows": "to", "title": "Subtopic of Stacking_Neurons", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "HousingPricePrediction", "to": "DerivedFeatures", "arrows": "to", "title": "Subtopic of HousingPricePrediction", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "DerivedFeatures", "to": "FamilySize", "arrows": "to", "title": "Subtopic of DerivedFeatures", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "DerivedFeatures", "to": "WalkableNeighborhood", "arrows": "to", "title": "Subtopic of DerivedFeatures", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "DerivedFeatures", "to": "SchoolQuality", "arrows": "to", "title": "Subtopic of DerivedFeatures", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "NeuralNetworks", "to": "InputFeatures", "arrows": "to", "title": "Subtopic of NeuralNetworks", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "NeuralNetworks", "to": "HiddenUnits", "arrows": "to", "title": "Subtopic of NeuralNetworks", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "NeuralNetworks", "to": "ReLUActivation", "arrows": "to", "title": "Subtopic of NeuralNetworks", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningOverview", "to": "NeuralNetworksInspiration", "arrows": "to", "title": "Subtopic of MachineLearningOverview", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningOverview", "to": "TwoLayerNN", "arrows": "to", "title": "Subtopic of MachineLearningOverview", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "NeuralNetworksInspiration", "to": "ParametersTheta", "arrows": "to", "title": "Subtopic of NeuralNetworksInspiration", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "NeuralNetworksInspiration", "to": "BiologicalSimilarity", "arrows": "to", "title": "Subtopic of NeuralNetworksInspiration", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "TwoLayerNN", "to": "PriorKnowledge", "arrows": "to", "title": "Subtopic of TwoLayerNN", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningModels", "to": "FullyConnectedNN", "arrows": "to", "title": "Subtopic of MachineLearningModels", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "FullyConnectedNN", "to": "IntermediateVariables", "arrows": "to", "title": "Subtopic of FullyConnectedNN", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "FullyConnectedNN", "to": "Parameterization", "arrows": "to", "title": "Subtopic of FullyConnectedNN", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningModels", "to": "Vectorization", "arrows": "to", "title": "Subtopic of MachineLearningModels", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "VectorizationInNN", "to": "SpeedPerspective", "arrows": "to", "title": "Subtopic of VectorizationInNN", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "VectorizationInNN", "to": "ParallelismGPUs", "arrows": "to", "title": "Subtopic of VectorizationInNN", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "VectorizationInNN", "to": "MatrixAlgebra", "arrows": "to", "title": "Subtopic of VectorizationInNN", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "VectorizationInNN", "to": "TwoLayerNetwork", "arrows": "to", "title": "Subtopic of VectorizationInNN", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "NeuralNetworks", "to": "WeightMatrices", "arrows": "to", "title": "Subtopic of NeuralNetworks", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "NeuralNetworks", "to": "BiasVectors", "arrows": "to", "title": "Subtopic of NeuralNetworks", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "NeuralNetworks", "to": "ActivationFunctions", "arrows": "to", "title": "Subtopic of NeuralNetworks", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "NeuralNetworks", "to": "LayerArchitecture", "arrows": "to", "title": "Subtopic of NeuralNetworks", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Multi-layer Neural Networks", "to": "Weight Matrices and Biases", "arrows": "to", "title": "Subtopic of Multi-layer Neural Networks", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Multi-layer Neural Networks", "to": "ReLU Activation Function", "arrows": "to", "title": "Subtopic of Multi-layer Neural Networks", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Multi-layer Neural Networks", "to": "Total Number of Neurons", "arrows": "to", "title": "Subtopic of Multi-layer Neural Networks", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Multi-layer Neural Networks", "to": "Total Parameters Count", "arrows": "to", "title": "Subtopic of Multi-layer Neural Networks", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Multi-layer Neural Networks", "to": "Notational Consistency", "arrows": "to", "title": "Subtopic of Multi-layer Neural Networks", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "ActivationFunctions", "to": "TanhFunction", "arrows": "to", "title": "Subtopic of ActivationFunctions", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "ActivationFunctions", "to": "LeakyReLUFunction", "arrows": "to", "title": "Subtopic of ActivationFunctions", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "ActivationFunctions", "to": "GELUFunction", "arrows": "to", "title": "Subtopic of ActivationFunctions", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "ActivationFunctions", "to": "SoftplusFunction", "arrows": "to", "title": "Subtopic of ActivationFunctions", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "ActivationFunctions", "to": "IdentityFunction", "arrows": "to", "title": "Subtopic of ActivationFunctions", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning", "to": "Feature_Engineering", "arrows": "to", "title": "Subtopic of Machine_Learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning", "to": "Deep_Learning", "arrows": "to", "title": "Subtopic of Machine_Learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Deep_Learning", "to": "Feature_Maps", "arrows": "to", "title": "Subtopic of Deep_Learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Feature_Maps", "to": "Linear_Model", "arrows": "to", "title": "Subtopic of Feature_Maps", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Deep Learning Representations", "to": "House Price Prediction Example", "arrows": "to", "title": "Subtopic of Deep Learning Representations", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Deep Learning Representations", "to": "Feature Discovery", "arrows": "to", "title": "Subtopic of Deep Learning Representations", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Deep Learning Representations", "to": "Black Box Nature", "arrows": "to", "title": "Subtopic of Deep Learning Representations", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Modern Neural Network Modules", "to": "Matrix Multiplication Module", "arrows": "to", "title": "Subtopic of Modern Neural Network Modules", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Modern Neural Network Modules", "to": "Nonlinear Activation Module", "arrows": "to", "title": "Subtopic of Modern Neural Network Modules", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Modern Neural Network Modules", "to": "MLP Composition", "arrows": "to", "title": "Subtopic of Modern Neural Network Modules", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningOverview", "to": "MLPArchitecture", "arrows": "to", "title": "Subtopic of MachineLearningOverview", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MLPArchitecture", "to": "MatrixMultiplicationModule", "arrows": "to", "title": "Subtopic of MLPArchitecture", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MLPArchitecture", "to": "NonlinearActivationModule", "arrows": "to", "title": "Subtopic of MLPArchitecture", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "ResNetOverview", "to": "ResidualBlockDefinition", "arrows": "to", "title": "Subtopic of ResNetOverview", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "ResNetOverview", "to": "ResNetComposition", "arrows": "to", "title": "Subtopic of ResNetOverview", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningOverview", "to": "ResNetArchitecture", "arrows": "to", "title": "Subtopic of MachineLearningOverview", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "ResNetArchitecture", "to": "ConvolutionalLayers", "arrows": "to", "title": "Subtopic of ResNetArchitecture", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "ResNetArchitecture", "to": "BatchNormalization", "arrows": "to", "title": "Subtopic of ResNetArchitecture", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "LayerNormalization", "to": "LN-SModule", "arrows": "to", "title": "Subtopic of LayerNormalization", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "LayerNormalization", "to": "LN-S", "arrows": "to", "title": "Subtopic of LayerNormalization", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "LayerNormalization", "to": "AffineTransformation", "arrows": "to", "title": "Subtopic of LayerNormalization", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "LayerNormalization", "to": "LearnableParameters", "arrows": "to", "title": "Subtopic of LayerNormalization", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "ScalingInvariantProperty", "to": "MM_Wb", "arrows": "to", "title": "Subtopic of ScalingInvariantProperty", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Normalization Techniques", "to": "Layer Normalization (LN)", "arrows": "to", "title": "Subtopic of Normalization Techniques", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Layer Normalization (LN)", "to": "Equation 7.43", "arrows": "to", "title": "Subtopic of Layer Normalization (LN)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Layer Normalization (LN)", "to": "Equation 7.44-7.47", "arrows": "to", "title": "Subtopic of Layer Normalization (LN)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Normalization Techniques", "to": "Scale-Invariant Property", "arrows": "to", "title": "Subtopic of Normalization Techniques", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Normalization Techniques", "to": "Batch Normalization (BN)", "arrows": "to", "title": "Subtopic of Normalization Techniques", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Normalization Techniques", "to": "Group Normalization", "arrows": "to", "title": "Subtopic of Normalization Techniques", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Convolutional Layers", "to": "1-D Convolution (Conv1D)", "arrows": "to", "title": "Subtopic of Convolutional Layers", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning_Concepts", "to": "Convolutional_Neural_Networks", "arrows": "to", "title": "Subtopic of Machine_Learning_Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Convolutional_Neural_Networks", "to": "1D_Convolution", "arrows": "to", "title": "Subtopic of Convolutional_Neural_Networks", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "1D_Convolution", "to": "Simplified_1D_Convolution", "arrows": "to", "title": "Subtopic of 1D_Convolution", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Simplified_1D_Convolution", "to": "Filter_Vector", "arrows": "to", "title": "Subtopic of Simplified_1D_Convolution", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Simplified_1D_Convolution", "to": "Bias_Scalar", "arrows": "to", "title": "Subtopic of Simplified_1D_Convolution", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Simplified_1D_Convolution", "to": "Matrix_Multiplication", "arrows": "to", "title": "Subtopic of Simplified_1D_Convolution", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning_Concepts", "to": "Convolutional_Layers", "arrows": "to", "title": "Subtopic of Machine_Learning_Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Convolutional_Layers", "to": "Parameter_Sharing", "arrows": "to", "title": "Subtopic of Convolutional_Layers", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Convolutional_Layers", "to": "Efficiency_of_Convolution", "arrows": "to", "title": "Subtopic of Convolutional_Layers", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Convolutional_Layers", "to": "Conv1D_Channel_Variants", "arrows": "to", "title": "Subtopic of Convolutional_Layers", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningOverview", "to": "Conv1D-S", "arrows": "to", "title": "Subtopic of MachineLearningOverview", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Conv1D-S", "to": "TotalParametersConv1D", "arrows": "to", "title": "Subtopic of Conv1D-S", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "TotalParametersConv1D", "to": "LinearMappingComparison", "arrows": "to", "title": "Subtopic of TotalParametersConv1D", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "TotalParametersConv1D", "to": "ParameterTensorRepresentation", "arrows": "to", "title": "Subtopic of TotalParametersConv1D", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningOverview", "to": "Conv2D-S", "arrows": "to", "title": "Subtopic of MachineLearningOverview", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Conv2D-S", "to": "TotalParametersConv2D", "arrows": "to", "title": "Subtopic of Conv2D-S", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Backpropagation", "to": "Differentiable Circuit", "arrows": "to", "title": "Subtopic of Backpropagation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Backpropagation", "to": "Gradient Computation", "arrows": "to", "title": "Subtopic of Backpropagation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Backpropagation", "to": "Theorem 7.4.1", "arrows": "to", "title": "Subtopic of Backpropagation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Gradient Computation", "to": "Chain Rule", "arrows": "to", "title": "Subtopic of Gradient Computation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Backpropagation", "to": "Auto-differentiation", "arrows": "to", "title": "Subtopic of Backpropagation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "Deep Learning Packages", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Neural Networks", "to": "MLPs (Multilayer Perceptrons)", "arrows": "to", "title": "Subtopic of Neural Networks", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning_Concepts", "to": "Partial_Derivatives", "arrows": "to", "title": "Subtopic of Machine_Learning_Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning_Concepts", "to": "Chain_Rule", "arrows": "to", "title": "Subtopic of Machine_Learning_Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Chain_Rule", "to": "Scalar_Variable_J", "arrows": "to", "title": "Subtopic of Chain_Rule", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Chain_Rule", "to": "Vectorized_Notation", "arrows": "to", "title": "Subtopic of Chain_Rule", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning_Backward_Propagation", "to": "Backward_Function_Linear_Map", "arrows": "to", "title": "Subtopic of Machine_Learning_Backward_Propagation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Backward_Function_Linear_Map", "to": "Jacobian_Matrix_Transpose", "arrows": "to", "title": "Subtopic of Backward_Function_Linear_Map", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Jacobian_Matrix_Transpose", "to": "Complexity_of_Jacobian_Matrices", "arrows": "to", "title": "Subtopic of Jacobian_Matrix_Transpose", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Backward_Function_Linear_Map", "to": "Equation_7.53_Usefulness", "arrows": "to", "title": "Subtopic of Backward_Function_Linear_Map", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Equation_7.53_Usefulness", "to": "Derivations_Section_7.4.3", "arrows": "to", "title": "Subtopic of Equation_7.53_Usefulness", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning_Backward_Propagation", "to": "Chain_Rule_Interpretation", "arrows": "to", "title": "Subtopic of Machine_Learning_Backward_Propagation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Backpropagation", "to": "Loss Function Composition", "arrows": "to", "title": "Subtopic of Backpropagation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Backpropagation", "to": "Modules", "arrows": "to", "title": "Subtopic of Backpropagation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningOverview", "to": "BinaryClassificationProblem", "arrows": "to", "title": "Subtopic of MachineLearningOverview", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "BinaryClassificationProblem", "to": "MLPModelDefinition", "arrows": "to", "title": "Subtopic of BinaryClassificationProblem", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "BinaryClassificationProblem", "to": "LossFunctionFormulation", "arrows": "to", "title": "Subtopic of BinaryClassificationProblem", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MLPModelDefinition", "to": "ModulesInMLP", "arrows": "to", "title": "Subtopic of MLPModelDefinition", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "ModulesInMLP", "to": "ParameterizationOfModules", "arrows": "to", "title": "Subtopic of ModulesInMLP", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "IntermediateVariables", "to": "ForwardPass", "arrows": "to", "title": "Subtopic of IntermediateVariables", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "IntermediateVariables", "to": "BackwardPass", "arrows": "to", "title": "Subtopic of IntermediateVariables", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning_Backpropagation", "to": "Chain_Rule_Application", "arrows": "to", "title": "Subtopic of Machine_Learning_Backpropagation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning_Backpropagation", "to": "Efficient_Backward_Propagation", "arrows": "to", "title": "Subtopic of Machine_Learning_Backpropagation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningOverview", "to": "NeuralNetworksComposition", "arrows": "to", "title": "Subtopic of MachineLearningOverview", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningOverview", "to": "BackpropagationDiscussion", "arrows": "to", "title": "Subtopic of MachineLearningOverview", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningOverview", "to": "ModulesInPractice", "arrows": "to", "title": "Subtopic of MachineLearningOverview", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "BackwardFunctionsBasics", "to": "LossFunctionBackward", "arrows": "to", "title": "Subtopic of BackwardFunctionsBasics", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningConcepts", "to": "BackwardFunctionWb", "arrows": "to", "title": "Subtopic of MachineLearningConcepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "BackwardFunctionWb", "to": "VectorizedNotation", "arrows": "to", "title": "Subtopic of BackwardFunctionWb", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "BackwardFunctionWb", "to": "EfficiencyConsiderations", "arrows": "to", "title": "Subtopic of BackwardFunctionWb", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "ActivationFunctions", "to": "BackwardFunctionActivations", "arrows": "to", "title": "Subtopic of ActivationFunctions", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning_Backward_Functions", "to": "Efficiency_of_Backward_Pass", "arrows": "to", "title": "Subtopic of Machine_Learning_Backward_Functions", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning_Backward_Functions", "to": "Vectorized_Notation_Backward_Func", "arrows": "to", "title": "Subtopic of Machine_Learning_Backward_Functions", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning_Backward_Functions", "to": "Squared_Loss_Backward", "arrows": "to", "title": "Subtopic of Machine_Learning_Backward_Functions", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning_Backward_Functions", "to": "Logistic_Loss_Backward", "arrows": "to", "title": "Subtopic of Machine_Learning_Backward_Functions", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning_Backward_Functions", "to": "Cross_Entropy_Loss_Backward", "arrows": "to", "title": "Subtopic of Machine_Learning_Backward_Functions", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "Loss Functions", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Loss Functions", "to": "Logistic Loss", "arrows": "to", "title": "Subtopic of Loss Functions", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Backpropagation for MLPs", "to": "Forward Pass", "arrows": "to", "title": "Subtopic of Backpropagation for MLPs", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Backpropagation Algorithm", "to": "Intermediate Values Storage", "arrows": "to", "title": "Subtopic of Backpropagation Algorithm", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningBasics", "to": "TrainingSetExamples", "arrows": "to", "title": "Subtopic of MachineLearningBasics", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningBasics", "to": "MatrixNotation", "arrows": "to", "title": "Subtopic of MachineLearningBasics", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "TrainingSetExamples", "to": "LayerActivations", "arrows": "to", "title": "Subtopic of TrainingSetExamples", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MatrixNotation", "to": "VectorizationTechniques", "arrows": "to", "title": "Subtopic of MatrixNotation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "VectorizationTechniques", "to": "Broadcasting", "arrows": "to", "title": "Subtopic of VectorizationTechniques", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningBasics", "to": "LayerGeneralization", "arrows": "to", "title": "Subtopic of MachineLearningBasics", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning", "to": "Matricization Approach", "arrows": "to", "title": "Subtopic of Machine Learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Matricization Approach", "to": "Data Matrix Representation", "arrows": "to", "title": "Subtopic of Matricization Approach", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Matricization Approach", "to": "Conversion Between Representations", "arrows": "to", "title": "Subtopic of Matricization Approach", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Generalization", "to": "Training Loss Function", "arrows": "to", "title": "Subtopic of Generalization", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Loss_Functions", "to": "Training_Loss", "arrows": "to", "title": "Subtopic of Loss_Functions", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Loss_Functions", "to": "Test_Error", "arrows": "to", "title": "Subtopic of Loss_Functions", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning_Concepts", "to": "Empirical_Distribution", "arrows": "to", "title": "Subtopic of Machine_Learning_Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning_Concepts", "to": "Population_Distribution", "arrows": "to", "title": "Subtopic of Machine_Learning_Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning_Concepts", "to": "Training_Data_Set", "arrows": "to", "title": "Subtopic of Machine_Learning_Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning_Concepts", "to": "Test_Data_Set", "arrows": "to", "title": "Subtopic of Machine_Learning_Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning_Concepts", "to": "Learning_Settings", "arrows": "to", "title": "Subtopic of Machine_Learning_Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Learning_Settings", "to": "Training_Distribution", "arrows": "to", "title": "Subtopic of Learning_Settings", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Learning_Settings", "to": "Test_Distribution", "arrows": "to", "title": "Subtopic of Learning_Settings", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Learning_Settings", "to": "Domain_Shift", "arrows": "to", "title": "Subtopic of Learning_Settings", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning_Concepts", "to": "Overfitting_Underfitting", "arrows": "to", "title": "Subtopic of Machine_Learning_Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Overfitting_Underfitting", "to": "Test_Error_Training_Error", "arrows": "to", "title": "Subtopic of Overfitting_Underfitting", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Test_Error_Training_Error", "to": "Generalization_Gap", "arrows": "to", "title": "Subtopic of Test_Error_Training_Error", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Bias_Variance_Tradoff", "to": "Model_Parameterizations", "arrows": "to", "title": "Subtopic of Bias_Variance_Tradoff", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "Bias-Variance Tradeoff", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "Double Descent Phenomenon", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Bias-Variance Tradeoff", "to": "Training and Test Datasets", "arrows": "to", "title": "Subtopic of Bias-Variance Tradeoff", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Bias-Variance Tradeoff", "to": "Linear Regression Example", "arrows": "to", "title": "Subtopic of Bias-Variance Tradeoff", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Bias-Variance Tradeoff", "to": "Model Complexity", "arrows": "to", "title": "Subtopic of Bias-Variance Tradeoff", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning_Bias_Variance_Tradeoff", "to": "Linear_Model_Failure", "arrows": "to", "title": "Subtopic of Machine_Learning_Bias_Variance_Tradeoff", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning_Bias_Variance_Tradeoff", "to": "Bias_Definition", "arrows": "to", "title": "Subtopic of Machine_Learning_Bias_Variance_Tradeoff", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning_Bias_Variance_Tradeoff", "to": "5th_Degree_Polynomial_Failure", "arrows": "to", "title": "Subtopic of Machine_Learning_Bias_Variance_Tradeoff", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning_Bias_Variance_Tradeoff", "to": "Generalization_Error", "arrows": "to", "title": "Subtopic of Machine_Learning_Bias_Variance_Tradeoff", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningConcepts", "to": "PolynomialFitting", "arrows": "to", "title": "Subtopic of MachineLearningConcepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "PolynomialFitting", "to": "Variance", "arrows": "to", "title": "Subtopic of PolynomialFitting", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningConcepts", "to": "BiasVsVarianceTradeoff", "arrows": "to", "title": "Subtopic of MachineLearningConcepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Bias-Variance Tradeoff", "to": "Test Error Decomposition", "arrows": "to", "title": "Subtopic of Bias-Variance Tradeoff", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "RegressionProblems", "to": "BiasVarianceTradeoff", "arrows": "to", "title": "Subtopic of RegressionProblems", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "BiasVarianceTradeoff", "to": "TrainingDataset", "arrows": "to", "title": "Subtopic of BiasVarianceTradeoff", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "BiasVarianceTradeoff", "to": "TestExample", "arrows": "to", "title": "Subtopic of BiasVarianceTradeoff", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "BiasVarianceTradeoff", "to": "MSE", "arrows": "to", "title": "Subtopic of BiasVarianceTradeoff", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "BiasVarianceTradeoff", "to": "Claim8.1.1", "arrows": "to", "title": "Subtopic of BiasVarianceTradeoff", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningConcepts", "to": "MSEDecomposition", "arrows": "to", "title": "Subtopic of MachineLearningConcepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MSEDecomposition", "to": "AverageModel", "arrows": "to", "title": "Subtopic of MSEDecomposition", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MSEDecomposition", "to": "BiasTerm", "arrows": "to", "title": "Subtopic of MSEDecomposition", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MSEDecomposition", "to": "VarianceTerm", "arrows": "to", "title": "Subtopic of MSEDecomposition", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning_Bias_Variance", "to": "Bias_Term", "arrows": "to", "title": "Subtopic of Machine_Learning_Bias_Variance", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning_Bias_Variance", "to": "Variance_Term", "arrows": "to", "title": "Subtopic of Machine_Learning_Bias_Variance", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning_Bias_Variance", "to": "Noise_Prediction_Impact", "arrows": "to", "title": "Subtopic of Machine_Learning_Bias_Variance", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning_Bias_Variance", "to": "Bias_Variance_Classification", "arrows": "to", "title": "Subtopic of Machine_Learning_Bias_Variance", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Double_Descent_Phenomenon", "to": "Model_Wise_Double_Descent", "arrows": "to", "title": "Subtopic of Double_Descent_Phenomenon", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Double Descent Phenomenon", "to": "Model-wise Double Descent", "arrows": "to", "title": "Subtopic of Double Descent Phenomenon", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Double Descent Phenomenon", "to": "Sample-wise Double Descent", "arrows": "to", "title": "Subtopic of Double Descent Phenomenon", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "Overparameterized Models", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Double Descent Phenomenon", "to": "Historical Context", "arrows": "to", "title": "Subtopic of Double Descent Phenomenon", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Model-wise Double Descent", "to": "Optimal Regularization", "arrows": "to", "title": "Subtopic of Model-wise Double Descent", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Double Descent Phenomenon", "to": "Implicit Regularization", "arrows": "to", "title": "Subtopic of Double Descent Phenomenon", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "Regularization Techniques", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Model Complexity", "to": "Parameter Count vs. Model Norm", "arrows": "to", "title": "Subtopic of Model Complexity", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "Linear Regression Setup", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Sample Complexity Bounds", "to": "Model Selection Methods", "arrows": "to", "title": "Subtopic of Sample Complexity Bounds", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Sample Complexity Bounds", "to": "Generalization Error", "arrows": "to", "title": "Subtopic of Sample Complexity Bounds", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Learning Theory Proofs", "to": "Union Bound Lemma", "arrows": "to", "title": "Subtopic of Learning Theory Proofs", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Learning Theory Proofs", "to": "Hoeffding Inequality", "arrows": "to", "title": "Subtopic of Learning Theory Proofs", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Binary Classification", "to": "Training Set", "arrows": "to", "title": "Subtopic of Binary Classification", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning_Basics", "to": "Binary_Classification", "arrows": "to", "title": "Subtopic of Machine_Learning_Basics", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Binary_Classification", "to": "Hypothesis", "arrows": "to", "title": "Subtopic of Binary_Classification", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Binary_Classification", "to": "Training_Error", "arrows": "to", "title": "Subtopic of Binary_Classification", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Binary_Classification", "to": "PAC_Assumptions", "arrows": "to", "title": "Subtopic of Binary_Classification", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning_Basics", "to": "Linear_Classification", "arrows": "to", "title": "Subtopic of Machine_Learning_Basics", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Linear_Classification", "to": "Empirical_Risk_Minimization", "arrows": "to", "title": "Subtopic of Linear_Classification", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning", "to": "Hypothesis_Class", "arrows": "to", "title": "Subtopic of Machine_Learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Empirical_Risk_Minimization", "to": "Finite_Hypothesis_Class", "arrows": "to", "title": "Subtopic of Empirical_Risk_Minimization", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning_Concepts", "to": "Hoeffding_Inequality", "arrows": "to", "title": "Subtopic of Machine_Learning_Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Generalization_Error", "to": "Bernoulli_Random_Variables", "arrows": "to", "title": "Subtopic of Generalization_Error", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Generalization_Error_Bound", "to": "Uniform_Convergence", "arrows": "to", "title": "Subtopic of Generalization_Error_Bound", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Uniform_Convergence", "to": "Training_Error_Generalization_Error_Difference", "arrows": "to", "title": "Subtopic of Uniform_Convergence", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Uniform_Convergence", "to": "Union_Bound_Application", "arrows": "to", "title": "Subtopic of Uniform_Convergence", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Training_Error_Generalization_Error_Difference", "to": "Probability_Error_Bounds", "arrows": "to", "title": "Subtopic of Training_Error_Generalization_Error_Difference", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Quantities_of_Interest", "to": "Sample_Size_Calculation", "arrows": "to", "title": "Subtopic of Quantities_of_Interest", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Quantities_of_Interest", "to": "Error_Margin_Determination", "arrows": "to", "title": "Subtopic of Quantities_of_Interest", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning_Theory", "to": "Generalization_Error_Bound", "arrows": "to", "title": "Subtopic of Machine_Learning_Theory", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning_Theory", "to": "Sample_Complexity", "arrows": "to", "title": "Subtopic of Machine_Learning_Theory", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Sample_Complexity", "to": "Hypothesis_Space_Size", "arrows": "to", "title": "Subtopic of Sample_Complexity", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning_Theory", "to": "Best_Hypothesis", "arrows": "to", "title": "Subtopic of Machine_Learning_Theory", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning_Theorem", "to": "Uniform_Convergence_Assumption", "arrows": "to", "title": "Subtopic of Machine_Learning_Theorem", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning_Theorem", "to": "Bias_Variance_Tradeoff", "arrows": "to", "title": "Subtopic of Machine_Learning_Theorem", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning_Bias_Variance_Tradeoff", "to": "Hypothesis_Class_Switching", "arrows": "to", "title": "Subtopic of Machine_Learning_Bias_Variance_Tradeoff", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Hypothesis_Class_Switching", "to": "Bias_Decrease", "arrows": "to", "title": "Subtopic of Hypothesis_Class_Switching", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Hypothesis_Class_Switching", "to": "Variance_Increase", "arrows": "to", "title": "Subtopic of Hypothesis_Class_Switching", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning_Bias_Variance_Tradeoff", "to": "Sample_Complexity_Bound", "arrows": "to", "title": "Subtopic of Machine_Learning_Bias_Variance_Tradeoff", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning_Bias_Variance_Tradeoff", "to": "Infinite_Hypothesis_Classes", "arrows": "to", "title": "Subtopic of Machine_Learning_Bias_Variance_Tradeoff", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Sample_Complexity", "to": "Hypothesis_Class_Size", "arrows": "to", "title": "Subtopic of Sample_Complexity", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Sample_Complexity", "to": "Parameterization_Impact", "arrows": "to", "title": "Subtopic of Sample_Complexity", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningConcepts", "to": "HypothesisClassParameterization", "arrows": "to", "title": "Subtopic of MachineLearningConcepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "HypothesisClassParameterization", "to": "LinearClassifierDefinition", "arrows": "to", "title": "Subtopic of HypothesisClassParameterization", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningConcepts", "to": "ShatteringConcept", "arrows": "to", "title": "Subtopic of MachineLearningConcepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningConcepts", "to": "VCDimensionIntroduction", "arrows": "to", "title": "Subtopic of MachineLearningConcepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "VC Dimension", "to": "Shattering", "arrows": "to", "title": "Subtopic of VC Dimension", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Vapnik's Theorem", "to": "Uniform Convergence", "arrows": "to", "title": "Subtopic of Vapnik's Theorem", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Regularization", "to": "ModelComplexity", "arrows": "to", "title": "Subtopic of Regularization", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Regularization", "to": "RegularizerFunction", "arrows": "to", "title": "Subtopic of Regularization", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Regularization", "to": "TrainingLoss", "arrows": "to", "title": "Subtopic of Regularization", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Regularization", "to": "RegularizationParameter", "arrows": "to", "title": "Subtopic of Regularization", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Regularized Loss Function", "to": "Loss J(\u03b8)", "arrows": "to", "title": "Subtopic of Regularized Loss Function", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Regularized Loss Function", "to": "Regularizer R(\u03b8)", "arrows": "to", "title": "Subtopic of Regularized Loss Function", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Regularized Loss Function", "to": "Regularization Parameter \u03bb", "arrows": "to", "title": "Subtopic of Regularized Loss Function", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Regularizer R(\u03b8)", "to": "L2 Regularization", "arrows": "to", "title": "Subtopic of Regularizer R(\u03b8)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "L2 Regularization", "to": "Weight Decay", "arrows": "to", "title": "Subtopic of L2 Regularization", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Regularizer R(\u03b8)", "to": "Sparsity Inducing Regularization", "arrows": "to", "title": "Subtopic of Regularizer R(\u03b8)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Regularization in Machine Learning", "to": "Sparsity Regularization", "arrows": "to", "title": "Subtopic of Regularization in Machine Learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Sparsity Regularization", "to": "L1 Norm (LASSO)", "arrows": "to", "title": "Subtopic of Sparsity Regularization", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Regularization in Machine Learning", "to": "L2 Norm Regularization", "arrows": "to", "title": "Subtopic of Regularization in Machine Learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Sparsity Regularization", "to": "Gradient Descent Incompatibility", "arrows": "to", "title": "Subtopic of Sparsity Regularization", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Regularization in Machine Learning", "to": "Kernel Methods Compatibility", "arrows": "to", "title": "Subtopic of Regularization in Machine Learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Regularization in Machine Learning", "to": "Deep Learning Regularization Techniques", "arrows": "to", "title": "Subtopic of Regularization in Machine Learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Regularization in Deep Learning", "to": "Explicit Regularization Techniques", "arrows": "to", "title": "Subtopic of Regularization in Deep Learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Regularization in Deep Learning", "to": "Implicit Regularization Effect", "arrows": "to", "title": "Subtopic of Regularization in Deep Learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Implicit Regularization Effect", "to": "Global Minima Diversity", "arrows": "to", "title": "Subtopic of Implicit Regularization Effect", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Implicit Regularization Effect", "to": "Optimizer Impact", "arrows": "to", "title": "Subtopic of Implicit Regularization Effect", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Optimizers", "to": "Global Minima", "arrows": "to", "title": "Subtopic of Optimizers", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Optimizers", "to": "Learning Rate Schedules", "arrows": "to", "title": "Subtopic of Optimizers", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Model Selection", "to": "Cross Validation", "arrows": "to", "title": "Subtopic of Model Selection", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Model Selection", "to": "Polynomial Regression Model", "arrows": "to", "title": "Subtopic of Model Selection", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Cross Validation", "to": "Bias and Variance Tradeoff", "arrows": "to", "title": "Subtopic of Cross Validation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Model Selection", "to": "Model Set M", "arrows": "to", "title": "Subtopic of Model Selection", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Model Set M", "to": "SVM", "arrows": "to", "title": "Subtopic of Model Set M", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Model Set M", "to": "Neural Network", "arrows": "to", "title": "Subtopic of Model Set M", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningAlgorithms", "to": "EmpiricalRiskMinimization", "arrows": "to", "title": "Subtopic of MachineLearningAlgorithms", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "CrossValidation", "to": "HoldOutCrossValidation", "arrows": "to", "title": "Subtopic of CrossValidation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "EmpiricalRiskMinimization", "to": "TrainingSetS", "arrows": "to", "title": "Subtopic of EmpiricalRiskMinimization", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "EmpiricalRiskMinimization", "to": "HypothesesHi", "arrows": "to", "title": "Subtopic of EmpiricalRiskMinimization", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "EmpiricalRiskMinimization", "to": "TrainingError", "arrows": "to", "title": "Subtopic of EmpiricalRiskMinimization", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "EmpiricalRiskMinimization", "to": "DegreeOfPolynomial", "arrows": "to", "title": "Subtopic of EmpiricalRiskMinimization", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "HoldOutCrossValidation", "to": "S_train", "arrows": "to", "title": "Subtopic of HoldOutCrossValidation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "HoldOutCrossValidation", "to": "S_cv", "arrows": "to", "title": "Subtopic of HoldOutCrossValidation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "HoldOutCrossValidation", "to": "ValidationError", "arrows": "to", "title": "Subtopic of HoldOutCrossValidation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning_Techniques", "to": "Model_Selection", "arrows": "to", "title": "Subtopic of Machine_Learning_Techniques", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Model_Selection", "to": "Validation_Set_Size", "arrows": "to", "title": "Subtopic of Model_Selection", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Model_Selection", "to": "Cross_Validation", "arrows": "to", "title": "Subtopic of Model_Selection", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Cross_Validation", "to": "Hold_Out_Cross_Validation", "arrows": "to", "title": "Subtopic of Cross_Validation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Cross_Validation", "to": "k_Fold_Cross_Validation", "arrows": "to", "title": "Subtopic of Cross_Validation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "k_Fold_Cross_Validation", "to": "Leave_One_Out_Cross_Validation", "arrows": "to", "title": "Subtopic of k_Fold_Cross_Validation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "k_Fold_Cross_Validation", "to": "Training_and_Testing_Process", "arrows": "to", "title": "Subtopic of k_Fold_Cross_Validation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Cross Validation", "to": "Leave-One-Out CV", "arrows": "to", "title": "Subtopic of Cross Validation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Bayesian Statistics", "to": "MLE", "arrows": "to", "title": "Subtopic of Bayesian Statistics", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Bayesian Statistics", "to": "Prior Distribution", "arrows": "to", "title": "Subtopic of Bayesian Statistics", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Bayesian Machine Learning", "to": "Posterior Distribution", "arrows": "to", "title": "Subtopic of Bayesian Machine Learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Posterior Distribution", "to": "Bayes' Theorem", "arrows": "to", "title": "Subtopic of Posterior Distribution", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Bayes' Theorem", "to": "Likelihood Function", "arrows": "to", "title": "Subtopic of Bayes' Theorem", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Posterior Distribution", "to": "Prediction on New Data", "arrows": "to", "title": "Subtopic of Posterior Distribution", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Prediction on New Data", "to": "Expected Value Prediction", "arrows": "to", "title": "Subtopic of Prediction on New Data", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Bayesian Machine Learning", "to": "Fully Bayesian Prediction", "arrows": "to", "title": "Subtopic of Bayesian Machine Learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Fully Bayesian Prediction", "to": "Computational Challenges", "arrows": "to", "title": "Subtopic of Fully Bayesian Prediction", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningOverview", "to": "BayesianInference", "arrows": "to", "title": "Subtopic of MachineLearningOverview", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "BayesianInference", "to": "PosteriorApproximation", "arrows": "to", "title": "Subtopic of BayesianInference", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "PosteriorApproximation", "to": "MAPEstimate", "arrows": "to", "title": "Subtopic of PosteriorApproximation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MAPEstimate", "to": "MLEvsMAP", "arrows": "to", "title": "Subtopic of MAPEstimate", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "PosteriorApproximation", "to": "PriorDistributions", "arrows": "to", "title": "Subtopic of PosteriorApproximation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "UnsupervisedLearning", "to": "Clustering", "arrows": "to", "title": "Subtopic of UnsupervisedLearning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Clustering", "to": "KMeansAlgorithm", "arrows": "to", "title": "Subtopic of Clustering", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "k-means_algorithm", "to": "distortion_function", "arrows": "to", "title": "Subtopic of k-means_algorithm", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "k-means_algorithm", "to": "centroid_initialization", "arrows": "to", "title": "Subtopic of k-means_algorithm", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "k-means_algorithm", "to": "inner_loop_steps", "arrows": "to", "title": "Subtopic of k-means_algorithm", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "distortion_function", "to": "coordinate_descent", "arrows": "to", "title": "Subtopic of distortion_function", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "k-means Algorithm", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "k-means Algorithm", "to": "Distortion Function J", "arrows": "to", "title": "Subtopic of k-means Algorithm", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "k-means Algorithm", "to": "Convergence in k-means", "arrows": "to", "title": "Subtopic of k-means Algorithm", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "EM Algorithms", "to": "Mixture of Gaussians", "arrows": "to", "title": "Subtopic of EM Algorithms", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Unsupervised Learning", "to": "Mixture of Gaussians Model", "arrows": "to", "title": "Subtopic of Unsupervised Learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Mixture of Gaussians Model", "to": "Joint Distribution", "arrows": "to", "title": "Subtopic of Mixture of Gaussians Model", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Joint Distribution", "to": "Latent Variables", "arrows": "to", "title": "Subtopic of Joint Distribution", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Mixture of Gaussians Model", "to": "Parameter Estimation", "arrows": "to", "title": "Subtopic of Mixture of Gaussians Model", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Parameter Estimation", "to": "Closed Form Solution", "arrows": "to", "title": "Subtopic of Parameter Estimation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningConcepts", "to": "DensityEstimation", "arrows": "to", "title": "Subtopic of MachineLearningConcepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "DensityEstimation", "to": "GaussianMixtureModel", "arrows": "to", "title": "Subtopic of DensityEstimation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "GaussianMixtureModel", "to": "EMAlgorithm", "arrows": "to", "title": "Subtopic of GaussianMixtureModel", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Optimization_Techniques", "to": "EM_Algorithm", "arrows": "to", "title": "Subtopic of Optimization_Techniques", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "EM_Algorithm", "to": "E_Step", "arrows": "to", "title": "Subtopic of EM_Algorithm", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "EM_Algorithm", "to": "M_Step", "arrows": "to", "title": "Subtopic of EM_Algorithm", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "E_Step", "to": "Gaussian_Mixture_Models", "arrows": "to", "title": "Subtopic of E_Step", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "EM_Algorithm", "to": "Soft_Assignments", "arrows": "to", "title": "Subtopic of EM_Algorithm", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "EM_Algorithm", "to": "K_Means_Clustering", "arrows": "to", "title": "Subtopic of EM_Algorithm", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "EM_Algorithm", "to": "Convergence_Issues", "arrows": "to", "title": "Subtopic of EM_Algorithm", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning_Concepts", "to": "EM_Algorithm_Generalization", "arrows": "to", "title": "Subtopic of Machine_Learning_Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Jensens_Inequality", "to": "Convex_Functions", "arrows": "to", "title": "Subtopic of Jensens_Inequality", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Jensens_Inequality", "to": "Theorem_Jensens_Inequality", "arrows": "to", "title": "Subtopic of Jensens_Inequality", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Jensen's Inequality", "to": "Convex Function", "arrows": "to", "title": "Subtopic of Jensen's Inequality", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Jensen's Inequality", "to": "Concave Function", "arrows": "to", "title": "Subtopic of Jensen's Inequality", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Jensen's Inequality", "to": "E[f(X)] vs f(E[X])", "arrows": "to", "title": "Subtopic of Jensen's Inequality", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Variational Inference", "to": "EM Algorithm", "arrows": "to", "title": "Subtopic of Variational Inference", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningOverview", "to": "OptimizationChallenges", "arrows": "to", "title": "Subtopic of MachineLearningOverview", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningOverview", "to": "EMAlgorithmIntroduction", "arrows": "to", "title": "Subtopic of MachineLearningOverview", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "EMAlgorithmIntroduction", "to": "LatentVariables", "arrows": "to", "title": "Subtopic of EMAlgorithmIntroduction", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "EMAlgorithmIntroduction", "to": "EStepMStepProcess", "arrows": "to", "title": "Subtopic of EMAlgorithmIntroduction", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "LikelihoodFunction", "to": "SingleExampleOptimization", "arrows": "to", "title": "Subtopic of LikelihoodFunction", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "SingleExampleOptimization", "to": "SummationNotEssential", "arrows": "to", "title": "Subtopic of SingleExampleOptimization", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningConcepts", "to": "ProbabilityDistributions", "arrows": "to", "title": "Subtopic of MachineLearningConcepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningConcepts", "to": "JensensInequality", "arrows": "to", "title": "Subtopic of MachineLearningConcepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningConcepts", "to": "LogLikelihoodBound", "arrows": "to", "title": "Subtopic of MachineLearningConcepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningConcepts", "to": "EvidenceLowerBoundELBO", "arrows": "to", "title": "Subtopic of MachineLearningConcepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "EM_Algorithm", "to": "Log_Likelihood_Optimization", "arrows": "to", "title": "Subtopic of EM_Algorithm", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Log_Likelihood_Optimization", "to": "Evidence_Lower_Bound_(ELBO)", "arrows": "to", "title": "Subtopic of Log_Likelihood_Optimization", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Evidence_Lower_Bound_(ELBO)", "to": "Multiple_Examples_Consideration", "arrows": "to", "title": "Subtopic of Evidence_Lower_Bound_(ELBO)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningOverview", "to": "ExpectationMaximizationAlgorithm", "arrows": "to", "title": "Subtopic of MachineLearningOverview", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "ExpectationMaximizationAlgorithm", "to": "EStep", "arrows": "to", "title": "Subtopic of ExpectationMaximizationAlgorithm", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "ExpectationMaximizationAlgorithm", "to": "MStep", "arrows": "to", "title": "Subtopic of ExpectationMaximizationAlgorithm", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "ExpectationMaximizationAlgorithm", "to": "LogLikelihoodImprovement", "arrows": "to", "title": "Subtopic of ExpectationMaximizationAlgorithm", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "EStep", "to": "ELBO", "arrows": "to", "title": "Subtopic of EStep", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningConcepts", "to": "ELBOExplanation", "arrows": "to", "title": "Subtopic of MachineLearningConcepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "ELBOExplanation", "to": "AlternativeFormulationsOfELBO", "arrows": "to", "title": "Subtopic of ELBOExplanation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "AlternativeFormulationsOfELBO", "to": "MarginalDistributionIndependence", "arrows": "to", "title": "Subtopic of AlternativeFormulationsOfELBO", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MarginalDistributionIndependence", "to": "ConditionalLikelihoodSimplification", "arrows": "to", "title": "Subtopic of MarginalDistributionIndependence", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningConcepts", "to": "EMAlgorithmOverview", "arrows": "to", "title": "Subtopic of MachineLearningConcepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "EMAlgorithmOverview", "to": "MixtureOfGaussiansExample", "arrows": "to", "title": "Subtopic of EMAlgorithmOverview", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MStep", "to": "PhiParameterUpdate", "arrows": "to", "title": "Subtopic of MStep", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MStep", "to": "MuParameterUpdate", "arrows": "to", "title": "Subtopic of MStep", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MStep", "to": "SigmaParameterUpdate", "arrows": "to", "title": "Subtopic of MStep", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "ExpectationMaximizationAlgorithm", "to": "MStepUpdateRule", "arrows": "to", "title": "Subtopic of ExpectationMaximizationAlgorithm", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "PhiParameterUpdate", "to": "LagrangianConstruction", "arrows": "to", "title": "Subtopic of PhiParameterUpdate", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "VariationalInference", "to": "VariationalAutoEncoder", "arrows": "to", "title": "Subtopic of VariationalInference", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning", "to": "Variational_Autoencoder", "arrows": "to", "title": "Subtopic of Machine_Learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning", "to": "EM_Algorithms", "arrows": "to", "title": "Subtopic of Machine_Learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning", "to": "Variational_Inference", "arrows": "to", "title": "Subtopic of Machine_Learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Variational_Autoencoder", "to": "Reparametrization_Trick", "arrows": "to", "title": "Subtopic of Variational_Autoencoder", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning", "to": "Posterior_Distribution", "arrows": "to", "title": "Subtopic of Machine_Learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Variational Inference", "to": "ELBO Lower Bound", "arrows": "to", "title": "Subtopic of Variational Inference", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Variational Inference", "to": "Mean Field Assumption", "arrows": "to", "title": "Subtopic of Variational Inference", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Variational Inference", "to": "Continuous Latent Variables", "arrows": "to", "title": "Subtopic of Variational Inference", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning_Concepts", "to": "Latent_Variables", "arrows": "to", "title": "Subtopic of Machine_Learning_Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Latent_Variables", "to": "Gaussian_Distribution_Qi", "arrows": "to", "title": "Subtopic of Latent_Variables", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Gaussian_Distribution_Qi", "to": "Mean_and_Variance_Functions", "arrows": "to", "title": "Subtopic of Gaussian_Distribution_Qi", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Variational_Autoencoder", "to": "Encoder_Decoder_Networks", "arrows": "to", "title": "Subtopic of Variational_Autoencoder", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning_Concepts", "to": "ELBO_Optimization", "arrows": "to", "title": "Subtopic of Machine_Learning_Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningOverview", "to": "ELBOOptimization", "arrows": "to", "title": "Subtopic of MachineLearningOverview", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "ELBOOptimization", "to": "QFormRequirements", "arrows": "to", "title": "Subtopic of ELBOOptimization", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "ELBOOptimization", "to": "EfficientEvaluationOfELBO", "arrows": "to", "title": "Subtopic of ELBOOptimization", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "EfficientEvaluationOfELBO", "to": "GaussianDistributionQ_i", "arrows": "to", "title": "Subtopic of EfficientEvaluationOfELBO", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "ELBOOptimization", "to": "GradientAscentOptimization", "arrows": "to", "title": "Subtopic of ELBOOptimization", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning_Concepts", "to": "ELBO_Gradient_Computation", "arrows": "to", "title": "Subtopic of Machine_Learning_Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "ELBO_Gradient_Computation", "to": "Gradient_Simple_Case", "arrows": "to", "title": "Subtopic of ELBO_Gradient_Computation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "ELBO_Gradient_Computation", "to": "Complexity_in_Computing_Gradients", "arrows": "to", "title": "Subtopic of ELBO_Gradient_Computation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Complexity_in_Computing_Gradients", "to": "Reparameterization_Trick", "arrows": "to", "title": "Subtopic of Complexity_in_Computing_Gradients", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Reparameterization_Trick", "to": "Mathematical_Formulation_Reparametrization", "arrows": "to", "title": "Subtopic of Reparameterization_Trick", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning_Concepts", "to": "Gradient_Estimation", "arrows": "to", "title": "Subtopic of Machine_Learning_Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "PCA_Method", "to": "Dataset_Analysis", "arrows": "to", "title": "Subtopic of PCA_Method", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "RedundancyDetection", "to": "CarAttributesExample", "arrows": "to", "title": "Subtopic of RedundancyDetection", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "RedundancyDetection", "to": "PilotSurveyExample", "arrows": "to", "title": "Subtopic of RedundancyDetection", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "PCAAlgorithm", "to": "DataNormalization", "arrows": "to", "title": "Subtopic of PCAAlgorithm", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Normalization Techniques", "to": "Mean Normalization", "arrows": "to", "title": "Subtopic of Normalization Techniques", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Normalization Techniques", "to": "Variance Scaling", "arrows": "to", "title": "Subtopic of Normalization Techniques", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Normalization Techniques", "to": "Data Rescaling", "arrows": "to", "title": "Subtopic of Normalization Techniques", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Major Axis of Variation", "to": "Projection Direction", "arrows": "to", "title": "Subtopic of Major Axis of Variation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "PrincipalComponentAnalysis", "to": "ProjectionOntoDirectionU", "arrows": "to", "title": "Subtopic of PrincipalComponentAnalysis", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "ProjectionOntoDirectionU", "to": "VarianceMaximization", "arrows": "to", "title": "Subtopic of ProjectionOntoDirectionU", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "PrincipalComponentAnalysis", "to": "EmpiricalCovarianceMatrix", "arrows": "to", "title": "Subtopic of PrincipalComponentAnalysis", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "VarianceMaximization", "to": "LagrangeMultipliersMethod", "arrows": "to", "title": "Subtopic of VarianceMaximization", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "VarianceMaximization", "to": "PrincipalEigenvector", "arrows": "to", "title": "Subtopic of VarianceMaximization", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "PrincipalComponentAnalysis", "to": "kDimensionalSubspace", "arrows": "to", "title": "Subtopic of PrincipalComponentAnalysis", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Dimensionality Reduction Techniques", "to": "Principal Component Analysis (PCA)", "arrows": "to", "title": "Subtopic of Dimensionality Reduction Techniques", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Principal Component Analysis (PCA)", "to": "Dimensionality Reduction", "arrows": "to", "title": "Subtopic of Principal Component Analysis (PCA)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Principal Component Analysis (PCA)", "to": "Eigenvectors and Eigenvalues", "arrows": "to", "title": "Subtopic of Principal Component Analysis (PCA)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Principal Component Analysis (PCA)", "to": "Orthogonal Basis", "arrows": "to", "title": "Subtopic of Principal Component Analysis (PCA)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Principal Component Analysis (PCA)", "to": "Approximation Error Minimization", "arrows": "to", "title": "Subtopic of Principal Component Analysis (PCA)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Principal Component Analysis (PCA)", "to": "Data Visualization", "arrows": "to", "title": "Subtopic of Principal Component Analysis (PCA)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Principal Component Analysis (PCA)", "to": "Compression", "arrows": "to", "title": "Subtopic of Principal Component Analysis (PCA)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Principal Component Analysis (PCA)", "to": "Plotting Similarity", "arrows": "to", "title": "Subtopic of Principal Component Analysis (PCA)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Principal Component Analysis (PCA)", "to": "Dimension Reduction Before Supervised Learning", "arrows": "to", "title": "Subtopic of Principal Component Analysis (PCA)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Principal Component Analysis (PCA)", "to": "Noise Reduction", "arrows": "to", "title": "Subtopic of Principal Component Analysis (PCA)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Principal Component Analysis (PCA)", "to": "Eigenfaces Method", "arrows": "to", "title": "Subtopic of Principal Component Analysis (PCA)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning_Topics", "to": "ICA", "arrows": "to", "title": "Subtopic of Machine_Learning_Topics", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "ICA", "to": "Cocktail_Party_Problem", "arrows": "to", "title": "Subtopic of ICA", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "ICA", "to": "Mixing_Matrix_A", "arrows": "to", "title": "Subtopic of ICA", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "ICA", "to": "Unmixing_Matrix_W", "arrows": "to", "title": "Subtopic of ICA", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "ICA", "to": "ICA_Ambiguities", "arrows": "to", "title": "Subtopic of ICA", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "ICA Ambiguities", "to": "Permutation Matrix", "arrows": "to", "title": "Subtopic of ICA Ambiguities", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "ICA Ambiguities", "to": "Scaling Ambiguity", "arrows": "to", "title": "Subtopic of ICA Ambiguities", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Scaling Ambiguity", "to": "Volume Adjustment", "arrows": "to", "title": "Subtopic of Scaling Ambiguity", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "ICA_Ambiguities", "to": "Scaling_Impact", "arrows": "to", "title": "Subtopic of ICA_Ambiguities", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "ICA_Ambiguities", "to": "Sign_Ignorance", "arrows": "to", "title": "Subtopic of ICA_Ambiguities", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "ICA_Ambiguities", "to": "Non_Gaussian_Sources", "arrows": "to", "title": "Subtopic of ICA_Ambiguities", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "ICA_Ambiguities", "to": "Gaussian_Data_Issue", "arrows": "to", "title": "Subtopic of ICA_Ambiguities", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Gaussian_Data_Issue", "to": "Mixing_Matrix_Rotation", "arrows": "to", "title": "Subtopic of Gaussian_Data_Issue", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningConcepts", "to": "ICAOnGaussianData", "arrows": "to", "title": "Subtopic of MachineLearningConcepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "ICAOnGaussianData", "to": "RotationalSymmetry", "arrows": "to", "title": "Subtopic of ICAOnGaussianData", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningConcepts", "to": "NonGaussianDataRecovery", "arrows": "to", "title": "Subtopic of MachineLearningConcepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningConcepts", "to": "LinearTransformationsEffect", "arrows": "to", "title": "Subtopic of MachineLearningConcepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Density Transformation", "to": "1D Example", "arrows": "to", "title": "Subtopic of Density Transformation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Density Transformation", "to": "General Case", "arrows": "to", "title": "Subtopic of Density Transformation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "General Case", "to": "Volume Calculation", "arrows": "to", "title": "Subtopic of General Case", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "ICA Algorithm", "to": "Bell and Sejnowski's Method", "arrows": "to", "title": "Subtopic of ICA Algorithm", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningOverview", "to": "ICAConcepts", "arrows": "to", "title": "Subtopic of MachineLearningOverview", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "ICAConcepts", "to": "JointDistributionModeling", "arrows": "to", "title": "Subtopic of ICAConcepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MaximumLikelihoodEstimation", "to": "CumulativeDistributionFunction", "arrows": "to", "title": "Subtopic of MaximumLikelihoodEstimation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningOverview", "to": "DataPreprocessing", "arrows": "to", "title": "Subtopic of MachineLearningOverview", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningOverview", "to": "LogisticFunctionDerivative", "arrows": "to", "title": "Subtopic of MachineLearningOverview", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningOverview", "to": "ModelParameters", "arrows": "to", "title": "Subtopic of MachineLearningOverview", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningOverview", "to": "LogLikelihoodCalculation", "arrows": "to", "title": "Subtopic of MachineLearningOverview", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningOverview", "to": "ConvergenceAndSourceRecovery", "arrows": "to", "title": "Subtopic of MachineLearningOverview", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningOverview", "to": "IndependenceAssumption", "arrows": "to", "title": "Subtopic of MachineLearningOverview", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "Stochastic Gradient Ascent", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Self-supervised Learning", "to": "Foundation Models", "arrows": "to", "title": "Subtopic of Self-supervised Learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Foundation Models", "to": "Pretraining and Adaptation", "arrows": "to", "title": "Subtopic of Foundation Models", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning", "to": "Transfer_Learning", "arrows": "to", "title": "Subtopic of Machine_Learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Transfer_Learning", "to": "Pretraining_Phase", "arrows": "to", "title": "Subtopic of Transfer_Learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Transfer_Learning", "to": "Adaptation_Phase", "arrows": "to", "title": "Subtopic of Transfer_Learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Pretraining_Phase", "to": "Unlabeled_Dataset", "arrows": "to", "title": "Subtopic of Pretraining_Phase", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Adaptation_Phase", "to": "Labeled_Task_Dataset", "arrows": "to", "title": "Subtopic of Adaptation_Phase", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Pretraining_Phase", "to": "Model_Parameter_Theta", "arrows": "to", "title": "Subtopic of Pretraining_Phase", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Pretraining_Phase", "to": "Embedding_Features", "arrows": "to", "title": "Subtopic of Pretraining_Phase", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Pretraining_Phase", "to": "Self_Supervised_Loss", "arrows": "to", "title": "Subtopic of Pretraining_Phase", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning_Adaptation", "to": "Downstream_Task_Dataset", "arrows": "to", "title": "Subtopic of Machine_Learning_Adaptation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Downstream_Task_Dataset", "to": "Zero_Shot_Learning", "arrows": "to", "title": "Subtopic of Downstream_Task_Dataset", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Downstream_Task_Dataset", "to": "Few_Shot_Learning", "arrows": "to", "title": "Subtopic of Downstream_Task_Dataset", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning_Adaptation", "to": "Adaptation_Algorithm", "arrows": "to", "title": "Subtopic of Machine_Learning_Adaptation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Adaptation_Algorithm", "to": "Linear_Probe_Method", "arrows": "to", "title": "Subtopic of Adaptation_Algorithm", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Adaptation_Algorithm", "to": "Finetuning_Method", "arrows": "to", "title": "Subtopic of Adaptation_Algorithm", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning_Adaptation", "to": "Language_Problems_Adaptation", "arrows": "to", "title": "Subtopic of Machine_Learning_Adaptation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning_Adaptation_Methods", "to": "Finetuning_Pretrained_Models", "arrows": "to", "title": "Subtopic of Machine_Learning_Adaptation_Methods", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Finetuning_Pretrained_Models", "to": "Linear_Head_Initiation", "arrows": "to", "title": "Subtopic of Finetuning_Pretrained_Models", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Finetuning_Pretrained_Models", "to": "Optimization_Objective", "arrows": "to", "title": "Subtopic of Finetuning_Pretrained_Models", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Pretraining_Methods_Computer_Vision", "to": "Supervised_Pretraining", "arrows": "to", "title": "Subtopic of Pretraining_Methods_Computer_Vision", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Pretraining_Methods_Computer_Vision", "to": "Contrastive_Learning", "arrows": "to", "title": "Subtopic of Pretraining_Methods_Computer_Vision", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Self-Supervised Learning", "to": "Representation Function", "arrows": "to", "title": "Subtopic of Self-Supervised Learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Representation Function", "to": "Positive Pair", "arrows": "to", "title": "Subtopic of Representation Function", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Representation Function", "to": "Negative Pair", "arrows": "to", "title": "Subtopic of Representation Function", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Self-Supervised Learning", "to": "Data Augmentation", "arrows": "to", "title": "Subtopic of Self-Supervised Learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Self-Supervised Learning", "to": "Supervised Contrastive Algorithms", "arrows": "to", "title": "Subtopic of Self-Supervised Learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearning", "to": "ContrastiveLearning", "arrows": "to", "title": "Subtopic of MachineLearning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "ContrastiveLearning", "to": "SIMCLRAlgorithm", "arrows": "to", "title": "Subtopic of ContrastiveLearning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "SIMCLRAlgorithm", "to": "AugmentationTechniques", "arrows": "to", "title": "Subtopic of SIMCLRAlgorithm", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "ContrastiveLearning", "to": "PositivePairs", "arrows": "to", "title": "Subtopic of ContrastiveLearning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "ContrastiveLearning", "to": "NegativePairs", "arrows": "to", "title": "Subtopic of ContrastiveLearning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Pretrained_Models", "to": "Natural_Language_Processing", "arrows": "to", "title": "Subtopic of Pretrained_Models", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Natural_Language_Processing", "to": "Language_Models", "arrows": "to", "title": "Subtopic of Natural_Language_Processing", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningModels", "to": "ConditionalProbabilityModeling", "arrows": "to", "title": "Subtopic of MachineLearningModels", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "ConditionalProbabilityModeling", "to": "ParameterizedFunction", "arrows": "to", "title": "Subtopic of ConditionalProbabilityModeling", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "ParameterizedFunction", "to": "Embeddings", "arrows": "to", "title": "Subtopic of ParameterizedFunction", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningModels", "to": "TransformerModel", "arrows": "to", "title": "Subtopic of MachineLearningModels", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "TransformerModel", "to": "InputOutputInterface", "arrows": "to", "title": "Subtopic of TransformerModel", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Transformer_Models", "to": "Conditional_Probability", "arrows": "to", "title": "Subtopic of Transformer_Models", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Transformer_Models", "to": "Training_Transformer", "arrows": "to", "title": "Subtopic of Transformer_Models", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Transformer_Models", "to": "Autoregressive_Decoding", "arrows": "to", "title": "Subtopic of Transformer_Models", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningConcepts", "to": "LanguageModels", "arrows": "to", "title": "Subtopic of MachineLearningConcepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "ConditionalProbability", "to": "TemperatureParameter", "arrows": "to", "title": "Subtopic of ConditionalProbability", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "LanguageModels", "to": "TextGeneration", "arrows": "to", "title": "Subtopic of LanguageModels", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "ConditionalProbability", "to": "AdaptiveSampling", "arrows": "to", "title": "Subtopic of ConditionalProbability", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningConcepts", "to": "ModelAdaptation", "arrows": "to", "title": "Subtopic of MachineLearningConcepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "ModelAdaptation", "to": "Finetuning", "arrows": "to", "title": "Subtopic of ModelAdaptation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "ModelAdaptation", "to": "ZeroShotLearning", "arrows": "to", "title": "Subtopic of ModelAdaptation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "ModelAdaptation", "to": "InContextLearning", "arrows": "to", "title": "Subtopic of ModelAdaptation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning_Adaptation_Techniques", "to": "Zero-Shot_Adaptation", "arrows": "to", "title": "Subtopic of Machine_Learning_Adaptation_Techniques", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning_Adaptation_Techniques", "to": "In-Context_Learning", "arrows": "to", "title": "Subtopic of Machine_Learning_Adaptation_Techniques", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Zero-Shot_Adaptation", "to": "Task_Formulation", "arrows": "to", "title": "Subtopic of Zero-Shot_Adaptation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "In-Context_Learning", "to": "Prompting_Strategy", "arrows": "to", "title": "Subtopic of In-Context_Learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning_Adaptation_Techniques", "to": "Model_Optimization", "arrows": "to", "title": "Subtopic of Machine_Learning_Adaptation_Techniques", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning", "to": "Reinforcement Learning", "arrows": "to", "title": "Subtopic of Machine Learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Reinforcement Learning", "to": "Sequential Decision Making", "arrows": "to", "title": "Subtopic of Reinforcement Learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Reinforcement Learning", "to": "Markov Decision Processes (MDP)", "arrows": "to", "title": "Subtopic of Reinforcement Learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Markov Decision Processes (MDP)", "to": "States", "arrows": "to", "title": "Subtopic of Markov Decision Processes (MDP)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Markov Decision Processes (MDP)", "to": "Actions", "arrows": "to", "title": "Subtopic of Markov Decision Processes (MDP)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Markov Decision Processes (MDP)", "to": "State Transition Probabilities", "arrows": "to", "title": "Subtopic of Markov Decision Processes (MDP)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Markov Decision Processes (MDP)", "to": "Discount Factor", "arrows": "to", "title": "Subtopic of Markov Decision Processes (MDP)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Markov Decision Processes (MDP)", "to": "Reward Function", "arrows": "to", "title": "Subtopic of Markov Decision Processes (MDP)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Markov Decision Process (MDP)", "to": "State Transition", "arrows": "to", "title": "Subtopic of Markov Decision Process (MDP)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Markov Decision Process (MDP)", "to": "Action Selection", "arrows": "to", "title": "Subtopic of Markov Decision Process (MDP)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Markov Decision Process (MDP)", "to": "Total Payoff", "arrows": "to", "title": "Subtopic of Markov Decision Process (MDP)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Total Payoff", "to": "Discount Factor (\u03b3)", "arrows": "to", "title": "Subtopic of Total Payoff", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Policy", "to": "Value Function", "arrows": "to", "title": "Subtopic of Policy", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Value Function", "to": "Bellman Equations", "arrows": "to", "title": "Subtopic of Value Function", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Bellman Equations", "to": "Immediate Reward", "arrows": "to", "title": "Subtopic of Bellman Equations", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Bellman Equations", "to": "Future Discounted Rewards", "arrows": "to", "title": "Subtopic of Bellman Equations", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Value Function", "to": "Policy Evaluation", "arrows": "to", "title": "Subtopic of Value Function", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Value Function", "to": "Optimal Value Function", "arrows": "to", "title": "Subtopic of Value Function", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Value Function", "to": "Bellman's Equation", "arrows": "to", "title": "Subtopic of Value Function", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Value Function", "to": "Optimal Policy", "arrows": "to", "title": "Subtopic of Value Function", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning", "to": "Optimal_Policy", "arrows": "to", "title": "Subtopic of Machine_Learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning", "to": "MDP_Finite_State_Space", "arrows": "to", "title": "Subtopic of Machine_Learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MDP_Finite_State_Space", "to": "Value_Iteration", "arrows": "to", "title": "Subtopic of MDP_Finite_State_Space", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Value_Iteration", "to": "Synchronous_Update", "arrows": "to", "title": "Subtopic of Value_Iteration", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Value_Iteration", "to": "Asynchronous_Update", "arrows": "to", "title": "Subtopic of Value_Iteration", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningConcepts", "to": "ValueIterationAlgorithm", "arrows": "to", "title": "Subtopic of MachineLearningConcepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningConcepts", "to": "PolicyIterationAlgorithm", "arrows": "to", "title": "Subtopic of MachineLearningConcepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningConcepts", "to": "ConvergenceOfAlgorithms", "arrows": "to", "title": "Subtopic of MachineLearningConcepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningConcepts", "to": "ComparisonValuePolicyIteration", "arrows": "to", "title": "Subtopic of MachineLearningConcepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningConcepts", "to": "BellmanEquations", "arrows": "to", "title": "Subtopic of MachineLearningConcepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning_Algorithms", "to": "Policy_Iteration", "arrows": "to", "title": "Subtopic of Machine_Learning_Algorithms", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Learning_Model_for_MDP", "to": "Inverted_Pendulum_Problem", "arrows": "to", "title": "Subtopic of Learning_Model_for_MDP", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Learning_Model_for_MDP", "to": "State_Transition_Probabilities", "arrows": "to", "title": "Subtopic of Learning_Model_for_MDP", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MDP_Model_Learning", "to": "Expected_Immediate_Rewards", "arrows": "to", "title": "Subtopic of MDP_Model_Learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MDP_Model_Learning", "to": "Optimization_Techniques", "arrows": "to", "title": "Subtopic of MDP_Model_Learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning_Concepts", "to": "Continuous_State_MDPs", "arrows": "to", "title": "Subtopic of Machine_Learning_Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Continuous_State_MDPs", "to": "Discretization_Method", "arrows": "to", "title": "Subtopic of Continuous_State_MDPs", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "Discretization in MDPs", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Discretization in MDPs", "to": "Value Iteration", "arrows": "to", "title": "Subtopic of Discretization in MDPs", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Discretization in MDPs", "to": "Policy Iteration", "arrows": "to", "title": "Subtopic of Discretization in MDPs", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Discretization in MDPs", "to": "Piecewise Constant Representation", "arrows": "to", "title": "Subtopic of Discretization in MDPs", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "Curse of Dimensionality", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningConcepts", "to": "StateRepresentation", "arrows": "to", "title": "Subtopic of MachineLearningConcepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "StateRepresentation", "to": "GridCellRepresentation", "arrows": "to", "title": "Subtopic of StateRepresentation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "StateRepresentation", "to": "CurseOfDimensionality", "arrows": "to", "title": "Subtopic of StateRepresentation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningOverview", "to": "ValueFunctionApproximation", "arrows": "to", "title": "Subtopic of MachineLearningOverview", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "ValueFunctionApproximation", "to": "ModelOrSimulator", "arrows": "to", "title": "Subtopic of ValueFunctionApproximation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Model Creation Methods", "to": "Physics Simulation", "arrows": "to", "title": "Subtopic of Model Creation Methods", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Physics Simulation", "to": "Off-the-Shelf Software", "arrows": "to", "title": "Subtopic of Physics Simulation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Model Creation Methods", "to": "Learning from Data", "arrows": "to", "title": "Subtopic of Model Creation Methods", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Learning from Data", "to": "Data Collection Process", "arrows": "to", "title": "Subtopic of Learning from Data", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningModels", "to": "LinearModelPrediction", "arrows": "to", "title": "Subtopic of MachineLearningModels", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningModels", "to": "DeterministicModel", "arrows": "to", "title": "Subtopic of MachineLearningModels", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningModels", "to": "StochasticModel", "arrows": "to", "title": "Subtopic of MachineLearningModels", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningModels", "to": "NonLinearFeatureMapping", "arrows": "to", "title": "Subtopic of MachineLearningModels", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningModels", "to": "LossFunctions", "arrows": "to", "title": "Subtopic of MachineLearningModels", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Models", "to": "Non-linear Feature Mappings", "arrows": "to", "title": "Subtopic of Machine Learning Models", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Models", "to": "MDP Simulators", "arrows": "to", "title": "Subtopic of Machine Learning Models", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Fitted Value Iteration", "to": "Continuous State Space", "arrows": "to", "title": "Subtopic of Fitted Value Iteration", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Fitted Value Iteration", "to": "Discrete Action Space", "arrows": "to", "title": "Subtopic of Fitted Value Iteration", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Fitted Value Iteration", "to": "Value Function Approximation", "arrows": "to", "title": "Subtopic of Fitted Value Iteration", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "ValueFunctionApproximation", "to": "FittedValueIteration", "arrows": "to", "title": "Subtopic of ValueFunctionApproximation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningOverview", "to": "SupervisedLearning", "arrows": "to", "title": "Subtopic of MachineLearningOverview", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "FittedValueIteration", "to": "StateSampling", "arrows": "to", "title": "Subtopic of FittedValueIteration", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "FittedValueIteration", "to": "ActionEvaluation", "arrows": "to", "title": "Subtopic of FittedValueIteration", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning", "to": "Supervised_Learning", "arrows": "to", "title": "Subtopic of Machine_Learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Supervised_Learning", "to": "Linear_Regression", "arrows": "to", "title": "Subtopic of Supervised_Learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning", "to": "Fitted_Value_Iteration", "arrows": "to", "title": "Subtopic of Machine_Learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Fitted_Value_Iteration", "to": "Deterministic_Simulator", "arrows": "to", "title": "Subtopic of Fitted_Value_Iteration", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning", "to": "Policy_Definition", "arrows": "to", "title": "Subtopic of Machine_Learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Value_Iteration", "to": "Expectation_Computation", "arrows": "to", "title": "Subtopic of Value_Iteration", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Expectation_Computation", "to": "Gaussian_Noise_Model", "arrows": "to", "title": "Subtopic of Expectation_Computation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Value_Iteration", "to": "Approximation_Methods", "arrows": "to", "title": "Subtopic of Value_Iteration", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Policy_Iteration", "to": "Linear_System_Solver", "arrows": "to", "title": "Subtopic of Policy_Iteration", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Policy_Iteration", "to": "Bellman_Updates", "arrows": "to", "title": "Subtopic of Policy_Iteration", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Algorithms", "to": "Algorithm 6", "arrows": "to", "title": "Subtopic of Machine Learning Algorithms", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Algorithm 6", "to": "Procedure VE", "arrows": "to", "title": "Subtopic of Algorithm 6", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Procedure VE", "to": "Option 1 and Option 2", "arrows": "to", "title": "Subtopic of Procedure VE", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Chapter_15_Summary", "to": "k_steps_update_frequency", "arrows": "to", "title": "Subtopic of Chapter_15_Summary", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Chapter_15_Summary", "to": "Policy_Iteration_Speedup", "arrows": "to", "title": "Subtopic of Chapter_15_Summary", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Chapter_15_Summary", "to": "Value_Iteration_Preference", "arrows": "to", "title": "Subtopic of Chapter_15_Summary", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Chapter_16_LQR_DDP_LQG", "to": "Finite_Horizon_MDPs", "arrows": "to", "title": "Subtopic of Chapter_16_LQR_DDP_LQG", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Finite_Horizon_MDPs", "to": "Optimal_Bellman_Equation", "arrows": "to", "title": "Subtopic of Finite_Horizon_MDPs", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Finite_Horizon_MDPs", "to": "General_Setting_Equations", "arrows": "to", "title": "Subtopic of Finite_Horizon_MDPs", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "ExpectationRewriting", "to": "RewardsDependency", "arrows": "to", "title": "Subtopic of ExpectationRewriting", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "RewardsDependency", "to": "OptimalActionComputation", "arrows": "to", "title": "Subtopic of RewardsDependency", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "FiniteHorizonMDP", "to": "TimeHorizonTuple", "arrows": "to", "title": "Subtopic of FiniteHorizonMDP", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "FiniteHorizonMDP", "to": "PayoffDefinition", "arrows": "to", "title": "Subtopic of FiniteHorizonMDP", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "FiniteHorizonMDP", "to": "DiscountFactorImpact", "arrows": "to", "title": "Subtopic of FiniteHorizonMDP", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Non-Stationary Policies", "to": "Finite Horizon MDP Dynamics", "arrows": "to", "title": "Subtopic of Non-Stationary Policies", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Finite Horizon MDP Dynamics", "to": "Time Dependent Transition Probabilities", "arrows": "to", "title": "Subtopic of Finite Horizon MDP Dynamics", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Non-Stationary Policies", "to": "Optimal Policy in Finite Horizon", "arrows": "to", "title": "Subtopic of Non-Stationary Policies", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Finite Horizon MDP Dynamics", "to": "Value Function for Non-Stationary MDPs", "arrows": "to", "title": "Subtopic of Finite Horizon MDP Dynamics", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning", "to": "Reinforcement_Learning", "arrows": "to", "title": "Subtopic of Machine_Learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Reinforcement_Learning", "to": "Value_Functions", "arrows": "to", "title": "Subtopic of Reinforcement_Learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Value_Functions", "to": "Policy_Evaluation", "arrows": "to", "title": "Subtopic of Value_Functions", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Value_Functions", "to": "Bellman_Equation", "arrows": "to", "title": "Subtopic of Value_Functions", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Reinforcement_Learning", "to": "Dynamic_Programming", "arrows": "to", "title": "Subtopic of Reinforcement_Learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Value_Iteration", "to": "Bellman_Equations", "arrows": "to", "title": "Subtopic of Value_Iteration", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Value_Iteration", "to": "Geometric_Convergence", "arrows": "to", "title": "Subtopic of Value_Iteration", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Linear_Quadratic_Regulation_(LQR)", "to": "Continuous_Model_Assumptions", "arrows": "to", "title": "Subtopic of Linear_Quadratic_Regulation_(LQR)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Continuous_Model_Assumptions", "to": "Linear_Transitions", "arrows": "to", "title": "Subtopic of Continuous_Model_Assumptions", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Linear_Transitions", "to": "Gaussian_Noise", "arrows": "to", "title": "Subtopic of Linear_Transitions", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Continuous_Model_Assumptions", "to": "Quadratic_Rewards", "arrows": "to", "title": "Subtopic of Continuous_Model_Assumptions", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningOverview", "to": "LQRModelAssumptions", "arrows": "to", "title": "Subtopic of MachineLearningOverview", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningOverview", "to": "LQRAlgorithmSteps", "arrows": "to", "title": "Subtopic of MachineLearningOverview", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "LQRAlgorithmSteps", "to": "Step1Estimation", "arrows": "to", "title": "Subtopic of LQRAlgorithmSteps", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "LQRAlgorithmSteps", "to": "Step2OptimalPolicy", "arrows": "to", "title": "Subtopic of LQRAlgorithmSteps", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningOverview", "to": "DynamicProgrammingApplication", "arrows": "to", "title": "Subtopic of MachineLearningOverview", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Optimal_Value_Function", "to": "Quadratic_Form", "arrows": "to", "title": "Subtopic of Optimal_Value_Function", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Optimal_Value_Function", "to": "Dynamics_Model", "arrows": "to", "title": "Subtopic of Optimal_Value_Function", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Optimal_Policy", "to": "Linear_Policy_Derivation", "arrows": "to", "title": "Subtopic of Optimal_Policy", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "Optimal Policy in LQR", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Optimal Policy in LQR", "to": "Discrete Ricatti Equations", "arrows": "to", "title": "Subtopic of Optimal Policy in LQR", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Optimal Policy in LQR", "to": "Independence of Noise", "arrows": "to", "title": "Subtopic of Optimal Policy in LQR", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Optimal Policy in LQR", "to": "LQR Algorithm Steps", "arrows": "to", "title": "Subtopic of Optimal Policy in LQR", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "LQR Algorithm Steps", "to": "Efficiency Improvement", "arrows": "to", "title": "Subtopic of LQR Algorithm Steps", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Non-linear Dynamics and LQR", "to": "Inverted Pendulum Example", "arrows": "to", "title": "Subtopic of Non-linear Dynamics and LQR", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Inverted_Pendulum_Model", "to": "State_Transitions", "arrows": "to", "title": "Subtopic of Inverted_Pendulum_Model", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Differential_Dynamic_Programming_(DDP)", "to": "Linearization_of_Dynamics", "arrows": "to", "title": "Subtopic of Differential_Dynamic_Programming_(DDP)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Linearization_of_Dynamics", "to": "Taylor_Expansion_Method", "arrows": "to", "title": "Subtopic of Linearization_of_Dynamics", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Linearization_of_Dynamics", "to": "LQR_Assumptions", "arrows": "to", "title": "Subtopic of Linearization_of_Dynamics", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning", "to": "Optimization_Methods", "arrows": "to", "title": "Subtopic of Machine_Learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Optimization_Methods", "to": "Differential_Dynamic_Programming_(DDP)", "arrows": "to", "title": "Subtopic of Optimization_Methods", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Differential_Dynamic_Programming_(DDP)", "to": "Nominal_Trajectory", "arrows": "to", "title": "Subtopic of Differential_Dynamic_Programming_(DDP)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Linearization_of_Dynamics", "to": "Rewriting_Dynamics", "arrows": "to", "title": "Subtopic of Linearization_of_Dynamics", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Differential_Dynamic_Programming_(DDP)", "to": "Reward_Function_Approximation", "arrows": "to", "title": "Subtopic of Differential_Dynamic_Programming_(DDP)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning_Concepts", "to": "Optimization_Frameworks", "arrows": "to", "title": "Subtopic of Machine_Learning_Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Optimization_Frameworks", "to": "Linear_Quadratic_Regulator_(LQR)", "arrows": "to", "title": "Subtopic of Optimization_Frameworks", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Optimization_Frameworks", "to": "Hessian_Matrix", "arrows": "to", "title": "Subtopic of Optimization_Frameworks", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Linear_Quadratic_Regulator_(LQR)", "to": "LQR_Framework_Applications", "arrows": "to", "title": "Subtopic of Linear_Quadratic_Regulator_(LQR)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Optimization_Frameworks", "to": "Trajectory_Generation", "arrows": "to", "title": "Subtopic of Optimization_Frameworks", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "Observation vs State", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "Partially Observable MDPs (POMDP)", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Partially Observable MDPs (POMDP)", "to": "Belief State", "arrows": "to", "title": "Subtopic of Partially Observable MDPs (POMDP)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "LQR Extension to POMDP", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "LQR Extension to POMDP", "to": "Kalman Filter", "arrows": "to", "title": "Subtopic of LQR Extension to POMDP", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Kalman Filter", "to": "Step 1", "arrows": "to", "title": "Subtopic of Kalman Filter", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Kalman Filter", "to": "Predict Step", "arrows": "to", "title": "Subtopic of Kalman Filter", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Kalman Filter", "to": "Update Step", "arrows": "to", "title": "Subtopic of Kalman Filter", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Kalman Filter", "to": "Computational Efficiency", "arrows": "to", "title": "Subtopic of Kalman Filter", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Kalman Filter", "to": "Belief States Update", "arrows": "to", "title": "Subtopic of Kalman Filter", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Predict Step", "to": "Gaussian Distribution", "arrows": "to", "title": "Subtopic of Predict Step", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Update Step", "to": "Kalman Gain (K_t)", "arrows": "to", "title": "Subtopic of Update Step", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Kalman Filter", "to": "Backward Pass (LQR Updates)", "arrows": "to", "title": "Subtopic of Kalman Filter", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Chapter_17_Policy_Gradient_REINFORCE", "to": "Finite_Horizon_Case", "arrows": "to", "title": "Subtopic of Chapter_17_Policy_Gradient_REINFORCE", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Chapter_17_Policy_Gradient_REINFORCE", "to": "Randomized_Policy", "arrows": "to", "title": "Subtopic of Chapter_17_Policy_Gradient_REINFORCE", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Chapter_17_Policy_Gradient_REINFORCE", "to": "Transition_Probabilities_Sampling", "arrows": "to", "title": "Subtopic of Chapter_17_Policy_Gradient_REINFORCE", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Chapter_17_Policy_Gradient_REINFORCE", "to": "Reward_Function_Querying", "arrows": "to", "title": "Subtopic of Chapter_17_Policy_Gradient_REINFORCE", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Chapter_17_Policy_Gradient_REINFORCE", "to": "Expected_Total_Payoff_Optimization", "arrows": "to", "title": "Subtopic of Chapter_17_Policy_Gradient_REINFORCE", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "Policy Gradient Methods", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Policy Gradient Methods", "to": "Gradient Ascent", "arrows": "to", "title": "Subtopic of Policy Gradient Methods", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Policy Gradient Methods", "to": "Reward Function Estimation", "arrows": "to", "title": "Subtopic of Policy Gradient Methods", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "Variational Auto-Encoder (VAE)", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Variational Auto-Encoder (VAE)", "to": "Re-parametrization Technique", "arrows": "to", "title": "Subtopic of Variational Auto-Encoder (VAE)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Policy Gradient Methods", "to": "REINFORCE Algorithm", "arrows": "to", "title": "Subtopic of Policy Gradient Methods", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "GradientEstimation", "to": "PolicyGradients", "arrows": "to", "title": "Subtopic of GradientEstimation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "PolicyGradients", "to": "SampleBasedEstimator", "arrows": "to", "title": "Subtopic of PolicyGradients", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "PolicyGradients", "to": "LogProbabilityDerivative", "arrows": "to", "title": "Subtopic of PolicyGradients", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "LogProbabilityDerivative", "to": "AnalyticalFormula", "arrows": "to", "title": "Subtopic of LogProbabilityDerivative", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "LogProbabilityDerivative", "to": "AutoDifferentiation", "arrows": "to", "title": "Subtopic of LogProbabilityDerivative", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Policy_Gradient_Theorem", "to": "Log_Probability_Ratio", "arrows": "to", "title": "Subtopic of Policy_Gradient_Theorem", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Policy_Gradient_Theorem", "to": "Vanilla_REINFORCE_Algorithm", "arrows": "to", "title": "Subtopic of Policy_Gradient_Theorem", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Gradient_Estimation", "to": "Empirical_Sample_Trajectories", "arrows": "to", "title": "Subtopic of Gradient_Estimation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Log_Probability_Ratio", "to": "Trajectory_Probability_Change", "arrows": "to", "title": "Subtopic of Log_Probability_Ratio", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MachineLearningOverview", "to": "PolicyGradientMethods", "arrows": "to", "title": "Subtopic of MachineLearningOverview", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "PolicyGradientMethods", "to": "TrajectoryProbability", "arrows": "to", "title": "Subtopic of PolicyGradientMethods", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "PolicyGradientMethods", "to": "RewardFunction", "arrows": "to", "title": "Subtopic of PolicyGradientMethods", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "PolicyGradientMethods", "to": "ExpectationCalculation", "arrows": "to", "title": "Subtopic of PolicyGradientMethods", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "PolicyGradientMethods", "to": "SimplificationOfFormulas", "arrows": "to", "title": "Subtopic of PolicyGradientMethods", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Reinforcement_Learning", "to": "Policy_Gradient_Methods", "arrows": "to", "title": "Subtopic of Reinforcement_Learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Policy_Gradient_Methods", "to": "Law_of_Total_Expectation", "arrows": "to", "title": "Subtopic of Policy_Gradient_Methods", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Policy_Gradient_Methods", "to": "Value_Function_Estimation", "arrows": "to", "title": "Subtopic of Policy_Gradient_Methods", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Policy Gradient Methods", "to": "Gradient Estimation", "arrows": "to", "title": "Subtopic of Policy Gradient Methods", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Gradient Estimation", "to": "Baseline Function", "arrows": "to", "title": "Subtopic of Gradient Estimation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Gradient Estimation", "to": "Algorithm 7", "arrows": "to", "title": "Subtopic of Gradient Estimation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning_Papers", "to": "Double_Descent_Weak_Features", "arrows": "to", "title": "Subtopic of Machine_Learning_Papers", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning_Papers", "to": "Variational_Inference_Review", "arrows": "to", "title": "Subtopic of Machine_Learning_Papers", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning_Papers", "to": "Foundation_Models", "arrows": "to", "title": "Subtopic of Machine_Learning_Papers", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning_Papers", "to": "Contrastive_Learning_Visual_Representations", "arrows": "to", "title": "Subtopic of Machine_Learning_Papers", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning_Papers", "to": "BERT_Model", "arrows": "to", "title": "Subtopic of Machine_Learning_Papers", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning_Papers", "to": "Implicit_Bias_Noise_Covariance", "arrows": "to", "title": "Subtopic of Machine_Learning_Papers", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning_Papers", "to": "High_Dimensional_Statistics", "arrows": "to", "title": "Subtopic of Machine_Learning_Papers", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Papers", "to": "Implicit Bias in Machine Learning", "arrows": "to", "title": "Subtopic of Machine Learning Papers", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Papers", "to": "High-Dimensional Interpolation", "arrows": "to", "title": "Subtopic of Machine Learning Papers", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Papers", "to": "Deep Residual Learning", "arrows": "to", "title": "Subtopic of Machine Learning Papers", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Papers", "to": "Statistical Learning Textbook", "arrows": "to", "title": "Subtopic of Machine Learning Papers", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Papers", "to": "Optimization Methods", "arrows": "to", "title": "Subtopic of Machine Learning Papers", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Papers", "to": "Variational Autoencoders", "arrows": "to", "title": "Subtopic of Machine Learning Papers", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Papers", "to": "Model-Based Reinforcement Learning", "arrows": "to", "title": "Subtopic of Machine Learning Papers", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Papers", "to": "Generalization Error Analysis", "arrows": "to", "title": "Subtopic of Machine Learning Papers", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Papers", "to": "Learning Theory Fundamentals", "arrows": "to", "title": "Subtopic of Machine Learning Papers", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "double_descent", "to": "statistical_mechanics_of_learning", "arrows": "to", "title": "Subtopic of double_descent", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "statistical_mechanics_of_learning", "to": "generalization", "arrows": "to", "title": "Subtopic of statistical_mechanics_of_learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "double_descent", "to": "learning_to_generalize", "arrows": "to", "title": "Subtopic of double_descent", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}];
        const otherEdgesData = [{"id": "other_Regularization Parameter \u03bb_Regularized Loss Function_subtopic", "from": "Regularization Parameter \u03bb", "to": "Regularized Loss Function", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine_Learning_Algorithms_Value_Iteration_contains", "from": "Machine_Learning_Algorithms", "to": "Value_Iteration", "arrows": "to", "title": "contains", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Logistic_Regression_Gradient_Descent_Machine_Learning_Concepts_subtopic", "from": "Logistic_Regression_Gradient_Descent", "to": "Machine_Learning_Concepts", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_SchoolQuality_DerivedFeatures_subtopic", "from": "SchoolQuality", "to": "DerivedFeatures", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Learning a model for an MDP_Continuous state MDPs_contains", "from": "Learning a model for an MDP", "to": "Continuous state MDPs", "arrows": "to", "title": "contains", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Differentiable Circuit_Gradient Computation_depends_on", "from": "Differentiable Circuit", "to": "Gradient Computation", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_GradientAscentRule_ConvergenceAndSourceRecovery_depends_on", "from": "GradientAscentRule", "to": "ConvergenceAndSourceRecovery", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_ValueIterationAlgorithm_ConvergenceOfAlgorithms_depends_on", "from": "ValueIterationAlgorithm", "to": "ConvergenceOfAlgorithms", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_LogLikelihoodImprovement_ExpectationMaximizationAlgorithm_related_to", "from": "LogLikelihoodImprovement", "to": "ExpectationMaximizationAlgorithm", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_PoissonDistributionModeling_MachineLearningOverview_depends_on", "from": "PoissonDistributionModeling", "to": "MachineLearningOverview", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_IndependenceAssumption_MachineLearningOverview_related_to", "from": "IndependenceAssumption", "to": "MachineLearningOverview", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Deep Learning Introduction_Machine Learning Overview_related_to", "from": "Deep Learning Introduction", "to": "Machine Learning Overview", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_SIMCLRAlgorithm_LossFunction_depends_on", "from": "SIMCLRAlgorithm", "to": "LossFunction", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Probability_Error_Bounds_Training_Error_Generalization_Error_Difference_related_to", "from": "Probability_Error_Bounds", "to": "Training_Error_Generalization_Error_Difference", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_KernelsAsSimilarityMetrics_MachineLearningConcepts_subtopic", "from": "KernelsAsSimilarityMetrics", "to": "MachineLearningConcepts", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_DynamicProgrammingApplication_MachineLearningOverview_related_to", "from": "DynamicProgrammingApplication", "to": "MachineLearningOverview", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_RegularizerFunction_TrainingLoss_part_of", "from": "RegularizerFunction", "to": "TrainingLoss", "arrows": "to", "title": "part_of", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_NewWordDetection_NaiveBayesFilter_subtopic", "from": "NewWordDetection", "to": "NaiveBayesFilter", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Best_Hypothesis_Machine_Learning_Theory_related_to", "from": "Best_Hypothesis", "to": "Machine_Learning_Theory", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_NeurIPS20xxSubmission_MachineLearningConferences_related_to", "from": "NeurIPS20xxSubmission", "to": "MachineLearningConferences", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Bernoulli_Random_Variables_Generalization_Error_depends_on", "from": "Bernoulli_Random_Variables", "to": "Generalization_Error", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Optimizers_Initialization_related_to", "from": "Optimizers", "to": "Initialization", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Discrete Action Space_Fitted Value Iteration_subtopic", "from": "Discrete Action Space", "to": "Fitted Value Iteration", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Parameter Estimation_Likelihood Function_depends_on", "from": "Parameter Estimation", "to": "Likelihood Function", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Generalization_Error_Bound_Sample_Complexity_depends_on", "from": "Generalization_Error_Bound", "to": "Sample_Complexity", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning_Generalization_related_to", "from": "Machine Learning", "to": "Generalization", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Optimization_Techniques_MDP_Model_Learning_subtopic", "from": "Optimization_Techniques", "to": "MDP_Model_Learning", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Self_Supervised_Loss_Pretraining_Phase_depends_on", "from": "Self_Supervised_Loss", "to": "Pretraining_Phase", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Reparametrization_Trick_Variational_Autoencoder_technique_used_in", "from": "Reparametrization_Trick", "to": "Variational_Autoencoder", "arrows": "to", "title": "technique_used_in", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Adaptation_Phase_Transfer_Learning_subtopic", "from": "Adaptation_Phase", "to": "Transfer_Learning", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Policy Evaluation_Value Function_depends_on", "from": "Policy Evaluation", "to": "Value Function", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine_Learning_Optimization_Lagrange_Duality_related_to", "from": "Machine_Learning_Optimization", "to": "Lagrange_Duality", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Sample-wise Double Descent_Optimal Regularization_depends_on", "from": "Sample-wise Double Descent", "to": "Optimal Regularization", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_ActivationFunctions_ReLUFunction_subtopic", "from": "ActivationFunctions", "to": "ReLUFunction", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_NaiveBayesClassifier_MaximumLikelihoodEstimation_depends_on", "from": "NaiveBayesClassifier", "to": "MaximumLikelihoodEstimation", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Variational_Autoencoder_EM_Algorithms_extends", "from": "Variational_Autoencoder", "to": "EM_Algorithms", "arrows": "to", "title": "extends", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Algorithms_Neural Networks_related_to", "from": "Machine Learning Algorithms", "to": "Neural Networks", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_NeurIPS20xxSubmission_NewWordDetection_depends_on", "from": "NeurIPS20xxSubmission", "to": "NewWordDetection", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_MachineLearningModels_LogisticRegression_related_to", "from": "MachineLearningModels", "to": "LogisticRegression", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_JensensInequality_LogLikelihoodBound_related_to", "from": "JensensInequality", "to": "LogLikelihoodBound", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Chain_Rule_Application_Gradient_Computation_depends_on", "from": "Chain_Rule_Application", "to": "Gradient_Computation", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Implicit_Bias_Noise_Covariance_Machine_Learning_Papers_belongs_to", "from": "Implicit_Bias_Noise_Covariance", "to": "Machine_Learning_Papers", "arrows": "to", "title": "belongs_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Sufficient Conditions for Kernels_Feature Mapping_related_to", "from": "Sufficient Conditions for Kernels", "to": "Feature Mapping", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Concepts_Backpropagation_related_to", "from": "Machine Learning Concepts", "to": "Backpropagation", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Step1Estimation_LQRAlgorithmSteps_subtopic", "from": "Step1Estimation", "to": "LQRAlgorithmSteps", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Dual Problem_Relationship Between Primal and Dual_related_to", "from": "Dual Problem", "to": "Relationship Between Primal and Dual", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_LMSUpdateRule_GradientDescentAlgorithm_related_to", "from": "LMSUpdateRule", "to": "GradientDescentAlgorithm", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_TwoLayerNN_SoftplusFunction_depends_on", "from": "TwoLayerNN", "to": "SoftplusFunction", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Dimensionality Reduction Techniques_Independent Components Analysis (ICA)_related_to", "from": "Dimensionality Reduction Techniques", "to": "Independent Components Analysis (ICA)", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_GaussianDiscriminantAnalysis(GDA)_LikelihoodFunction_depends_on", "from": "GaussianDiscriminantAnalysis(GDA)", "to": "LikelihoodFunction", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_RegressionProblems_TrainingDataset_depends_on", "from": "RegressionProblems", "to": "TrainingDataset", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Concepts_Overfitting_related_to", "from": "Machine Learning Concepts", "to": "Overfitting", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Concepts_Gradient Computation_depends_on", "from": "Machine Learning Concepts", "to": "Gradient Computation", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Newton's Method_Gradient Descent_compared_with", "from": "Newton's Method", "to": "Gradient Descent", "arrows": "to", "title": "compared_with", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Jensens_Inequality_Machine_Learning_Concepts_related_to", "from": "Jensens_Inequality", "to": "Machine_Learning_Concepts", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Reward_Function_Querying_Chapter_17_Policy_Gradient_REINFORCE_subtopic", "from": "Reward_Function_Querying", "to": "Chapter_17_Policy_Gradient_REINFORCE", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_MachineLearningConcepts_GeneralizedLinearModels_related_to", "from": "MachineLearningConcepts", "to": "GeneralizedLinearModels", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine_Learning_Neural_Networks_related_to", "from": "Machine_Learning", "to": "Neural_Networks", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Backpropagation_Chain Rule_depends_on", "from": "Backpropagation", "to": "Chain Rule", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_KKT Conditions_Support Vector Machine (SVM)_related_to", "from": "KKT Conditions", "to": "Support Vector Machine (SVM)", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Zero-Shot_Adaptation_Language_Models_depends_on", "from": "Zero-Shot_Adaptation", "to": "Language_Models", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine_Learning_Empirical_Risk_Minimization_depends_on", "from": "Machine_Learning", "to": "Empirical_Risk_Minimization", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Kalman Filter_Forward Pass_includes", "from": "Kalman Filter", "to": "Forward Pass", "arrows": "to", "title": "includes", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Design Matrix_Least Squares Revisited_subtopic", "from": "Design Matrix", "to": "Least Squares Revisited", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_OptimalActionComputation_RewardsDependency_subtopic", "from": "OptimalActionComputation", "to": "RewardsDependency", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Nonlinear Activation Module_MLP Composition_depends_on", "from": "Nonlinear Activation Module", "to": "MLP Composition", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Concepts_Backpropagation Algorithm_depends_on", "from": "Machine Learning Concepts", "to": "Backpropagation Algorithm", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_LogisticRegression_LinearRegression_compared_to", "from": "LogisticRegression", "to": "LinearRegression", "arrows": "to", "title": "compared_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_ParallelismGPUs_VectorizationInNN_related_to", "from": "ParallelismGPUs", "to": "VectorizationInNN", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Support_Vector_Machines_SVM_Linear_Constraints_has_subtopic", "from": "Support_Vector_Machines_SVM", "to": "Linear_Constraints", "arrows": "to", "title": "has_subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_BinaryFeatures_NaiveBayesAlgorithm_subtopic", "from": "BinaryFeatures", "to": "NaiveBayesAlgorithm", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Functional Margins_Machine Learning Concepts_subtopic", "from": "Functional Margins", "to": "Machine Learning Concepts", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_LinearRegression_LocallyWeightedLinearRegression_subtopic", "from": "LinearRegression", "to": "LocallyWeightedLinearRegression", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Binary_Classification_Generalization_Error_subtopic", "from": "Binary_Classification", "to": "Generalization_Error", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Transfer_Learning_Machine_Learning_subtopic", "from": "Transfer_Learning", "to": "Machine_Learning", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_VariationalAutoEncoder_VariationalInference_subtopic", "from": "VariationalAutoEncoder", "to": "VariationalInference", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Gradient_Estimation_Reparameterization_Trick_depends_on", "from": "Gradient_Estimation", "to": "Reparameterization_Trick", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Foundation Models_Machine Learning Overview_subtopic", "from": "Foundation Models", "to": "Machine Learning Overview", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Sample_Size_Calculation_Quantities_of_Interest_subtopic", "from": "Sample_Size_Calculation", "to": "Quantities_of_Interest", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_MachineLearningModels_LogisticRegression_contains", "from": "MachineLearningModels", "to": "LogisticRegression", "arrows": "to", "title": "contains", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_BERT_Model_Machine_Learning_Papers_belongs_to", "from": "BERT_Model", "to": "Machine_Learning_Papers", "arrows": "to", "title": "belongs_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Class Priors_Bayesian Classification_depends_on", "from": "Class Priors", "to": "Bayesian Classification", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Continuous State Space_Fitted Value Iteration_subtopic", "from": "Continuous State Space", "to": "Fitted Value Iteration", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_MachineLearningOverview_Backpropagation_subtopic", "from": "MachineLearningOverview", "to": "Backpropagation", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Variational_Autoencoder_Neural_Networks_uses", "from": "Variational_Autoencoder", "to": "Neural_Networks", "arrows": "to", "title": "uses", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_MachineLearningConcepts_LikelihoodFunction_contains", "from": "MachineLearningConcepts", "to": "LikelihoodFunction", "arrows": "to", "title": "contains", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Bellman's Equation_Optimal Value Function_defines", "from": "Bellman's Equation", "to": "Optimal Value Function", "arrows": "to", "title": "defines", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_MachineLearningConcepts_ActivationFunctions_related_to", "from": "MachineLearningConcepts", "to": "ActivationFunctions", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_GradientDescentAlgorithm_StochasticGradientDescent_subtopic", "from": "GradientDescentAlgorithm", "to": "StochasticGradientDescent", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Evidence_Lower_Bound_(ELBO)_Log_Likelihood_Optimization_depends_on", "from": "Evidence_Lower_Bound_(ELBO)", "to": "Log_Likelihood_Optimization", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_MDP_Finite_State_Space_Machine_Learning_subtopic", "from": "MDP_Finite_State_Space", "to": "Machine_Learning", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_LikelihoodFunction_MaximumLikelihoodEstimation_leads_to", "from": "LikelihoodFunction", "to": "MaximumLikelihoodEstimation", "arrows": "to", "title": "leads_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_MachineLearningOverview_LayerNormalization_contains", "from": "MachineLearningOverview", "to": "LayerNormalization", "arrows": "to", "title": "contains", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Dual_Problem_Formulation_KKT_Conditions_related_to", "from": "Dual_Problem_Formulation", "to": "KKT_Conditions", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Mathematical Decomposition for Regression_Bias-Variance Tradeoff_depends_on", "from": "Mathematical Decomposition for Regression", "to": "Bias-Variance Tradeoff", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Generalization Error Analysis_Machine Learning Papers_subtopic", "from": "Generalization Error Analysis", "to": "Machine Learning Papers", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Future Discounted Rewards_Bellman Equations_related_to", "from": "Future Discounted Rewards", "to": "Bellman Equations", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Bayesian Machine Learning_Training Set_depends_on", "from": "Bayesian Machine Learning", "to": "Training Set", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_SingleTrainingExampleCase_PartialDerivativeCalculation_subtopic", "from": "SingleTrainingExampleCase", "to": "PartialDerivativeCalculation", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_MachineLearningOverview_UnsupervisedLearning_depends_on", "from": "MachineLearningOverview", "to": "UnsupervisedLearning", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Compression_Principal Component Analysis (PCA)_subtopic", "from": "Compression", "to": "Principal Component Analysis (PCA)", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_FamilySize_DerivedFeatures_subtopic", "from": "FamilySize", "to": "DerivedFeatures", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Optimization_Problems_KKT_Conditions_depends_on", "from": "Optimization_Problems", "to": "KKT_Conditions", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Regularizer R(\u03b8)_Regularized Loss Function_related_to", "from": "Regularizer R(\u03b8)", "to": "Regularized Loss Function", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_EvidenceLowerBoundELBO_EMAlgorithm_related_to", "from": "EvidenceLowerBoundELBO", "to": "EMAlgorithm", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Discount Factor (\u03b3)_Total Payoff_related_to", "from": "Discount Factor (\u03b3)", "to": "Total Payoff", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Linear_Policy_Derivation_Optimal_Value_Function_depends_on", "from": "Linear_Policy_Derivation", "to": "Optimal_Value_Function", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_MatrixMultiplicationModule_NeuralNetworksComposition_depends_on", "from": "MatrixMultiplicationModule", "to": "NeuralNetworksComposition", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Geometric Margins_Machine Learning Concepts_subtopic", "from": "Geometric Margins", "to": "Machine Learning Concepts", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Regularization_BiasVarianceTradeoff_related_to", "from": "Regularization", "to": "BiasVarianceTradeoff", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_TrainingSetExamples_MatrixNotation_depends_on", "from": "TrainingSetExamples", "to": "MatrixNotation", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Volume Calculation_General Case_subtopic", "from": "Volume Calculation", "to": "General Case", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_MachineLearningAlgorithms_CrossValidation_related_to", "from": "MachineLearningAlgorithms", "to": "CrossValidation", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Bias-Variance Tradeoff_Double Descent Phenomenon_related_to", "from": "Bias-Variance Tradeoff", "to": "Double Descent Phenomenon", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_BackwardFunctionsBasics_ActivationFunctions_has_subtopic", "from": "BackwardFunctionsBasics", "to": "ActivationFunctions", "arrows": "to", "title": "has_subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Naive Bayes Algorithm_Discretization_subtopic", "from": "Naive Bayes Algorithm", "to": "Discretization", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Mean Vector_Multivariate Normal Distribution_related_to", "from": "Mean Vector", "to": "Multivariate Normal Distribution", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Optimization Problem in SVM_Functional Margin_related_to", "from": "Optimization Problem in SVM", "to": "Functional Margin", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_LogLikelihoodCalculation_GradientAscentRule_subtopic", "from": "LogLikelihoodCalculation", "to": "GradientAscentRule", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_PilotSurveyExample_RedundancyDetection_subtopic", "from": "PilotSurveyExample", "to": "RedundancyDetection", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Value_Function_Estimation_Policy_Gradient_Methods_subtopic", "from": "Value_Function_Estimation", "to": "Policy_Gradient_Methods", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Logistic_Regression_Gradient_Descent_Perceptron_Algorithm_related_to", "from": "Logistic_Regression_Gradient_Descent", "to": "Perceptron_Algorithm", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Concepts_Model Complexity_has_subtopic", "from": "Machine Learning Concepts", "to": "Model Complexity", "arrows": "to", "title": "has_subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine_Learning_Algorithms_SMO_Algorithm_has_subtopic", "from": "Machine_Learning_Algorithms", "to": "SMO_Algorithm", "arrows": "to", "title": "has_subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Embedding_Features_Pretraining_Phase_subtopic", "from": "Embedding_Features", "to": "Pretraining_Phase", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine_Learning_Concepts_Naive_Bayes_Classifier_subtopic", "from": "Machine_Learning_Concepts", "to": "Naive_Bayes_Classifier", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Foundation_Models_Machine_Learning_Papers_belongs_to", "from": "Foundation_Models", "to": "Machine_Learning_Papers", "arrows": "to", "title": "belongs_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_MultiClassClassification_SoftmaxFunction_depends_on", "from": "MultiClassClassification", "to": "SoftmaxFunction", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Continuous Latent Variables_Variational Inference_subtopic", "from": "Continuous Latent Variables", "to": "Variational Inference", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_EStep_ExpectationMaximizationAlgorithm_subtopic", "from": "EStep", "to": "ExpectationMaximizationAlgorithm", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine_Learning_Topic_Text_Classification_has_subtopic", "from": "Machine_Learning_Topic", "to": "Text_Classification", "arrows": "to", "title": "has_subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_High_Dimensional_Statistics_Machine_Learning_Papers_belongs_to", "from": "High_Dimensional_Statistics", "to": "Machine_Learning_Papers", "arrows": "to", "title": "belongs_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Few_Shot_Learning_Machine_Learning_Papers_belongs_to", "from": "Few_Shot_Learning", "to": "Machine_Learning_Papers", "arrows": "to", "title": "belongs_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_HiddenUnits_NeuralNetworks_related_to", "from": "HiddenUnits", "to": "NeuralNetworks", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_OptimizationMethods_NewtonMethod_contains", "from": "OptimizationMethods", "to": "NewtonMethod", "arrows": "to", "title": "contains", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Statistical Learning Textbook_Machine Learning Papers_subtopic", "from": "Statistical Learning Textbook", "to": "Machine Learning Papers", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Multivariate Normal Distribution_Gaussian Discriminant Analysis (GDA)_depends_on", "from": "Multivariate Normal Distribution", "to": "Gaussian Discriminant Analysis (GDA)", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Representation Function_Self-Supervised Learning_subtopic", "from": "Representation Function", "to": "Self-Supervised Learning", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Learning Theory Proofs_Binary Classification_subtopic", "from": "Learning Theory Proofs", "to": "Binary Classification", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Non-linear Feature Mappings_Machine Learning Models_subtopic", "from": "Non-linear Feature Mappings", "to": "Machine Learning Models", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_LanguageModels_ConditionalProbability_depends_on", "from": "LanguageModels", "to": "ConditionalProbability", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_CarAttributesExample_RedundancyDetection_subtopic", "from": "CarAttributesExample", "to": "RedundancyDetection", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Notational Consistency_Multi-layer Neural Networks_subtopic", "from": "Notational Consistency", "to": "Multi-layer Neural Networks", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Value Function Approximation_Fitted Value Iteration_subtopic", "from": "Value Function Approximation", "to": "Fitted Value Iteration", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_KernelFunctions_FeatureMapping_related_to", "from": "KernelFunctions", "to": "FeatureMapping", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_SupervisedLearning_LinearRegression_related_to", "from": "SupervisedLearning", "to": "LinearRegression", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_MachineLearningConcepts_MaximumLikelihoodEstimation_contains", "from": "MachineLearningConcepts", "to": "MaximumLikelihoodEstimation", "arrows": "to", "title": "contains", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Conditional Probability p(x|y)_Bayesian Classification_depends_on", "from": "Conditional Probability p(x|y)", "to": "Bayesian Classification", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Backpropagation for MLPs_Gradient Calculation_depends_on", "from": "Backpropagation for MLPs", "to": "Gradient Calculation", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_MatrixAlgebra_VectorizationInNN_subtopic", "from": "MatrixAlgebra", "to": "VectorizationInNN", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_PropertiesOfKernels_FeatureMapsAndKernels_related_to", "from": "PropertiesOfKernels", "to": "FeatureMapsAndKernels", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_GaussianKernel_KernelsAsSimilarityMetrics_subtopic", "from": "GaussianKernel", "to": "KernelsAsSimilarityMetrics", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Functional Margins_Geometric Margins_related_to", "from": "Functional Margins", "to": "Geometric Margins", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_MachineLearningModels_MultinomialEventModel_has_subtopic", "from": "MachineLearningModels", "to": "MultinomialEventModel", "arrows": "to", "title": "has_subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Sequential Minimal Optimization (SMO) Algorithm_Support Vector Machines (SVM)_subtopic", "from": "Sequential Minimal Optimization (SMO) Algorithm", "to": "Support Vector Machines (SVM)", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Empirical_Risk_Minimization_Machine_Learning_Concepts_subtopic", "from": "Empirical_Risk_Minimization", "to": "Machine_Learning_Concepts", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_LQR Extension to POMDP_Machine Learning Concepts_subtopic", "from": "LQR Extension to POMDP", "to": "Machine Learning Concepts", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Deep Residual Learning_Machine Learning Papers_subtopic", "from": "Deep Residual Learning", "to": "Machine Learning Papers", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Logistic Regression_Classification Problem_related_to", "from": "Logistic Regression", "to": "Classification Problem", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Data Visualization_Principal Component Analysis (PCA)_subtopic", "from": "Data Visualization", "to": "Principal Component Analysis (PCA)", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_PartialDerivativeCalculation_GradientDescentAlgorithm_subtopic", "from": "PartialDerivativeCalculation", "to": "GradientDescentAlgorithm", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Primal Problem_Relationship Between Primal and Dual_related_to", "from": "Primal Problem", "to": "Relationship Between Primal and Dual", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Theorem 7.4.1_Backpropagation_related_to", "from": "Theorem 7.4.1", "to": "Backpropagation", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine_Learning_Challenges_k_Fold_Cross_Validation_has_subtopic", "from": "Machine_Learning_Challenges", "to": "k_Fold_Cross_Validation", "arrows": "to", "title": "has_subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Gradient Calculation_Matrix Derivatives_subtopic", "from": "Gradient Calculation", "to": "Matrix Derivatives", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_State_Transition_Probabilities_MDP_Model_Learning_depends_on", "from": "State_Transition_Probabilities", "to": "MDP_Model_Learning", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Support Vector Machines (SVM)_Machine Learning Overview_depends_on", "from": "Support Vector Machines (SVM)", "to": "Machine Learning Overview", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Convex_Functions_Jensens_Inequality_subtopic", "from": "Convex_Functions", "to": "Jensens_Inequality", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Generalization_Error_Machine_Learning_Concepts_subtopic", "from": "Generalization_Error", "to": "Machine_Learning_Concepts", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Locally Weighted Linear Regression_Value Function Approximation_related_to", "from": "Locally Weighted Linear Regression", "to": "Value Function Approximation", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_EM Algorithm_Convex Function_related_to", "from": "EM Algorithm", "to": "Convex Function", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_ParameterEstimation_NaiveBayesAlgorithm_subtopic", "from": "ParameterEstimation", "to": "NaiveBayesAlgorithm", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Quadratic_Form_Dynamics_Model_depends_on", "from": "Quadratic_Form", "to": "Dynamics_Model", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Step 1_Gaussian Distributions_depends_on", "from": "Step 1", "to": "Gaussian Distributions", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Transition_Probabilities_Sampling_Chapter_17_Policy_Gradient_REINFORCE_subtopic", "from": "Transition_Probabilities_Sampling", "to": "Chapter_17_Policy_Gradient_REINFORCE", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Variational Autoencoders_Machine Learning Papers_subtopic", "from": "Variational Autoencoders", "to": "Machine Learning Papers", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_EM Algorithm_Variational Inference_related_to", "from": "EM Algorithm", "to": "Variational Inference", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_PayoffDefinition_FiniteHorizonMDP_subtopic", "from": "PayoffDefinition", "to": "FiniteHorizonMDP", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_DataPreprocessing_LogisticFunctionDerivative_depends_on", "from": "DataPreprocessing", "to": "LogisticFunctionDerivative", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Multiple_Examples_Consideration_Evidence_Lower_Bound_(ELBO)_subtopic", "from": "Multiple_Examples_Consideration", "to": "Evidence_Lower_Bound_(ELBO)", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_MStepUpdateRule_ExpectationMaximizationAlgorithm_subtopic", "from": "MStepUpdateRule", "to": "ExpectationMaximizationAlgorithm", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Expectation_Computation_Deterministic_Simulator_subtopic", "from": "Expectation_Computation", "to": "Deterministic_Simulator", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Eigenvectors and Eigenvalues_Principal Component Analysis (PCA)_subtopic", "from": "Eigenvectors and Eigenvalues", "to": "Principal Component Analysis (PCA)", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Algorithms_Value Iteration_related_to", "from": "Machine Learning Algorithms", "to": "Value Iteration", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Hypothesis_Space_Size_Sample_Complexity_subtopic", "from": "Hypothesis_Space_Size", "to": "Sample_Complexity", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Asynchronous_Update_Value_Iteration_subtopic", "from": "Asynchronous_Update", "to": "Value_Iteration", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_GaussianDiscriminantAnalysis_ModelAssumptions_subtopic", "from": "GaussianDiscriminantAnalysis", "to": "ModelAssumptions", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Maximize_Geometric_Margin_Optimal_Margin_Classifier_subtopic", "from": "Maximize_Geometric_Margin", "to": "Optimal_Margin_Classifier", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_KernelMethods_KernelTrick_has_subtopic", "from": "KernelMethods", "to": "KernelTrick", "arrows": "to", "title": "has_subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Labeled_Task_Dataset_Adaptation_Phase_depends_on", "from": "Labeled_Task_Dataset", "to": "Adaptation_Phase", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Finding Roots_Maximizing Functions_related_to", "from": "Finding Roots", "to": "Maximizing Functions", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Finite_Horizon_Case_Chapter_17_Policy_Gradient_REINFORCE_subtopic", "from": "Finite_Horizon_Case", "to": "Chapter_17_Policy_Gradient_REINFORCE", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_NaturalParameterForBernoulli_SigmoidFunction_depends_on", "from": "NaturalParameterForBernoulli", "to": "SigmoidFunction", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Locally Weighted Linear Regression_Machine Learning Models_subtopic", "from": "Locally Weighted Linear Regression", "to": "Machine Learning Models", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Gaussian Discriminant Analysis (GDA)_Bayesian Classification_subtopic", "from": "Gaussian Discriminant Analysis (GDA)", "to": "Bayesian Classification", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_BinaryClassification_LogisticFunction_depends_on", "from": "BinaryClassification", "to": "LogisticFunction", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_GaussianDiscriminantAnalysis(GDA)_DecisionBoundary_has_subtopic", "from": "GaussianDiscriminantAnalysis(GDA)", "to": "DecisionBoundary", "arrows": "to", "title": "has_subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_TwoLayerNN_GELUFunction_depends_on", "from": "TwoLayerNN", "to": "GELUFunction", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_TwoLayerNN_TanhFunction_depends_on", "from": "TwoLayerNN", "to": "TanhFunction", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Concepts_EM Algorithms_related_to", "from": "Machine Learning Concepts", "to": "EM Algorithms", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Model-Based Reinforcement Learning_Machine Learning Papers_subtopic", "from": "Model-Based Reinforcement Learning", "to": "Machine Learning Papers", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_TextGeneration_ConditionalProbability_depends_on", "from": "TextGeneration", "to": "ConditionalProbability", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_State Transition_Markov Decision Process (MDP)_depends_on", "from": "State Transition", "to": "Markov Decision Process (MDP)", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine_Learning_Optimization_Normal_Equations_Method_related_to", "from": "Machine_Learning_Optimization", "to": "Normal_Equations_Method", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Regularization_Overfitting_addresses", "from": "Regularization", "to": "Overfitting", "arrows": "to", "title": "addresses", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_MachineLearningConcepts_GaussianDiscriminantAnalysisModel_contains", "from": "MachineLearningConcepts", "to": "GaussianDiscriminantAnalysisModel", "arrows": "to", "title": "contains", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_KernelTrick_MachineLearningConcepts_related_to", "from": "KernelTrick", "to": "MachineLearningConcepts", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine_Learning_Concepts_Differential_Dynamic_Programming_related_to", "from": "Machine_Learning_Concepts", "to": "Differential_Dynamic_Programming", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_ReLU Activation Function_Multi-layer Neural Networks_subtopic", "from": "ReLU Activation Function", "to": "Multi-layer Neural Networks", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_MDP Simulators_Machine Learning Models_subtopic", "from": "MDP Simulators", "to": "Machine Learning Models", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_ReLUActivation_NeuralNetworks_subtopic", "from": "ReLUActivation", "to": "NeuralNetworks", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_AdaptiveSampling_TemperatureParameter_related_to", "from": "AdaptiveSampling", "to": "TemperatureParameter", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Uniform_Convergence_Generalization_Error_Bound_related_to", "from": "Uniform_Convergence", "to": "Generalization_Error_Bound", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine_Learning_Basics_LMS_Algorithm_depends_on", "from": "Machine_Learning_Basics", "to": "LMS_Algorithm", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_LogisticRegression_HypothesisFunction_uses", "from": "LogisticRegression", "to": "HypothesisFunction", "arrows": "to", "title": "uses", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Variational_Inference_Review_Machine_Learning_Papers_belongs_to", "from": "Variational_Inference_Review", "to": "Machine_Learning_Papers", "arrows": "to", "title": "belongs_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Optimal_Margin_Classifier_Machine_Learning_Concepts_related_to", "from": "Optimal_Margin_Classifier", "to": "Machine_Learning_Concepts", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_ProbabilityDistributions_JensensInequality_depends_on", "from": "ProbabilityDistributions", "to": "JensensInequality", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Support_Vector_Machines_SVM_Optimal_Margin_Classifier_leads_to", "from": "Support_Vector_Machines_SVM", "to": "Optimal_Margin_Classifier", "arrows": "to", "title": "leads_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Log_Probability_Ratio_Gradient_Estimation_depends_on", "from": "Log_Probability_Ratio", "to": "Gradient_Estimation", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Value_Iteration_MDP_Model_Learning_related_to", "from": "Value_Iteration", "to": "MDP_Model_Learning", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine_Learning_Concepts_Support_Vector_Machines_SVM_contains", "from": "Machine_Learning_Concepts", "to": "Support_Vector_Machines_SVM", "arrows": "to", "title": "contains", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_MachineLearningOverview_KernelMethods_contains", "from": "MachineLearningOverview", "to": "KernelMethods", "arrows": "to", "title": "contains", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_MachineLearningModels_GeneralizedLinearModels_contains", "from": "MachineLearningModels", "to": "GeneralizedLinearModels", "arrows": "to", "title": "contains", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Predict Step_Update Step_related_to", "from": "Predict Step", "to": "Update Step", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Computational Efficiency_Kalman Filter_depends_on", "from": "Computational Efficiency", "to": "Kalman Filter", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Training_Transformer_Cross_Entropy_Loss_related_to", "from": "Training_Transformer", "to": "Cross_Entropy_Loss", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Hoeffding_Inequality_Machine_Learning_Concepts_subtopic", "from": "Hoeffding_Inequality", "to": "Machine_Learning_Concepts", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_ELBO Lower Bound_Variational Inference_depends_on", "from": "ELBO Lower Bound", "to": "Variational Inference", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_ICAConcepts_MaximumLikelihoodEstimation_subtopic", "from": "ICAConcepts", "to": "MaximumLikelihoodEstimation", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_DiscriminativeLearning_LogisticRegression_subtopic", "from": "DiscriminativeLearning", "to": "LogisticRegression", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Training_Error_Generalization_Error_Difference_Uniform_Convergence_depends_on", "from": "Training_Error_Generalization_Error_Difference", "to": "Uniform_Convergence", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Randomized_Policy_Chapter_17_Policy_Gradient_REINFORCE_subtopic", "from": "Randomized_Policy", "to": "Chapter_17_Policy_Gradient_REINFORCE", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_TwoLayerNN_SigmoidFunction_depends_on", "from": "TwoLayerNN", "to": "SigmoidFunction", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Bayes' Theorem_Prior Distribution_related_to", "from": "Bayes' Theorem", "to": "Prior Distribution", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Optimal Value Function_Value Function_subtopic", "from": "Optimal Value Function", "to": "Value Function", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Optimal_Policy_Machine_Learning_subtopic", "from": "Optimal_Policy", "to": "Machine_Learning", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_SVM_Model Set M_subtopic", "from": "SVM", "to": "Model Set M", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Deep_Learning_Neural_Networks_depends_on", "from": "Deep_Learning", "to": "Neural_Networks", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Dimensionality Reduction_Principal Component Analysis (PCA)_depends_on", "from": "Dimensionality Reduction", "to": "Principal Component Analysis (PCA)", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine_Learning_Optimization_Techniques_related_to", "from": "Machine_Learning", "to": "Optimization_Techniques", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_TwoLayerNN_IdentityFunction_related_to", "from": "TwoLayerNN", "to": "IdentityFunction", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_ELBO_EStep_depends_on", "from": "ELBO", "to": "EStep", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Fitted_Value_Iteration_Convergence_Issues_related_to", "from": "Fitted_Value_Iteration", "to": "Convergence_Issues", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_SupportVectorsConcept_KernelTrickIntroduction_related_to", "from": "SupportVectorsConcept", "to": "KernelTrickIntroduction", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Expected_Total_Payoff_Optimization_Chapter_17_Policy_Gradient_REINFORCE_subtopic", "from": "Expected_Total_Payoff_Optimization", "to": "Chapter_17_Policy_Gradient_REINFORCE", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Contrastive_Learning_Visual_Representations_Machine_Learning_Papers_belongs_to", "from": "Contrastive_Learning_Visual_Representations", "to": "Machine_Learning_Papers", "arrows": "to", "title": "belongs_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_ModelParameters_LogLikelihoodCalculation_related_to", "from": "ModelParameters", "to": "LogLikelihoodCalculation", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Learning a model for an MDP_Connections between Policy and Value Iteration (Optional)_contains", "from": "Learning a model for an MDP", "to": "Connections between Policy and Value Iteration (Optional)", "arrows": "to", "title": "contains", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Value_Iteration_MDP_Finite_State_Space_subtopic", "from": "Value_Iteration", "to": "MDP_Finite_State_Space", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_MachineLearningBasics_GaussianDistribution_contains", "from": "MachineLearningBasics", "to": "GaussianDistribution", "arrows": "to", "title": "contains", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Variational_Inference_Variational_Autoencoder_component_of", "from": "Variational_Inference", "to": "Variational_Autoencoder", "arrows": "to", "title": "component_of", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Step2OptimalPolicy_LQRAlgorithmSteps_subtopic", "from": "Step2OptimalPolicy", "to": "LQRAlgorithmSteps", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine_Learning_Theory_Empirical_Risk_Minimization_related_to", "from": "Machine_Learning_Theory", "to": "Empirical_Risk_Minimization", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Support_Vector_Machines_SVMs_SMO_Algorithm_related_to", "from": "Support_Vector_Machines_SVMs", "to": "SMO_Algorithm", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_ComparisonValuePolicyIteration_MachineLearningConcepts_subtopic", "from": "ComparisonValuePolicyIteration", "to": "MachineLearningConcepts", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Linear_Model_Failure_Bias_Definition_depends_on", "from": "Linear_Model_Failure", "to": "Bias_Definition", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Observation vs State_Partially Observable MDPs (POMDP)_depends_on", "from": "Observation vs State", "to": "Partially Observable MDPs (POMDP)", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Log_Probability_Ratio_Policy_Gradient_Methods_subtopic", "from": "Log_Probability_Ratio", "to": "Policy_Gradient_Methods", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Total Parameters Count_Multi-layer Neural Networks_subtopic", "from": "Total Parameters Count", "to": "Multi-layer Neural Networks", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_GaussianDiscriminantAnalysisModel_BernoulliDistribution_subtopic", "from": "GaussianDiscriminantAnalysisModel", "to": "BernoulliDistribution", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_1D Example_Density Transformation_depends_on", "from": "1D Example", "to": "Density Transformation", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Uniform_Convergence_Assumption_Generalization_Error_Bound_depends_on", "from": "Uniform_Convergence_Assumption", "to": "Generalization_Error_Bound", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine_Learning_Concepts_Value_Iteration_depends_on", "from": "Machine_Learning_Concepts", "to": "Value_Iteration", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Theorem_Jensens_Inequality_Jensens_Inequality_subtopic", "from": "Theorem_Jensens_Inequality", "to": "Jensens_Inequality", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Binary_Classification_Training_Set_depends_on", "from": "Binary_Classification", "to": "Training_Set", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Support_Vector_Machines_SVM_SMO_Algorithm_contains", "from": "Support_Vector_Machines_SVM", "to": "SMO_Algorithm", "arrows": "to", "title": "contains", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Feature Discovery_Black Box Nature_subtopic", "from": "Feature Discovery", "to": "Black Box Nature", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_LayerNormalization_TransformerArchitecture_related_to", "from": "LayerNormalization", "to": "TransformerArchitecture", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Loss Functions_Cross-Entropy Loss_subtopic", "from": "Loss Functions", "to": "Cross-Entropy Loss", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Expected_Immediate_Rewards_MDP_Model_Learning_depends_on", "from": "Expected_Immediate_Rewards", "to": "MDP_Model_Learning", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Pretraining_Phase_Transfer_Learning_subtopic", "from": "Pretraining_Phase", "to": "Transfer_Learning", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_OrdinaryLeastSquares_ExponentialFamilyDistribution_uses_distribution", "from": "OrdinaryLeastSquares", "to": "ExponentialFamilyDistribution", "arrows": "to", "title": "uses_distribution", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_ParameterizedModel_SoftmaxFunction_uses", "from": "ParameterizedModel", "to": "SoftmaxFunction", "arrows": "to", "title": "uses", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_ConditionalProbability_JointLikelihood_depends_on", "from": "ConditionalProbability", "to": "JointLikelihood", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Gradient Computation_Theorem 7.4.1_subtopic", "from": "Gradient Computation", "to": "Theorem 7.4.1", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Bias-Variance Tradeoff_Underfitting_depends_on", "from": "Bias-Variance Tradeoff", "to": "Underfitting", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Union_Bound_Application_Uniform_Convergence_subtopic", "from": "Union_Bound_Application", "to": "Uniform_Convergence", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Neural Network_Model Set M_subtopic", "from": "Neural Network", "to": "Model Set M", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_MultinomialEventModel_ParameterEstimation_has_subtopic", "from": "MultinomialEventModel", "to": "ParameterEstimation", "arrows": "to", "title": "has_subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Target Vector_Least Squares Revisited_subtopic", "from": "Target Vector", "to": "Least Squares Revisited", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_SpeedPerspective_VectorizationInNN_depends_on", "from": "SpeedPerspective", "to": "VectorizationInNN", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Policy_Iteration_MDP_Model_Learning_related_to", "from": "Policy_Iteration", "to": "MDP_Model_Learning", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_CostFunctionJ_GradientDescentAlgorithm_related_to", "from": "CostFunctionJ", "to": "GradientDescentAlgorithm", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Training_Error_Generalization_Error_Bound_depends_on", "from": "Training_Error", "to": "Generalization_Error_Bound", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Bias_Variance_Tradeoff_Machine_Learning_Papers_belongs_to", "from": "Bias_Variance_Tradeoff", "to": "Machine_Learning_Papers", "arrows": "to", "title": "belongs_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Optimization Methods_Machine Learning Papers_subtopic", "from": "Optimization Methods", "to": "Machine Learning Papers", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_TemperatureParameter_ConditionalProbability_subtopic", "from": "TemperatureParameter", "to": "ConditionalProbability", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Optimizers_Implicit Regularization_depends_on", "from": "Optimizers", "to": "Implicit Regularization", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_DistanceToBoundary_DecisionBoundary_subtopic", "from": "DistanceToBoundary", "to": "DecisionBoundary", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Action Selection_Markov Decision Process (MDP)_depends_on", "from": "Action Selection", "to": "Markov Decision Process (MDP)", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Approximation Error Minimization_Principal Component Analysis (PCA)_subtopic", "from": "Approximation Error Minimization", "to": "Principal Component Analysis (PCA)", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Sequential Decision Making_Reinforcement Learning_subtopic", "from": "Sequential Decision Making", "to": "Reinforcement Learning", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine_Learning_Concepts_Variational_Autoencoder_related_to", "from": "Machine_Learning_Concepts", "to": "Variational_Autoencoder", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Dynamic_Programming_Value_Iteration_subtopic", "from": "Dynamic_Programming", "to": "Value_Iteration", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_PolynomialFitting_Overfitting_depends_on", "from": "PolynomialFitting", "to": "Overfitting", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_RewardsDependency_ExpectationRewriting_depends_on", "from": "RewardsDependency", "to": "ExpectationRewriting", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_KernelTrickIntroduction_LagrangianFormulation_depends_on", "from": "KernelTrickIntroduction", "to": "LagrangianFormulation", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Mini-batch Stochastic Gradient Descent_Stochastic Gradient Descent (SGD)_variant_of", "from": "Mini-batch Stochastic Gradient Descent", "to": "Stochastic Gradient Descent (SGD)", "arrows": "to", "title": "variant_of", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Bell and Sejnowski's Method_ICA Algorithm_subtopic", "from": "Bell and Sejnowski's Method", "to": "ICA Algorithm", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Efficiency_of_Backward_Pass_Vectorized_Notation_Backward_Func_depends_on", "from": "Efficiency_of_Backward_Pass", "to": "Vectorized_Notation_Backward_Func", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Law_of_Total_Expectation_Log_Probability_Ratio_depends_on", "from": "Law_of_Total_Expectation", "to": "Log_Probability_Ratio", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_JointLikelihood_MaximumLikelihoodEstimation_related_to", "from": "JointLikelihood", "to": "MaximumLikelihoodEstimation", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_TwoLayerNN_ReLUFunction_depends_on", "from": "TwoLayerNN", "to": "ReLUFunction", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Weight Matrices and Biases_Multi-layer Neural Networks_subtopic", "from": "Weight Matrices and Biases", "to": "Multi-layer Neural Networks", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_LayerActivations_VectorizationTechniques_related_to", "from": "LayerActivations", "to": "VectorizationTechniques", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Optimal Parameters w and b_Dual Optimization Problem_subtopic", "from": "Optimal Parameters w and b", "to": "Dual Optimization Problem", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Optimizers_Pretraining_Phase_related_to", "from": "Optimizers", "to": "Pretraining_Phase", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Unsupervised learning_Self-supervised learning and foundation models_related_to", "from": "Unsupervised learning", "to": "Self-supervised learning and foundation models", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Kalman Filter_Belief State_related_to", "from": "Kalman Filter", "to": "Belief State", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Bellman Equations_Value Function_subtopic", "from": "Bellman Equations", "to": "Value Function", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Total Payoff_Markov Decision Process (MDP)_subtopic", "from": "Total Payoff", "to": "Markov Decision Process (MDP)", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_SMO_Algorithm_KKT_Conditions_has_subtopic", "from": "SMO_Algorithm", "to": "KKT_Conditions", "arrows": "to", "title": "has_subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_RegularizerFunction_RegularizationParameter_depends_on", "from": "RegularizerFunction", "to": "RegularizationParameter", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_MachineLearningModels_ExponentialFamilyDistributions_subtopic", "from": "MachineLearningModels", "to": "ExponentialFamilyDistributions", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Learning Theory Fundamentals_Machine Learning Papers_subtopic", "from": "Learning Theory Fundamentals", "to": "Machine Learning Papers", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Total Number of Neurons_Multi-layer Neural Networks_subtopic", "from": "Total Number of Neurons", "to": "Multi-layer Neural Networks", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Lagrangian Function_Dual Optimization Problem_depends_on", "from": "Lagrangian Function", "to": "Dual Optimization Problem", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_LogisticRegression_ExponentialFamilyDistribution_uses_distribution", "from": "LogisticRegression", "to": "ExponentialFamilyDistribution", "arrows": "to", "title": "uses_distribution", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Posterior Distribution p(y|x)_Bayesian Classification_related_to", "from": "Posterior Distribution p(y|x)", "to": "Bayesian Classification", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_OptimizationChallenges_LikelihoodFunction_subtopic", "from": "OptimizationChallenges", "to": "LikelihoodFunction", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_MStep_ExpectationMaximizationAlgorithm_subtopic", "from": "MStep", "to": "ExpectationMaximizationAlgorithm", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_NonStationaryOptimalPolicy_FiniteHorizonMDP_follows_from", "from": "NonStationaryOptimalPolicy", "to": "FiniteHorizonMDP", "arrows": "to", "title": "follows_from", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Multi_Class_Classification_Machine_Learning_Concepts_subtopic", "from": "Multi_Class_Classification", "to": "Machine_Learning_Concepts", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Orthogonal Basis_Principal Component Analysis (PCA)_subtopic", "from": "Orthogonal Basis", "to": "Principal Component Analysis (PCA)", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_ParameterEstimation_NaiveBayesFilter_subtopic", "from": "ParameterEstimation", "to": "NaiveBayesFilter", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Error_Margin_Determination_Quantities_of_Interest_subtopic", "from": "Error_Margin_Determination", "to": "Quantities_of_Interest", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Overfitting_Variance_depends_on", "from": "Overfitting", "to": "Variance", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_House Price Prediction Example_Feature Discovery_related_to", "from": "House Price Prediction Example", "to": "Feature Discovery", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Confidence in Predictions_Machine Learning Concepts_subtopic", "from": "Confidence in Predictions", "to": "Machine Learning Concepts", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine_Learning_Concepts_Linear_Quadratic_Gaussian_(LQG)_next_topic", "from": "Machine_Learning_Concepts", "to": "Linear_Quadratic_Gaussian_(LQG)", "arrows": "to", "title": "next_topic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Gaussian_Mixture_Models_Posterior_Distribution_has_analytical_solution_for", "from": "Gaussian_Mixture_Models", "to": "Posterior_Distribution", "arrows": "to", "title": "has_analytical_solution_for", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_TwoLayerNetwork_VectorizationInNN_subtopic", "from": "TwoLayerNetwork", "to": "VectorizationInNN", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Decision Boundary_Machine Learning Concepts_subtopic", "from": "Decision Boundary", "to": "Machine Learning Concepts", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_LinearModelPrediction_StochasticModel_related_to", "from": "LinearModelPrediction", "to": "StochasticModel", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_RegressionProblems_TestExample_related_to", "from": "RegressionProblems", "to": "TestExample", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Scaling_Parameters_Machine_Learning_Concepts_depends_on", "from": "Scaling_Parameters", "to": "Machine_Learning_Concepts", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine_Learning_Algorithms_Support_Vector_Machines_SVM_related_to", "from": "Machine_Learning_Algorithms", "to": "Support_Vector_Machines_SVM", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Geometric_Margin_Machine_Learning_Concepts_subtopic", "from": "Geometric_Margin", "to": "Machine_Learning_Concepts", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_General Case_Density Transformation_subtopic", "from": "General Case", "to": "Density Transformation", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Sparsity Inducing Regularization_Regularizer R(\u03b8)_subtopic", "from": "Sparsity Inducing Regularization", "to": "Regularizer R(\u03b8)", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Concepts_Vectorization Over Training Examples_related_to", "from": "Machine Learning Concepts", "to": "Vectorization Over Training Examples", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_5th_Degree_Polynomial_Failure_Generalization_Error_related_to", "from": "5th_Degree_Polynomial_Failure", "to": "Generalization_Error", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Kernels_in_Machine_Learning_Feature_Map_Phi_depends_on", "from": "Kernels_in_Machine_Learning", "to": "Feature_Map_Phi", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Conditional_Probability_Softmax_Function_depends_on", "from": "Conditional_Probability", "to": "Softmax_Function", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_LQR Algorithm_Kalman Filter_subtopic", "from": "LQR Algorithm", "to": "Kalman Filter", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_TwoLayerNN_LeakyReLUFunction_depends_on", "from": "TwoLayerNN", "to": "LeakyReLUFunction", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Separating Hyperplane_Decision Boundary_subtopic", "from": "Separating Hyperplane", "to": "Decision Boundary", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_UnitVectorW_VectorW_related_to", "from": "UnitVectorW", "to": "VectorW", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Value Function_Policy Execution_depends_on", "from": "Value Function", "to": "Policy Execution", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Belief State_Partially Observable MDPs (POMDP)_subtopic", "from": "Belief State", "to": "Partially Observable MDPs (POMDP)", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Unlabeled_Dataset_Pretraining_Phase_related_to", "from": "Unlabeled_Dataset", "to": "Pretraining_Phase", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_PhiParameterUpdate_MStepUpdateRule_subtopic", "from": "PhiParameterUpdate", "to": "MStepUpdateRule", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Double_Descent_Weak_Features_Machine_Learning_Papers_belongs_to", "from": "Double_Descent_Weak_Features", "to": "Machine_Learning_Papers", "arrows": "to", "title": "belongs_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_MaximumLikelihoodEstimation_SigmoidFunction_related_to", "from": "MaximumLikelihoodEstimation", "to": "SigmoidFunction", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_ExpectationMaximizationAlgorithm_MachineLearningOverview_depends_on", "from": "ExpectationMaximizationAlgorithm", "to": "MachineLearningOverview", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_LQRModelAssumptions_MachineLearningOverview_depends_on", "from": "LQRModelAssumptions", "to": "MachineLearningOverview", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Empirical_Risk_Minimization_Training_Error_depends_on", "from": "Empirical_Risk_Minimization", "to": "Training_Error", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_FullyConnectedNN_ReLUActivation_uses", "from": "FullyConnectedNN", "to": "ReLUActivation", "arrows": "to", "title": "uses", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_MachineLearning_ExpectationMaximizationAlgorithm_contains", "from": "MachineLearning", "to": "ExpectationMaximizationAlgorithm", "arrows": "to", "title": "contains", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_GLMAssumptions_GeneralizedLinearModels_subtopic", "from": "GLMAssumptions", "to": "GeneralizedLinearModels", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Logistic Regression_Model Set M_subtopic", "from": "Logistic Regression", "to": "Model Set M", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_MachineLearningModels_BernoulliEventModel_related_to", "from": "MachineLearningModels", "to": "BernoulliEventModel", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_FeatureMapping_MachineLearningConcepts_depends_on", "from": "FeatureMapping", "to": "MachineLearningConcepts", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Alpha Updates_Sequential Minimal Optimization (SMO) Algorithm_subtopic", "from": "Alpha Updates", "to": "Sequential Minimal Optimization (SMO) Algorithm", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Mean Field Assumption_Variational Inference_subtopic", "from": "Mean Field Assumption", "to": "Variational Inference", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine_Learning_Concepts_Inverted_Pendulum_Model_related_to", "from": "Machine_Learning_Concepts", "to": "Inverted_Pendulum_Model", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Model_Parameter_Theta_Pretraining_Phase_related_to", "from": "Model_Parameter_Theta", "to": "Pretraining_Phase", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning_Deep Learning Packages_related_to", "from": "Machine Learning", "to": "Deep Learning Packages", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Loss J(\u03b8)_Regularized Loss Function_depends_on", "from": "Loss J(\u03b8)", "to": "Regularized Loss Function", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine_Learning_Adaptation_Methods_Pretraining_Methods_Computer_Vision_has_subtopic", "from": "Machine_Learning_Adaptation_Methods", "to": "Pretraining_Methods_Computer_Vision", "arrows": "to", "title": "has_subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_MachineLearningConcepts_SupportVectorMachines_related_to", "from": "MachineLearningConcepts", "to": "SupportVectorMachines", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Algorithms_Policy Iteration_related_to", "from": "Machine Learning Algorithms", "to": "Policy Iteration", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_LQRAlgorithmSteps_MachineLearningOverview_subtopic", "from": "LQRAlgorithmSteps", "to": "MachineLearningOverview", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_LeastSquaresRegression_MaximumLikelihoodEstimation_related_to", "from": "LeastSquaresRegression", "to": "MaximumLikelihoodEstimation", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Implicit Bias in Machine Learning_High-Dimensional Interpolation_related_to", "from": "Implicit Bias in Machine Learning", "to": "High-Dimensional Interpolation", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Quantities_of_Interest_Uniform_Convergence_related_to", "from": "Quantities_of_Interest", "to": "Uniform_Convergence", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_VectorW_DecisionBoundary_depends_on", "from": "VectorW", "to": "DecisionBoundary", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Optimal_Policy_Quadratic_Form_related_to", "from": "Optimal_Policy", "to": "Quadratic_Form", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Bias-Variance Tradeoff_Overfitting_depends_on", "from": "Bias-Variance Tradeoff", "to": "Overfitting", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Chapter9_Regularization_contains", "from": "Chapter9", "to": "Regularization", "arrows": "to", "title": "contains", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_LagrangianConstruction_PhiParameterUpdate_depends_on", "from": "LagrangianConstruction", "to": "PhiParameterUpdate", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Perceptron_Algorithm_Machine_Learning_Concepts_subtopic", "from": "Perceptron_Algorithm", "to": "Machine_Learning_Concepts", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Concepts_Underfitting_related_to", "from": "Machine Learning Concepts", "to": "Underfitting", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_PolicyIterationAlgorithm_BellmanEquations_related_to", "from": "PolicyIterationAlgorithm", "to": "BellmanEquations", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Synchronous_Update_Value_Iteration_subtopic", "from": "Synchronous_Update", "to": "Value_Iteration", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_LinearModelPrediction_DeterministicModel_depends_on", "from": "LinearModelPrediction", "to": "DeterministicModel", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_MultiClassClassification_Logits_related_to", "from": "MultiClassClassification", "to": "Logits", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Policy Gradient Methods_Value Function_relates_to", "from": "Policy Gradient Methods", "to": "Value Function", "arrows": "to", "title": "relates_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_LayerArchitecture_TwoLayerNN_subtopic", "from": "LayerArchitecture", "to": "TwoLayerNN", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Matrix Multiplication Module_MLP Composition_depends_on", "from": "Matrix Multiplication Module", "to": "MLP Composition", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_MachineLearningConcepts_LocallyWeightedLinearRegression_contains", "from": "MachineLearningConcepts", "to": "LocallyWeightedLinearRegression", "arrows": "to", "title": "contains", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Model Set M_Model Selection_subtopic", "from": "Model Set M", "to": "Model Selection", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_L2 Regularization_Regularizer R(\u03b8)_subtopic", "from": "L2 Regularization", "to": "Regularizer R(\u03b8)", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Weight Decay_L2 Regularization_related_to", "from": "Weight Decay", "to": "L2 Regularization", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_EM_Algorithm_Generalization_Machine_Learning_Concepts_subtopic", "from": "EM_Algorithm_Generalization", "to": "Machine_Learning_Concepts", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_DataNormalization_PCAAlgorithm_depends_on", "from": "DataNormalization", "to": "PCAAlgorithm", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_WalkableNeighborhood_DerivedFeatures_subtopic", "from": "WalkableNeighborhood", "to": "DerivedFeatures", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_DiscountFactorImpact_FiniteHorizonMDP_related_to", "from": "DiscountFactorImpact", "to": "FiniteHorizonMDP", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_BackwardFunctionsBasics_MatrixMultiplicationModule_has_subtopic", "from": "BackwardFunctionsBasics", "to": "MatrixMultiplicationModule", "arrows": "to", "title": "has_subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Polynomial Regression Model_Model Selection_related_to", "from": "Polynomial Regression Model", "to": "Model Selection", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_GeneralizedLinearModels_MachineLearningOverview_subtopic", "from": "GeneralizedLinearModels", "to": "MachineLearningOverview", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_VariationalInference_MachineLearningOverview_related_to", "from": "VariationalInference", "to": "MachineLearningOverview", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_LogisticRegression_ModelAssumptions_subtopic", "from": "LogisticRegression", "to": "ModelAssumptions", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Loss_Functions_Cross_Entropy_Loss_has_subtopic", "from": "Loss_Functions", "to": "Cross_Entropy_Loss", "arrows": "to", "title": "has_subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_TimeHorizonTuple_FiniteHorizonMDP_subtopic", "from": "TimeHorizonTuple", "to": "FiniteHorizonMDP", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_NeuralNetworks_ClassificationProblem_subtopic", "from": "NeuralNetworks", "to": "ClassificationProblem", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine_Learning_Theorem_Generalization_Error_Bound_subtopic", "from": "Machine_Learning_Theorem", "to": "Generalization_Error_Bound", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_LossFunctionFormulation_IntermediateVariables_leads_to", "from": "LossFunctionFormulation", "to": "IntermediateVariables", "arrows": "to", "title": "leads_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Corollary of Vapnik's Theorem_Vapnik's Theorem_derives_from", "from": "Corollary of Vapnik's Theorem", "to": "Vapnik's Theorem", "arrows": "to", "title": "derives_from", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_MachineLearningOverview_LogisticRegression_contains", "from": "MachineLearningOverview", "to": "LogisticRegression", "arrows": "to", "title": "contains", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Non-linear Feature Mappings_MDP Simulators_depends_on", "from": "Non-linear Feature Mappings", "to": "MDP Simulators", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_ClassificationModelAssumptions_LikelihoodFunction_subtopic", "from": "ClassificationModelAssumptions", "to": "LikelihoodFunction", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Other Activation Functions_Multi-layer Neural Networks_related_to", "from": "Other Activation Functions", "to": "Multi-layer Neural Networks", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_ConvergenceOfAlgorithms_PolicyIterationAlgorithm_depends_on", "from": "ConvergenceOfAlgorithms", "to": "PolicyIterationAlgorithm", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Matrix Derivatives_Least Squares Revisited_depends_on", "from": "Matrix Derivatives", "to": "Least Squares Revisited", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Sample Complexity Bounds_Bias-Variance Tradeoff_subtopic", "from": "Sample Complexity Bounds", "to": "Bias-Variance Tradeoff", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine_Learning_Concepts_Bias_Variance_Tradoff_subtopic", "from": "Machine_Learning_Concepts", "to": "Bias_Variance_Tradoff", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Covariance Matrix_Multivariate Normal Distribution_related_to", "from": "Covariance Matrix", "to": "Multivariate Normal Distribution", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Supervised Learning with Non-Linear Models_Deep Learning Introduction_subtopic", "from": "Supervised Learning with Non-Linear Models", "to": "Deep Learning Introduction", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Immediate Reward_Bellman Equations_related_to", "from": "Immediate Reward", "to": "Bellman Equations", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_MultinomialDistribution_ParameterizedModel_has_subtopic", "from": "MultinomialDistribution", "to": "ParameterizedModel", "arrows": "to", "title": "has_subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Optimization Problem in SVM_Geometric Margin_related_to", "from": "Optimization Problem in SVM", "to": "Geometric Margin", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_LearningRate_GradientDescentAlgorithm_depends_on", "from": "LearningRate", "to": "GradientDescentAlgorithm", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_ValueFunctionApproximation_FeatureMapping_related_to", "from": "ValueFunctionApproximation", "to": "FeatureMapping", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Backpropagation Algorithm_Gradient Calculation_subtopic", "from": "Backpropagation Algorithm", "to": "Gradient Calculation", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_JensensInequality_EvidenceLowerBoundELBO_depends_on", "from": "JensensInequality", "to": "EvidenceLowerBoundELBO", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Optimal Policy_Optimal Value Function_related_to", "from": "Optimal Policy", "to": "Optimal Value Function", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}];
        const detailsDivId = "nodeDetails";
        // ------------------------------------------

        const nodes = new vis.DataSet(nodesData);
        const hierarchicalEdges = new vis.DataSet(hierarchicalEdgesData);
        const dynamicOtherEdges = new vis.DataSet([]); // For tracking dynamic edges

        const container = document.getElementById('mynetwork');
        const detailsDiv = document.getElementById(detailsDivId);
        const loadingOverlay = document.getElementById('loadingOverlay');

        // Start with only hierarchical edges visible
        const allEdges = new vis.DataSet(hierarchicalEdges.get());

        const data = {
            nodes: nodes,
            edges: allEdges
        };

        const options = {
            layout: {
                hierarchical: {
                    enabled: true,
                    levelSeparation: 150,
                    nodeSpacing: 120, // Might need slight adjustment with wrapped labels
                    treeSpacing: 220, // Might need slight adjustment
                    direction: 'UD',
                    sortMethod: 'directed'
                }
            },
            physics: {
                enabled: true,
                hierarchicalRepulsion: {
                    centralGravity: 0.0,
                    springLength: 100,
                    springConstant: 0.01,
                    nodeDistance: 120, // Base distance
                    damping: 0.09
                },
                minVelocity: 0.75,
                solver: 'hierarchicalRepulsion'
            },
            interaction: {
                 hover: true,
                 tooltipDelay: 300
            },
            nodes: {
                borderWidth: 1,
                borderWidthSelected: 2,
                // No specific wrapping option needed here, Vis.js uses the '
'
                font: {
                    multi: 'html', // Allow multi-line labels (handles '
')
                    // size: 12 // Optional: Adjust font size if needed
                }
            },
            edges: {
                smooth: {
                    type: "cubicBezier",
                    forceDirection: "vertical",
                    roundness: 0.4
                }
            }
        };

        const network = new vis.Network(container, data, options);
        let currentlyShownOtherEdges = [];

        // --- Event Listeners (Keep as before) ---
        network.on("stabilizationIterationsDone", function () {
            loadingOverlay.style.display = 'none';
        });

        network.on("selectNode", function (params) {
            if (params.nodes.length > 0) {
                const selectedNodeId = params.nodes[0];
                const nodeData = nodes.get(selectedNodeId);

                if (nodeData && detailsDiv) {
                    // Display original (unwrapped) ID in details for clarity if needed
                    // Or use nodeData.label which has the wrapped text
                    detailsDiv.innerHTML = `
                        <h3>${nodeData.id} Details</h3> <!-- Show original ID -->
                        <p><b>Type:</b> ${nodeData.node_type || 'unknown'}</p>
                        <p><b>Description:</b><br>${nodeData.description || 'N/A'}</p>
                    `;
                } else {
                     detailsDiv.innerHTML = "<h3>Node Details</h3><p>Details not found.</p>";
                }

                if (currentlyShownOtherEdges.length > 0) {
                    allEdges.remove(currentlyShownOtherEdges);
                    currentlyShownOtherEdges = [];
                }

                const relevantEdges = otherEdgesData.filter(edge => edge.from === selectedNodeId || edge.to === selectedNodeId);
                if (relevantEdges.length > 0) {
                    allEdges.add(relevantEdges);
                    currentlyShownOtherEdges = relevantEdges.map(edge => edge.id);
                    detailsDiv.innerHTML += `<p><b>Other Relationships Shown (${relevantEdges.length})</b></p>`;
                }
            }
        });

         network.on("deselectNode", function (params) {
             detailsDiv.innerHTML = "<h3>Node Details</h3><p>Select a node to see details.</p>";
             if (currentlyShownOtherEdges.length > 0) {
                 allEdges.remove(currentlyShownOtherEdges);
                 currentlyShownOtherEdges = [];
             }
         });

         network.on("click", function (params) {
             if (params.nodes.length === 0 && params.edges.length === 0) {
                 detailsDiv.innerHTML = "<h3>Node Details</h3><p>Select a node to see details.</p>";
                 if (currentlyShownOtherEdges.length > 0) {
                     allEdges.remove(currentlyShownOtherEdges);
                     currentlyShownOtherEdges = [];
                 }
             }
         });

    </script>
</body>
</html>
