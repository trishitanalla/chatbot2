{
  "nodes": [
    {
      "id": "I Supervised learning",
      "type": "major",
      "parent": null,
      "description": "Overview of supervised learning techniques in machine learning."
    },
    {
      "id": "1 Linear regression",
      "type": "subnode",
      "parent": "I Supervised learning",
      "description": "Introduction to linear regression and its applications."
    },
    {
      "id": "1.1 LMS algorithm",
      "type": "subnode",
      "parent": "1 Linear regression",
      "description": "Least Mean Squares (LMS) algorithm for updating weights in linear models."
    },
    {
      "id": "1.2 The normal equations",
      "type": "subnode",
      "parent": "1 Linear regression",
      "description": "Derivation and use of the normal equations to solve linear regression problems."
    },
    {
      "id": "1.2.1 Matrix derivatives",
      "type": "subnode",
      "parent": "1.2 The normal equations",
      "description": "Calculation of matrix derivatives relevant for solving normal equations."
    },
    {
      "id": "1.2.2 Least squares revisited",
      "type": "subnode",
      "parent": "1.2 The normal equations",
      "description": "Revisiting least squares method in the context of linear regression."
    },
    {
      "id": "1.3 Probabilistic interpretation",
      "type": "subnode",
      "parent": "1 Linear regression",
      "description": "Probabilistic view of linear regression models and assumptions."
    },
    {
      "id": "1.4 Locally weighted linear regression (optional reading)",
      "type": "subnode",
      "parent": "1 Linear regression",
      "description": "Advanced topic: locally weighted linear regression for non-stationary data."
    },
    {
      "id": "2 Classification and logistic regression",
      "type": "subnode",
      "parent": "I Supervised learning",
      "description": "Introduction to classification problems and logistic regression models."
    },
    {
      "id": "2.1 Logistic regression",
      "type": "subnode",
      "parent": "2 Classification and logistic regression",
      "description": "Logistic regression model for binary classification tasks."
    },
    {
      "id": "2.2 Digression: the perceptron learning algorithm",
      "type": "subnode",
      "parent": "2 Classification and logistic regression",
      "description": "Discussion on the perceptron learning algorithm as a precursor to modern classifiers."
    },
    {
      "id": "2.3 Multi-class classification",
      "type": "subnode",
      "parent": "2 Classification and logistic regression",
      "description": "Techniques for extending binary classifiers to multi-class problems."
    },
    {
      "id": "2.4 Another algorithm for maximizing \\(\\ell(\\theta)\\)",
      "type": "subnode",
      "parent": "2 Classification and logistic regression",
      "description": "Alternative methods for optimizing the likelihood function in classification models."
    },
    {
      "id": "3 Generalized linear models",
      "type": "subnode",
      "parent": "I Supervised learning",
      "description": "Introduction to generalized linear models (GLMs) extending beyond linear and logistic regression."
    },
    {
      "id": "3.1 The exponential family",
      "type": "subnode",
      "parent": "3 Generalized linear models",
      "description": "Overview of the exponential family distributions used in GLMs."
    },
    {
      "id": "3.2 Constructing GLMs",
      "type": "subnode",
      "parent": "3 Generalized linear models",
      "description": "Steps and methods for constructing generalized linear models."
    },
    {
      "id": "3.2.1 Ordinary least squares",
      "type": "subnode",
      "parent": "3.2 Constructing GLMs",
      "description": "Ordinary least squares method in the context of GLMs."
    },
    {
      "id": "3.2.2 Logistic regression",
      "type": "subnode",
      "parent": "3.2 Constructing GLMs",
      "description": "Logistic regression as a specific case of generalized linear models."
    },
    {
      "id": "4 Generative learning algorithms",
      "type": "subnode",
      "parent": "I Supervised learning",
      "description": "Introduction to generative approaches in machine learning for classification tasks."
    },
    {
      "id": "4.1 Gaussian discriminant analysis",
      "type": "subnode",
      "parent": "4 Generative learning algorithms",
      "description": "Gaussian Discriminant Analysis (GDA) model for binary and multi-class classification."
    },
    {
      "id": "4.1.1 The multivariate normal distribution",
      "type": "subnode",
      "parent": "4.1 Gaussian discriminant analysis",
      "description": "Properties of the multivariate normal distribution used in GDA."
    },
    {
      "id": "4.1.2 The Gaussian discriminant analysis model",
      "type": "subnode",
      "parent": "4.1 Gaussian discriminant analysis",
      "description": "Formulation and application of the Gaussian Discriminant Analysis model."
    },
    {
      "id": "4.1.3 Discussion: GDA and logistic regression",
      "type": "subnode",
      "parent": "4.1 Gaussian discriminant analysis",
      "description": "Comparison between GDA and logistic regression models."
    },
    {
      "id": "4.2 Naive bayes (Option Reading)",
      "type": "subnode",
      "parent": "4 Generative learning algorithms",
      "description": "Introduction to the naive Bayes classifier for text classification tasks."
    },
    {
      "id": "4.2.1 Laplace smoothing",
      "type": "subnode",
      "parent": "4.2 Naive bayes (Option Reading)",
      "description": "Technique of Laplace smoothing in naive Bayes classifiers."
    },
    {
      "id": "4.2.2 Event models for text classification",
      "type": "subnode",
      "parent": "4.2 Naive bayes (Option Reading)",
      "description": "Event models used in naive Bayes for text classification tasks."
    },
    {
      "id": "5 Kernel methods",
      "type": "subnode",
      "parent": "I Supervised learning",
      "description": "Introduction to kernel methods and their applications in machine learning."
    },
    {
      "id": "5.1 Feature maps",
      "type": "subnode",
      "parent": "5 Kernel methods",
      "description": "Concept of feature mapping in the context of kernel methods."
    },
    {
      "id": "5.2 LMS (least mean squares) with features",
      "type": "subnode",
      "parent": "5 Kernel methods",
      "description": "Application of LMS algorithm using feature maps."
    },
    {
      "id": "5.3 LMS with the kernel trick",
      "type": "subnode",
      "parent": "5 Kernel methods",
      "description": "Use of the kernel trick in LMS for non-linear data."
    },
    {
      "id": "5.4 Properties of kernels",
      "type": "subnode",
      "parent": "5 Kernel methods",
      "description": "Properties and characteristics of valid kernels in machine learning."
    },
    {
      "id": "6 Support vector machines",
      "type": "subnode",
      "parent": "I Supervised learning",
      "description": "Introduction to support vector machines (SVMs) for classification tasks."
    },
    {
      "id": "6.1 Margins: intuition",
      "type": "subnode",
      "parent": "6 Support vector machines",
      "description": "Intuitive explanation of margins in SVMs."
    },
    {
      "id": "II Deep learning",
      "type": "major",
      "parent": null,
      "description": "Overview of deep learning concepts and techniques."
    },
    {
      "id": "7 Deep learning",
      "type": "subnode",
      "parent": "II Deep learning",
      "description": "Introduction to the field of deep learning including neural networks and backpropagation."
    },
    {
      "id": "Modern Neural Networks",
      "type": "major",
      "parent": null,
      "description": "Overview of modern neural network concepts and techniques."
    },
    {
      "id": "Modules in Modern Neural Networks",
      "type": "subnode",
      "parent": "Modern Neural Networks",
      "description": "Discussion on the components that make up contemporary neural networks."
    },
    {
      "id": "Backpropagation",
      "type": "subnode",
      "parent": "Modern Neural Networks",
      "description": "Introduction to backpropagation for efficient gradient computation in neural networks."
    },
    {
      "id": "Preliminaries on partial derivatives",
      "type": "subnode",
      "parent": "Backpropagation",
      "description": "Introduction to the mathematical concepts needed for backpropagation."
    },
    {
      "id": "General strategy of backpropagation",
      "type": "subnode",
      "parent": "Backpropagation",
      "description": "Overview of the algorithmic steps in backpropagation."
    },
    {
      "id": "Backward functions for basic modules",
      "type": "subnode",
      "parent": "Backpropagation",
      "description": "Details on how to implement backward functions for simple network components."
    },
    {
      "id": "Back-propagation for MLPs",
      "type": "subnode",
      "parent": "Backpropagation",
      "description": "Specific implementation of backpropagation in multi-layer perceptrons."
    },
    {
      "id": "Vectorization over training examples",
      "type": "subnode",
      "parent": "Modern Neural Networks",
      "description": "Techniques for efficient computation using vector operations."
    },
    {
      "id": "Generalization and regularization",
      "type": "major",
      "parent": null,
      "description": "Focuses on methods to improve model performance on unseen data."
    },
    {
      "id": "Generalization",
      "type": "subnode",
      "parent": "Generalization and regularization",
      "description": "Exploration of concepts that enhance a model's ability to generalize from training data."
    },
    {
      "id": "Bias-variance tradeoff",
      "type": "subnode",
      "parent": "Generalization",
      "description": "Analysis of the balance between model complexity and error due to variance or bias."
    },
    {
      "id": "A mathematical decomposition (for regression)",
      "type": "subnode",
      "parent": "Bias-variance tradeoff",
      "description": "Mathematical breakdown for understanding regression models' performance."
    },
    {
      "id": "The double descent phenomenon",
      "type": "subnode",
      "parent": "Generalization",
      "description": "Observation of model performance as complexity increases, showing a second peak and decline in error."
    },
    {
      "id": "Sample complexity bounds (optional readings)",
      "type": "subnode",
      "parent": "Generalization",
      "description": "Theoretical limits on the number of samples needed for learning tasks."
    },
    {
      "id": "Preliminaries",
      "type": "subnode",
      "parent": "Sample complexity bounds (optional readings)",
      "description": "Introduction to necessary concepts and definitions."
    },
    {
      "id": "The case of finite H",
      "type": "subnode",
      "parent": "Sample complexity bounds (optional readings)",
      "description": "Analysis when the hypothesis space is limited in size."
    },
    {
      "id": "The case of infinite H",
      "type": "subnode",
      "parent": "Sample complexity bounds (optional readings)",
      "description": "Discussion on scenarios where the hypothesis space is unbounded."
    },
    {
      "id": "Regularization and model selection",
      "type": "major",
      "parent": null,
      "description": "Techniques to prevent overfitting by penalizing overly complex models or selecting optimal ones."
    },
    {
      "id": "Regularization",
      "type": "subnode",
      "parent": "Regularization and model selection",
      "description": "Techniques that add penalty terms to loss functions to prevent overfitting."
    },
    {
      "id": "Implicit regularization effect (optional reading)",
      "type": "subnode",
      "parent": "Regularization and model selection",
      "description": "Exploration of how certain algorithms inherently regularize models."
    },
    {
      "id": "Model selection via cross validation",
      "type": "subnode",
      "parent": "Regularization and model selection",
      "description": "Use of cross-validation to choose the best model from a set of candidates."
    },
    {
      "id": "Bayesian statistics and regularization",
      "type": "subnode",
      "parent": "Regularization and model selection",
      "description": "Application of Bayesian principles in regularizing models."
    },
    {
      "id": "Unsupervised learning",
      "type": "major",
      "parent": null,
      "description": "Techniques for learning from data without labeled responses."
    },
    {
      "id": "Clustering and the k-means algorithm",
      "type": "subnode",
      "parent": "Unsupervised learning",
      "description": "Introduction to clustering techniques with a focus on the k-means method."
    },
    {
      "id": "EM algorithms",
      "type": "subnode",
      "parent": "Unsupervised learning",
      "description": "Exploration of Expectation-Maximization algorithms for probabilistic modeling."
    },
    {
      "id": "EM for mixture of Gaussians",
      "type": "subnode",
      "parent": "EM algorithms",
      "description": "Application of EM to model data with a mixture of Gaussian distributions."
    },
    {
      "id": "Jensen's inequality",
      "type": "subnode",
      "parent": "EM algorithms",
      "description": "Mathematical principle used in the derivation and understanding of EM algorithms."
    },
    {
      "id": "General EM algorithms",
      "type": "subnode",
      "parent": "EM algorithms",
      "description": "Extension of basic EM to more complex scenarios, including variational inference."
    },
    {
      "id": "Other interpretation of ELBO",
      "type": "subnode",
      "parent": "General EM algorithms",
      "description": "Alternative perspectives on the Evidence Lower Bound (ELBO) in variational inference."
    },
    {
      "id": "Mixture of Gaussians revisited",
      "type": "subnode",
      "parent": "EM algorithms",
      "description": "Re-examination and advanced topics related to Gaussian mixture models using EM."
    },
    {
      "id": "Variational inference and variational auto-encoder (optional reading)",
      "type": "subnode",
      "parent": "EM algorithms",
      "description": "Advanced topic on probabilistic modeling with variational methods and neural networks."
    },
    {
      "id": "Principal components analysis",
      "type": "subnode",
      "parent": "Unsupervised learning",
      "description": "Dimensionality reduction technique that identifies principal components in data."
    },
    {
      "id": "Independent components analysis",
      "type": "subnode",
      "parent": "Unsupervised learning",
      "description": "Technique for separating mixed signals into independent sources."
    },
    {
      "id": "ICA ambiguities",
      "type": "subnode",
      "parent": "Independent components analysis",
      "description": "Discussion on the inherent limitations and issues in ICA solutions."
    },
    {
      "id": "Densities and linear transformations",
      "type": "subnode",
      "parent": "Independent components analysis",
      "description": "Analysis of how densities change under linear transformations relevant to ICA."
    },
    {
      "id": "ICA algorithm",
      "type": "subnode",
      "parent": "Independent components analysis",
      "description": "Detailed explanation of the Independent Components Analysis procedure."
    },
    {
      "id": "Self-supervised learning and foundation models",
      "type": "major",
      "parent": null,
      "description": "Approaches to training models with self-generated supervision signals and foundational architectures."
    },
    {
      "id": "Pretraining and adaptation",
      "type": "subnode",
      "parent": "Self-supervised learning and foundation models",
      "description": "Overview of the process of pre-training large models followed by fine-tuning on specific tasks."
    },
    {
      "id": "Pretraining methods in computer vision",
      "type": "subnode",
      "parent": "Self-supervised learning and foundation models",
      "description": "Exploration of self-supervised techniques for image data."
    },
    {
      "id": "Pretrained large language models",
      "type": "subnode",
      "parent": "Self-supervised learning and foundation models",
      "description": "Discussion on the development and applications of pre-trained language models."
    },
    {
      "id": "Open up the blackbox of Transformers",
      "type": "subnode",
      "parent": "Pretrained large language models",
      "description": "Insight into the architecture and workings of Transformer-based models."
    },
    {
      "id": "Zero-shot learning and in-context learning",
      "type": "subnode",
      "parent": "Pretrained large language models",
      "description": "Exploration of capabilities for learning without explicit training data or with minimal context."
    },
    {
      "id": "Reinforcement Learning and Control",
      "type": "major",
      "parent": null,
      "description": "Techniques for agents to learn optimal actions through interaction in an environment."
    },
    {
      "id": "Reinforcement learning",
      "type": "subnode",
      "parent": "Reinforcement Learning and Control",
      "description": "Introduction to the principles of reinforcement learning and its applications."
    },
    {
      "id": "Markov decision processes",
      "type": "subnode",
      "parent": "Reinforcement learning",
      "description": "Foundation for understanding state transitions and rewards in RL problems."
    },
    {
      "id": "Value iteration and policy iteration",
      "type": "subnode",
      "parent": "Reinforcement learning",
      "description": "Algorithms that compute optimal policies or value functions through iterations."
    },
    {
      "id": "Learning a model for an MDP",
      "type": "subnode",
      "parent": "Reinforcement learning",
      "description": "Techniques to infer the dynamics of an environment from observations."
    },
    {
      "id": "Continuous state MDPs",
      "type": "subnode",
      "parent": "Reinforcement learning",
      "description": "Handling environments with a continuum of states rather than discrete ones."
    },
    {
      "id": "Discretization",
      "type": "subnode",
      "parent": "Continuous state MDPs",
      "description": "Method to approximate continuous state spaces by dividing them into discrete bins."
    },
    {
      "id": "Value function approximation",
      "type": "subnode",
      "parent": "Continuous state MDPs",
      "description": "Techniques for approximating value functions in large or continuous state spaces"
    },
    {
      "id": "Connections between Policy and Value Iteration (Optional)",
      "type": "subnode",
      "parent": "Reinforcement learning",
      "description": "Analysis of the interplay and equivalences between policy and value iteration methods."
    },
    {
      "id": "LQR, DDP and LQG",
      "type": "major",
      "parent": null,
      "description": "Advanced control techniques for linear systems with quadratic costs."
    },
    {
      "id": "Finite-horizon MDPs",
      "type": "subnode",
      "parent": "LQR, DDP and LQG",
      "description": "Formulation of decision-making problems in environments with a limited time horizon."
    },
    {
      "id": "Linear Quadratic Regulation (LQR)",
      "type": "subnode",
      "parent": "LQR, DDP and LQG",
      "description": "Special case of finite-horizon setting with tractable exact solutions used in robotics."
    },
    {
      "id": "From non-linear dynamics to LQR",
      "type": "subnode",
      "parent": "LQR, DDP and LQG",
      "description": "Approach to converting non-linear dynamics into a form suitable for LQR analysis"
    },
    {
      "id": "Linearization of dynamics",
      "type": "subnode",
      "parent": "From non-linear dynamics to LQR",
      "description": "Process of approximating non-linear systems with linear models"
    },
    {
      "id": "Differential Dynamic Programming (DDP)",
      "type": "subnode",
      "parent": "From non-linear dynamics to LQR",
      "description": "Optimization technique for nonlinear control problems"
    },
    {
      "id": "Linear Quadratic Gaussian (LQG)",
      "type": "subnode",
      "parent": "LQR, DDP and LQG",
      "description": "Control theory framework combining LQR with stochastic noise models"
    },
    {
      "id": "Policy Gradient (REINFORCE)",
      "type": "major",
      "parent": null,
      "description": "Method for learning policies in reinforcement learning using gradient ascent on the expected reward"
    },
    {
      "id": "Supervised Learning Examples",
      "type": "major",
      "parent": null,
      "description": "Introduction to supervised learning through examples of predicting house prices based on living area"
    },
    {
      "id": "Supervised Learning Problem",
      "type": "major",
      "parent": null,
      "description": "Formal description of the goal in supervised learning"
    },
    {
      "id": "Regression",
      "type": "subnode",
      "parent": "Supervised Learning Problem",
      "description": "Learning problem with continuous target variable"
    },
    {
      "id": "Classification",
      "type": "subnode",
      "parent": "Supervised Learning Problem",
      "description": "Learning problem with discrete target variables"
    },
    {
      "id": "Hypothesis",
      "type": "subnode",
      "parent": "Supervised Learning Problem",
      "description": "Function learned by the model to predict y from x"
    },
    {
      "id": "Linear Regression",
      "type": "major",
      "parent": "Machine Learning Concepts",
      "description": "Basic supervised learning method for predicting continuous values based on input features."
    },
    {
      "id": "Housing Example",
      "type": "subnode",
      "parent": "Linear Regression",
      "description": "Example dataset with living area and number of bedrooms as features"
    },
    {
      "id": "Feature Selection",
      "type": "subnode",
      "parent": "Linear Regression",
      "description": "Process of choosing which features to include in the model"
    },
    {
      "id": "MachineLearningBasics",
      "type": "major",
      "parent": null,
      "description": "Fundamental concepts in machine learning including models and loss functions."
    },
    {
      "id": "FunctionRepresentation",
      "type": "subnode",
      "parent": "MachineLearningBasics",
      "description": "How functions and hypotheses are represented in machine learning models."
    },
    {
      "id": "LinearHypothesis",
      "type": "subnode",
      "parent": "FunctionRepresentation",
      "description": "Introduction to linear hypothesis functions used in regression problems."
    },
    {
      "id": "ParametersWeights",
      "type": "subnode",
      "parent": "LinearHypothesis",
      "description": "Explanation of parameters and weights in the context of linear models."
    },
    {
      "id": "CostFunction",
      "type": "subnode",
      "parent": "MachineLearningBasics",
      "description": "Definition and purpose of a cost function in machine learning."
    },
    {
      "id": "OrdinaryLeastSquares",
      "type": "subnode",
      "parent": "CostFunction",
      "description": "Explanation of ordinary least squares regression as an example of minimizing the cost function."
    },
    {
      "id": "LMSAlgorithm",
      "type": "major",
      "parent": null,
      "description": "Introduction to the Least Mean Squares algorithm for parameter optimization."
    },
    {
      "id": "Machine_Learning_Algorithms",
      "type": "major",
      "parent": null,
      "description": "Collection of algorithms used in machine learning for prediction and decision making."
    },
    {
      "id": "Gradient_Descent",
      "type": "subnode",
      "parent": "Machine_Learning_Algorithms",
      "description": "Optimization algorithm that minimizes a function by iteratively moving towards the minimum value of that function."
    },
    {
      "id": "Learning_Rate",
      "type": "subnode",
      "parent": "Gradient_Descent",
      "description": "Hyperparameter in gradient descent determining the step size at each iteration."
    },
    {
      "id": "Cost_Function_J_theta",
      "type": "subnode",
      "parent": "Gradient_Descent",
      "description": "Function that measures the performance of a machine learning model for given data and parameters."
    },
    {
      "id": "Partial_Derivative_Calculation",
      "type": "subnode",
      "parent": "Gradient_Descent",
      "description": "Calculation of partial derivatives to update parameters in gradient descent."
    },
    {
      "id": "LMS_Update_Rule",
      "type": "subnode",
      "parent": "Gradient_Descent",
      "description": "Rule for updating model parameters based on the least mean squares method."
    },
    {
      "id": "LMS_Rule",
      "type": "major",
      "parent": null,
      "description": "Least Mean Squares update rule for adjusting parameters in machine learning."
    },
    {
      "id": "Widrow_Hoff_Learning_Rule",
      "type": "subnode",
      "parent": "LMS_Rule",
      "description": "Alternative name for the LMS update rule."
    },
    {
      "id": "Error_Term",
      "type": "subnode",
      "parent": "LMS_Rule",
      "description": "The difference between actual and predicted values, guiding parameter updates."
    },
    {
      "id": "Single_Training_Example",
      "type": "subnode",
      "parent": "LMS_Rule",
      "description": "Derivation of LMS rule for a single training example scenario."
    },
    {
      "id": "Batch_Gradient_Descent",
      "type": "major",
      "parent": null,
      "description": "Method that uses the entire dataset to update parameters in each iteration."
    },
    {
      "id": "LinearRegressionOptimization",
      "type": "major",
      "parent": null,
      "description": "Exploration of the optimization problem in linear regression."
    },
    {
      "id": "GradientDescentConvergence",
      "type": "subnode",
      "parent": "LinearRegressionOptimization",
      "description": "Discussion on how gradient descent converges to a global minimum for convex functions."
    },
    {
      "id": "BatchGradientDescentExample",
      "type": "subnode",
      "parent": "LinearRegressionOptimization",
      "description": "Illustration of batch gradient descent with specific parameters and results."
    },
    {
      "id": "StochasticGradientDescent",
      "type": "subnode",
      "parent": "LinearRegressionOptimization",
      "description": "Variant of gradient descent that uses a single or subset of data points to update the model parameters."
    },
    {
      "id": "MachineLearningOptimization",
      "type": "major",
      "parent": null,
      "description": "Methods for minimizing cost functions in machine learning."
    },
    {
      "id": "BatchGradientDescent",
      "type": "subnode",
      "parent": "MachineLearningOptimization",
      "description": "Description of the batch gradient descent algorithm in machine learning."
    },
    {
      "id": "NormalEquationsMethod",
      "type": "subnode",
      "parent": "MachineLearningOptimization",
      "description": "Explicit minimization of cost function without iterative algorithm."
    },
    {
      "id": "MatrixDerivatives",
      "type": "subnode",
      "parent": "NormalEquationsMethod",
      "description": "Notation for calculus with matrices in machine learning context."
    },
    {
      "id": "Matrix Derivatives",
      "type": "major",
      "parent": null,
      "description": "Derivation of matrix derivatives for functions mapping matrices to real numbers."
    },
    {
      "id": "Gradient Calculation",
      "type": "subnode",
      "parent": "Matrix Derivatives",
      "description": "Calculation of the gradient for a given function and matrix."
    },
    {
      "id": "Least Squares Revisited",
      "type": "major",
      "parent": null,
      "description": "Revisiting least squares using matrix derivatives."
    },
    {
      "id": "Design Matrix",
      "type": "subnode",
      "parent": "Least Squares Revisited",
      "description": "Matrix containing training examples' input values in its rows."
    },
    {
      "id": "Target Vector",
      "type": "subnode",
      "parent": "Least Squares Revisited",
      "description": "Vector containing target values from the training set."
    },
    {
      "id": "MachineLearningOverview",
      "type": "major",
      "parent": null,
      "description": "Introduction to machine learning concepts and EM algorithm overview."
    },
    {
      "id": "LinearRegression",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Algorithm for modeling the relationship between a scalar dependent variable and one or more explanatory variables."
    },
    {
      "id": "CostFunctionJ",
      "type": "subnode",
      "parent": "LinearRegression",
      "description": "Definition and minimization process of the cost function J(theta)."
    },
    {
      "id": "NormalEquations",
      "type": "subnode",
      "parent": "CostFunctionJ",
      "description": "Derivation and explanation of normal equations for finding theta that minimizes J(theta)."
    },
    {
      "id": "InvertibleMatrixAssumption",
      "type": "subnode",
      "parent": "LinearRegression",
      "description": "Assumption that $X^{T}X$ is invertible for solving linear equations."
    },
    {
      "id": "ProbabilisticInterpretation",
      "type": "major",
      "parent": null,
      "description": "Explanation of the probabilistic assumptions underlying least-squares regression."
    },
    {
      "id": "RegressionProblem",
      "type": "subnode",
      "parent": "ProbabilisticInterpretation",
      "description": "Discussion on why linear regression is a reasonable choice for regression problems."
    },
    {
      "id": "ErrorTermAssumption",
      "type": "subnode",
      "parent": "ProbabilisticInterpretation",
      "description": "Assumption that error terms are IID and follow a Gaussian distribution with mean zero."
    },
    {
      "id": "GaussianDistribution",
      "type": "subnode",
      "parent": "ErrorTermAssumption",
      "description": "Description of the Gaussian distribution used for modeling errors in regression analysis."
    },
    {
      "id": "ProbabilisticModeling",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Models that use probability distributions to describe data generation processes."
    },
    {
      "id": "ConditionalProbabilityDistribution",
      "type": "subnode",
      "parent": "ProbabilisticModeling",
      "description": "Describes the distribution of one random variable given another."
    },
    {
      "id": "DesignMatrixX",
      "type": "subnode",
      "parent": "ProbabilisticModeling",
      "description": "Contains all input data vectors for a model."
    },
    {
      "id": "LikelihoodFunction",
      "type": "subnode",
      "parent": "ProbabilisticModeling",
      "description": "Probability of observed data given parameters, viewed as function of parameters."
    },
    {
      "id": "IndependenceAssumption",
      "type": "subnode",
      "parent": "ProbabilisticModeling",
      "description": "Simplifying assumption that training examples are independent, though this may not hold for time series or correlated data."
    },
    {
      "id": "MaximumLikelihoodEstimation",
      "type": "subnode",
      "parent": "ProbabilisticModeling",
      "description": "Justification of least squares regression using maximum likelihood estimation under probabilistic assumptions."
    },
    {
      "id": "LogLikelihoodFunction",
      "type": "subnode",
      "parent": "MaximumLikelihoodEstimation",
      "description": "Natural logarithm of the likelihood function, simplifies calculations."
    },
    {
      "id": "LogLikelihood",
      "type": "subnode",
      "parent": "LikelihoodFunction",
      "description": "Measure of how likely a given set of parameters is to produce observed data, crucial for model fitting."
    },
    {
      "id": "MaximizingLogLikelihood",
      "type": "subnode",
      "parent": "LogLikelihood",
      "description": "Process of maximizing the likelihood function to estimate model parameters."
    },
    {
      "id": "LeastSquaresRegression",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Linear regression technique that minimizes the sum of squared residuals."
    },
    {
      "id": "LocallyWeightedLinearRegression",
      "type": "major",
      "parent": "MachineLearningConcepts",
      "description": "Algorithm that assigns weights to training examples based on proximity to the query point."
    },
    {
      "id": "MachineLearningConcepts",
      "type": "major",
      "parent": null,
      "description": "Overview of machine learning concepts including convolutional neural networks and backpropagation."
    },
    {
      "id": "PolynomialFeatures",
      "type": "subnode",
      "parent": "LinearRegression",
      "description": "Adding polynomial terms to improve model fit."
    },
    {
      "id": "Underfitting",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Occurs when a model is too simple to capture underlying patterns in data."
    },
    {
      "id": "Overfitting",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Result of using too complex models, needs proper model complexity for optimal bias-variance tradeoff"
    },
    {
      "id": "FeatureSelection",
      "type": "subnode",
      "parent": "LinearRegression",
      "description": "Choosing relevant features to improve model performance."
    },
    {
      "id": "WeightsCalculation",
      "type": "subnode",
      "parent": "LocallyWeightedLinearRegression",
      "description": "Method for calculating weights using a Gaussian-like function of distance from the query point."
    },
    {
      "id": "BandwidthParameter",
      "type": "subnode",
      "parent": "WeightsCalculation",
      "description": "Controls how quickly weight decreases with distance, affecting model flexibility and overfitting."
    },
    {
      "id": "Machine Learning Concepts",
      "type": "major",
      "parent": null,
      "description": "General concepts in machine learning including training and performance considerations."
    },
    {
      "id": "Locally Weighted Linear Regression",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "A non-linear learning algorithm for estimating future states based on current state and action."
    },
    {
      "id": "Classification Problem",
      "type": "major",
      "parent": null,
      "description": "Task of predicting discrete values for input data."
    },
    {
      "id": "Binary Classification",
      "type": "subnode",
      "parent": "Classification Problem",
      "description": "Example of classifying emails into two categories (spam or not-spam)"
    },
    {
      "id": "Logistic Regression",
      "type": "subnode",
      "parent": "Classification Problem",
      "description": "Generalizing Newton's method for multidimensional settings in logistic regression."
    },
    {
      "id": "Machine_Learning",
      "type": "major",
      "parent": null,
      "description": "Study of algorithms and statistical models for computer systems to perform tasks without explicit instructions."
    },
    {
      "id": "Logistic_Regression",
      "type": "subnode",
      "parent": "Machine_Learning",
      "description": "Statistical method for binary classification problems using the logistic function."
    },
    {
      "id": "Linear_Regression",
      "type": "subnode",
      "parent": "Machine_Learning",
      "description": "Method for modeling relationships between a scalar response and one or more explanatory variables."
    },
    {
      "id": "Logistic_Function",
      "type": "subnode",
      "parent": "Logistic_Regression",
      "description": "S-shaped curve function used in logistic regression to model probabilities."
    },
    {
      "id": "Sigmoid_Function",
      "type": "subnode",
      "parent": "Logistic_Function",
      "description": "Alternative name for the logistic function, named due to its S-shape."
    },
    {
      "id": "Derivative_of_Sigmoid",
      "type": "subnode",
      "parent": "Sigmoid_Function",
      "description": "Mathematical expression describing how the sigmoid changes with input."
    },
    {
      "id": "ClassificationModelAssumptions",
      "type": "subnode",
      "parent": "MaximumLikelihoodEstimation",
      "description": "Probabilistic assumptions for classification models in machine learning."
    },
    {
      "id": "LogisticRegressionFunction",
      "type": "subnode",
      "parent": "ClassificationModelAssumptions",
      "description": "Mathematical function representing the probability of class membership given input features."
    },
    {
      "id": "GradientAscentUpdateRule",
      "type": "subnode",
      "parent": "MaximumLikelihoodEstimation",
      "description": "Algorithm for updating model parameters in the direction that increases likelihood."
    },
    {
      "id": "LogisticRegression",
      "type": "subnode",
      "parent": "MachineLearningBasics",
      "description": "Discriminative model used to predict the probability of a binary outcome based on input variables."
    },
    {
      "id": "GradientAscentRule",
      "type": "subnode",
      "parent": "LogisticRegression",
      "description": "Update rule for parameters in logistic regression."
    },
    {
      "id": "LMSUpdateRule",
      "type": "subnode",
      "parent": "LogisticRegression",
      "description": "Least Mean Squares update rule comparison with logistic gradient ascent."
    },
    {
      "id": "LogisticLossFunction",
      "type": "subnode",
      "parent": "LogisticRegression",
      "description": "Definition and properties of the logistic loss function."
    },
    {
      "id": "NegativeLogLikelihood",
      "type": "subnode",
      "parent": "LogisticLossFunction",
      "description": "Connection between negative log-likelihood and logistic loss."
    },
    {
      "id": "Logit",
      "type": "subnode",
      "parent": "LogisticLossFunction",
      "description": "Definition of the logit in relation to logistic regression."
    },
    {
      "id": "Logistic Regression Derivation",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Derivation and interpretation of logistic regression equations."
    },
    {
      "id": "Perceptron Learning Algorithm",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Historical algorithm for binary classification with a threshold function."
    },
    {
      "id": "Multi-class Classification",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Classification problems where the response variable can take on multiple values."
    },
    {
      "id": "Multinomial Distribution",
      "type": "subnode",
      "parent": "2.3 Multi-class classification",
      "description": "Probability distribution over non-negative integers used for modeling features with multiple categories"
    },
    {
      "id": "Parameterized Model",
      "type": "subnode",
      "parent": "2.3 Multi-class classification",
      "description": "Model outputs probabilities for each of the k outcomes given input x"
    },
    {
      "id": "Softmax Function",
      "type": "subnode",
      "parent": "2.3 Multi-class classification",
      "description": "Function that transforms a vector of real numbers into a probability distribution"
    },
    {
      "id": "Logits",
      "type": "subnode",
      "parent": "Softmax Function",
      "description": "Inputs to the softmax function before transformation"
    },
    {
      "id": "Probability Vector",
      "type": "subnode",
      "parent": "Softmax Function",
      "description": "Output of softmax, a vector with nonnegative entries summing to 1"
    },
    {
      "id": "Probabilistic Model",
      "type": "major",
      "parent": null,
      "description": "Model using softmax outputs as probabilities for different outcomes given input x and parameters \u03b8"
    },
    {
      "id": "Negative Log-Likelihood",
      "type": "subnode",
      "parent": "Probabilistic Model",
      "description": "Measure of model's performance on a single example (x,y)"
    },
    {
      "id": "Loss Function",
      "type": "major",
      "parent": null,
      "description": "Function measuring discrepancy between predicted and actual values."
    },
    {
      "id": "Cross-Entropy Loss",
      "type": "subnode",
      "parent": "Loss Function",
      "description": "Alternative definition for the loss function using cross-entropy"
    },
    {
      "id": "Machine_Learning_Concepts",
      "type": "major",
      "parent": null,
      "description": "Overview of concepts in machine learning including loss functions and optimization."
    },
    {
      "id": "Cross_Entropy_Loss",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Function used to measure the performance of a classification model whose output is a probability value between 0 and 1."
    },
    {
      "id": "Softmax_Function",
      "type": "subnode",
      "parent": "Cross_Entropy_Loss",
      "description": "Transformation that converts raw scores into probabilities for each class in multi-class classification problems."
    },
    {
      "id": "Gradient_Calculation",
      "type": "subnode",
      "parent": "Cross_Entropy_Loss",
      "description": "Derivation of gradients used in backpropagation to update model parameters during training."
    },
    {
      "id": "Loss_Functions",
      "type": "subnode",
      "parent": "Machine_Learning",
      "description": "Quantitative measures used to evaluate the performance of a model during training."
    },
    {
      "id": "Newton_Method",
      "type": "subnode",
      "parent": "Machine_Learning",
      "description": "Optimization algorithm that uses second-order derivatives to find the minimum of a function."
    },
    {
      "id": "Newton's Method",
      "type": "major",
      "parent": null,
      "description": "An iterative method to find the roots of a real-valued function."
    },
    {
      "id": "Finding Roots",
      "type": "subnode",
      "parent": "Newton's Method",
      "description": "Using Newton's method to solve for \u03b8 where f(\u03b8) = 0."
    },
    {
      "id": "Maximizing Functions",
      "type": "subnode",
      "parent": "Newton's Method",
      "description": "Applying Newton's method to maximize a function by finding its critical points."
    },
    {
      "id": "Hessian Matrix",
      "type": "subnode",
      "parent": "Logistic Regression",
      "description": "Matrix of second-order partial derivatives used to generalize Newton's method."
    },
    {
      "id": "Convergence Rate",
      "type": "subnode",
      "parent": "Newton's Method",
      "description": "Newton's method typically converges faster than gradient descent, requiring fewer iterations."
    },
    {
      "id": "OptimizationMethods",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Techniques for finding optimal parameters in models such as Newton's method."
    },
    {
      "id": "HessianMatrix",
      "type": "subnode",
      "parent": "OptimizationMethods",
      "description": "Second-order derivative matrix used in optimization methods like Newton's method."
    },
    {
      "id": "FisherScoring",
      "type": "subnode",
      "parent": "OptimizationMethods",
      "description": "Application of Newton's method to maximize logistic regression likelihood."
    },
    {
      "id": "GeneralizedLinearModels",
      "type": "major",
      "parent": null,
      "description": "Family of models that includes both regression and classification methods."
    },
    {
      "id": "ExponentialFamilyDistributions",
      "type": "subnode",
      "parent": "GeneralizedLinearModels",
      "description": "A class of probability distributions that includes many common distributions such as Poisson, binomial, etc."
    },
    {
      "id": "NaturalParameter",
      "type": "subnode",
      "parent": "ExponentialFamilyDistributions",
      "description": "The parameter \u03b7 that characterizes the distribution in exponential form."
    },
    {
      "id": "SufficientStatistic",
      "type": "subnode",
      "parent": "ExponentialFamilyDistributions",
      "description": "A function T(y) summarizing data relevant to parameter estimation."
    },
    {
      "id": "LogPartitionFunction",
      "type": "subnode",
      "parent": "ExponentialFamilyDistributions",
      "description": "The function a(\u03b7) ensuring the distribution sums/integrates to 1."
    },
    {
      "id": "BernoulliDistribution",
      "type": "major",
      "parent": "GLMFormulation",
      "description": "A binary random variable with parameter \u03c6 representing success probability."
    },
    {
      "id": "NaturalParameterOfBernoulli",
      "type": "subnode",
      "parent": "BernoulliDistribution",
      "description": "\u03b7 = log(\u03c6/(1-\u03c6)), also known as the sigmoid function."
    },
    {
      "id": "SufficientStatisticForBernoulli",
      "type": "subnode",
      "parent": "BernoulliDistribution",
      "description": "T(y) = y, indicating that the statistic is simply the outcome itself."
    },
    {
      "id": "LogPartitionFunctionOfBernoulli",
      "type": "subnode",
      "parent": "BernoulliDistribution",
      "description": "a(\u03b7) = log(1 + e^\u03b7), ensuring normalization of the distribution."
    },
    {
      "id": "MachineLearningModels",
      "type": "major",
      "parent": null,
      "description": "Overview of machine learning models including GDA and logistic regression."
    },
    {
      "id": "GLMFormulation",
      "type": "subnode",
      "parent": "MachineLearningModels",
      "description": "Formulation of Generalized Linear Models (GLMs) using exponential family distributions."
    },
    {
      "id": "OtherDistributions",
      "type": "subnode",
      "parent": "GLMFormulation",
      "description": "Overview of other distributions in exponential family such as Poisson, Gamma, etc."
    },
    {
      "id": "ConstructingGLMs",
      "type": "major",
      "parent": null,
      "description": "Process for constructing GLMs using various distributions."
    },
    {
      "id": "PoissonDistribution",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "A statistical distribution used for modeling count data such as website visitors."
    },
    {
      "id": "GeneralizedLinearModel",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Models that extend linear regression to accommodate non-normal error distributions and nonlinear relationships."
    },
    {
      "id": "GLMAssumptions",
      "type": "subnode",
      "parent": "GeneralizedLinearModel",
      "description": "Three key assumptions made when constructing GLMs: conditional distribution form, prediction goal, and linear relationship between natural parameter and input variables."
    },
    {
      "id": "GeneralizedLinearModelsGLMs",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "A class of statistical models that includes linear regression, logistic regression, etc."
    },
    {
      "id": "AssumptionsOfGLMs",
      "type": "subnode",
      "parent": "GeneralizedLinearModelsGLMs",
      "description": "Three assumptions/design choices for deriving GLM algorithms."
    },
    {
      "id": "OrdinaryLeastSquaresOLS",
      "type": "subnode",
      "parent": "GeneralizedLinearModelsGLMs",
      "description": "A special case of GLMs where the target variable is continuous and modeled as Gaussian."
    },
    {
      "id": "Machine Learning Models",
      "type": "major",
      "parent": null,
      "description": "Models used in machine learning for prediction and decision-making."
    },
    {
      "id": "Conditional Distribution Modeling",
      "type": "subnode",
      "parent": "Machine Learning Models",
      "description": "Modeling the distribution of y given x."
    },
    {
      "id": "Bernoulli Distribution",
      "type": "subnode",
      "parent": "Conditional Distribution Modeling",
      "description": "Probability distribution for binary outcomes."
    },
    {
      "id": "Exponential Family Distributions",
      "type": "subnode",
      "parent": "Bernoulli Distribution",
      "description": "Family of distributions including Bernoulli, Gaussian, etc."
    },
    {
      "id": "Hypothesis Function",
      "type": "subnode",
      "parent": "Conditional Distribution Modeling",
      "description": "Function predicting the expected value of y given x."
    },
    {
      "id": "Canonical Response Function",
      "type": "subnode",
      "parent": "Exponential Family Distributions",
      "description": "Function relating natural parameter to distribution mean."
    },
    {
      "id": "Canonical Link Function",
      "type": "subnode",
      "parent": "Exponential Family Distributions",
      "description": "Inverse of canonical response function, maps x to natural parameter."
    },
    {
      "id": "MachineLearningAlgorithms",
      "type": "major",
      "parent": null,
      "description": "Overview of different types of machine learning algorithms."
    },
    {
      "id": "DiscriminativeAlgorithms",
      "type": "subnode",
      "parent": "MachineLearningAlgorithms",
      "description": "Algorithms that learn p(y|x) directly to distinguish between classes."
    },
    {
      "id": "PerceptronAlgorithm",
      "type": "subnode",
      "parent": "DiscriminativeAlgorithms",
      "description": "Linear classifier that separates data with a decision boundary."
    },
    {
      "id": "GenerativeAlgorithms",
      "type": "subnode",
      "parent": "MachineLearningAlgorithms",
      "description": "Model p(x|y) and p(y) to derive posterior distribution on y given x."
    },
    {
      "id": "ClassPriors",
      "type": "subnode",
      "parent": "GenerativeAlgorithms",
      "description": "Probability of each class before observing data."
    },
    {
      "id": "BayesRule",
      "type": "subnode",
      "parent": "GenerativeAlgorithms",
      "description": "Uses p(x|y) and p(y) to calculate posterior probability p(y|x)."
    },
    {
      "id": "Bayes Rule Application",
      "type": "major",
      "parent": null,
      "description": "Using Bayes rule to derive posterior distribution on y given x."
    },
    {
      "id": "Class Priors",
      "type": "subnode",
      "parent": "Bayes Rule Application",
      "description": "Probability of each class before observing data."
    },
    {
      "id": "Conditional Probability p(x|y)",
      "type": "subnode",
      "parent": "Bayes Rule Application",
      "description": "Distribution of features given the class label."
    },
    {
      "id": "Gaussian Discriminant Analysis (GDA)",
      "type": "major",
      "parent": null,
      "description": "Generative learning algorithm assuming p(x|y) is multivariate normal distribution."
    },
    {
      "id": "Multivariate Normal Distribution",
      "type": "subnode",
      "parent": "Gaussian Discriminant Analysis (GDA)",
      "description": "Distribution parameterized by mean vector and covariance matrix."
    },
    {
      "id": "Mean Vector",
      "type": "subnode",
      "parent": "Multivariate Normal Distribution",
      "description": "Vector representing the expected value of each feature in d-dimensions."
    },
    {
      "id": "Covariance Matrix",
      "type": "subnode",
      "parent": "Multivariate Normal Distribution",
      "description": "Matrix describing the variance and covariance between features."
    },
    {
      "id": "Random_Variables",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Introduction to random variables and their properties."
    },
    {
      "id": "Normal_Distribution",
      "type": "subnode",
      "parent": "Random_Variables",
      "description": "Properties of the normal distribution in machine learning."
    },
    {
      "id": "Mean",
      "type": "subnode",
      "parent": "Normal_Distribution",
      "description": "Definition and calculation of mean for a normal distribution."
    },
    {
      "id": "Covariance_Matrix",
      "type": "subnode",
      "parent": "Normal_Distribution",
      "description": "Explanation of covariance matrix in the context of multivariate distributions."
    },
    {
      "id": "Standard_Normal_Distribution",
      "type": "subnode",
      "parent": "Normal_Distribution",
      "description": "Definition and properties of standard normal distribution."
    },
    {
      "id": "Density_Properties",
      "type": "subnode",
      "parent": "Normal_Distribution",
      "description": "Properties of density functions for different covariance matrices."
    },
    {
      "id": "CovarianceMatrixEffects",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Exploration of how varying the covariance matrix affects density contours."
    },
    {
      "id": "GaussianDiscriminantAnalysis",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Model that assumes data is generated from a multivariate Gaussian distribution."
    },
    {
      "id": "GDAParameters",
      "type": "subnode",
      "parent": "GaussianDiscriminantAnalysis",
      "description": "Model parameters including phi, mu_0, mu_1, and Sigma."
    },
    {
      "id": "MLEstimation",
      "type": "subnode",
      "parent": "GaussianDiscriminantAnalysis",
      "description": "Process to find maximum likelihood estimates for phi, mu_0, mu_1, and Sigma."
    },
    {
      "id": "DecisionBoundary",
      "type": "subnode",
      "parent": "GaussianDiscriminantAnalysis",
      "description": "The boundary that separates classes in a classification problem."
    },
    {
      "id": "RelationshipToLogisticRegression",
      "type": "subnode",
      "parent": "MachineLearningModels",
      "description": "Connection between GDA and logistic regression models."
    },
    {
      "id": "DecisionBoundaries",
      "type": "subnode",
      "parent": "MachineLearningModels",
      "description": "Boundaries that separate different classes in feature space."
    },
    {
      "id": "ModelAssumptions",
      "type": "subnode",
      "parent": "MachineLearningModels",
      "description": "Theoretical assumptions made by models about the data generation process."
    },
    {
      "id": "AsymptoticEfficiency",
      "type": "subnode",
      "parent": "GaussianDiscriminantAnalysis",
      "description": "Property of GDA that ensures optimal performance with large datasets under correct assumptions."
    },
    {
      "id": "GDA",
      "type": "subnode",
      "parent": "MachineLearningModels",
      "description": "Generative Discriminative Algorithm with strong modeling assumptions."
    },
    {
      "id": "RobustnessToAssumptions",
      "type": "subnode",
      "parent": "MachineLearningModels",
      "description": "Comparison of GDA and Logistic Regression in terms of assumption sensitivity."
    },
    {
      "id": "NaiveBayes",
      "type": "major",
      "parent": null,
      "description": "Classification algorithm for discrete-valued features (optional reading)."
    },
    {
      "id": "DiscreteFeatures",
      "type": "subnode",
      "parent": "NaiveBayes",
      "description": "Introduction to Naive Bayes with focus on discrete feature vectors."
    },
    {
      "id": "EmailSpamFiltering",
      "type": "subnode",
      "parent": "NaiveBayes",
      "description": "Example application of Naive Bayes in email spam detection."
    },
    {
      "id": "Text_Classification",
      "type": "subnode",
      "parent": "Machine_Learning",
      "description": "Techniques for classifying text into categories such as spam or non-spam emails."
    },
    {
      "id": "Spam_Filtering",
      "type": "subnode",
      "parent": "Text_Classification",
      "description": "Application of machine learning to distinguish spam from non-spam emails."
    },
    {
      "id": "Training_Set",
      "type": "subnode",
      "parent": "Spam_Filtering",
      "description": "Set of labeled examples used to train a machine learning model."
    },
    {
      "id": "Feature_Vector",
      "type": "subnode",
      "parent": "Spam_Filtering",
      "description": "Vector representation of an email based on its words."
    },
    {
      "id": "Vocabulary",
      "type": "subnode",
      "parent": "Feature_Vector",
      "description": "Set of unique words used to create the feature vector."
    },
    {
      "id": "Stop_Words",
      "type": "subnode",
      "parent": "Training_Set",
      "description": "Commonly occurring words removed from analysis due to lack of content value."
    },
    {
      "id": "Machine_Learning_Topic",
      "type": "major",
      "parent": null,
      "description": "Overview of machine learning concepts and techniques."
    },
    {
      "id": "Feature_Vector_Selection",
      "type": "subnode",
      "parent": "Text_Classification",
      "description": "Process of selecting relevant features from the text, excluding stop words."
    },
    {
      "id": "Generative_Modeling",
      "type": "subnode",
      "parent": "Text_Classification",
      "description": "Building models that generate data similar to the training set, focusing on conditional probabilities."
    },
    {
      "id": "Naive_Bayes_Assumption",
      "type": "subnode",
      "parent": "Generative_Modeling",
      "description": "Assumption of conditional independence between features given a class label in Naive Bayes classifiers."
    },
    {
      "id": "Conditional_Independence",
      "type": "subnode",
      "parent": "Naive_Bayes_Assumption",
      "description": "Concept that variables are independent when conditioned on another variable, specifically the class label in text classification."
    },
    {
      "id": "NaiveBayesAlgorithm",
      "type": "major",
      "parent": null,
      "description": "A probabilistic classifier based on applying Bayes' theorem with strong independence assumptions between the features."
    },
    {
      "id": "ConditionalProbability",
      "type": "subnode",
      "parent": "NaiveBayesAlgorithm",
      "description": "Calculates probability of features given a class label under Naive Bayes assumption."
    },
    {
      "id": "ParameterEstimation",
      "type": "subnode",
      "parent": "NaiveBayesAlgorithm",
      "description": "Involves estimating parameters from training data to maximize likelihood."
    },
    {
      "id": "Prediction",
      "type": "subnode",
      "parent": "NaiveBayesAlgorithm",
      "description": "Uses estimated parameters and Bayes' theorem for class prediction on new examples."
    },
    {
      "id": "Machine Learning Algorithms",
      "type": "major",
      "parent": null,
      "description": "Collection of methods for learning from data to make predictions or decisions."
    },
    {
      "id": "Naive Bayes Algorithm",
      "type": "subnode",
      "parent": "Machine Learning Algorithms",
      "description": "Probabilistic classifier based on applying Bayes' theorem with strong independence assumptions between the features"
    },
    {
      "id": "Binary Features",
      "type": "subnode",
      "parent": "Naive Bayes Algorithm",
      "description": "Features that can take only two values, typically 0 or 1"
    },
    {
      "id": "Feature Discretization",
      "type": "subnode",
      "parent": "Naive Bayes Algorithm",
      "description": "Process of converting continuous-valued features into discrete values"
    },
    {
      "id": "Laplace Smoothing",
      "type": "subnode",
      "parent": "Machine Learning Algorithms",
      "description": "Technique to prevent zero probabilities in Naive Bayes by adding a small constant to counts"
    },
    {
      "id": "Spam Classification",
      "type": "subnode",
      "parent": "Naive Bayes Algorithm",
      "description": "Application of Naive Bayes for distinguishing between spam and non-spam emails"
    },
    {
      "id": "Naive_Bayes_Classifier",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "A probabilistic classifier based on applying Bayes' theorem with strong independence assumptions between the features."
    },
    {
      "id": "NeurIPS_Conference",
      "type": "major",
      "parent": null,
      "description": "One of the top machine learning conferences where researchers submit their work for publication."
    },
    {
      "id": "Training_Set_Issues",
      "type": "subnode",
      "parent": "Spam_Filtering",
      "description": "Challenges related to training sets in Naive Bayes classifiers when encountering new words like 'neurips'."
    },
    {
      "id": "ProbabilityEstimation",
      "type": "major",
      "parent": null,
      "description": "Avoiding estimation of probabilities as zero in finite training sets."
    },
    {
      "id": "MultinomialRandomVariable",
      "type": "subnode",
      "parent": "ProbabilityEstimation",
      "description": "Parameterizing a multinomial random variable with probabilities \u03c6_j."
    },
    {
      "id": "MaximumLikelihoodEstimates",
      "type": "subnode",
      "parent": "MultinomialRandomVariable",
      "description": "Estimating parameters using maximum likelihood from independent observations."
    },
    {
      "id": "LaplaceSmoothing",
      "type": "subnode",
      "parent": "ProbabilityEstimation",
      "description": "Adding a constant to numerator and denominator of probability estimates to avoid zero probabilities."
    },
    {
      "id": "NaiveBayesClassifier",
      "type": "major",
      "parent": null,
      "description": "Using Laplace smoothing in the Naive Bayes classifier for better parameter estimation."
    },
    {
      "id": "EventModelsTextClassification",
      "type": "subnode",
      "parent": "ProbabilityEstimation",
      "description": "Application of event models to text classification problems with Laplace smoothing."
    },
    {
      "id": "EventModelsForTextClassification",
      "type": "major",
      "parent": null,
      "description": "Overview of event models for text classification in machine learning."
    },
    {
      "id": "BernoulliEventModel",
      "type": "subnode",
      "parent": "EventModelsForTextClassification",
      "description": "A model where each word is included independently according to probabilities given the class."
    },
    {
      "id": "MultinomialEventModel",
      "type": "subnode",
      "parent": "EventModelsForTextClassification",
      "description": "An alternative model using a different notation and feature set for representing emails."
    },
    {
      "id": "Machine_Learning_Models",
      "type": "major",
      "parent": null,
      "description": "Models used in machine learning for sequence prediction and natural language processing."
    },
    {
      "id": "Multinomial_Event_Model",
      "type": "subnode",
      "parent": "Machine_Learning_Models",
      "description": "A model where each event is a draw from a multinomial distribution over words."
    },
    {
      "id": "Spam_Detection",
      "type": "subnode",
      "parent": "Multinomial_Event_Model",
      "description": "Application of the multinomial event model to classify emails as spam or non-spam."
    },
    {
      "id": "Parameter_Estimation",
      "type": "subnode",
      "parent": "Machine_Learning_Models",
      "description": "Process of estimating parameters in machine learning models from training data."
    },
    {
      "id": "Likelihood_Function",
      "type": "subnode",
      "parent": "Parameter_Estimation",
      "description": "Function used to measure how well a set of parameter values fits the observed data."
    },
    {
      "id": "Laplace_Smoothing",
      "type": "subnode",
      "parent": "Parameter_Estimation",
      "description": "Technique to prevent zero probability estimates by adding a small constant to observed counts."
    },
    {
      "id": "Kernel_Methods",
      "type": "major",
      "parent": "Machine_Learning_Techniques",
      "description": "Techniques for handling non-linear relationships in data using feature maps and kernels."
    },
    {
      "id": "Feature_Maps",
      "type": "subnode",
      "parent": "Kernel_Methods",
      "description": "Transformation of input features to a higher-dimensional space to capture non-linear patterns."
    },
    {
      "id": "Cubic_Features",
      "type": "subnode",
      "parent": "Feature_Maps",
      "description": "Example feature transformation for fitting cubic functions to data."
    },
    {
      "id": "FeatureMap",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Mapping from input attributes to feature variables."
    },
    {
      "id": "CubicFunctionRepresentation",
      "type": "subnode",
      "parent": "FeatureMap",
      "description": "Representing a cubic function as a linear function over features."
    },
    {
      "id": "LinearFunctionOverFeatures",
      "type": "subnode",
      "parent": "FeatureMap",
      "description": "Rewriting the model using feature variables."
    },
    {
      "id": "LMSAlgorithmWithFeatures",
      "type": "major",
      "parent": null,
      "description": "Derivation of LMS algorithm for fitting models with features."
    },
    {
      "id": "BatchGradientDescentUpdate",
      "type": "subnode",
      "parent": "LMSAlgorithmWithFeatures",
      "description": "Updating the model parameters using batch gradient descent."
    },
    {
      "id": "FeatureMapping",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Transformation of input data into a higher-dimensional space for better separability."
    },
    {
      "id": "GradientDescentAlgorithm",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Optimization technique for minimizing cost functions in machine learning models."
    },
    {
      "id": "KernelTrick",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Technique for efficiently computing high-dimensional feature mappings without explicitly calculating them."
    },
    {
      "id": "ComputationalComplexity",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Analysis of the computational cost associated with different machine learning algorithms and techniques."
    },
    {
      "id": "KernelTrickIntroduction",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Explanation of the kernel trick in machine learning for efficient computation."
    },
    {
      "id": "PhiFunctionExpansion",
      "type": "subnode",
      "parent": "KernelTrickIntroduction",
      "description": "Description and expansion of the \u03c6(x) function used in the kernel method."
    },
    {
      "id": "ThetaVectorInitialization",
      "type": "subnode",
      "parent": "KernelTrickIntroduction",
      "description": "Details on initializing the \u03b8 vector to zero for simplicity."
    },
    {
      "id": "IterativeUpdateRule",
      "type": "subnode",
      "parent": "KernelTrickIntroduction",
      "description": "Explanation of iterative update rules in the context of kernel methods."
    },
    {
      "id": "LinearCombinationRepresentation",
      "type": "subnode",
      "parent": "IterativeUpdateRule",
      "description": "Description of how \u03b8 can be represented as a linear combination of \u03c6(x) vectors."
    },
    {
      "id": "InductiveProofOfRepresentation",
      "type": "subnode",
      "parent": "LinearCombinationRepresentation",
      "description": "Inductive proof showing that \u03b8 remains a linear combination after updates."
    },
    {
      "id": "UpdateRuleForCoefficients",
      "type": "subnode",
      "parent": "IterativeUpdateRule",
      "description": "Derivation of the update rule for coefficients in the linear combination representation."
    },
    {
      "id": "MachineLearningAlgorithm",
      "type": "major",
      "parent": null,
      "description": "Overview of machine learning algorithms and their implementations."
    },
    {
      "id": "BetaUpdateEquation",
      "type": "subnode",
      "parent": "BatchGradientDescent",
      "description": "The equation used to update \u03b2 during each iteration of batch gradient descent."
    },
    {
      "id": "ThetaRepresentation",
      "type": "subnode",
      "parent": "BatchGradientDescent",
      "description": "Expression for \u03b8 in terms of feature vectors and their inner products."
    },
    {
      "id": "InnerProductEfficiency",
      "type": "subnode",
      "parent": "BatchGradientDescent",
      "description": "Discussion on the efficiency of computing inner products between feature maps."
    },
    {
      "id": "FeatureMapsAndKernels",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Discussion on feature maps and their corresponding kernel functions."
    },
    {
      "id": "KernelFunctionDefinition",
      "type": "subnode",
      "parent": "FeatureMapsAndKernels",
      "description": "Definition of the kernel function based on inner products in a transformed space."
    },
    {
      "id": "EfficientComputationOfKernels",
      "type": "subnode",
      "parent": "FeatureMapsAndKernels",
      "description": "Algorithm for computing kernel values efficiently using equation (5.9)."
    },
    {
      "id": "UpdateRepresentationBeta",
      "type": "subnode",
      "parent": "FeatureMapsAndKernels",
      "description": "Process of updating the representation beta with O(n) time per update."
    },
    {
      "id": "PredictionUsingKernelFunction",
      "type": "subnode",
      "parent": "FeatureMapsAndKernels",
      "description": "Explanation on how to predict using the kernel function and representation beta."
    },
    {
      "id": "Kernels_in_Machine_Learning",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Discussion on kernel functions and their properties in machine learning algorithms."
    },
    {
      "id": "Feature_Map_Phi",
      "type": "subnode",
      "parent": "Kernels_in_Machine_Learning",
      "description": "Introduction to the feature map \u03c6 that induces a kernel function K(x,z)."
    },
    {
      "id": "Kernel_Function_K",
      "type": "subnode",
      "parent": "Kernels_in_Machine_Learning",
      "description": "Definition and properties of the kernel function K(\u22c5,\u22c5) in machine learning."
    },
    {
      "id": "Algorithm_Independence_of_Phi",
      "type": "subnode",
      "parent": "Kernel_Function_K",
      "description": "Explanation that algorithms can operate without explicit feature map \u03c6, relying solely on K(\u22c5,\u22c5)."
    },
    {
      "id": "Characterization_of_Valid_Kernels",
      "type": "subnode",
      "parent": "Kernel_Function_K",
      "description": "Discussion on how to characterize valid kernel functions that correspond to some feature map \u03c6."
    },
    {
      "id": "Concrete_Examples_of_Kernels",
      "type": "subnode",
      "parent": "Characterization_of_Valid_Kernels",
      "description": "Exploration of specific examples like K(x,z)=(x^Tz)^2 to illustrate valid kernels."
    },
    {
      "id": "KernelFunctions",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Functions used in machine learning for pattern analysis."
    },
    {
      "id": "PolynomialKernels",
      "type": "subnode",
      "parent": "KernelFunctions",
      "description": "A type of kernel function that maps input data into a higher-dimensional space."
    },
    {
      "id": "ComputationalEfficiency",
      "type": "subnode",
      "parent": "PolynomialKernels",
      "description": "Discussion on the efficiency of calculating kernel functions vs. direct computation in high-dimensional spaces."
    },
    {
      "id": "KernelsAsSimilarityMetrics",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Using kernels as a measure of similarity between data points."
    },
    {
      "id": "GaussianKernel",
      "type": "subnode",
      "parent": "KernelsAsSimilarityMetrics",
      "description": "A specific kernel function that measures the similarity based on the Gaussian distribution."
    },
    {
      "id": "ValidKernelConditions",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Criteria a function must meet to be considered a valid kernel in machine learning."
    },
    {
      "id": "Kernel Functions",
      "type": "major",
      "parent": null,
      "description": "Functions that satisfy certain properties to be valid kernels."
    },
    {
      "id": "Necessary Conditions for Valid Kernels",
      "type": "subnode",
      "parent": "Kernel Functions",
      "description": "Properties a kernel must have to be considered valid."
    },
    {
      "id": "Symmetry Property",
      "type": "subnode",
      "parent": "Necessary Conditions for Valid Kernels",
      "description": "A valid kernel matrix is symmetric."
    },
    {
      "id": "Positive Semi-Definite Property",
      "type": "subnode",
      "parent": "Necessary Conditions for Valid Kernels",
      "description": "A valid kernel matrix must be positive semi-definite."
    },
    {
      "id": "Feature Mapping",
      "type": "subnode",
      "parent": "Kernel Functions",
      "description": "Mapping from input space to a higher-dimensional feature space."
    },
    {
      "id": "Kernel Matrix",
      "type": "subnode",
      "parent": "Necessary Conditions for Valid Kernels",
      "description": "Matrix representation of kernel function values between points."
    },
    {
      "id": "Kernel_Matrix",
      "type": "major",
      "parent": null,
      "description": "Symmetric positive semidefinite matrix corresponding to kernel function."
    },
    {
      "id": "Valid_Kernels_Conditions",
      "type": "subnode",
      "parent": "Kernel_Matrix",
      "description": "Conditions for a function K to be considered a valid Mercer kernel."
    },
    {
      "id": "Mercer_Theorem",
      "type": "subnode",
      "parent": "Valid_Kernels_Conditions",
      "description": "Necessary and sufficient condition for K to be a valid kernel, involving symmetric positive semidefinite matrices."
    },
    {
      "id": "Testing_Valid_Kernel",
      "type": "subnode",
      "parent": "Mercer_Theorem",
      "description": "Another way of testing if a given function is a valid kernel using Mercer's theorem."
    },
    {
      "id": "Examples_of_Kernels",
      "type": "major",
      "parent": null,
      "description": "Various examples demonstrating the application and effectiveness of different types of kernels in machine learning problems."
    },
    {
      "id": "Digit_Recognition_Problem",
      "type": "subnode",
      "parent": "Examples_of_Kernels",
      "description": "Example illustrating the use of polynomial or Gaussian kernel with SVMs for digit recognition tasks."
    },
    {
      "id": "String_Classification",
      "type": "subnode",
      "parent": "Examples_of_Kernels",
      "description": "Brief mention of string classification problems, such as classifying amino acid sequences into proteins."
    },
    {
      "id": "Machine_Learning_Techniques",
      "type": "major",
      "parent": null,
      "description": "Techniques used in machine learning for classification and regression."
    },
    {
      "id": "Feature_Vectors",
      "type": "subnode",
      "parent": "Machine_Learning_Techniques",
      "description": "Vectors representing features of input data."
    },
    {
      "id": "Support_Vector_Machines",
      "type": "major",
      "parent": null,
      "description": "Supervised learning algorithm for classification and regression analysis."
    },
    {
      "id": "String_Features",
      "type": "subnode",
      "parent": "Feature_Vectors",
      "description": "Features derived from string data such as amino acid sequences."
    },
    {
      "id": "Kernel_Tricks",
      "type": "subnode",
      "parent": "Kernel_Methods",
      "description": "Technique to extend linear classifiers into non-linear ones using kernels."
    },
    {
      "id": "Margins_Concept",
      "type": "subnode",
      "parent": "Support_Vector_Machines",
      "description": "Introduces the concept of margins in SVMs to separate data with a large gap."
    },
    {
      "id": "Optimal_Margin_Classifier",
      "type": "subnode",
      "parent": "Support_Vector_Machines",
      "description": "Finding the classifier that maximizes the geometric margin for linearly separable datasets."
    },
    {
      "id": "Lagrange_Duality",
      "type": "subnode",
      "parent": "Support_Vector_Machines",
      "description": "Explains duality theory used in optimization problems of SVMs."
    },
    {
      "id": "Kernels_in_SVM",
      "type": "subnode",
      "parent": "Support_Vector_Machines",
      "description": "Describes how kernels enable SVM to work efficiently in high-dimensional spaces."
    },
    {
      "id": "SMO_Algorithm",
      "type": "subnode",
      "parent": "Support_Vector_Machines",
      "description": "Algorithm that updates two alpha values simultaneously to maintain constraint satisfaction."
    },
    {
      "id": "Decision Boundary",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "A line or hyperplane that separates different classes of data points."
    },
    {
      "id": "Functional Margins",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Measure of confidence in classification decisions based on the distance from the decision boundary."
    },
    {
      "id": "Geometric Margins",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Distance-based measure to formalize confident predictions far from the separating hyperplane."
    },
    {
      "id": "Training Set",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Collection of data points used for training a machine learning model."
    },
    {
      "id": "Classification Confidence",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Level of certainty in predicting the class label based on distance from decision boundary."
    },
    {
      "id": "Support Vector Machines (SVMs)",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Binary classification algorithm that maximizes the margin between classes."
    },
    {
      "id": "Notation for SVMs",
      "type": "subnode",
      "parent": "Support Vector Machines (SVMs)",
      "description": "Introduction to notation used in discussing SVMs, including parameters w and b."
    },
    {
      "id": "Functional Margin",
      "type": "subnode",
      "parent": "Support Vector Machines (SVMs)",
      "description": "Definition of functional margin for a training example with respect to classifier parameters."
    },
    {
      "id": "Geometric Margin",
      "type": "subnode",
      "parent": "Support Vector Machines (SVMs)",
      "description": "Conceptual understanding and formalization of geometric margins in SVM context."
    },
    {
      "id": "FunctionalMargin",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Measure indicating the confidence and correctness of predictions for linear classifiers."
    },
    {
      "id": "ConfidenceAndCorrectness",
      "type": "subnode",
      "parent": "FunctionalMargin",
      "description": "Relationship between functional margin and prediction accuracy."
    },
    {
      "id": "ScalingImpact",
      "type": "subnode",
      "parent": "FunctionalMargin",
      "description": "Effect of scaling weights and bias on the functional margin."
    },
    {
      "id": "NormalizationCondition",
      "type": "subnode",
      "parent": "FunctionalMargin",
      "description": "Proposed normalization to address issues with scaling in functional margins."
    },
    {
      "id": "FunctionMarginTrainingSet",
      "type": "subnode",
      "parent": "FunctionalMargin",
      "description": "Definition of function margin considering a training set."
    },
    {
      "id": "GeometricMargins",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Introduction to geometric margins in machine learning context."
    },
    {
      "id": "VectorW",
      "type": "subnode",
      "parent": "DecisionBoundary",
      "description": "A vector orthogonal to the decision boundary."
    },
    {
      "id": "DistanceToBoundary",
      "type": "subnode",
      "parent": "DecisionBoundary",
      "description": "The perpendicular distance from a point to the decision boundary."
    },
    {
      "id": "UnitVectorW",
      "type": "subnode",
      "parent": "DistanceToBoundary",
      "description": "A unit vector in the direction of W."
    },
    {
      "id": "TrainingExample",
      "type": "subnode",
      "parent": "DecisionBoundary",
      "description": "An input example with a label used for training."
    },
    {
      "id": "GeometricMargin",
      "type": "major",
      "parent": null,
      "description": "The perpendicular distance from a data point to the decision boundary scaled by the class label."
    },
    {
      "id": "Parameter_Scaling_Invariance",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Invariance to scaling of parameters w and b."
    },
    {
      "id": "Geometric_Margin",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Definition and calculation of geometric margin in training data."
    },
    {
      "id": "Linear_Separability",
      "type": "subnode",
      "parent": "Optimal_Margin_Classifier",
      "description": "Condition where positive and negative examples can be separated by a hyperplane."
    },
    {
      "id": "Maximize_Geometric_Margin_Optimization",
      "type": "subnode",
      "parent": "Optimal_Margin_Classifier",
      "description": "Formulation of optimization problem to maximize geometric margin."
    },
    {
      "id": "Support Vector Machines (SVM)",
      "type": "major",
      "parent": "Machine Learning Overview",
      "description": "Machine learning model for classification and regression analysis."
    },
    {
      "id": "Optimization Problem",
      "type": "subnode",
      "parent": "Support Vector Machines (SVM)",
      "description": "Formulation of SVM optimization with quadratic objective and linear constraints."
    },
    {
      "id": "Non-Convex Constraint",
      "type": "subnode",
      "parent": "Optimization Problem",
      "description": "Constraint that makes the optimization problem difficult to solve."
    },
    {
      "id": "Scaling Constraint",
      "type": "subnode",
      "parent": "Optimization Problem",
      "description": "Constraint used to simplify the optimization problem by scaling parameters."
    },
    {
      "id": "Convex Quadratic Objective",
      "type": "subnode",
      "parent": "Optimization Problem",
      "description": "Objective function to minimize in the SVM problem."
    },
    {
      "id": "Linear Constraints",
      "type": "subnode",
      "parent": "Optimization Problem",
      "description": "Constraints ensuring data points are correctly classified with a margin of at least 1."
    },
    {
      "id": "Optimal Margin Classifier",
      "type": "subnode",
      "parent": "Optimization Problem",
      "description": "Resulting classifier from solving the optimization problem."
    },
    {
      "id": "Lagrange Duality",
      "type": "major",
      "parent": null,
      "description": "Theory explaining how to find dual form of constrained optimization problems."
    },
    {
      "id": "Dual Formulation",
      "type": "subnode",
      "parent": "Lagrange Duality",
      "description": "Alternative formulation allowing efficient computation in high-dimensional spaces."
    },
    {
      "id": "Lagrangian Function",
      "type": "subnode",
      "parent": "Lagrange Duality",
      "description": "Combination of objective function and constraints using Lagrange multipliers."
    },
    {
      "id": "Lagrange Multipliers",
      "type": "subnode",
      "parent": "Lagrange Duality",
      "description": "Coefficients used in the Lagrangian to enforce equality constraints."
    },
    {
      "id": "ConstrainedOptimization",
      "type": "major",
      "parent": null,
      "description": "Generalization of optimization problems with equality and inequality constraints."
    },
    {
      "id": "LagrangeMultipliers",
      "type": "subnode",
      "parent": "ConstrainedOptimization",
      "description": "Scalars used in Lagrangian to incorporate constraints into the objective function."
    },
    {
      "id": "PrimalProblem",
      "type": "subnode",
      "parent": "ConstrainedOptimization",
      "description": "Minimizing a function subject to inequality and equality constraints."
    },
    {
      "id": "GeneralizedLagrangian",
      "type": "subnode",
      "parent": "PrimalProblem",
      "description": "Combination of the objective function with Lagrange multipliers for constraints."
    },
    {
      "id": "ThetaP",
      "type": "subnode",
      "parent": "PrimalProblem",
      "description": "Function that evaluates to f(w) if w satisfies primal constraints, otherwise infinity."
    },
    {
      "id": "Primal Problem",
      "type": "major",
      "parent": null,
      "description": "Original optimization problem in machine learning."
    },
    {
      "id": "Objective Function Primal",
      "type": "subnode",
      "parent": "Primal Problem",
      "description": "Function to be minimized or maximized in the primal problem."
    },
    {
      "id": "Dual Problem",
      "type": "major",
      "parent": null,
      "description": "Optimization problem derived from the primal by exchanging min and max operations."
    },
    {
      "id": "Objective Function Dual",
      "type": "subnode",
      "parent": "Dual Problem",
      "description": "Function to be maximized in the dual problem."
    },
    {
      "id": "Primal-Dual Relationship",
      "type": "major",
      "parent": null,
      "description": "Relationship between primal and dual problems including inequalities between their optimal values."
    },
    {
      "id": "OptimizationInML",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Techniques for optimizing functions in machine learning models."
    },
    {
      "id": "PrimalDualProblem",
      "type": "subnode",
      "parent": "OptimizationInML",
      "description": "Explanation of primal and dual problems in the context of machine learning optimization."
    },
    {
      "id": "dStarAndPStarEquality",
      "type": "subnode",
      "parent": "PrimalDualProblem",
      "description": "Conditions under which d* equals p*, allowing solving the dual problem instead of the primal one."
    },
    {
      "id": "ConvexityAssumptions",
      "type": "subnode",
      "parent": "dStarAndPStarEquality",
      "description": "Convex functions and affine constraints assumptions for equality between primal and dual problems."
    },
    {
      "id": "FeasibilityConstraints",
      "type": "subnode",
      "parent": "dStarAndPStarEquality",
      "description": "Strict feasibility conditions ensuring the existence of a solution satisfying all constraints."
    },
    {
      "id": "KKTConditions",
      "type": "subnode",
      "parent": "PrimalDualProblem",
      "description": "Karush-Kuhn-Tucker (KKT) conditions for optimality in constrained optimization problems."
    },
    {
      "id": "Optimal_Margin_Classifiers",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Focus on classifiers that maximize the margin between classes."
    },
    {
      "id": "KKT_Conditions",
      "type": "subnode",
      "parent": "Optimal_Margin_Classifiers",
      "description": "Conditions that must be satisfied by a solution to an optimization problem with inequality constraints."
    },
    {
      "id": "Dual_Complementarity",
      "type": "subnode",
      "parent": "KKT_Conditions",
      "description": "Condition stating that if alpha is positive, the constraint holds with equality."
    },
    {
      "id": "Primal_Problem",
      "type": "subnode",
      "parent": "Optimal_Margin_Classifiers",
      "description": "Minimization problem for finding optimal margin classifier in primal form."
    },
    {
      "id": "Dual_Form",
      "type": "subnode",
      "parent": "Optimal_Margin_Classifiers",
      "description": "Formulation of the optimization problem in dual space, focusing on alpha values."
    },
    {
      "id": "SupportVectors",
      "type": "major",
      "parent": null,
      "description": "Training examples that define the decision boundary in SVMs."
    },
    {
      "id": "AlphaCoefficients",
      "type": "subnode",
      "parent": "SupportVectors",
      "description": "Lagrange multipliers corresponding to support vectors."
    },
    {
      "id": "InnerProduct",
      "type": "subnode",
      "parent": "KernelTrick",
      "description": "Expression \u03c4(x^(i), x^(j)) representing similarity between points."
    },
    {
      "id": "LagrangianFormulation",
      "type": "major",
      "parent": null,
      "description": "Optimization problem formulation for SVMs using Lagrange multipliers."
    },
    {
      "id": "DualProblem",
      "type": "subnode",
      "parent": "LagrangianFormulation",
      "description": "Transformation of the primal optimization problem into a dual form."
    },
    {
      "id": "DerivativeWithRespectToW",
      "type": "subnode",
      "parent": "LagrangianFormulation",
      "description": "Calculation to find optimal w in terms of \u03b1 and x."
    },
    {
      "id": "DerivativeWithRespectToB",
      "type": "subnode",
      "parent": "LagrangianFormulation",
      "description": "Calculation showing sum of \u03b1y=0 for all training examples."
    },
    {
      "id": "Lagrangian_Formulation",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Formulation involving Lagrangian for optimization problems in machine learning."
    },
    {
      "id": "Equation_6.9",
      "type": "subnode",
      "parent": "Lagrangian_Formulation",
      "description": "Initial Lagrangian equation used in the formulation."
    },
    {
      "id": "Equation_6.10",
      "type": "subnode",
      "parent": "Lagrangian_Formulation",
      "description": "Definition of w used to simplify the Lagrangian."
    },
    {
      "id": "Equation_6.11",
      "type": "subnode",
      "parent": "Lagrangian_Formulation",
      "description": "Constraint equation that simplifies the Lagrangian expression."
    },
    {
      "id": "Dual_Optimization_Problem",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Formulation of the dual optimization problem derived from the primal problem."
    },
    {
      "id": "Equation_6.12",
      "type": "subnode",
      "parent": "Dual_Optimization_Problem",
      "description": "Maximization problem for finding alpha values in the dual formulation."
    },
    {
      "id": "Finding_w_and_b",
      "type": "subnode",
      "parent": "Dual_Optimization_Problem",
      "description": "Steps to find optimal w and b after solving the dual problem."
    },
    {
      "id": "Optimal_Parameter_Finding",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Finding optimal parameters for a model."
    },
    {
      "id": "Dual_Form_Optimization",
      "type": "subnode",
      "parent": "Support_Vector_Machines",
      "description": "Optimization problem viewed from a dual perspective."
    },
    {
      "id": "Inner_Products_Calculation",
      "type": "subnode",
      "parent": "Dual_Form_Optimization",
      "description": "Calculating inner products for predictions."
    },
    {
      "id": "Regularization_and_Kernels",
      "type": "major",
      "parent": null,
      "description": "Handling non-separable data with regularization and kernels."
    },
    {
      "id": "Non-Separable Case",
      "type": "subnode",
      "parent": "Support Vector Machines (SVM)",
      "description": "Handling datasets where data points cannot be separated linearly"
    },
    {
      "id": "L1 Regularization",
      "type": "subnode",
      "parent": "Regularization",
      "description": "Penalizes the absolute value of coefficients to reduce model complexity"
    },
    {
      "id": "Support_Vector_Machines_SVM",
      "type": "subnode",
      "parent": "Machine_Learning",
      "description": "Algorithm for classification and regression analysis based on statistical learning theory."
    },
    {
      "id": "Dual_Problem_Formulation",
      "type": "subnode",
      "parent": "Support_Vector_Machines_SMO",
      "description": "Formulation of SVM optimization problem in dual space to simplify computation."
    },
    {
      "id": "Lagrange_Multipliers",
      "type": "subnode",
      "parent": "Dual_Problem_Formulation",
      "description": "Multipliers used in the method of Lagrange multipliers for solving constrained optimization problems."
    },
    {
      "id": "Support_Vector_Machines_SVMs",
      "type": "subnode",
      "parent": "Machine_Learning_Algorithms",
      "description": "Overview of SVMs including the derivation and optimization problem."
    },
    {
      "id": "Sequential_Minimal_Optimization_SMO",
      "type": "subnode",
      "parent": "Support_Vector_Machines_SVMs",
      "description": "Efficient algorithm for solving the dual problem in SVM derivation."
    },
    {
      "id": "Coordinate_Ascend_Algorithm",
      "type": "major",
      "parent": null,
      "description": "Optimization technique used to solve unconstrained optimization problems."
    },
    {
      "id": "Unconstrained_Optimization_Problem",
      "type": "subnode",
      "parent": "Coordinate_Ascend_Algorithm",
      "description": "Mathematical problem of finding the maximum value of a function W with respect to parameters \u03b1i's."
    },
    {
      "id": "Machine_Learning_Optimization",
      "type": "major",
      "parent": null,
      "description": "Optimization techniques in machine learning including coordinate ascent and SMO."
    },
    {
      "id": "Coordinate_Ascend_Method",
      "type": "subnode",
      "parent": "Machine_Learning_Optimization",
      "description": "A method for optimizing functions by moving along one variable at a time."
    },
    {
      "id": "Quadratic_Function_Optimization",
      "type": "subnode",
      "parent": "Coordinate_Ascend_Method",
      "description": "Optimizing quadratic functions using coordinate ascent starting from (2,-2)."
    },
    {
      "id": "SVM_Dual_Optimization_Problem",
      "type": "subnode",
      "parent": "Support_Vector_Machines_SVMs",
      "description": "The dual form of the optimization problem for SVMs with constraints."
    },
    {
      "id": "Constraint_Satisfaction",
      "type": "subnode",
      "parent": "SMO_Algorithm",
      "description": "Ensuring constraints are met during optimization by updating multiple alphas."
    },
    {
      "id": "Constraints_Satisfaction",
      "type": "subnode",
      "parent": "SMO_Algorithm",
      "description": "Ensuring constraints are met during the optimization process."
    },
    {
      "id": "Convergence_Tolerance",
      "type": "subnode",
      "parent": "Constraints_Satisfaction",
      "description": "Parameter defining the acceptable range for convergence checks."
    },
    {
      "id": "Efficient_Update_Mechanism",
      "type": "subnode",
      "parent": "SMO_Algorithm",
      "description": "Methodology for efficiently updating alpha values in SMO."
    },
    {
      "id": "Alpha_Updating_Process",
      "type": "subnode",
      "parent": "Efficient_Update_Mechanism",
      "description": "Details on how specific alphas are updated while others remain fixed."
    },
    {
      "id": "Alpha_Parameters",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Parameters \u03b1 that are constrained within a box [0,C]x[0,C]."
    },
    {
      "id": "Constraint_Equation",
      "type": "subnode",
      "parent": "Alpha_Parameters",
      "description": "Equation defining the relationship between \u03b1 parameters and y values."
    },
    {
      "id": "Objective_Function",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Function W(\u03b1) that depends on \u03b1 parameters."
    },
    {
      "id": "Quadratic_Formulation",
      "type": "subnode",
      "parent": "Objective_Function",
      "description": "W(\u03b1) expressed as a quadratic function in terms of \u03b12."
    },
    {
      "id": "Machine Learning Overview",
      "type": "major",
      "parent": null,
      "description": "General introduction to machine learning concepts and techniques."
    },
    {
      "id": "Sequential Minimal Optimization (SMO) Algorithm",
      "type": "subnode",
      "parent": "Support Vector Machines (SVM)",
      "description": "Efficient algorithm for solving the optimization problem in SVM."
    },
    {
      "id": "Alpha Updates",
      "type": "subnode",
      "parent": "Sequential Minimal Optimization (SMO) Algorithm",
      "description": "Process of updating alpha values during SMO."
    },
    {
      "id": "Deep Learning Introduction",
      "type": "major",
      "parent": null,
      "description": "Introduction to deep learning concepts and neural networks."
    },
    {
      "id": "Supervised Learning with Non-Linear Models",
      "type": "subnode",
      "parent": "Deep Learning Introduction",
      "description": "Overview of supervised learning using non-linear models like neural networks."
    },
    {
      "id": "NonLinearModels",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Uses non-linear functions and feature mappings for predictions"
    },
    {
      "id": "TrainingExamples",
      "type": "subnode",
      "parent": "NonLinearModels",
      "description": "Definition of training examples and their role in defining the cost function."
    },
    {
      "id": "RegressionProblems",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Exploration of regression problems using real number outputs."
    },
    {
      "id": "LeastSquareCostFunction",
      "type": "subnode",
      "parent": "RegressionProblems",
      "description": "Definition and explanation of the least square cost function for individual examples."
    },
    {
      "id": "MeanSquareCostFunction",
      "type": "subnode",
      "parent": "RegressionProblems",
      "description": "Explanation of mean-square cost function used in regression problems."
    },
    {
      "id": "BinaryClassification",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Focuses on binary classification problems where labels are either 0 or 1."
    },
    {
      "id": "LogisticFunction",
      "type": "subnode",
      "parent": "BinaryClassification",
      "description": "Use of logistic function to convert logit into a probability for binary classification."
    },
    {
      "id": "MulticlassClassification",
      "type": "subnode",
      "parent": "MachineLearningBasics",
      "description": "Extension of binary classification for multiple classes using softmax function."
    },
    {
      "id": "LogitFunction",
      "type": "subnode",
      "parent": "LogisticRegression",
      "description": "Linear combination of input features and weights before applying logistic or softmax functions."
    },
    {
      "id": "ProbabilityPrediction",
      "type": "subnode",
      "parent": "LogisticRegression",
      "description": "Conversion of logit to probability using sigmoid function for binary classification."
    },
    {
      "id": "NegativeLikelihoodLoss",
      "type": "subnode",
      "parent": "LogisticRegression",
      "description": "Loss function used in logistic regression that measures the discrepancy between predicted and actual outcomes."
    },
    {
      "id": "SoftmaxFunction",
      "type": "subnode",
      "parent": "MulticlassClassification",
      "description": "Transformation of logits into probabilities for multi-class classification problems."
    },
    {
      "id": "LogitsInMultiClass",
      "type": "subnode",
      "parent": "MulticlassClassification",
      "description": "Vector of predicted values before applying softmax function in multi-class scenarios."
    },
    {
      "id": "Average Loss",
      "type": "subnode",
      "parent": "Loss Function",
      "description": "Overall loss calculated as mean of individual losses."
    },
    {
      "id": "Conditional Probabilistic Model",
      "type": "subnode",
      "parent": "Negative Log-Likelihood",
      "description": "Model where output distribution depends on input features."
    },
    {
      "id": "Exponential Family Distribution",
      "type": "subnode",
      "parent": "Conditional Probabilistic Model",
      "description": "Family of distributions with exponential form, used in probabilistic models."
    },
    {
      "id": "Optimizers",
      "type": "major",
      "parent": "Pretraining_Phase",
      "description": "Optimization algorithms used to minimize the pretraining loss."
    },
    {
      "id": "Gradient Descent (GD)",
      "type": "subnode",
      "parent": "Optimizers",
      "description": "Algorithm that iteratively minimizes the cost function by moving in direction of steepest descent."
    },
    {
      "id": "Stochastic Gradient Descent (SGD)",
      "type": "subnode",
      "parent": "Optimizers",
      "description": "Variant of gradient descent using a single or subset of data points for each iteration."
    },
    {
      "id": "Learning Rate",
      "type": "subnode",
      "parent": "Gradient Descent (GD)",
      "description": "Hyperparameter controlling the step size during optimization."
    },
    {
      "id": "Mini-batch Stochastic Gradient Descent",
      "type": "subnode",
      "parent": "Stochastic Gradient Descent (SGD)",
      "description": "Variant of SGD using a small batch of data for each iteration."
    },
    {
      "id": "Hyperparameters",
      "type": "subnode",
      "parent": "Machine Learning Algorithms",
      "description": "Parameters set before the learning process that control the overall behavior of the algorithm."
    },
    {
      "id": "Learning Rate (\u03b1)",
      "type": "subnode",
      "parent": "Hyperparameters",
      "description": "Step size at each iteration while moving toward a minimum of a loss function."
    },
    {
      "id": "Number of Iterations (n_iter)",
      "type": "subnode",
      "parent": "Hyperparameters",
      "description": "Total number of iterations over which to optimize the model."
    },
    {
      "id": "Batch Size (B)",
      "type": "subnode",
      "parent": "Mini-batch Stochastic Gradient Descent",
      "description": "Number of samples per gradient update in mini-batch SGD."
    },
    {
      "id": "Initialization",
      "type": "subnode",
      "parent": "Machine Learning Algorithms",
      "description": "Process of setting initial values for model parameters before training begins."
    },
    {
      "id": "Neural Networks",
      "type": "major",
      "parent": null,
      "description": "Non-linear models using combinations of matrix multiplications and non-linear operations."
    },
    {
      "id": "Parametrization (h_\u03b8(x))",
      "type": "subnode",
      "parent": "Neural Networks",
      "description": "Function that maps input data to output predictions with parameters \u03b8."
    },
    {
      "id": "ClassificationProblem",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Categorizing inputs into discrete classes."
    },
    {
      "id": "NeuralNetworks",
      "type": "major",
      "parent": "MachineLearningOverview",
      "description": "Introduction to neural networks including fully-connected architectures."
    },
    {
      "id": "SingleNeuronNN",
      "type": "subnode",
      "parent": "NeuralNetworks",
      "description": "A simple neural network with a single neuron for regression tasks."
    },
    {
      "id": "HousingPricePrediction",
      "type": "subnode",
      "parent": "SingleNeuronNN",
      "description": "Example of predicting housing prices using a single neuron model."
    },
    {
      "id": "ReLUActivationFunction",
      "type": "subnode",
      "parent": "SingleNeuronNN",
      "description": "Use of ReLU activation function in computing intermediate variables a1, a2, and a3."
    },
    {
      "id": "Parameterization",
      "type": "major",
      "parent": "FullyConnectedNN",
      "description": "Discussion on parameterization including weights and biases for neural network layers."
    },
    {
      "id": "Activation Function",
      "type": "subnode",
      "parent": "Neural Networks",
      "description": "Function that introduces non-linearity to the model's output."
    },
    {
      "id": "ReLU",
      "type": "subnode",
      "parent": "Activation Function",
      "description": "Rectified Linear Unit, a popular activation function in neural networks."
    },
    {
      "id": "Bias Term",
      "type": "subnode",
      "parent": "Neural Networks",
      "description": "Adjustment term added to the input before applying the activation function."
    },
    {
      "id": "Weight Vector",
      "type": "subnode",
      "parent": "Neural Networks",
      "description": "Vector containing weights for each feature in the input data."
    },
    {
      "id": "Single Neuron Model",
      "type": "subnode",
      "parent": "Neural Networks",
      "description": "A model with a single neuron, including activation function and bias term."
    },
    {
      "id": "Stacking Neurons",
      "type": "subnode",
      "parent": "Neural Networks",
      "description": "Process of combining multiple neurons to form complex neural networks."
    },
    {
      "id": "Complex Neural Network Example",
      "type": "subnode",
      "parent": "Stacking Neurons",
      "description": "Example using house features and family size prediction."
    },
    {
      "id": "HousingPricesModel",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "A model predicting housing prices based on derived features."
    },
    {
      "id": "DerivedFeatures",
      "type": "subnode",
      "parent": "HousingPricesModel",
      "description": "Family size, walkability, and school quality as key factors in determining house price."
    },
    {
      "id": "IntermediateVariables",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Hidden units (a1, a2, a3) representing intermediate computations in the neural network."
    },
    {
      "id": "NeuralNetworkStructure",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Description of how input features are processed through hidden layers to produce output."
    },
    {
      "id": "OutputParameterization",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Final output as a linear combination of hidden units with parameters \u03b89 to \u03b812."
    },
    {
      "id": "NeuralNetworkParameters",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Collection of parameters in neural networks excluding those from the last layer."
    },
    {
      "id": "BiologicalInspiration",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Explanation of the inspiration from biological neural networks."
    },
    {
      "id": "TwoLayerNetworks",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Description and construction of two-layer fully-connected neural networks."
    },
    {
      "id": "FullyConnectedNN",
      "type": "subnode",
      "parent": "NeuralNetworks",
      "description": "Description of a two-layer fully connected neural network with m hidden units and d-dimensional input."
    },
    {
      "id": "Vectorization",
      "type": "subnode",
      "parent": "NeuralNetworks",
      "description": "Importance of vectorization in simplifying expressions and improving implementation efficiency."
    },
    {
      "id": "VectorizationInNN",
      "type": "major",
      "parent": null,
      "description": "Process of converting loops into matrix operations for efficiency."
    },
    {
      "id": "EfficiencyConcerns",
      "type": "subnode",
      "parent": "VectorizationInNN",
      "description": "Discussion on the inefficiency of using for loops in neural networks."
    },
    {
      "id": "MatrixAlgebra",
      "type": "subnode",
      "parent": "VectorizationInNN",
      "description": "Use of matrix algebra to speed up computations."
    },
    {
      "id": "BLASOptimization",
      "type": "subnode",
      "parent": "VectorizationInNN",
      "description": "Utilizing BLAS for optimized numerical linear algebra operations."
    },
    {
      "id": "TwoLayerNetwork",
      "type": "subnode",
      "parent": "VectorizationInNN",
      "description": "Example of vectorizing a two-layer fully-connected neural network."
    },
    {
      "id": "WeightMatrices",
      "type": "subnode",
      "parent": "NeuralNetworks",
      "description": "Description of weight matrices in neural network models."
    },
    {
      "id": "BiasVectors",
      "type": "subnode",
      "parent": "NeuralNetworks",
      "description": "Explanation of bias vectors and their role in neural networks."
    },
    {
      "id": "ActivationFunctions",
      "type": "subnode",
      "parent": "NeuralNetworks",
      "description": "Explanation of activation functions used in neural networks, including ReLU and others."
    },
    {
      "id": "LayerStructure",
      "type": "subnode",
      "parent": "NeuralNetworks",
      "description": "Description of layer structure in neural networks including hidden layers."
    },
    {
      "id": "WeightMatricesBiases",
      "type": "subnode",
      "parent": "NeuralNetworks",
      "description": "Description of weight matrices and biases in a neural network."
    },
    {
      "id": "ReLUFunction",
      "type": "subnode",
      "parent": "ActivationFunctions",
      "description": "Description of the Rectified Linear Unit (ReLU) function as an activation function."
    },
    {
      "id": "SigmoidTanh",
      "type": "subnode",
      "parent": "ActivationFunctions",
      "description": "Explanation of sigmoid and tanh functions as alternative non-linear activation functions."
    },
    {
      "id": "SigmoidFunction",
      "type": "subnode",
      "parent": "ActivationFunctions",
      "description": "A bounded function that maps real numbers to (0, 1)."
    },
    {
      "id": "TanhFunction",
      "type": "subnode",
      "parent": "ActivationFunctions",
      "description": "Similar to sigmoid but ranges from -1 to 1."
    },
    {
      "id": "LeakyReLUFunction",
      "type": "subnode",
      "parent": "ReLUFunction",
      "description": "A variant of ReLU that allows a small gradient when the unit is not active."
    },
    {
      "id": "GELUFunction",
      "type": "subnode",
      "parent": "ReLUFunction",
      "description": "A smooth approximation to the rectified linear function used in NLP models."
    },
    {
      "id": "SoftplusFunction",
      "type": "subnode",
      "parent": "ActivationFunctions",
      "description": "Smoothed version of ReLU with a proper second-order derivative."
    },
    {
      "id": "IdentityFunction",
      "type": "subnode",
      "parent": "ActivationFunctions",
      "description": "A linear function where the output is equal to the input."
    },
    {
      "id": "FeatureEngineering",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Process of selecting and preparing the features for a model."
    },
    {
      "id": "DeepLearning",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Subfield of machine learning that uses neural networks to learn representations from data."
    },
    {
      "id": "FeatureMaps",
      "type": "subnode",
      "parent": "DeepLearning",
      "description": "Mappings created by deep learning models to transform input into useful features."
    },
    {
      "id": "LinearModelOnTopOfFeatureMap",
      "type": "subnode",
      "parent": "DeepLearning",
      "description": "Using a linear model with features generated by deep learning models."
    },
    {
      "id": "LearnedFeaturesRepresentations",
      "type": "subnode",
      "parent": "DeepLearning",
      "description": "Intermediate outputs from penultimate layers used as learned representations."
    },
    {
      "id": "Deep Learning Representations",
      "type": "major",
      "parent": null,
      "description": "Discussion on feature representations in deep learning."
    },
    {
      "id": "House Price Prediction Example",
      "type": "subnode",
      "parent": "Deep Learning Representations",
      "description": "Example of using a fully-connected neural network for house price prediction."
    },
    {
      "id": "Feature Discovery",
      "type": "subnode",
      "parent": "Deep Learning Representations",
      "description": "Automatic discovery of useful features by neural networks."
    },
    {
      "id": "Black Box Nature",
      "type": "subnode",
      "parent": "Deep Learning Representations",
      "description": "Complexity and interpretability issues in neural network feature discovery."
    },
    {
      "id": "Modern Neural Network Modules",
      "type": "major",
      "parent": null,
      "description": "Introduction to building blocks of modern neural networks."
    },
    {
      "id": "Matrix Multiplication Module",
      "type": "subnode",
      "parent": "Modern Neural Network Modules",
      "description": "Building block for matrix multiplication operations in neural networks."
    },
    {
      "id": "MLP Composition",
      "type": "subnode",
      "parent": "Modern Neural Network Modules",
      "description": "Composition of MLP using multiple matrix multiplication and nonlinear activation modules."
    },
    {
      "id": "MLPArchitecture",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Multi-layer perceptron architecture using matrix multiplication and activation functions."
    },
    {
      "id": "MatrixMultiplicationModule",
      "type": "subnode",
      "parent": "MLPArchitecture",
      "description": "Explanation of the backward function for matrix multiplication module (MM)."
    },
    {
      "id": "NonlinearActivationModule",
      "type": "subnode",
      "parent": "MLPArchitecture",
      "description": "Component of MLP applying nonlinear transformation to matrix multiplication output."
    },
    {
      "id": "ResidualConnections",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Technique used in ResNet for deep learning architectures to mitigate vanishing gradient problem."
    },
    {
      "id": "MachineLearningArchitectures",
      "type": "major",
      "parent": null,
      "description": "Overview of different architectures in machine learning."
    },
    {
      "id": "ResNetArchitecture",
      "type": "subnode",
      "parent": "MachineLearningArchitectures",
      "description": "Deep residual network architecture using convolution layers and batch normalization."
    },
    {
      "id": "LayerNormalization",
      "type": "subnode",
      "parent": "MachineLearningArchitectures",
      "description": "Technique to normalize the layer inputs by scaling and shifting them."
    },
    {
      "id": "LN-SModule",
      "type": "subnode",
      "parent": "LayerNormalization",
      "description": "Sub-module for layer normalization that normalizes each element of the input vector."
    },
    {
      "id": "AffineTransformation",
      "type": "subnode",
      "parent": "LayerNormalization",
      "description": "Transforms the output of LN-S to have desired mean and standard deviation using learnable parameters \u03b2 and \u03b3."
    },
    {
      "id": "LN-S",
      "type": "subnode",
      "parent": "LayerNormalization",
      "description": "Standardized version of LN that normalizes input vectors without learnable parameters."
    },
    {
      "id": "LearnableParameters",
      "type": "subnode",
      "parent": "LayerNormalization",
      "description": "Scalars \u03b2 and \u03b3 that are learned during training to adjust the mean and standard deviation of inputs."
    },
    {
      "id": "ScalingInvariantProperty",
      "type": "major",
      "parent": null,
      "description": "Property ensuring model output remains unchanged under scaling transformations in preceding layers."
    },
    {
      "id": "ScaleInvariantProperty",
      "type": "subnode",
      "parent": "LayerNormalization",
      "description": "Property of modern DL architectures regarding weight scaling."
    },
    {
      "id": "OtherNormalizationLayers",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Alternative normalization techniques used in neural networks."
    },
    {
      "id": "BatchNormalization",
      "type": "subnode",
      "parent": "OtherNormalizationLayers",
      "description": "Normalization technique commonly used in computer vision applications."
    },
    {
      "id": "GroupNormalization",
      "type": "subnode",
      "parent": "OtherNormalizationLayers",
      "description": "Normalization method suitable for grouped channels in neural networks."
    },
    {
      "id": "ConvolutionalLayers",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Neural network layers designed for spatial hierarchies in data."
    },
    {
      "id": "1DConvolution",
      "type": "subnode",
      "parent": "ConvolutionalLayers",
      "description": "Simplified version of 1-D convolution layer used in neural networks."
    },
    {
      "id": "Machine_Learning_Topics",
      "type": "major",
      "parent": null,
      "description": "Main topics in machine learning."
    },
    {
      "id": "Convolutional_Neural_Networks",
      "type": "subnode",
      "parent": "Machine_Learning_Topics",
      "description": "Neural networks that use convolution operations."
    },
    {
      "id": "1D_Convolution",
      "type": "subnode",
      "parent": "Convolutional_Neural_Networks",
      "description": "One-dimensional convolution used in sequence data processing."
    },
    {
      "id": "2D_Convolution",
      "type": "subnode",
      "parent": "Convolutional_Neural_Networks",
      "description": "Two-dimensional convolution typically used for image data."
    },
    {
      "id": "Filter_Vector",
      "type": "subnode",
      "parent": "1D_Convolution",
      "description": "Vector of weights applied to input sequences."
    },
    {
      "id": "Bias_Scalar",
      "type": "subnode",
      "parent": "1D_Convolution",
      "description": "Scalar bias added to convolution output."
    },
    {
      "id": "Kernel_Method",
      "type": "subnode",
      "parent": "Machine_Learning_Topics",
      "description": "Technique for pattern analysis using a kernel function."
    },
    {
      "id": "ParameterSharing",
      "type": "subnode",
      "parent": "ConvolutionalLayers",
      "description": "Explanation of parameter sharing in convolutional layers."
    },
    {
      "id": "EfficiencyOfConvolution",
      "type": "subnode",
      "parent": "ConvolutionalLayers",
      "description": "Comparison of computational efficiency between convolution and generic matrix multiplication."
    },
    {
      "id": "ChannelConcepts",
      "type": "subnode",
      "parent": "ConvolutionalLayers",
      "description": "Introduction to the concept of channels in convolutional layers."
    },
    {
      "id": "Conv1DModule",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "One-dimensional convolution module with multiple channels."
    },
    {
      "id": "Conv2DModule",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Two-dimensional convolution module with one channel and its extension to multi-channel inputs."
    },
    {
      "id": "Differentiable_Circuit",
      "type": "subnode",
      "parent": "Backpropagation",
      "description": "Composition of arithmetic operations and elementary functions that can be differentiated."
    },
    {
      "id": "Gradient_Computation",
      "type": "subnode",
      "parent": "Backpropagation",
      "description": "Process of computing gradients for a real-valued function using backpropagation."
    },
    {
      "id": "Loss Function J(theta)",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Function representing the error between predicted values and actual values in a model."
    },
    {
      "id": "Gradient Computation",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Process of calculating gradients to optimize parameters in machine learning models."
    },
    {
      "id": "Backpropagation Algorithm",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Method for efficiently computing gradients in neural networks."
    },
    {
      "id": "Deep Learning Packages",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Software libraries like TensorFlow and PyTorch that implement backpropagation."
    },
    {
      "id": "Chain Rule",
      "type": "major",
      "parent": "Backpropagation",
      "description": "Mathematical rule for computing derivatives of composite functions."
    },
    {
      "id": "Basic Chain Rule Perspective",
      "type": "subnode",
      "parent": "Chain Rule",
      "description": "New perspective on the chain rule useful for understanding backpropagation."
    },
    {
      "id": "Backpropagation Strategy",
      "type": "major",
      "parent": null,
      "description": "General approach to implementing backpropagation in neural networks."
    },
    {
      "id": "Backward Function Computation",
      "type": "subnode",
      "parent": "Backpropagation Strategy",
      "description": "Method for calculating gradients of basic modules used in neural networks."
    },
    {
      "id": "Concrete Backprop Algorithm",
      "type": "subnode",
      "parent": "Backpropagation Strategy",
      "description": "Specific algorithm for implementing backpropagation in MLPs."
    },
    {
      "id": "Partial Derivatives",
      "type": "major",
      "parent": null,
      "description": "Concept of partial derivatives and their application in machine learning."
    },
    {
      "id": "Scalar Variable J",
      "type": "subnode",
      "parent": "Partial Derivatives",
      "description": "Variable representing the scalar output dependent on other variables z."
    },
    {
      "id": "PartialDerivatives",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Explanation and challenges related to partial derivatives in multi-variate functions."
    },
    {
      "id": "ChainRule",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Review of the chain rule for auto-differentiation purposes."
    },
    {
      "id": "ScalarFunctionDerivatives",
      "type": "subnode",
      "parent": "PartialDerivatives",
      "description": "Focus on derivatives of scalar functions with respect to vectors, matrices, or tensors."
    },
    {
      "id": "MathematicalNotationChallenges",
      "type": "subnode",
      "parent": "PartialDerivatives",
      "description": "Discussion on the complexities in notation for multi-variate partial derivatives."
    },
    {
      "id": "Machine_Learning_Backward_Propagation",
      "type": "major",
      "parent": null,
      "description": "Overview of backward propagation in machine learning"
    },
    {
      "id": "Chain_Rule_Applications",
      "type": "subnode",
      "parent": "Machine_Learning_Backward_Propagation",
      "description": "Using the chain rule to compute gradients"
    },
    {
      "id": "Jacobian_Matrix",
      "type": "subnode",
      "parent": "Machine_Learning_Backward_Propagation",
      "description": "Introduction to Jacobian matrix in context of machine learning"
    },
    {
      "id": "Complex_Functions",
      "type": "subnode",
      "parent": "Chain_Rule_Applications",
      "description": "Handling complex functions where direct computation is difficult"
    },
    {
      "id": "Modules Composition",
      "type": "subnode",
      "parent": "Backpropagation",
      "description": "Viewing neural networks as compositions of simple modules"
    },
    {
      "id": "Loss Function J",
      "type": "subnode",
      "parent": "Gradient Computation",
      "description": "Abstract representation of loss function in terms of module composition"
    },
    {
      "id": "BinaryClassificationProblem",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "A specific problem in machine learning involving binary outcomes."
    },
    {
      "id": "MLPModel",
      "type": "subnode",
      "parent": "BinaryClassificationProblem",
      "description": "Multi-layer perceptron model used for solving the classification problem."
    },
    {
      "id": "LossFunction",
      "type": "subnode",
      "parent": "BinaryClassificationProblem",
      "description": "Mathematical function that measures the error between predicted and actual outcomes."
    },
    {
      "id": "ModulesInMLP",
      "type": "subnode",
      "parent": "MLPModel",
      "description": "Components of MLP including linear transformations and activation functions."
    },
    {
      "id": "BackwardPass",
      "type": "subnode",
      "parent": "LossFunction",
      "description": "Process of computing gradients to update model parameters based on intermediate variables and loss function derivatives."
    },
    {
      "id": "Machine_Learning_Backpropagation",
      "type": "major",
      "parent": null,
      "description": "Overview of backpropagation in neural networks."
    },
    {
      "id": "Chain_Rule_Application",
      "type": "subnode",
      "parent": "Machine_Learning_Backpropagation",
      "description": "Application of the chain rule in computing gradients."
    },
    {
      "id": "Backward_Propagation_Equation",
      "type": "subnode",
      "parent": "Chain_Rule_Application",
      "description": "Equations detailing the backward propagation process."
    },
    {
      "id": "Computational_Graph_Illustration",
      "type": "subnode",
      "parent": "Gradient_Computation",
      "description": "Visualization of computational graph for backpropagation."
    },
    {
      "id": "Efficiency_of_Modules",
      "type": "subnode",
      "parent": "Machine_Learning_Backpropagation",
      "description": "Discussion on the efficiency and granularity of neural network modules."
    },
    {
      "id": "NeuralNetworksComposition",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Understanding neural networks as compositions of basic operations."
    },
    {
      "id": "BackpropagationDiscussion",
      "type": "subnode",
      "parent": "NeuralNetworksComposition",
      "description": "Detailed discussion on backpropagation in the context of neural network training."
    },
    {
      "id": "BackwardFunctionsBasics",
      "type": "subnode",
      "parent": "BackpropagationDiscussion",
      "description": "Introduction to backward functions for basic modules used in networks."
    },
    {
      "id": "LossFunctionBackward",
      "type": "subnode",
      "parent": "BackwardFunctionsBasics",
      "description": "Explanation of the backward function computation for loss functions."
    },
    {
      "id": "Backward Function Overview",
      "type": "major",
      "parent": null,
      "description": "Overview of backward functions in machine learning."
    },
    {
      "id": "Matrix Multiplication Backward Function",
      "type": "subnode",
      "parent": "Backward Function Overview",
      "description": "Explanation and computation efficiency for the matrix multiplication backward function."
    },
    {
      "id": "Bias Backward Function",
      "type": "subnode",
      "parent": "Backward Function Overview",
      "description": "Derivation of the bias backward function using equation (7.59)."
    },
    {
      "id": "Activation Function Backward Function",
      "type": "subnode",
      "parent": "Backward Function Overview",
      "description": "Explanation and derivation for element-wise activation functions' backward computation."
    },
    {
      "id": "Equation 7.64",
      "type": "subnode",
      "parent": "Matrix Multiplication Backward Function",
      "description": "Derivation of the matrix multiplication backward function using equation (7.64)."
    },
    {
      "id": "Vectorized Notation",
      "type": "subnode",
      "parent": "Matrix Multiplication Backward Function",
      "description": "Expression in vectorized notation for the matrix multiplication backward function."
    },
    {
      "id": "Equation 7.65",
      "type": "subnode",
      "parent": "Vectorized Notation",
      "description": "Vectorized form of equation (7.64) with dimensions specified."
    },
    {
      "id": "Equation 7.66",
      "type": "subnode",
      "parent": "Bias Backward Function",
      "description": "Derivation and simplified expression for the bias backward function using equation (7.59)."
    },
    {
      "id": "Activation Derivative Matrix",
      "type": "subnode",
      "parent": "Activation Function Backward Function",
      "description": "Matrix representation of derivative terms in activation functions."
    },
    {
      "id": "Element-wise Operation",
      "type": "subnode",
      "parent": "Activation Function Backward Function",
      "description": "Simplified form using element-wise operations for the backward function of activations."
    },
    {
      "id": "Machine_Learning_Backward_Functions",
      "type": "major",
      "parent": null,
      "description": "Overview of backward functions in machine learning modules."
    },
    {
      "id": "Efficiency_of_Backward_Pass",
      "type": "subnode",
      "parent": "Machine_Learning_Backward_Functions",
      "description": "Discussion on the efficiency and simplification of the backward pass computation."
    },
    {
      "id": "Vectorized_Notation_Backward",
      "type": "subnode",
      "parent": "Machine_Learning_Backward_Functions",
      "description": "Explanation of vectorized notation for backward functions in machine learning modules."
    },
    {
      "id": "Squared_Loss_Backward",
      "type": "subnode",
      "parent": "Vectorized_Notation_Backward",
      "description": "Derivation and explanation of the backward function for squared loss (MSE)."
    },
    {
      "id": "Logistic_Loss_Backward",
      "type": "subnode",
      "parent": "Vectorized_Notation_Backward",
      "description": "Explanation of the backward function for logistic loss."
    },
    {
      "id": "Cross_Entropy_Loss_Backward",
      "type": "subnode",
      "parent": "Vectorized_Notation_Backward",
      "description": "Explanation of the backward function for cross-entropy loss."
    },
    {
      "id": "Machine Learning Loss Functions",
      "type": "major",
      "parent": null,
      "description": "Overview of loss functions used in machine learning models."
    },
    {
      "id": "Logistic Loss Function",
      "type": "subnode",
      "parent": "Machine Learning Loss Functions",
      "description": "Function used for binary classification problems."
    },
    {
      "id": "Cross-Entropy Loss Function",
      "type": "subnode",
      "parent": "Machine Learning Loss Functions",
      "description": "Alternative loss function to logistic loss, commonly used in classification tasks."
    },
    {
      "id": "Back-propagation Algorithm",
      "type": "major",
      "parent": null,
      "description": "Algorithm for computing gradients of the loss function with respect to parameters in neural networks."
    },
    {
      "id": "MLP Backpropagation",
      "type": "subnode",
      "parent": "Back-propagation Algorithm",
      "description": "Specific application of backpropagation to Multi-Layer Perceptrons (MLPs)."
    },
    {
      "id": "Forward Pass",
      "type": "subnode",
      "parent": "MLP Backpropagation",
      "description": "Sequence of operations to compute the loss function in a multi-layer perceptron."
    },
    {
      "id": "Backward Pass",
      "type": "subnode",
      "parent": "MLP Backpropagation",
      "description": "Process of computing gradients by applying backward functions sequentially."
    },
    {
      "id": "BackpropagationAlgorithm",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Details on the backpropagation algorithm used in neural networks."
    },
    {
      "id": "GradientComputation",
      "type": "subnode",
      "parent": "BackpropagationAlgorithm",
      "description": "Computation of gradients for weights and biases using backpropagation equations."
    },
    {
      "id": "IntermediateValuesStorage",
      "type": "subnode",
      "parent": "BackpropagationAlgorithm",
      "description": "Storing intermediate values like activations and pre-activations during forward pass."
    },
    {
      "id": "ParallelTrainingExamples",
      "type": "major",
      "parent": null,
      "description": "Discussion on handling multiple training examples in neural networks using vectorization."
    },
    {
      "id": "VectorizedForwardPass",
      "type": "subnode",
      "parent": "ParallelTrainingExamples",
      "description": "Matrix notation for the forward pass of a neural network with multiple examples."
    },
    {
      "id": "TrainingSetExamples",
      "type": "subnode",
      "parent": "MachineLearningBasics",
      "description": "Explanation of training set examples and their representation."
    },
    {
      "id": "MatrixNotation",
      "type": "subnode",
      "parent": "MachineLearningBasics",
      "description": "Use of matrix notation in representing multiple training examples."
    },
    {
      "id": "LayerActivations",
      "type": "subnode",
      "parent": "TrainingSetExamples",
      "description": "First-layer activations for each example using matrix operations."
    },
    {
      "id": "Broadcasting",
      "type": "subnode",
      "parent": "Vectorization",
      "description": "Technique used to add bias terms across multiple columns in matrix operations."
    },
    {
      "id": "GeneralizationToLayers",
      "type": "subnode",
      "parent": "MachineLearningBasics",
      "description": "Discussion on generalizing the matricization approach to multiple layers."
    },
    {
      "id": "Machine Learning",
      "type": "major",
      "parent": null,
      "description": "Field of study focusing on algorithms that learn from and make predictions on data."
    },
    {
      "id": "Matricization Approach",
      "type": "subnode",
      "parent": "Machine Learning",
      "description": "Method to represent multi-layer neural networks using matrices."
    },
    {
      "id": "Implementation Details",
      "type": "subnode",
      "parent": "Matricization Approach",
      "description": "Specifics of implementing matricization in deep learning packages."
    },
    {
      "id": "Data Representation",
      "type": "subnode",
      "parent": "Implementation Details",
      "description": "Discussion on how data points are represented as rows or columns in matrices."
    },
    {
      "id": "Conversion Rules",
      "type": "subnode",
      "parent": "Implementation Details",
      "description": "Rules for converting between row and column vector representations."
    },
    {
      "id": "Training Loss Function",
      "type": "subnode",
      "parent": "Generalization",
      "description": "Function used to train models by minimizing error on the training dataset."
    },
    {
      "id": "Training_Loss",
      "type": "subnode",
      "parent": "Loss_Functions",
      "description": "Measure of how well a model fits the training data, often referred to as empirical loss."
    },
    {
      "id": "Test_Error",
      "type": "subnode",
      "parent": "Loss_Functions",
      "description": "Evaluation metric for a model's performance on unseen test examples, also known as population error."
    },
    {
      "id": "Mean_Squared_Error",
      "type": "subnode",
      "parent": "Loss_Functions",
      "description": "Common loss function used to quantify the difference between predicted and actual values."
    },
    {
      "id": "Training_Dataset",
      "type": "subnode",
      "parent": "Machine_Learning",
      "description": "Collection of examples used for training a model, drawn from an empirical distribution."
    },
    {
      "id": "Test_Dataset",
      "type": "subnode",
      "parent": "Machine_Learning",
      "description": "Set of unseen data points used to evaluate the performance of a trained model."
    },
    {
      "id": "Empirical_Distribution",
      "type": "subnode",
      "parent": "Training_Dataset",
      "description": "Uniform distribution over training examples, representing an empirical approximation of the true distribution."
    },
    {
      "id": "Training_Test_Distributions",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Explanation of training and test data distributions."
    },
    {
      "id": "Domain_Shift",
      "type": "subnode",
      "parent": "Training_Test_Distributions",
      "description": "Scenario where training and test distributions differ."
    },
    {
      "id": "Overfitting_Underfitting",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Conditions leading to overfitting or underfitting of models."
    },
    {
      "id": "Generalization_Error",
      "type": "subnode",
      "parent": "Overfitting_Underfitting",
      "description": "Difference between true error and empirical error for a hypothesis in the hypothesis space."
    },
    {
      "id": "Bias_Variance_Tradeoff",
      "type": "major",
      "parent": "Machine_Learning_Papers",
      "description": "Decomposition of test error into bias and variance terms to understand model performance."
    },
    {
      "id": "Test_Error_Influence",
      "type": "subnode",
      "parent": "Bias_Variance_Tradeoff",
      "description": "Factors influencing the test error in machine learning models."
    },
    {
      "id": "Double_Descent_Phenomenon",
      "type": "major",
      "parent": "Machine_Learning_Concepts",
      "description": "Description of the double descent phenomenon in model and sample complexity."
    },
    {
      "id": "Overfitting and Underfitting",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Discussion on when overfitting and underfitting occur and how to avoid them."
    },
    {
      "id": "Bias-Variance Tradeoff",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Balancing model complexity to minimize prediction error by adjusting bias and variance."
    },
    {
      "id": "Double Descent Phenomenon",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Phenomenon where model performance initially improves then worsens and improves again with increasing complexity."
    },
    {
      "id": "Training Dataset Example",
      "type": "subnode",
      "parent": "Bias-Variance Tradeoff",
      "description": "Example illustrating training data and model fitting issues."
    },
    {
      "id": "Test Error Analysis",
      "type": "subnode",
      "parent": "Bias-Variance Tradeoff",
      "description": "Analysis of test errors when using different types of models."
    },
    {
      "id": "Linear Regression Example",
      "type": "subnode",
      "parent": "Test Error Analysis",
      "description": "Example showing issues with fitting a linear model to non-linear data."
    },
    {
      "id": "MachineLearningIssues",
      "type": "major",
      "parent": null,
      "description": "Discussion of issues in machine learning models."
    },
    {
      "id": "LinearModelLimitations",
      "type": "subnode",
      "parent": "MachineLearningIssues",
      "description": "Exploration of limitations with linear models."
    },
    {
      "id": "BiasInModels",
      "type": "subnode",
      "parent": "MachineLearningIssues",
      "description": "Definition and discussion on model bias."
    },
    {
      "id": "UnderfittingLinearModel",
      "type": "subnode",
      "parent": "LinearModelLimitations",
      "description": "Analysis of underfitting with linear models."
    },
    {
      "id": "FifthDegreePolynomial",
      "type": "subnode",
      "parent": "MachineLearningIssues",
      "description": "Discussion on fitting a 5th-degree polynomial to data."
    },
    {
      "id": "OverfittingFifthDegree",
      "type": "subnode",
      "parent": "FifthDegreePolynomial",
      "description": "Analysis of overfitting with fifth-degree polynomials."
    },
    {
      "id": "GeneralizationFailure",
      "type": "subnode",
      "parent": "MachineLearningIssues",
      "description": "Discussion on the model's inability to generalize well."
    },
    {
      "id": "PolynomialFitting",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Discussion on fitting polynomials to data sets."
    },
    {
      "id": "Variance",
      "type": "subnode",
      "parent": "PolynomialFitting",
      "description": "Description and impact of variance in model fitting."
    },
    {
      "id": "BiasVsVarianceTradeoff",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Overview of the trade-off between bias and variance in models."
    },
    {
      "id": "Model Complexity",
      "type": "subnode",
      "parent": "Bias-Variance Tradeoff",
      "description": "Degree of flexibility and capacity of a model to fit data, impacting bias and variance."
    },
    {
      "id": "Test Error Decomposition",
      "type": "subnode",
      "parent": "Bias-Variance Tradeoff",
      "description": "Breaking down test error into components of bias and variance."
    },
    {
      "id": "Quadratic Model Example",
      "type": "subnode",
      "parent": "Test Error Decomposition",
      "description": "Illustration showing how quadratic models balance bias and variance effectively."
    },
    {
      "id": "TrainingDataset",
      "type": "subnode",
      "parent": "RegressionProblems",
      "description": "Description of a training dataset for regression analysis."
    },
    {
      "id": "ModelTraining",
      "type": "subnode",
      "parent": "RegressionProblems",
      "description": "Process of training a model on the given dataset."
    },
    {
      "id": "TestExample",
      "type": "subnode",
      "parent": "RegressionProblems",
      "description": "An example used to test the trained model's performance."
    },
    {
      "id": "ExpectedTestError",
      "type": "subnode",
      "parent": "RegressionProblems",
      "description": "Calculation of expected error on a test set."
    },
    {
      "id": "MSEDecomposition",
      "type": "subnode",
      "parent": "ExpectedTestError",
      "description": "Breaking down the mean squared error into bias and variance terms."
    },
    {
      "id": "Claim811",
      "type": "subnode",
      "parent": "MSEDecomposition",
      "description": "Mathematical claim used to decompose MSE into bias and variance."
    },
    {
      "id": "BiasVarianceTradeoff",
      "type": "subnode",
      "parent": "ExpectedTestError",
      "description": "Concept describing the trade-off between model complexity and generalization ability."
    },
    {
      "id": "AverageModel",
      "type": "subnode",
      "parent": "MSEDecomposition",
      "description": "Definition of the average model as a theoretical construct for analysis"
    },
    {
      "id": "BiasTerm",
      "type": "subnode",
      "parent": "MSEDecomposition",
      "description": "Explanation and definition of the bias term in MSE decomposition"
    },
    {
      "id": "VarianceTerm",
      "type": "subnode",
      "parent": "MSEDecomposition",
      "description": "Explanation and definition of the variance term in MSE decomposition"
    },
    {
      "id": "Variance Term",
      "type": "subnode",
      "parent": "Bias-Variance Tradeoff",
      "description": "Error due to model sensitivity to training data randomness."
    },
    {
      "id": "Model-wise Double Descent",
      "type": "subnode",
      "parent": "Double Descent Phenomenon",
      "description": "Test error decreases, increases, then decreases again as model complexity grows."
    },
    {
      "id": "Model_Wise_Double_Descent",
      "type": "subnode",
      "parent": "Double_Descent_Phenomenon",
      "description": "Explanation of how test error changes with respect to model parameters."
    },
    {
      "id": "Sample_Wise_Double_Descent",
      "type": "subnode",
      "parent": "Double_Descent_Phenomenon",
      "description": "Description of the relationship between sample size and test error, similar to model-wise double descent."
    },
    {
      "id": "Overparameterized_Models",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Discussion on the benefits and implications of using overparameterized models in machine learning."
    },
    {
      "id": "Sample-wise Double Descent",
      "type": "subnode",
      "parent": "Double Descent Phenomenon",
      "description": "Peak in test error occurs at n approximately equal to d due to suboptimal training algorithms."
    },
    {
      "id": "Optimization Algorithms",
      "type": "subnode",
      "parent": "Sample-wise Double Descent",
      "description": "Current algorithms are suboptimal at n approximately equal to d, leading to peak test error."
    },
    {
      "id": "Regularization Techniques",
      "type": "subnode",
      "parent": "Double Descent Phenomenon",
      "description": "Optimally tuned regularization can mitigate the double descent effect."
    },
    {
      "id": "Implicit Regularization",
      "type": "subnode",
      "parent": "Model-wise Double Descent",
      "description": "Common optimizers like gradient descent provide implicit regularization, improving generalization in overparameterized models."
    },
    {
      "id": "GradientDescentOptimizer",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Optimization technique that finds minimum norm solution in overparameterized models."
    },
    {
      "id": "MinimumNormSolution",
      "type": "subnode",
      "parent": "GradientDescentOptimizer",
      "description": "Solution found by gradient descent with zero initialization in linear models."
    },
    {
      "id": "OverparameterizationRegime",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Scenario where the number of parameters exceeds the number of samples."
    },
    {
      "id": "DoubleDescentPhenomenon",
      "type": "major",
      "parent": "MachineLearningOverview",
      "description": "A phenomenon in machine learning where performance improves after initially getting worse as model complexity increases."
    },
    {
      "id": "ModelComplexityMeasures",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Different measures of model complexity such as number of parameters and norm of the model."
    },
    {
      "id": "NormAsComplexityMeasure",
      "type": "subnode",
      "parent": "ModelComplexityMeasures",
      "description": "Using the norm of the learned model to measure complexity can avoid double descent in certain cases."
    },
    {
      "id": "DeepNeuralNetworks",
      "type": "major",
      "parent": null,
      "description": "Discussion on finding correct complexity measures for deep neural networks."
    },
    {
      "id": "Linear Regression Model",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Model used for predicting a quantitative response using linear relationships between variables."
    },
    {
      "id": "Sample Complexity Bounds",
      "type": "major",
      "parent": null,
      "description": "Theoretical analysis of the number of samples needed to ensure good generalization performance."
    },
    {
      "id": "Model Selection Methods",
      "type": "subnode",
      "parent": "Sample Complexity Bounds",
      "description": "Techniques for choosing the best model from a set of candidates based on training data performance."
    },
    {
      "id": "Generalization Error",
      "type": "subnode",
      "parent": "Sample Complexity Bounds",
      "description": "Error rate of a hypothesis when applied to unseen data, distinct from training error."
    },
    {
      "id": "Machine_Learning_Theory",
      "type": "major",
      "parent": null,
      "description": "Theoretical foundations of machine learning including concepts like generalization and sample complexity."
    },
    {
      "id": "Union_Bound",
      "type": "subnode",
      "parent": "Machine_Learning_Theory",
      "description": "Probability bound for the union of events in probability theory."
    },
    {
      "id": "Hoeffding_Inequality",
      "type": "subnode",
      "parent": "Machine_Learning_Theory",
      "description": "Bound on the deviation between sample mean and true mean for Bernoulli random variables."
    },
    {
      "id": "Chernoff_Bound",
      "type": "subnode",
      "parent": "Hoeffding_Inequality",
      "description": "Alternative name for Hoeffding inequality in learning theory, emphasizing small probability of large deviations."
    },
    {
      "id": "Binary_Classification",
      "type": "subnode",
      "parent": "Machine_Learning_Theory",
      "description": "Classification problem where labels are binary (0 or 1)."
    },
    {
      "id": "TrainingSet",
      "type": "subnode",
      "parent": "BinaryClassification",
      "description": "A collection of training examples used to train a machine learning model."
    },
    {
      "id": "HypothesisFunction",
      "type": "subnode",
      "parent": "BinaryClassification",
      "description": "A function that maps input data to predicted labels based on learned parameters."
    },
    {
      "id": "TrainingError",
      "type": "subnode",
      "parent": "BinaryClassification",
      "description": "The fraction of training examples misclassified by the hypothesis function."
    },
    {
      "id": "GeneralizationError",
      "type": "subnode",
      "parent": "BinaryClassification",
      "description": "Probability that a new example will be misclassified based on learned parameters."
    },
    {
      "id": "PACAssumptions",
      "type": "subnode",
      "parent": "BinaryClassification",
      "description": "Probably Approximately Correct framework assumptions including same distribution for training and testing."
    },
    {
      "id": "LinearClassification",
      "type": "subnode",
      "parent": "MachineLearningBasics",
      "description": "A type of classification where the decision boundary is linear."
    },
    {
      "id": "EmpiricalRiskMinimization",
      "type": "subnode",
      "parent": "BinaryClassification",
      "description": "Process of minimizing training error to find optimal parameters for a hypothesis function."
    },
    {
      "id": "Empirical_Risk_Minimization",
      "type": "subnode",
      "parent": "Machine_Learning",
      "description": "Process of minimizing training error by selecting parameters that minimize empirical risk."
    },
    {
      "id": "Hypothesis_Class",
      "type": "subnode",
      "parent": "Machine_Learning",
      "description": "Set of all classifiers considered by a learning algorithm, abstracting from specific parameterization."
    },
    {
      "id": "Finite_Hypothesis_Class",
      "type": "subnode",
      "parent": "Empirical_Risk_Minimization",
      "description": "Case where hypothesis class consists of finite number of hypotheses for empirical risk minimization."
    },
    {
      "id": "GeneralizationErrorGuarantees",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Strategies to ensure that training performance predicts generalization performance."
    },
    {
      "id": "BernoulliRandomVariableZ",
      "type": "subnode",
      "parent": "GeneralizationErrorGuarantees",
      "description": "A random variable indicating misclassification of a hypothesis on a single example."
    },
    {
      "id": "TrainingSetSampling",
      "type": "subnode",
      "parent": "GeneralizationErrorGuarantees",
      "description": "Process of sampling training examples independently from the distribution D."
    },
    {
      "id": "HoeffdingInequality",
      "type": "subnode",
      "parent": "GeneralizationErrorGuarantees",
      "description": "Probability bound on the difference between empirical and true error for a single hypothesis."
    },
    {
      "id": "Uniform_Convergence",
      "type": "major",
      "parent": null,
      "description": "A result showing that the empirical error is close to the true error for all hypotheses in a class."
    },
    {
      "id": "Training_Error_Generalization_Error",
      "type": "subnode",
      "parent": "Uniform_Convergence",
      "description": "The relationship between training error and generalization error under uniform convergence."
    },
    {
      "id": "Union_Bound_Application",
      "type": "subnode",
      "parent": "Uniform_Convergence",
      "description": "Using the union bound to extend a result from one hypothesis to all hypotheses in a class."
    },
    {
      "id": "Probability_Error",
      "type": "subnode",
      "parent": "Training_Error_Generalization_Error",
      "description": "The probability that training error is within a certain range of generalization error for all hypotheses."
    },
    {
      "id": "Sample_Size_Calculation",
      "type": "subnode",
      "parent": "Training_Error_Generalization_Error",
      "description": "Calculating the minimum sample size needed to ensure a given level of confidence in the proximity of training and generalization errors."
    },
    {
      "id": "Sample_Complexity",
      "type": "subnode",
      "parent": "Machine_Learning_Theory",
      "description": "Number of training examples required to achieve a certain level of performance with high probability."
    },
    {
      "id": "Uniform_Convergence_Bound",
      "type": "subnode",
      "parent": "Generalization_Error",
      "description": "Mathematical bound on the difference between empirical and true error for all hypotheses in the hypothesis space."
    },
    {
      "id": "Training_Error_vs_Generalization_Error",
      "type": "subnode",
      "parent": "Generalization_Error",
      "description": "Relationship between training set performance and actual model performance on unseen data."
    },
    {
      "id": "Hypothesis_Space_Size",
      "type": "subnode",
      "parent": "Uniform_Convergence_Bound",
      "description": "Impact of the size of hypothesis space on uniform convergence bound."
    },
    {
      "id": "Optimal_Hypothesis_Selection",
      "type": "subnode",
      "parent": "Machine_Learning_Theory",
      "description": "Process of selecting a hypothesis that minimizes empirical error and its implications for generalization performance."
    },
    {
      "id": "UniformConvergence",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Theoretical concept ensuring that the empirical risk is close to the true risk with high probability."
    },
    {
      "id": "HypothesisSpace",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Set of all possible hypotheses or models considered in a learning problem."
    },
    {
      "id": "Machine_Learning_Bias_Variance_Tradeoff",
      "type": "major",
      "parent": null,
      "description": "Exploration of bias-variance tradeoff in machine learning hypothesis classes."
    },
    {
      "id": "Hypothesis_Class_Switching",
      "type": "subnode",
      "parent": "Machine_Learning_Bias_Variance_Tradeoff",
      "description": "Analysis of switching to a larger hypothesis class and its impact on bias and variance."
    },
    {
      "id": "Sample_Complexity_Bound",
      "type": "subnode",
      "parent": "Machine_Learning_Bias_Variance_Tradeoff",
      "description": "Derivation of sample complexity bound for finite hypothesis classes."
    },
    {
      "id": "Infinite_Hypothesis_Classes",
      "type": "subnode",
      "parent": "Machine_Learning_Bias_Variance_Tradeoff",
      "description": "Discussion on handling infinite hypothesis classes in machine learning theory."
    },
    {
      "id": "Hypothesis_Class_Size",
      "type": "subnode",
      "parent": "Machine_Learning_Theory",
      "description": "Size of the hypothesis class in terms of model parameters."
    },
    {
      "id": "Floating_Point_Precision",
      "type": "subnode",
      "parent": "Hypothesis_Class_Size",
      "description": "Impact of floating point precision on the size of hypothesis class."
    },
    {
      "id": "Linear_Classifiers",
      "type": "subnode",
      "parent": "Machine_Learning_Theory",
      "description": "Class of classifiers that are linear in parameters."
    },
    {
      "id": "ParameterizationOfH",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Discussion on parameterizing the hypothesis class \u03a9."
    },
    {
      "id": "LinearClassifiers",
      "type": "subnode",
      "parent": "ParameterizationOfH",
      "description": "Explanation of linear classifiers with different parameter sets."
    },
    {
      "id": "ShatteringConcept",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Definition and explanation of the shattering concept in hypothesis classes."
    },
    {
      "id": "VCDimension",
      "type": "subnode",
      "parent": "ShatteringConcept",
      "description": "Introduction to Vapnik-Chervonenkis dimension as a measure of complexity for \u03a9."
    },
    {
      "id": "VC Dimension",
      "type": "major",
      "parent": null,
      "description": "Measure of the capacity of a statistical classification algorithm"
    },
    {
      "id": "Shattering",
      "type": "subnode",
      "parent": "VC Dimension",
      "description": "A set is shattered by a hypothesis class if every possible labeling can be achieved"
    },
    {
      "id": "Vapnik's Theorem",
      "type": "major",
      "parent": null,
      "description": "Theorem providing bounds on the difference between empirical and true error rates"
    },
    {
      "id": "Hypothesis Class",
      "type": "subnode",
      "parent": "VC Dimension",
      "description": "A collection of hypotheses or models used in learning theory"
    },
    {
      "id": "Uniform Convergence",
      "type": "subnode",
      "parent": "Vapnik's Theorem",
      "description": "Property where empirical error approximates true error for all hypotheses as sample size grows"
    },
    {
      "id": "Corollary",
      "type": "major",
      "parent": null,
      "description": "Result derived from Vapnik's theorem regarding the number of training examples needed"
    },
    {
      "id": "Chapter9",
      "type": "major",
      "parent": null,
      "description": "Regularization and model selection in machine learning"
    },
    {
      "id": "ModelComplexity",
      "type": "subnode",
      "parent": "Regularization",
      "description": "Measured by number of parameters or function of parameters like \u03b2\u2082 norm"
    },
    {
      "id": "TrainingLoss",
      "type": "subnode",
      "parent": "Regularization",
      "description": "Modified with regularizer to prevent overfitting, denoted as J_\u03bb(\u03b8)"
    },
    {
      "id": "Regularizer",
      "type": "subnode",
      "parent": "TrainingLoss",
      "description": "Additional term added to training loss, denoted by R(\u03b8), measures model complexity"
    },
    {
      "id": "LambdaParameter",
      "type": "subnode",
      "parent": "TrainingLoss",
      "description": "Regularization parameter \u03bb that balances original loss and regularizer"
    },
    {
      "id": "Regularized_Loss",
      "type": "subnode",
      "parent": "Regularization",
      "description": "Combination of original loss and regularizer, controlled by parameter lambda."
    },
    {
      "id": "Lambda_Parameter",
      "type": "subnode",
      "parent": "Regularized_Loss",
      "description": "Balances the trade-off between fitting data and model complexity."
    },
    {
      "id": "L2_Regularization",
      "type": "subnode",
      "parent": "Regularization",
      "description": "Encourages small L2 norm of parameters, often called weight decay in deep learning."
    },
    {
      "id": "Weight_Decay",
      "type": "subnode",
      "parent": "L2_Regularization",
      "description": "Gradient descent with regularization shrinks weights by a scalar factor."
    },
    {
      "id": "Inductive_Biases",
      "type": "subnode",
      "parent": "Regularization",
      "description": "Regulation can impose prior beliefs or structures on model parameters to guide learning."
    },
    {
      "id": "Regularization in Machine Learning",
      "type": "major",
      "parent": null,
      "description": "Techniques to prevent overfitting by adding constraints or penalties."
    },
    {
      "id": "Sparsity Regularization",
      "type": "subnode",
      "parent": "Regularization in Machine Learning",
      "description": "Encourages model parameters to be sparse (many zero values)."
    },
    {
      "id": "L0 Norm",
      "type": "subnode",
      "parent": "Sparsity Regularization",
      "description": "Counts the number of non-zero elements."
    },
    {
      "id": "L1 Norm (LASSO)",
      "type": "subnode",
      "parent": "Regularization in Machine Learning",
      "description": "Penalizes the sum of absolute values of coefficients."
    },
    {
      "id": "L2 Norm Regularization",
      "type": "subnode",
      "parent": "Regularization in Machine Learning",
      "description": "Penalizes the sum of squared values of coefficients, commonly used with kernel methods."
    },
    {
      "id": "Deep Learning Regularizers",
      "type": "subnode",
      "parent": "Regularization in Machine Learning",
      "description": "Techniques specific to deep learning models for regularization."
    },
    {
      "id": "Weight Decay",
      "type": "subnode",
      "parent": "Deep Learning Regularizers",
      "description": "Equivalent to L2 norm regularization, reduces model complexity."
    },
    {
      "id": "Dropout",
      "type": "subnode",
      "parent": "Deep Learning Regularizers",
      "description": "Randomly drops units during training to prevent overfitting."
    },
    {
      "id": "Data Augmentation",
      "type": "subnode",
      "parent": "Deep Learning Regularizers",
      "description": "Increases diversity of training data by applying transformations."
    },
    {
      "id": "Spectral Norm Regularization",
      "type": "subnode",
      "parent": "Deep Learning Regularizers",
      "description": "Penalizes the spectral norm of weight matrices to control model complexity."
    },
    {
      "id": "Lipschitzness Regularization",
      "type": "subnode",
      "parent": "Deep Learning Regularizers",
      "description": "Regulates the Lipschitz constant to ensure bounded sensitivity to input changes."
    },
    {
      "id": "Regularization in Deep Learning",
      "type": "major",
      "parent": null,
      "description": "Overview of regularization techniques and concepts in deep learning."
    },
    {
      "id": "Explicit Regularization Techniques",
      "type": "subnode",
      "parent": "Regularization in Deep Learning",
      "description": "Techniques like weight decay, dropout, data augmentation, spectral norm regularization, and Lipschitz regularizers."
    },
    {
      "id": "Implicit Regularization Effect",
      "type": "subnode",
      "parent": "Regularization in Deep Learning",
      "description": "The impact of optimizers on model generalization beyond explicit regularization."
    },
    {
      "id": "Optimizer Bias",
      "type": "subnode",
      "parent": "Implicit Regularization Effect",
      "description": "How different optimizers converge to different global minima with varying properties and generalization performance."
    },
    {
      "id": "Optimizers and Generalization",
      "type": "major",
      "parent": null,
      "description": "Discusses how optimizers affect model generalization."
    },
    {
      "id": "Global Minima and Generalization",
      "type": "subnode",
      "parent": "Optimizers and Generalization",
      "description": "Investigates how different global minima affect model performance."
    },
    {
      "id": "Learning Rate Schedules",
      "type": "subnode",
      "parent": "Optimizers and Generalization",
      "description": "Examines the impact of learning rate schedules on generalization."
    },
    {
      "id": "Model Selection via Cross Validation",
      "type": "major",
      "parent": null,
      "description": "Describes model selection techniques using cross-validation."
    },
    {
      "id": "Model Selection",
      "type": "major",
      "parent": null,
      "description": "Process of choosing the best model for a given task."
    },
    {
      "id": "Cross Validation",
      "type": "subnode",
      "parent": "Model Selection",
      "description": "Technique to evaluate models and select the best one based on performance metrics."
    },
    {
      "id": "Polynomial Regression Models",
      "type": "subnode",
      "parent": "Cross Validation",
      "description": "Models with varying degrees of polynomial terms for regression analysis."
    },
    {
      "id": "SVM and Regularization",
      "type": "subnode",
      "parent": "Model Selection",
      "description": "Support Vector Machines with regularization techniques like L1 to control model complexity."
    },
    {
      "id": "Finite Model Set",
      "type": "subnode",
      "parent": "Cross Validation",
      "description": "A predefined set of models from which the best one is selected using cross validation."
    },
    {
      "id": "Infinite Model Space",
      "type": "subnode",
      "parent": "Cross Validation",
      "description": "Techniques to handle continuous model parameters by discretization or optimization in a large space."
    },
    {
      "id": "Cross_Validation",
      "type": "major",
      "parent": null,
      "description": "Technique to evaluate and select models using a validation set."
    },
    {
      "id": "Hold_Out_Cross_Validation",
      "type": "subnode",
      "parent": "Cross_Validation",
      "description": "Split data into training and validation sets for model selection."
    },
    {
      "id": "Training_Error",
      "type": "subnode",
      "parent": "Empirical_Risk_Minimization",
      "description": "Error of a hypothesis on the training set used to train it."
    },
    {
      "id": "Validation_Error",
      "type": "subnode",
      "parent": "Hold_Out_Cross_Validation",
      "description": "Error of a hypothesis on the validation set not seen during training."
    },
    {
      "id": "MachineLearningValidationTechniques",
      "type": "major",
      "parent": null,
      "description": "Overview of validation techniques in machine learning."
    },
    {
      "id": "HoldoutCrossValidation",
      "type": "subnode",
      "parent": "MachineLearningValidationTechniques",
      "description": "Method where a portion of data is held out for validation purposes."
    },
    {
      "id": "kFoldCrossValidation",
      "type": "subnode",
      "parent": "MachineLearningValidationTechniques",
      "description": "A method to validate models by splitting data into k subsets and training on k-1 while testing on 1."
    },
    {
      "id": "ModelSelection",
      "type": "subnode",
      "parent": "HoldoutCrossValidation",
      "description": "Process of selecting a model based on validation error estimates."
    },
    {
      "id": "ValidationSetSize",
      "type": "subnode",
      "parent": "HoldoutCrossValidation",
      "description": "Discussion on the appropriate size for the validation set in relation to total data availability."
    },
    {
      "id": "RetrainingModel",
      "type": "subnode",
      "parent": "HoldoutCrossValidation",
      "description": "Option of retraining selected model with full dataset after cross-validation."
    },
    {
      "id": "MachineLearningChallenges",
      "type": "major",
      "parent": null,
      "description": "Challenges in machine learning when data is scarce."
    },
    {
      "id": "LeaveOneOutCV",
      "type": "subnode",
      "parent": "kFoldCrossValidation",
      "description": "A variant of cross validation where one example is held out for testing each time."
    },
    {
      "id": "DataSplitting",
      "type": "subnode",
      "parent": "kFoldCrossValidation",
      "description": "Process of splitting data into k disjoint subsets for training and validation."
    },
    {
      "id": "ModelEvaluation",
      "type": "subnode",
      "parent": "kFoldCrossValidation",
      "description": "Procedure to evaluate models using cross-validation techniques."
    },
    {
      "id": "GeneralizationErrorEstimation",
      "type": "subnode",
      "parent": "ModelEvaluation",
      "description": "Method to estimate the error of a model's performance on unseen data."
    },
    {
      "id": "Cross Validation Techniques",
      "type": "major",
      "parent": null,
      "description": "Techniques for model evaluation and selection."
    },
    {
      "id": "Leave-One-Out Cross Validation",
      "type": "subnode",
      "parent": "Cross Validation Techniques",
      "description": "Method where one training example is held out at a time."
    },
    {
      "id": "Bayesian Statistics",
      "type": "major",
      "parent": null,
      "description": "Approach to parameter estimation considering parameters as random variables."
    },
    {
      "id": "Frequentist View",
      "type": "subnode",
      "parent": "Bayesian Statistics",
      "description": "View where parameters are constant but unknown values."
    },
    {
      "id": "Maximum Likelihood Estimation (MLE)",
      "type": "subnode",
      "parent": "Frequentist View",
      "description": "Estimation method maximizing likelihood of observed data under parameter \u03b8."
    },
    {
      "id": "Bayesian Machine Learning",
      "type": "major",
      "parent": null,
      "description": "Overview of Bayesian approaches in machine learning."
    },
    {
      "id": "Posterior Distribution on Parameters",
      "type": "subnode",
      "parent": "Bayesian Machine Learning",
      "description": "Distribution of parameters given the data and prior knowledge."
    },
    {
      "id": "Model Specification",
      "type": "subnode",
      "parent": "Bayesian Machine Learning",
      "description": "Definition of the probabilistic model used for prediction."
    },
    {
      "id": "Logistic Regression Example",
      "type": "subnode",
      "parent": "Model Specification",
      "description": "Example using Bayesian logistic regression for binary classification."
    },
    {
      "id": "Prediction on New Data",
      "type": "subnode",
      "parent": "Bayesian Machine Learning",
      "description": "Process of predicting class labels or expected values given new data and posterior distribution."
    },
    {
      "id": "Fully Bayesian Prediction",
      "type": "subnode",
      "parent": "Prediction on New Data",
      "description": "Method that averages predictions over the posterior distribution of parameters."
    },
    {
      "id": "Computational Challenges",
      "type": "subnode",
      "parent": "Bayesian Machine Learning",
      "description": "Difficulties in computing high-dimensional integrals for posterior distributions."
    },
    {
      "id": "BayesianInference",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Techniques for estimating parameters using prior knowledge."
    },
    {
      "id": "MAPEstimation",
      "type": "subnode",
      "parent": "BayesianInference",
      "description": "Finding the most probable parameter values given data and priors."
    },
    {
      "id": "PriorDistributions",
      "type": "subnode",
      "parent": "BayesianInference",
      "description": "Selection of prior probabilities for parameters."
    },
    {
      "id": "UnsupervisedLearning",
      "type": "major",
      "parent": null,
      "description": "Techniques to find hidden structure in unlabeled data."
    },
    {
      "id": "Clustering",
      "type": "subnode",
      "parent": "UnsupervisedLearning",
      "description": "Grouping a set of objects into clusters based on similarity."
    },
    {
      "id": "KMeansAlgorithm",
      "type": "subnode",
      "parent": "Clustering",
      "description": "Clustering algorithm that partitions data into k clusters based on mean distance."
    },
    {
      "id": "k-means_algorithm",
      "type": "major",
      "parent": null,
      "description": "Clustering algorithm that partitions data into k clusters."
    },
    {
      "id": "distortion_function",
      "type": "subnode",
      "parent": "k-means_algorithm",
      "description": "Measures sum of squared distances between examples and cluster centroids."
    },
    {
      "id": "initialization_step",
      "type": "subnode",
      "parent": "k-means_algorithm",
      "description": "Randomly selects k training examples as initial cluster centroids."
    },
    {
      "id": "inner_loop_steps",
      "type": "subnode",
      "parent": "k-means_algorithm",
      "description": "Assigns examples to closest centroid and updates centroids based on assigned points."
    },
    {
      "id": "convergence_property",
      "type": "subnode",
      "parent": "k-means_algorithm",
      "description": "Guaranteed to converge in a certain sense, minimizing distortion function iteratively."
    },
    {
      "id": "DistortionFunctionJ",
      "type": "subnode",
      "parent": "KMeansAlgorithm",
      "description": "Non-convex function used to measure the quality of clustering in K-means."
    },
    {
      "id": "ConvergenceProperties",
      "type": "subnode",
      "parent": "KMeansAlgorithm",
      "description": "Properties related to convergence and local optima in K-means algorithm."
    },
    {
      "id": "EMAlgorithms",
      "type": "major",
      "parent": null,
      "description": "Set of notes discussing EM algorithms for density estimation."
    },
    {
      "id": "MixtureOfGaussians",
      "type": "subnode",
      "parent": "EMAlgorithms",
      "description": "Modeling data using a mixture of Gaussian distributions in an unsupervised setting."
    },
    {
      "id": "Unsupervised Learning",
      "type": "major",
      "parent": null,
      "description": "Learning setting without labeled data"
    },
    {
      "id": "Joint Distribution",
      "type": "subnode",
      "parent": "Unsupervised Learning",
      "description": "Distribution modeling both observed and latent variables"
    },
    {
      "id": "Mixture of Gaussians Model",
      "type": "subnode",
      "parent": "Joint Distribution",
      "description": "Model using multiple Gaussian distributions with hidden variables"
    },
    {
      "id": "Latent Variables",
      "type": "subnode",
      "parent": "Mixture of Gaussians Model",
      "description": "Hidden random variables that affect the observed data generation process"
    },
    {
      "id": "Model Parameters",
      "type": "subnode",
      "parent": "Joint Distribution",
      "description": "Parameters including \u03c6, \u03bc and \u03a3 for the model estimation"
    },
    {
      "id": "Likelihood Function",
      "type": "subnode",
      "parent": "Model Parameters",
      "description": "Function to estimate parameters by maximizing likelihood of observed data"
    },
    {
      "id": "DensityEstimation",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Techniques for estimating probability densities from data."
    },
    {
      "id": "GaussianMixtureModels",
      "type": "subnode",
      "parent": "DensityEstimation",
      "description": "Modeling data with a mixture of Gaussian distributions."
    },
    {
      "id": "EMAlgorithm",
      "type": "subnode",
      "parent": "GaussianMixtureModels",
      "description": "Iterative algorithm for finding maximum likelihood estimates in probabilistic models with latent variables."
    },
    {
      "id": "EStep",
      "type": "subnode",
      "parent": "EMAlgorithm",
      "description": "Expectation step where posterior probabilities of hidden variables are calculated."
    },
    {
      "id": "MStep",
      "type": "subnode",
      "parent": "EMAlgorithm",
      "description": "Maximization step where model parameters are updated based on the E-step's results."
    },
    {
      "id": "EM_Algorithm",
      "type": "major",
      "parent": null,
      "description": "Iterative method for finding maximum likelihood estimates of parameters in probabilistic models with latent variables."
    },
    {
      "id": "E_Step",
      "type": "subnode",
      "parent": "EM_Algorithm",
      "description": "Estimation step where posterior distribution of latent variables is calculated given observed data and current parameters."
    },
    {
      "id": "M_Step",
      "type": "subnode",
      "parent": "EM_Algorithm",
      "description": "Maximization step where parameters are updated to maximize the expected log-likelihood found in E-step."
    },
    {
      "id": "Bayes_Rule",
      "type": "subnode",
      "parent": "E_Step",
      "description": "Used to calculate posterior probabilities of latent variables given observed data and current parameter estimates."
    },
    {
      "id": "Gaussian_Distribution",
      "type": "subnode",
      "parent": "EM_Algorithm",
      "description": "Probability distribution used for modeling the likelihood of observations in EM algorithm."
    },
    {
      "id": "Soft_Guesses",
      "type": "subnode",
      "parent": "E_Step",
      "description": "Probabilistic assignments of latent variables, taking values between 0 and 1."
    },
    {
      "id": "Hard_Assignments",
      "type": "subnode",
      "parent": "EM_Algorithm",
      "description": "Binary assignments used in K-means clustering as opposed to probabilistic assignments in EM."
    },
    {
      "id": "K_Means_Clustering",
      "type": "major",
      "parent": null,
      "description": "Clustering algorithm that assigns data points to clusters based on hard assignments."
    },
    {
      "id": "Local_Optima",
      "type": "subnode",
      "parent": "EM_Algorithm",
      "description": "Potential issue where EM may converge to suboptimal solutions due to initialization and iterative nature."
    },
    {
      "id": "Convergence_Guarantees",
      "type": "subnode",
      "parent": "EM_Algorithm",
      "description": "Analysis of conditions under which the EM algorithm converges to a global optimum."
    },
    {
      "id": "ConvergenceGuarantees",
      "type": "subnode",
      "parent": "EMAlgorithm",
      "description": "Discussion on convergence guarantees for EM algorithm."
    },
    {
      "id": "JensensInequality",
      "type": "major",
      "parent": "MachineLearningConcepts",
      "description": "Introduction to Jensen's inequality and its application in machine learning."
    },
    {
      "id": "ConvexFunctionDefinition",
      "type": "subnode",
      "parent": "JensensInequality",
      "description": "Definition of convex functions including strict convexity."
    },
    {
      "id": "TheoremStatement",
      "type": "subnode",
      "parent": "JensensInequality",
      "description": "Formal statement of Jensen's inequality theorem and its implications."
    },
    {
      "id": "Jensen's Inequality",
      "type": "major",
      "parent": null,
      "description": "Inequality describing the relationship between expected values and convex functions."
    },
    {
      "id": "Convex Functions",
      "type": "subnode",
      "parent": "Jensen's Inequality",
      "description": "Functions where the line segment between any two points on the graph of the function lies above or on the graph."
    },
    {
      "id": "Concave Functions",
      "type": "subnode",
      "parent": "Jensen's Inequality",
      "description": "Negative of convex functions, with reversed inequality in Jensen's inequality."
    },
    {
      "id": "EM Algorithm",
      "type": "major",
      "parent": null,
      "description": "Algorithm used for maximum likelihood estimation in models with latent variables."
    },
    {
      "id": "Latent Variable Models",
      "type": "subnode",
      "parent": "EM Algorithm",
      "description": "Models where some of the variables are not observed directly but rather inferred from other observable variables."
    },
    {
      "id": "OptimizationChallenges",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Difficulties in optimizing parameters due to non-convex problems."
    },
    {
      "id": "SingleExampleLikelihood",
      "type": "subnode",
      "parent": "EMAlgorithm",
      "description": "Simplifying exposition by considering likelihood for a single example first."
    },
    {
      "id": "SummationNotEssential",
      "type": "subnode",
      "parent": "SingleExampleLikelihood",
      "description": "The summation is not crucial in the initial derivation of EM algorithm."
    },
    {
      "id": "LogLikelihoodExpression",
      "type": "subnode",
      "parent": "SingleExampleLikelihood",
      "description": "Rewriting the likelihood function for a single example as log p(x;\u03b8)."
    },
    {
      "id": "DistributionQ",
      "type": "subnode",
      "parent": "LogLikelihoodExpression",
      "description": "Introducing distribution Q over possible values of latent variable z."
    },
    {
      "id": "ProbabilityDistributions",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Discussion on probability distributions used in ML models."
    },
    {
      "id": "LogLikelihoodBound",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Derivation and explanation of the log-likelihood bound using Q distribution."
    },
    {
      "id": "EvidenceLowerBoundELBO",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Definition and significance of the evidence lower bound (ELBO)."
    },
    {
      "id": "PosteriorDistribution",
      "type": "subnode",
      "parent": "EvidenceLowerBoundELBO",
      "description": "Explanation of how to set Q(z) as the posterior distribution p(z|x;theta)."
    },
    {
      "id": "EqualityCondition",
      "type": "subnode",
      "parent": "JensensInequality",
      "description": "Conditions under which Jensen's inequality holds with equality."
    },
    {
      "id": "LogLikelihoodOptimization",
      "type": "subnode",
      "parent": "EMAlgorithm",
      "description": "Focuses on optimizing log-likelihood for a single example using EM."
    },
    {
      "id": "MultipleExamplesExtension",
      "type": "subnode",
      "parent": "EMAlgorithm",
      "description": "Extends the EM algorithm to handle multiple training examples."
    },
    {
      "id": "ELBOFormula",
      "type": "subnode",
      "parent": "MultipleExamplesExtension",
      "description": "Derivation and application of evidence lower bound formula for multiple examples."
    },
    {
      "id": "Log-Likelihood",
      "type": "major",
      "parent": null,
      "description": "Measure of how likely a given set of data is under a specific probability model."
    },
    {
      "id": "Convergence",
      "type": "subnode",
      "parent": "EM_Algorithm",
      "description": "Condition where the algorithm stops improving the log-likelihood and reaches an optimal solution."
    },
    {
      "id": "Jensen's_Inequality",
      "type": "major",
      "parent": null,
      "description": "Mathematical result stating that for a convex function f, the value of the function at the expected value of x is less than or equal to the expected value of the function at x."
    },
    {
      "id": "ELBO",
      "type": "subnode",
      "parent": "E_Step",
      "description": "Evidence Lower BOund used in variational inference and EM algorithm as a lower bound on log-likelihood."
    },
    {
      "id": "KL_Divergence",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Measure of difference between two probability distributions."
    },
    {
      "id": "MarginalDistribution",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Probability distribution of a subset of variables in the context of ELBO."
    },
    {
      "id": "ConditionalLikelihood",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Probability of observing data given latent variables in probabilistic models."
    },
    {
      "id": "Expectation-Maximization Algorithm",
      "type": "major",
      "parent": null,
      "description": "Iterative method for finding maximum likelihood or maximum a posteriori estimates of parameters in statistical models."
    },
    {
      "id": "E-step",
      "type": "subnode",
      "parent": "Expectation-Maximization Algorithm",
      "description": "Calculates the expected value of the log-likelihood evaluated using the current estimate for the parameters."
    },
    {
      "id": "M-step",
      "type": "subnode",
      "parent": "Expectation-Maximization Algorithm",
      "description": "Maximizes the expected log-likelihood found in the E step as a function of the parameters."
    },
    {
      "id": "Q-function",
      "type": "subnode",
      "parent": "E-step",
      "description": "Function used to calculate the probability of latent variables given observed data and current parameter estimates."
    },
    {
      "id": "Update Rule for \\(\\mu_j\\)",
      "type": "subnode",
      "parent": "M-step",
      "description": "Rule derived from maximizing Q-function with respect to \\(\\mu_j\\)."
    },
    {
      "id": "Update Rule for \\(\\phi_j\\)",
      "type": "subnode",
      "parent": "M-step",
      "description": "Rule derived from maximizing Q-function with respect to \\(\\phi_j\\), left as an exercise."
    },
    {
      "id": "Gaussian Mixture Model (GMM)",
      "type": "major",
      "parent": null,
      "description": "Statistical model used in machine learning for clustering and density estimation."
    },
    {
      "id": "Update_Rule",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Rule for updating parameters based on weighted inputs."
    },
    {
      "id": "M-step_Update_for_phi_j",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Derivation of the update rule for parameter phi_j in EM algorithm."
    },
    {
      "id": "Lagrangian_Method",
      "type": "subnode",
      "parent": "M-step_Update_for_phi_j",
      "description": "Use of Lagrangian to handle constraints during optimization."
    },
    {
      "id": "Variational_Inference",
      "type": "major",
      "parent": "Variational_Autoencoder",
      "description": "Techniques for approximating posterior distributions in complex models using variational methods and neural networks."
    },
    {
      "id": "Variational_Autoencoder",
      "type": "subnode",
      "parent": "Variational_Inference",
      "description": "Model used to learn an efficient encoding of the input data by using an encoder and a decoder network."
    },
    {
      "id": "EM_Algorithms",
      "type": "subnode",
      "parent": "Machine_Learning",
      "description": "Expectation-Maximization algorithm for finding maximum likelihood estimates in probabilistic models with latent variables."
    },
    {
      "id": "Reparametrization_Trick",
      "type": "subnode",
      "parent": "Variational_Autoencoder",
      "description": "Method enabling the use of gradient descent for variational inference in models with continuous latent variables."
    },
    {
      "id": "Gaussian_Mixture_Models",
      "type": "subnode",
      "parent": "Machine_Learning",
      "description": "Probabilistic model that assumes all data points are generated from a mixture of several Gaussian distributions with unknown parameters."
    },
    {
      "id": "Neural_Network_Parameterization",
      "type": "subnode",
      "parent": "Variational_Autoencoder",
      "description": "Parameterizing the distribution of latent variables using neural networks."
    },
    {
      "id": "Posterior_Distribution",
      "type": "subnode",
      "parent": "Machine_Learning",
      "description": "Probability distribution of a random variable given evidence about other related random variables."
    },
    {
      "id": "Variational Inference",
      "type": "major",
      "parent": null,
      "description": "Technique to approximate posterior distributions in Bayesian models."
    },
    {
      "id": "Mean Field Assumption",
      "type": "subnode",
      "parent": "Variational Inference",
      "description": "Assumption that latent variables are independent, simplifying the posterior approximation."
    },
    {
      "id": "Discrete Latent Variables",
      "type": "subnode",
      "parent": "Mean Field Assumption",
      "description": "Latent variables with discrete values, often modeled using mean field assumption."
    },
    {
      "id": "Continuous Latent Variables",
      "type": "subnode",
      "parent": "Variational Inference",
      "description": "Latent variables with continuous values requiring more complex approximations than mean field alone."
    },
    {
      "id": "VariationalAutoEncoder",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Technique for learning latent variable models using neural networks."
    },
    {
      "id": "LatentVariableModel",
      "type": "subnode",
      "parent": "VariationalAutoEncoder",
      "description": "Models that use unobserved variables to represent hidden structure in data."
    },
    {
      "id": "ContinuousLatentVariables",
      "type": "subnode",
      "parent": "LatentVariableModel",
      "description": "Use of continuous latent variables in probabilistic models."
    },
    {
      "id": "GaussianDistributionQ_i",
      "type": "subnode",
      "parent": "ContinuousLatentVariables",
      "description": "Representation of distribution Q_i as a Gaussian with mean and variance functions."
    },
    {
      "id": "MeanAndVarianceFunctions",
      "type": "subnode",
      "parent": "GaussianDistributionQ_i",
      "description": "Description of q(x;phi) for mean and v(x;psi) for variance in the Gaussian model."
    },
    {
      "id": "EncoderDecoderConcepts",
      "type": "subnode",
      "parent": "VariationalAutoEncoder",
      "description": "Introduction to encoder (q, v) and decoder (g(z;theta)) concepts in VAE."
    },
    {
      "id": "ELBO_Optimization",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Details on optimizing the Evidence Lower Bound (ELBO) in variational inference."
    },
    {
      "id": "Form_of_Q_i",
      "type": "subnode",
      "parent": "ELBO_Optimization",
      "description": "Description of the form and requirements for Q_i in ELBO optimization."
    },
    {
      "id": "Efficient_Evaluation_ELBO",
      "type": "subnode",
      "parent": "ELBO_Optimization",
      "description": "Conditions under which ELBO can be efficiently evaluated given fixed Q and theta."
    },
    {
      "id": "Gradient_Ascend_ELBO",
      "type": "subnode",
      "parent": "ELBO_Optimization",
      "description": "Process of gradient ascent optimization for the parameters phi, psi, and theta in ELBO."
    },
    {
      "id": "VariationalInference",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Technique for approximating probability densities using variational methods."
    },
    {
      "id": "ReparameterizationTrick",
      "type": "subnode",
      "parent": "GradientComputation",
      "description": "Technique used to simplify gradient computation by re-parameterizing distributions."
    },
    {
      "id": "Gradient_Estimation",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Estimating gradients for optimization in machine learning models."
    },
    {
      "id": "Reparameterization_Trick",
      "type": "subnode",
      "parent": "Gradient_Estimation",
      "description": "Technique to estimate gradients through sampling."
    },
    {
      "id": "PCA_Methods",
      "type": "major",
      "parent": null,
      "description": "Principal Components Analysis for dimensionality reduction."
    },
    {
      "id": "Data_Subspace_Identification",
      "type": "subnode",
      "parent": "PCA_Methods",
      "description": "Identifying the subspace where data approximately lies."
    },
    {
      "id": "Computational_Efficiency",
      "type": "subnode",
      "parent": "PCA_Methods",
      "description": "Efficient computation using eigenvector calculations."
    },
    {
      "id": "RedundancyDetection",
      "type": "major",
      "parent": null,
      "description": "Detecting and removing redundant attributes in data sets."
    },
    {
      "id": "CarAttributesExample",
      "type": "subnode",
      "parent": "RedundancyDetection",
      "description": "Illustration of redundancy with car maximum speed measurements."
    },
    {
      "id": "PilotSurveyExample",
      "type": "subnode",
      "parent": "RedundancyDetection",
      "description": "Example involving correlation between piloting skill and enjoyment in RC helicopter pilots."
    },
    {
      "id": "PCAAlgorithm",
      "type": "major",
      "parent": null,
      "description": "Principal Component Analysis for dimensionality reduction."
    },
    {
      "id": "DataNormalization",
      "type": "subnode",
      "parent": "PCAAlgorithm",
      "description": "Preprocessing step to normalize data features before PCA application."
    },
    {
      "id": "MeanRemoval",
      "type": "subnode",
      "parent": "DataNormalization",
      "description": "Subtracting the mean from each feature to center the data around zero."
    },
    {
      "id": "VarianceScaling",
      "type": "subnode",
      "parent": "DataNormalization",
      "description": "Dividing by standard deviation to ensure unit variance for all features."
    },
    {
      "id": "FeatureComparison",
      "type": "subnode",
      "parent": "VarianceScaling",
      "description": "Ensures comparability of attributes with different scales."
    },
    {
      "id": "MajorAxisOfVariation",
      "type": "major",
      "parent": null,
      "description": "Finding the direction in which data varies most significantly."
    },
    {
      "id": "ProjectionObjective",
      "type": "subnode",
      "parent": "MajorAxisOfVariation",
      "description": "Maximizing variance of projected data to find major axis."
    },
    {
      "id": "NormalizedDatasetExample",
      "type": "major",
      "parent": null,
      "description": "Illustrative dataset after normalization steps are applied."
    },
    {
      "id": "PrincipalComponentAnalysis",
      "type": "major",
      "parent": null,
      "description": "Technique for reducing dimensionality while preserving variance."
    },
    {
      "id": "ProjectionDirection",
      "type": "subnode",
      "parent": "PrincipalComponentAnalysis",
      "description": "Choosing the direction to maximize projection variance."
    },
    {
      "id": "VarianceMaximization",
      "type": "subnode",
      "parent": "ProjectionDirection",
      "description": "Objective is to maximize variance of projections onto a unit vector."
    },
    {
      "id": "EmpiricalCovarianceMatrix",
      "type": "subnode",
      "parent": "PrincipalComponentAnalysis",
      "description": "Matrix representing the covariance between data points."
    },
    {
      "id": "EigenvectorsEigenvalues",
      "type": "subnode",
      "parent": "PrincipalComponentAnalysis",
      "description": "Key components for finding optimal projection directions."
    },
    {
      "id": "kDimensionalSubspace",
      "type": "subnode",
      "parent": "PrincipalComponentAnalysis",
      "description": "Projection into a lower-dimensional space using top k eigenvectors."
    },
    {
      "id": "PCA",
      "type": "major",
      "parent": null,
      "description": "Principal Component Analysis for dimensionality reduction"
    },
    {
      "id": "Eigenvectors",
      "type": "subnode",
      "parent": "PCA",
      "description": "Top k eigenvectors of covariance matrix \u03a3 form new orthogonal basis"
    },
    {
      "id": "Dimensionality Reduction",
      "type": "subnode",
      "parent": "PCA",
      "description": "Reduces data from d dimensions to k dimensions"
    },
    {
      "id": "Principal Components",
      "type": "subnode",
      "parent": "Eigenvectors",
      "description": "First k eigenvectors of \u03a3 are principal components"
    },
    {
      "id": "Approximation Error Minimization",
      "type": "subnode",
      "parent": "PCA",
      "description": "Minimizes error from projecting data onto k-dimensional subspace"
    },
    {
      "id": "Data Visualization",
      "type": "subnode",
      "parent": "Dimensionality Reduction",
      "description": "Using PCA to visualize similarities and clusters in high-dimensional data."
    },
    {
      "id": "Compression",
      "type": "subnode",
      "parent": "Dimensionality Reduction",
      "description": "Compresses high-dimensional data to lower dimensions"
    },
    {
      "id": "Machine Learning Techniques",
      "type": "major",
      "parent": null,
      "description": "Overview of various machine learning techniques and applications."
    },
    {
      "id": "Principal Component Analysis (PCA)",
      "type": "subnode",
      "parent": "Machine Learning Techniques",
      "description": "Dimensionality reduction technique that transforms data into principal components."
    },
    {
      "id": "Computational Efficiency",
      "type": "subnode",
      "parent": "Principal Component Analysis (PCA)",
      "description": "Reduces computational load by decreasing the dimensionality of input data."
    },
    {
      "id": "Overfitting Prevention",
      "type": "subnode",
      "parent": "Principal Component Analysis (PCA)",
      "description": "Helps in avoiding overfitting by simplifying the hypothesis space."
    },
    {
      "id": "Noise Reduction",
      "type": "subnode",
      "parent": "Principal Component Analysis (PCA)",
      "description": "Estimates intrinsic features from noisy data, like 'piloting karma' from noisy measures."
    },
    {
      "id": "Eigenfaces Method",
      "type": "subnode",
      "parent": "Principal Component Analysis (PCA)",
      "description": "Face recognition technique using PCA to reduce dimensionality of face images."
    },
    {
      "id": "Independent Components Analysis (ICA)",
      "type": "major",
      "parent": null,
      "description": "Technique for finding independent components in data, different from PCA's goal."
    },
    {
      "id": "Cocktail Party Problem",
      "type": "subnode",
      "parent": "Independent Components Analysis (ICA)",
      "description": "Motivational example for ICA where multiple speakers' voices are separated using ICA."
    },
    {
      "id": "Independent Component Analysis (ICA)",
      "type": "major",
      "parent": null,
      "description": "Technique to separate mixed signals into their independent sources."
    },
    {
      "id": "Mixing Matrix (A)",
      "type": "subnode",
      "parent": "Independent Component Analysis (ICA)",
      "description": "Matrix representing how independent sources mix to form observed data."
    },
    {
      "id": "Unmixing Matrix (W)",
      "type": "subnode",
      "parent": "Independent Component Analysis (ICA)",
      "description": "Inverse of mixing matrix used to recover original source signals from mixed data."
    },
    {
      "id": "ICA Ambiguities",
      "type": "major",
      "parent": null,
      "description": "Discussion on the limitations and uncertainties in recovering unmixing matrix W without prior knowledge."
    },
    {
      "id": "Permutation Matrix",
      "type": "subnode",
      "parent": "ICA Ambiguities",
      "description": "Describes the role of permutation matrices in ICA ambiguities."
    },
    {
      "id": "Scaling Ambiguity",
      "type": "subnode",
      "parent": "ICA Ambiguities",
      "description": "Explains how scaling factors affect ICA recovery without changing observed data."
    },
    {
      "id": "Sign Changes",
      "type": "subnode",
      "parent": "ICA Ambiguities",
      "description": "Discusses the irrelevance of sign changes in source signals for ICA applications."
    },
    {
      "id": "ICA_Ambiguities",
      "type": "major",
      "parent": null,
      "description": "Discusses ambiguities in Independent Component Analysis (ICA)"
    },
    {
      "id": "Scaling_Signal",
      "type": "subnode",
      "parent": "ICA_Ambiguities",
      "description": "Explains the effect of scaling a signal on ICA"
    },
    {
      "id": "Non_Gaussian_Sources",
      "type": "subnode",
      "parent": "ICA_Ambiguities",
      "description": "States that non-Gaussian sources resolve ambiguities in ICA"
    },
    {
      "id": "Gaussian_Data_Issue",
      "type": "subnode",
      "parent": "ICA_Ambiguities",
      "description": "Describes the problem with Gaussian data in ICA"
    },
    {
      "id": "Mixing_Matrix_Rotation",
      "type": "subnode",
      "parent": "Gaussian_Data_Issue",
      "description": "Explains how rotation of mixing matrix affects Gaussian data"
    },
    {
      "id": "ICAAlgorithm",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Derivation and understanding of the Independent Component Analysis algorithm."
    },
    {
      "id": "MixingMatrix",
      "type": "subnode",
      "parent": "ICAAlgorithm",
      "description": "Explanation of mixing matrices in ICA, including their properties and effects on data distribution."
    },
    {
      "id": "RotationalSymmetry",
      "type": "subnode",
      "parent": "MixingMatrix",
      "description": "Discussion on the impact of rotational symmetry in multivariate standard normal distributions."
    },
    {
      "id": "NonGaussianDataRecovery",
      "type": "subnode",
      "parent": "ICAAlgorithm",
      "description": "Conditions and methods for recovering independent sources from non-Gaussian data."
    },
    {
      "id": "LinearTransformations",
      "type": "major",
      "parent": null,
      "description": "Effect of linear transformations on densities in the context of machine learning."
    },
    {
      "id": "DensityCalculation",
      "type": "subnode",
      "parent": "LinearTransformations",
      "description": "Correct method for calculating density after a linear transformation involving an inverse matrix."
    },
    {
      "id": "Density Transformation",
      "type": "major",
      "parent": null,
      "description": "Transformation of density functions under linear mappings"
    },
    {
      "id": "1D Example",
      "type": "subnode",
      "parent": "Density Transformation",
      "description": "Example in one dimension with specific values and transformations"
    },
    {
      "id": "General Case",
      "type": "subnode",
      "parent": "Density Transformation",
      "description": "Extension to vector-valued distributions and general matrices"
    },
    {
      "id": "Volume Mapping",
      "type": "subnode",
      "parent": "Density Transformation",
      "description": "Explanation of volume changes under linear transformations in higher dimensions"
    },
    {
      "id": "ICA Algorithm",
      "type": "major",
      "parent": null,
      "description": "Derivation and interpretation of an Independent Component Analysis algorithm"
    },
    {
      "id": "ICAIndependenceAssumption",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Independent Component Analysis (ICA) assumes sources are independent and modeled by product distribution."
    },
    {
      "id": "JointDistributionModeling",
      "type": "subnode",
      "parent": "ICAIndependenceAssumption",
      "description": "Modeling the joint distribution of sources as a product of marginals to capture independence."
    },
    {
      "id": "DensityFunctionTransformation",
      "type": "subnode",
      "parent": "ICAIndependenceAssumption",
      "description": "Transforming density functions based on linear transformation and determinant of mixing matrix."
    },
    {
      "id": "CumulativeDistributionFunction",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Definition and properties of cumulative distribution function (CDF)."
    },
    {
      "id": "SigmoidFunctionChoice",
      "type": "subnode",
      "parent": "ICAIndependenceAssumption",
      "description": "Choosing sigmoid function as a default cdf for source densities in ICA."
    },
    {
      "id": "DataPreprocessing",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Process of preparing data for analysis by normalizing or transforming it to meet certain criteria."
    },
    {
      "id": "ParameterMatrixW",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Square matrix parameter W used in the model for data transformation and analysis."
    },
    {
      "id": "StochasticGradientAscent",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Optimization technique used in machine learning to iteratively update model parameters based on training examples."
    },
    {
      "id": "ConvergenceAndRecovery",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Process of achieving a stable solution and recovering original data sources from transformed data."
    },
    {
      "id": "Stochastic Gradient Ascent",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Technique for optimizing model parameters using randomly selected data points."
    },
    {
      "id": "Self-supervised Learning",
      "type": "major",
      "parent": null,
      "description": "Learning method that uses the input data itself as a source of supervision to train models."
    },
    {
      "id": "Foundation Models",
      "type": "subnode",
      "parent": "Self-supervised Learning",
      "description": "Large-scale pre-trained models adaptable to various downstream tasks with limited labeled data."
    },
    {
      "id": "Pretraining and Adaptation",
      "type": "subnode",
      "parent": "Foundation Models",
      "description": "Two-phase process involving training on unlabeled data followed by adaptation to specific tasks."
    },
    {
      "id": "Machine_Learning_Pretraining_Adaptation",
      "type": "major",
      "parent": null,
      "description": "Overview of pretraining and adaptation phases in machine learning."
    },
    {
      "id": "Pretraining_Phase",
      "type": "subnode",
      "parent": "Machine_Learning_Pretraining_Adaptation",
      "description": "Training a model on unlabeled data to learn general representations."
    },
    {
      "id": "Adaptation_Phase",
      "type": "subnode",
      "parent": "Machine_Learning_Pretraining_Adaptation",
      "description": "Customizing the pretrained model for specific tasks with labeled data."
    },
    {
      "id": "Unlabeled_Dataset",
      "type": "subnode",
      "parent": "Pretraining_Phase",
      "description": "Dataset used in pretraining phase, consisting of unlabeled examples."
    },
    {
      "id": "Model_Parameterization",
      "type": "subnode",
      "parent": "Pretraining_Phase",
      "description": "Description of model parameters and input-output mapping."
    },
    {
      "id": "Pretraining_Loss_Function",
      "type": "subnode",
      "parent": "Pretraining_Phase",
      "description": "Loss function used to train the model during pretraining phase."
    },
    {
      "id": "Labeled_Dataset_Task",
      "type": "subnode",
      "parent": "Adaptation_Phase",
      "description": "Dataset with labeled examples for adaptation phase."
    },
    {
      "id": "Downstream_Tasks",
      "type": "subnode",
      "parent": "Machine_Learning_Pretraining_Adaptation",
      "description": "Tasks that use pretrained models and adapt them to specific domains."
    },
    {
      "id": "Machine Learning Tasks",
      "type": "major",
      "parent": null,
      "description": "Tasks in machine learning including zero-shot and few-shot learning."
    },
    {
      "id": "Zero-Shot Learning",
      "type": "subnode",
      "parent": "Machine Learning Tasks",
      "description": "Learning scenario with no labeled examples for the task."
    },
    {
      "id": "Few-Shot Learning",
      "type": "subnode",
      "parent": "Machine Learning Tasks",
      "description": "Scenario where only a small number of labeled examples are available."
    },
    {
      "id": "Adaptation Algorithms",
      "type": "major",
      "parent": null,
      "description": "Methods used to adapt pre-trained models for downstream tasks."
    },
    {
      "id": "Linear Probe",
      "type": "subnode",
      "parent": "Adaptation Algorithms",
      "description": "Uses a linear head on top of the representation to predict labels without modifying the pretrained model."
    },
    {
      "id": "Finetuning",
      "type": "subnode",
      "parent": "Adaptation Algorithms",
      "description": "Further trains both the pretrained model and the prediction model for downstream tasks."
    },
    {
      "id": "Downstream Dataset",
      "type": "major",
      "parent": null,
      "description": "Dataset used to adapt a pre-trained model for specific tasks."
    },
    {
      "id": "Machine_Learning_Adaptation",
      "type": "major",
      "parent": null,
      "description": "Overview of machine learning adaptation techniques."
    },
    {
      "id": "Finetuning_Pretrained_Models",
      "type": "subnode",
      "parent": "Machine_Learning_Adaptation",
      "description": "Process of finetuning pretrained models for downstream tasks."
    },
    {
      "id": "Prediction_Model_Structure",
      "type": "subnode",
      "parent": "Finetuning_Pretrained_Models",
      "description": "Structure of the prediction model using both pretrained and new parameters."
    },
    {
      "id": "Optimization_Objective",
      "type": "subnode",
      "parent": "Finetuning_Pretrained_Models",
      "description": "Objective function for optimizing the prediction model's parameters."
    },
    {
      "id": "Pretraining_Methods",
      "type": "major",
      "parent": null,
      "description": "Introduction to pretraining methods in machine learning."
    },
    {
      "id": "Supervised_Pretraining",
      "type": "subnode",
      "parent": "Pretraining_Methods",
      "description": "Method of using labeled datasets for supervised training of neural networks."
    },
    {
      "id": "Contrastive_Learning",
      "type": "subnode",
      "parent": "Pretraining_Methods",
      "description": "Technique for learning representations by contrasting similar and dissimilar instances."
    },
    {
      "id": "Self-Supervised Learning",
      "type": "major",
      "parent": null,
      "description": "Method using unlabeled data for pretraining."
    },
    {
      "id": "Representation Function",
      "type": "subnode",
      "parent": "Self-Supervised Learning",
      "description": "Function mapping images to representations."
    },
    {
      "id": "Positive Pair",
      "type": "subnode",
      "parent": "Self-Supervised Learning",
      "description": "Augmented versions of the same image."
    },
    {
      "id": "Negative Pair",
      "type": "subnode",
      "parent": "Self-Supervised Learning",
      "description": "Randomly selected augmented images from different originals."
    },
    {
      "id": "Supervised Contrastive Algorithms",
      "type": "subnode",
      "parent": "Self-Supervised Learning",
      "description": "Algorithms using labeled data for contrastive learning."
    },
    {
      "id": "SIMCLR",
      "type": "subnode",
      "parent": "Contrastive_Learning",
      "description": "Algorithm introduced in 2020 that uses contrastive learning to learn image representations."
    },
    {
      "id": "Loss_Function",
      "type": "subnode",
      "parent": "SIMCLR",
      "description": "Function used by SIMCLR to measure the similarity between augmented data pairs."
    },
    {
      "id": "Augmentation_Techniques",
      "type": "subnode",
      "parent": "SIMCLR",
      "description": "Techniques applied to input data to create diverse training samples."
    },
    {
      "id": "Negative_Pairs",
      "type": "subnode",
      "parent": "Contrastive_Learning",
      "description": "Pairs of instances that are not similar, used for contrast in learning algorithms."
    },
    {
      "id": "LossFunctionOptimization",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Techniques for optimizing loss functions in ML models."
    },
    {
      "id": "PretrainedLanguageModels",
      "type": "major",
      "parent": null,
      "description": "Introduction to pretrained large language models in NLP."
    },
    {
      "id": "NaturalLanguageProcessing",
      "type": "subnode",
      "parent": "PretrainedLanguageModels",
      "description": "Overview of natural language processing and its applications."
    },
    {
      "id": "DocumentProbabilityModeling",
      "type": "subnode",
      "parent": "PretrainedLanguageModels",
      "description": "Techniques for modeling the probability distribution of a document in NLP."
    },
    {
      "id": "Conditional_Probability_Modeling",
      "type": "subnode",
      "parent": "Machine_Learning_Models",
      "description": "Modeling the probability of a word given previous words in a sequence."
    },
    {
      "id": "Embeddings_and_Representations",
      "type": "subnode",
      "parent": "Machine_Learning_Models",
      "description": "Introduction and use of embeddings for discrete variables such as words."
    },
    {
      "id": "Transformer_Model",
      "type": "subnode",
      "parent": "Machine_Learning_Models",
      "description": "Description of the input-output interface of a Transformer model used in sequence prediction tasks."
    },
    {
      "id": "Autoregressive_Transformer",
      "type": "subnode",
      "parent": "Transformer_Model",
      "description": "Version of the Transformer that ensures each output depends only on previous inputs."
    },
    {
      "id": "Conditional_Probability",
      "type": "subnode",
      "parent": "Transformer_Model",
      "description": "Probability of the next input given previous inputs in the context of Transformer models."
    },
    {
      "id": "Training_Transformer",
      "type": "subnode",
      "parent": "Transformer_Model",
      "description": "Process of minimizing the negative log-likelihood using cross-entropy loss."
    },
    {
      "id": "Autoregressive_Decoding",
      "type": "major",
      "parent": null,
      "description": "Generating text sequentially from a Transformer model given an initial prefix."
    },
    {
      "id": "TemperatureParameter",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Introduction to the temperature parameter used in softmax functions for adjusting model output sharpness."
    },
    {
      "id": "TextGenerationModels",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Discussion on models that generate text based on learned probabilities."
    },
    {
      "id": "ZeroShotLearning",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Exploration of zero-shot learning in the context of adapting pre-trained language models to new tasks without task-specific data."
    },
    {
      "id": "Zero-Shot_Adaptation",
      "type": "subnode",
      "parent": "Machine_Learning_Adaptation",
      "description": "Technique where no input-output pairs from downstream tasks are available."
    },
    {
      "id": "In-Context_Learning",
      "type": "subnode",
      "parent": "Machine_Learning_Adaptation",
      "description": "Approach used for few-shot settings with a small number of labeled examples."
    },
    {
      "id": "Reinforcement Learning",
      "type": "major",
      "parent": null,
      "description": "Study of algorithms for sequential decision making and control."
    },
    {
      "id": "Supervised Learning vs Reinforcement Learning",
      "type": "subnode",
      "parent": "Reinforcement Learning",
      "description": "Comparison between supervised learning and reinforcement learning approaches."
    },
    {
      "id": "MachineLearning",
      "type": "major",
      "parent": null,
      "description": "Field of study focusing on algorithms that learn from data."
    },
    {
      "id": "ReinforcementLearning",
      "type": "subnode",
      "parent": "MachineLearning",
      "description": "Type of ML where agents learn by interacting with an environment to maximize rewards."
    },
    {
      "id": "MarkovDecisionProcesses",
      "type": "subnode",
      "parent": "ReinforcementLearning",
      "description": "Formal framework for modeling decision-making situations in RL."
    },
    {
      "id": "States",
      "type": "subnode",
      "parent": "MarkovDecisionProcesses",
      "description": "Set of all possible conditions or configurations an agent can be in."
    },
    {
      "id": "Actions",
      "type": "subnode",
      "parent": "MarkovDecisionProcesses",
      "description": "Set of all possible actions an agent can take in a given state."
    },
    {
      "id": "StateTransitionProbabilities",
      "type": "subnode",
      "parent": "MarkovDecisionProcesses",
      "description": "Probability distribution over states after taking an action from the current state."
    },
    {
      "id": "DiscountFactor",
      "type": "subnode",
      "parent": "MarkovDecisionProcesses",
      "description": "Parameter that discounts future rewards, ensuring finite sum of discounted rewards."
    },
    {
      "id": "RewardFunction",
      "type": "subnode",
      "parent": "MarkovDecisionProcesses",
      "description": "Function mapping state-action pairs to real numbers representing immediate reward."
    },
    {
      "id": "Reinforcement_Learning",
      "type": "major",
      "parent": "Machine_Learning",
      "description": "Field of machine learning focusing on decision-making in stochastic environments."
    },
    {
      "id": "Markov_Decision_Processes_(MDP)",
      "type": "subnode",
      "parent": "Reinforcement_Learning",
      "description": "Model for sequential decision making under uncertainty."
    },
    {
      "id": "State_Transitions",
      "type": "subnode",
      "parent": "Markov_Decision_Processes_(MDP)",
      "description": "Random transitions between states based on actions and transition probabilities."
    },
    {
      "id": "Discounted_Rewards",
      "type": "subnode",
      "parent": "Reinforcement_Learning",
      "description": "Rewards at future timesteps are discounted by a factor of gamma."
    },
    {
      "id": "Policy",
      "type": "subnode",
      "parent": "Reinforcement_Learning",
      "description": "Function mapping states to actions, guiding decision-making in MDPs."
    },
    {
      "id": "Value_Function",
      "type": "subnode",
      "parent": "Reinforcement_Learning",
      "description": "Expected cumulative reward starting from a state under a given policy."
    },
    {
      "id": "Policy Execution",
      "type": "major",
      "parent": null,
      "description": "Process of following a policy in state-action mapping."
    },
    {
      "id": "Value Function",
      "type": "subnode",
      "parent": "Policy Execution",
      "description": "Functions that represent the expected return starting from a state under a policy or optimally."
    },
    {
      "id": "Bellman Equations",
      "type": "subnode",
      "parent": "Value Function",
      "description": "Equations used to solve for the value function in an MDP."
    },
    {
      "id": "Immediate Reward",
      "type": "subnode",
      "parent": "Bellman Equations",
      "description": "Reward received immediately upon entering a state."
    },
    {
      "id": "Policy Evaluation",
      "type": "subnode",
      "parent": "Value Function",
      "description": "Evaluating the value of states under a given policy."
    },
    {
      "id": "Optimal Value Function",
      "type": "subnode",
      "parent": "Value Function",
      "description": "The best possible expected sum of discounted rewards for any state and policy."
    },
    {
      "id": "Bellman's Equation",
      "type": "subnode",
      "parent": "Value Function",
      "description": "Equations that define the value function recursively in terms of successor states."
    },
    {
      "id": "Optimal Policy",
      "type": "subnode",
      "parent": "Value Function",
      "description": "Policy that maximizes the expected utility from a given state."
    },
    {
      "id": "MDP (Markov Decision Process)",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Framework for modeling decision-making situations in AI."
    },
    {
      "id": "Value Iteration and Policy Iteration",
      "type": "major",
      "parent": null,
      "description": "Algorithms for solving finite-state MDPs."
    },
    {
      "id": "Finite-State MDPs",
      "type": "subnode",
      "parent": "Value Iteration and Policy Iteration",
      "description": "MDPs with a finite number of states and actions."
    },
    {
      "id": "State Transition Probabilities",
      "type": "subnode",
      "parent": "Finite-State MDPs",
      "description": "Probabilities of moving from one state to another in an MDP."
    },
    {
      "id": "Reward Function",
      "type": "subnode",
      "parent": "Finite-State MDPs",
      "description": "Function that assigns a numerical value for each state-action pair."
    },
    {
      "id": "Value Iteration Algorithm",
      "type": "subnode",
      "parent": "Value Iteration and Policy Iteration",
      "description": "Algorithm to iteratively update the estimated value function in an MDP."
    },
    {
      "id": "Synchronous Updates",
      "type": "subnode",
      "parent": "Value Iteration Algorithm",
      "description": "Updating all state values simultaneously before moving on."
    },
    {
      "id": "Asynchronous Updates",
      "type": "subnode",
      "parent": "Value Iteration Algorithm",
      "description": "Updating one state value at a time in sequence."
    },
    {
      "id": "Value_Iteration",
      "type": "subnode",
      "parent": "Machine_Learning_Algorithms",
      "description": "Algorithm that uses the Bellman equation to find optimal policies through iterative computation of value functions."
    },
    {
      "id": "Policy_Iteration",
      "type": "subnode",
      "parent": "Machine_Learning_Algorithms",
      "description": "Alternative algorithm to value iteration that directly optimizes the policy function."
    },
    {
      "id": "Convergence_to_Optimal_Value",
      "type": "subnode",
      "parent": "Value_Iteration",
      "description": "Process of value iteration converging to optimal state values."
    },
    {
      "id": "Optimal_Policy_Finding",
      "type": "subnode",
      "parent": "Value_Iteration",
      "description": "Using converged values to determine the best policy."
    },
    {
      "id": "Policy_Evaluation",
      "type": "subnode",
      "parent": "Policy_Iteration",
      "description": "Computing value function for current policy in each iteration."
    },
    {
      "id": "Policy_Improvement",
      "type": "subnode",
      "parent": "Policy_Iteration",
      "description": "Updating the policy based on the computed value function."
    },
    {
      "id": "Bellman_Equations",
      "type": "subnode",
      "parent": "Machine_Learning_Algorithms",
      "description": "Set of equations used to solve for optimal policies in MDPs."
    },
    {
      "id": "Greedy_Policy_With_Respect_To_V",
      "type": "subnode",
      "parent": "Policy_Iteration",
      "description": "Policy derived from value function that maximizes immediate reward."
    },
    {
      "id": "Model_Learning_for_MDPs",
      "type": "major",
      "parent": null,
      "description": "Learning state transition probabilities and rewards from data in MDPs."
    },
    {
      "id": "Inverted_Pendulum_Problem",
      "type": "subnode",
      "parent": "Model_Learning_for_MDPs",
      "description": "Example problem used to illustrate learning models from experience."
    },
    {
      "id": "State_Transition_Probabilities",
      "type": "subnode",
      "parent": "Model_Learning_for_MDPs",
      "description": "Probabilities that describe how an agent transitions between states based on actions taken."
    },
    {
      "id": "Maximum_Likelihood_Estimation",
      "type": "subnode",
      "parent": "State_Transition_Probabilities",
      "description": "Method for estimating parameters by maximizing the likelihood function based on observed data."
    },
    {
      "id": "Markov_Decision_Processes_MDPs",
      "type": "subnode",
      "parent": "Machine_Learning",
      "description": "Framework for modeling decision making in situations where outcomes are partly random and partly under the control of a decision maker."
    },
    {
      "id": "Reward_Functions",
      "type": "subnode",
      "parent": "Markov_Decision_Processes_MDPs",
      "description": "Function assigning a numerical value to each possible state or state-action pair in the MDP."
    },
    {
      "id": "Model_Learning",
      "type": "subnode",
      "parent": "Markov_Decision_Processes_MDPs",
      "description": "Process of learning an accurate model of the environment from experience."
    },
    {
      "id": "Unknown_State_Transitions_and_Rewards",
      "type": "subnode",
      "parent": "Model_Learning",
      "description": "Learning state transitions and rewards when they are not fully known in advance."
    },
    {
      "id": "Efficient_Experience_Use",
      "type": "subnode",
      "parent": "Unknown_State_Transitions_and_Rewards",
      "description": "Techniques for updating estimates based on new experience efficiently."
    },
    {
      "id": "Continuous_State_MDPs",
      "type": "major",
      "parent": null,
      "description": "MDPs with an infinite number of states, often represented in continuous space."
    },
    {
      "id": "Discretization_Method",
      "type": "subnode",
      "parent": "Continuous_State_MDPs",
      "description": "Approach to solving MDPs by converting the state space into a discrete form."
    },
    {
      "id": "Discretization in MDPs",
      "type": "subnode",
      "parent": "Machine Learning Overview",
      "description": "Process of converting continuous state spaces into discrete ones for easier computation."
    },
    {
      "id": "Value Iteration",
      "type": "subnode",
      "parent": "Discretization in MDPs",
      "description": "Algorithm for finding optimal policies in reinforcement learning through iterative updates."
    },
    {
      "id": "Policy Iteration",
      "type": "subnode",
      "parent": "Discretization in MDPs",
      "description": "An iterative method for finding optimal policies in reinforcement learning."
    },
    {
      "id": "Supervised Learning",
      "type": "major",
      "parent": null,
      "description": "Type of machine learning where models are trained on labeled data to predict outcomes."
    },
    {
      "id": "Discretization Limitations",
      "type": "major",
      "parent": null,
      "description": "Issues arising from using a piecewise constant representation in discretized models."
    },
    {
      "id": "Curse of Dimensionality",
      "type": "subnode",
      "parent": "Discretization Limitations",
      "description": "Problem where the volume of the state space increases exponentially with dimensionality, making it difficult to represent and learn from data."
    },
    {
      "id": "Discretization_Methods",
      "type": "subnode",
      "parent": "Machine_Learning_Topics",
      "description": "Techniques for discretizing continuous state spaces."
    },
    {
      "id": "Curse_of_Dimensionality",
      "type": "subnode",
      "parent": "Discretization_Methods",
      "description": "Exponential growth of discrete states with dimensionality."
    },
    {
      "id": "Value_Function_Approximation",
      "type": "major",
      "parent": "Reinforcement_Learning",
      "description": "Alternative method for finding policies in continuous-state MDPs without discretization."
    },
    {
      "id": "Model_or_Simulator",
      "type": "subnode",
      "parent": "Value_Function_Approximation",
      "description": "Black-box that simulates state transitions based on input actions and states."
    },
    {
      "id": "Model Creation Methods",
      "type": "major",
      "parent": null,
      "description": "Different methods to create a model for state transitions in machine learning."
    },
    {
      "id": "Physics Simulation",
      "type": "subnode",
      "parent": "Model Creation Methods",
      "description": "Using physical laws and parameters to simulate system behavior."
    },
    {
      "id": "Off-the-Shelf Physics Software",
      "type": "subnode",
      "parent": "Model Creation Methods",
      "description": "Utilizing existing physics simulation software packages."
    },
    {
      "id": "Learning from Data",
      "type": "subnode",
      "parent": "Model Creation Methods",
      "description": "Deriving models based on data collected through repeated trials in an MDP."
    },
    {
      "id": "Open Dynamics Engine",
      "type": "subnode",
      "parent": "Off-the-Shelf Physics Software",
      "description": "Free/open-source physics simulator used for simulating mechanical systems like inverted pendulum."
    },
    {
      "id": "MachineLearningModeling",
      "type": "major",
      "parent": null,
      "description": "Overview of modeling approaches in machine learning"
    },
    {
      "id": "StateTransitionModel",
      "type": "subnode",
      "parent": "MachineLearningModeling",
      "description": "Models predicting next state given current state and action"
    },
    {
      "id": "LinearModel",
      "type": "subnode",
      "parent": "StateTransitionModel",
      "description": "Predicts next state using linear functions of current state and action"
    },
    {
      "id": "DeterministicModel",
      "type": "subnode",
      "parent": "StateTransitionModel",
      "description": "Next state is exactly determined by inputs"
    },
    {
      "id": "StochasticModel",
      "type": "subnode",
      "parent": "StateTransitionModel",
      "description": "Next state includes a random noise term"
    },
    {
      "id": "Non-Linear Feature Mappings",
      "type": "subnode",
      "parent": "Machine Learning Models",
      "description": "Feature transformations that allow models to capture complex relationships between inputs and outputs."
    },
    {
      "id": "MDP Simulators",
      "type": "subnode",
      "parent": "Machine Learning Models",
      "description": "Simulations of Markov Decision Processes using learned models to predict outcomes."
    },
    {
      "id": "Fitted Value Iteration",
      "type": "major",
      "parent": null,
      "description": "Algorithm for approximating value functions in continuous state MDPs using supervised learning methods."
    },
    {
      "id": "Continuous State Space",
      "type": "subnode",
      "parent": "Fitted Value Iteration",
      "description": "The space of all possible states is continuous and multidimensional."
    },
    {
      "id": "Discrete Action Space",
      "type": "subnode",
      "parent": "Fitted Value Iteration",
      "description": "A finite set of actions available in the decision-making process."
    },
    {
      "id": "Value Function Approximation",
      "type": "subnode",
      "parent": "Fitted Value Iteration",
      "description": "Estimating value functions using a supervised learning approach over a sample of states."
    },
    {
      "id": "Supervised Learning Algorithm",
      "type": "subnode",
      "parent": "Fitted Value Iteration",
      "description": "Used to get V(s) close to y^(i)."
    },
    {
      "id": "State Sample",
      "type": "subnode",
      "parent": "Fitted Value Iteration",
      "description": "Randomly sampled states used for approximation."
    },
    {
      "id": "Action Evaluation",
      "type": "subnode",
      "parent": "Value Function Approximation",
      "description": "Evaluates Q(a) for each action a in A."
    },
    {
      "id": "Reward Estimation",
      "type": "subnode",
      "parent": "Action Evaluation",
      "description": "Estimates R(s)+gamma*V(s') using sampled transitions."
    },
    {
      "id": "Max Action Selection",
      "type": "subnode",
      "parent": "Value Function Approximation",
      "description": "Selects action with highest Q(a) as y^(i)."
    },
    {
      "id": "Parameter Update",
      "type": "subnode",
      "parent": "Fitted Value Iteration",
      "description": "Updates theta to minimize squared error between V(s) and y^(i)."
    },
    {
      "id": "SupervisedLearning",
      "type": "major",
      "parent": null,
      "description": "Machine learning technique using labeled data to predict outcomes."
    },
    {
      "id": "FittedValueIteration",
      "type": "subnode",
      "parent": "SupervisedLearning",
      "description": "Technique using regression algorithms to approximate value functions in reinforcement learning problems."
    },
    {
      "id": "DeterministicSimulator",
      "type": "subnode",
      "parent": "FittedValueIteration",
      "description": "Simplified model where a single sample is sufficient for exact computation of expectation."
    },
    {
      "id": "Expectation_Calculation",
      "type": "subnode",
      "parent": "Value_Function_Approximation",
      "description": "Method to calculate expected values under a probability distribution Psa."
    },
    {
      "id": "Sampling_Methods",
      "type": "subnode",
      "parent": "Expectation_Calculation",
      "description": "Techniques for approximating expectations through random sampling."
    },
    {
      "id": "Deterministic_Simulator",
      "type": "subnode",
      "parent": "Value_Function_Approximation",
      "description": "Scenario where the simulator is deterministic, simplifying expectation calculations."
    },
    {
      "id": "Gaussian_Noise_Model",
      "type": "subnode",
      "parent": "Expectation_Calculation",
      "description": "Model incorporating Gaussian noise for more realistic simulations."
    },
    {
      "id": "Value_Evaluation_Procedure",
      "type": "subnode",
      "parent": "Policy_Iteration",
      "description": "Procedure for evaluating value functions within policy iteration framework."
    },
    {
      "id": "Algorithm 6",
      "type": "subnode",
      "parent": "Machine Learning Algorithms",
      "description": "Variant of policy iteration that uses value evaluation procedure VE."
    },
    {
      "id": "VE Procedure",
      "type": "subnode",
      "parent": "Algorithm 6",
      "description": "Procedure for evaluating the value function in reinforcement learning algorithms."
    },
    {
      "id": "Option 1 Initialization",
      "type": "subnode",
      "parent": "VE Procedure",
      "description": "Initialization method where V(s) is set to zero."
    },
    {
      "id": "Option 2 Initialization",
      "type": "subnode",
      "parent": "VE Procedure",
      "description": "Initialization method using the current value function from the main algorithm."
    },
    {
      "id": "Update Rule (15.12)",
      "type": "subnode",
      "parent": "Algorithm 6",
      "description": "Rule for updating state values in reinforcement learning algorithms."
    },
    {
      "id": "Policy Update Rule (15.13)",
      "type": "subnode",
      "parent": "Algorithm 6",
      "description": "Rule for updating the policy based on the value function."
    },
    {
      "id": "Chapter16",
      "type": "major",
      "parent": null,
      "description": "Introduction to LQR, DDP and LQG in the context of finite-horizon MDPs."
    },
    {
      "id": "FiniteHorizonMDPs",
      "type": "subnode",
      "parent": "Chapter16",
      "description": "Discussion on Markov Decision Processes (MDPs) with a focus on finite horizon scenarios."
    },
    {
      "id": "OptimalBellmanEquation",
      "type": "subnode",
      "parent": "FiniteHorizonMDPs",
      "description": "Definition and explanation of the optimal Bellman equation for determining the optimal value function."
    },
    {
      "id": "ValueIterationPolicyIteration",
      "type": "subnode",
      "parent": "FiniteHorizonMDPs",
      "description": "Explanation of Value Iteration and Policy Iteration methods in solving MDPs."
    },
    {
      "id": "OptimalPolicyRecovery",
      "type": "subnode",
      "parent": "FiniteHorizonMDPs",
      "description": "Description of how to recover the optimal policy from the optimal value function."
    },
    {
      "id": "GeneralSettingEquations",
      "type": "subnode",
      "parent": "Chapter16",
      "description": "Introduction to equations that apply in both discrete and continuous settings for MDPs."
    },
    {
      "id": "ExpectationRewriting",
      "type": "major",
      "parent": null,
      "description": "Discusses rewriting expectations in finite and continuous cases."
    },
    {
      "id": "StateActionDependentRewards",
      "type": "subnode",
      "parent": "ExpectationRewriting",
      "description": "Explains rewards depending on both states and actions."
    },
    {
      "id": "InfiniteHorizonMDP",
      "type": "major",
      "parent": null,
      "description": "Describes the concept of an infinite horizon Markov Decision Process (MDP)."
    },
    {
      "id": "FiniteHorizonMDP",
      "type": "subnode",
      "parent": "InfiniteHorizonMDP",
      "description": "Introduces finite horizon MDPs and their characteristics."
    },
    {
      "id": "OptimalPolicyFiniteHorizon",
      "type": "subnode",
      "parent": "FiniteHorizonMDP",
      "description": "Discusses optimal policy changes over time in finite horizon settings."
    },
    {
      "id": "Markov Decision Processes (MDP)",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Models for decision making under uncertainty."
    },
    {
      "id": "Policy in MDPs",
      "type": "subnode",
      "parent": "Markov Decision Processes (MDP)",
      "description": "A strategy that defines the action to take at each state."
    },
    {
      "id": "Time-Dependent Policy",
      "type": "subnode",
      "parent": "Policy in MDPs",
      "description": "A policy that changes over time, denoted as \\(\\pi^{(t)}\\)."
    },
    {
      "id": "Finite Horizon Setting",
      "type": "subnode",
      "parent": "Markov Decision Processes (MDP)",
      "description": "An MDP with a fixed number of steps."
    },
    {
      "id": "Optimal Policy in Finite Horizon",
      "type": "subnode",
      "parent": "Finite Horizon Setting",
      "description": "The best strategy that changes based on time and remaining actions."
    },
    {
      "id": "Time Dependent Dynamics",
      "type": "subnode",
      "parent": "Markov Decision Processes (MDP)",
      "description": "Transition probabilities that change over time, denoted as \\(P_{sa}^{(t)}\\)."
    },
    {
      "id": "Value Function in MDPs",
      "type": "subnode",
      "parent": "Markov Decision Processes (MDP)",
      "description": "The expected utility of a policy starting from state \\(s\\) at time \\(t\\)."
    },
    {
      "id": "Value_Functions",
      "type": "subnode",
      "parent": "Reinforcement_Learning",
      "description": "Functions representing the expected cumulative reward from a given state under an optimal policy."
    },
    {
      "id": "Optimal_Value_Function",
      "type": "subnode",
      "parent": "Value_Functions",
      "description": "The value function for an optimal policy that maximizes the cumulative reward."
    },
    {
      "id": "Bellman_Equation",
      "type": "subnode",
      "parent": "Reinforcement_Learning",
      "description": "Equation defining the value of being in a state as the maximum expected utility over all possible actions."
    },
    {
      "id": "Dynamic_Programming",
      "type": "subnode",
      "parent": "Reinforcement_Learning",
      "description": "Methodology for solving complex problems by breaking them down into simpler sub-problems in a recursive manner."
    },
    {
      "id": "Bellman Update",
      "type": "subnode",
      "parent": "Value Iteration",
      "description": "Operator used to update value functions iteratively towards the optimal solution."
    },
    {
      "id": "Theorem",
      "type": "subnode",
      "parent": "Value Iteration",
      "description": "Mathematical proof of geometric convergence in value iteration."
    },
    {
      "id": "Continuous Setting",
      "type": "subnode",
      "parent": "Linear Quadratic Regulation (LQR)",
      "description": "Assumptions about state and action spaces as continuous real vectors."
    },
    {
      "id": "Linear Transitions",
      "type": "subnode",
      "parent": "Linear Quadratic Regulation (LQR)",
      "description": "Model of system dynamics with linear equations and Gaussian noise."
    },
    {
      "id": "Quadratic Rewards",
      "type": "subnode",
      "parent": "Linear Quadratic Regulation (LQR)",
      "description": "Reward function that is quadratic in state and action variables, ensuring negative rewards."
    },
    {
      "id": "LQRModelAssumptions",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Assumptions made for the Linear Quadratic Regulator (LQR) model in reinforcement learning."
    },
    {
      "id": "Step1Estimation",
      "type": "subnode",
      "parent": "LQRModelAssumptions",
      "description": "First step of LQR algorithm: estimating unknown matrices using value approximation and linear regression."
    },
    {
      "id": "ValueApproximation",
      "type": "subnode",
      "parent": "Step1Estimation",
      "description": "Technique for approximating values in reinforcement learning to estimate model parameters."
    },
    {
      "id": "Step2OptimalPolicy",
      "type": "subnode",
      "parent": "LQRModelAssumptions",
      "description": "Second step of LQR algorithm: deriving optimal policy using dynamic programming given known model parameters."
    },
    {
      "id": "DynamicProgramming",
      "type": "subnode",
      "parent": "Step2OptimalPolicy",
      "description": "Technique for solving the optimization problem to find the optimal value function V_t*."
    },
    {
      "id": "Optimal_Control",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Theoretical framework for finding optimal control policies."
    },
    {
      "id": "Quadratic_Value_Functions",
      "type": "subnode",
      "parent": "Value_Functions",
      "description": "Value functions that are quadratic in terms of state variables."
    },
    {
      "id": "Optimal_Policy_Derivation",
      "type": "subnode",
      "parent": "Optimal_Control",
      "description": "Deriving the optimal policy from value function properties and dynamics model."
    },
    {
      "id": "Linear_Optimal_Policy",
      "type": "subnode",
      "parent": "Optimal_Policy_Derivation",
      "description": "Result showing that under certain conditions, the optimal policy is linear in state variables."
    },
    {
      "id": "Optimal_Policy_Linear_Systems",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Discussion on optimal policies for linear systems."
    },
    {
      "id": "Discrete_Ricatti_Equations",
      "type": "subnode",
      "parent": "Optimal_Policy_Linear_Systems",
      "description": "Equations governing the evolution of \u03a6_t and \u03a8_t in discrete time."
    },
    {
      "id": "LQR_Algorithm",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Algorithm for solving Linear Quadratic Regulator problems."
    },
    {
      "id": "Nonlinear_Dynamics_to_LQR",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Reduction of nonlinear dynamics to LQR framework."
    },
    {
      "id": "Inverted_Pendulum",
      "type": "subnode",
      "parent": "Dynamics_and_Control_Systems",
      "description": "A classic control problem used to illustrate system dynamics."
    },
    {
      "id": "System_Dynamics",
      "type": "subnode",
      "parent": "Dynamics_and_Control_Systems",
      "description": "Study of how systems change over time and respond to inputs."
    },
    {
      "id": "Linearization_of_Dynamics",
      "type": "subnode",
      "parent": "System_Dynamics",
      "description": "Process of approximating nonlinear dynamics with linear equations."
    },
    {
      "id": "Taylor_Expansion",
      "type": "subnode",
      "parent": "Linearization_of_Dynamics",
      "description": "Mathematical technique for approximating functions using polynomials."
    },
    {
      "id": "LQR_Assumptions",
      "type": "subnode",
      "parent": "Linearization_of_Dynamics",
      "description": "Similarities between linearized dynamics and Linear Quadratic Regulator assumptions."
    },
    {
      "id": "Differential_Dynamic_Programming",
      "type": "major",
      "parent": null,
      "description": "Optimization technique for trajectory planning in nonlinear systems."
    },
    {
      "id": "DifferentialDynamicProgramming",
      "type": "major",
      "parent": null,
      "description": "Method used to optimize trajectories in dynamic systems."
    },
    {
      "id": "NominalTrajectoryGeneration",
      "type": "subnode",
      "parent": "DifferentialDynamicProgramming",
      "description": "Process of generating an initial trajectory approximation using a naive controller."
    },
    {
      "id": "LinearizationOfDynamics",
      "type": "subnode",
      "parent": "DifferentialDynamicProgramming",
      "description": "Technique for linearizing system dynamics around each trajectory point."
    },
    {
      "id": "RewritingDynamics",
      "type": "subnode",
      "parent": "DifferentialDynamicProgramming",
      "description": "Rewriting the dynamics using a linear approximation in non-stationary settings."
    },
    {
      "id": "RewardLinearization",
      "type": "subnode",
      "parent": "DifferentialDynamicProgramming",
      "description": "Second-order Taylor expansion for approximating rewards around trajectory points."
    },
    {
      "id": "LinearQuadraticRegulatorLQR",
      "type": "major",
      "parent": null,
      "description": "Control strategy for linear systems with quadratic cost functions."
    },
    {
      "id": "LQG",
      "type": "subnode",
      "parent": "LinearQuadraticRegulatorLQR",
      "description": "Extension of LQR to stochastic environments where the state is not fully observable."
    },
    {
      "id": "Partially Observable MDPs (POMDP)",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "MDPs with an additional observation layer to handle partial observability."
    },
    {
      "id": "Observation Layer",
      "type": "subnode",
      "parent": "Partially Observable MDPs (POMDP)",
      "description": "Introduces the concept of observations in POMDPs."
    },
    {
      "id": "Belief State",
      "type": "subnode",
      "parent": "Partially Observable MDPs (POMDP)",
      "description": "Maintains a distribution over states based on observations."
    },
    {
      "id": "Policy Mapping",
      "type": "subnode",
      "parent": "Partially Observable MDPs (POMDP)",
      "description": "Maps belief states to actions in POMDPs."
    },
    {
      "id": "LQR Extension",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Extension of Linear Quadratic Regulator to partially observable settings."
    },
    {
      "id": "Observation Model",
      "type": "subnode",
      "parent": "LQR Extension",
      "description": "Model describing how observations are generated from states in LQR extension."
    },
    {
      "id": "Kalman Filter",
      "type": "subnode",
      "parent": "LQR Extension",
      "description": "Algorithm for efficient computation of state estimates in dynamic systems with Gaussian noise."
    },
    {
      "id": "Step 1",
      "type": "subnode",
      "parent": "Kalman Filter",
      "description": "Initial step to set up the system dynamics and observation model."
    },
    {
      "id": "Gaussian Distributions",
      "type": "major",
      "parent": null,
      "description": "Statistical distributions used in modeling uncertainties in state estimation problems."
    },
    {
      "id": "Predict Step",
      "type": "subnode",
      "parent": "Kalman Filter",
      "description": "Step to predict the next state based on current observations and dynamics."
    },
    {
      "id": "Update Step",
      "type": "subnode",
      "parent": "Kalman Filter",
      "description": "Step to update the predicted state with new observation data."
    },
    {
      "id": "LQR Algorithm",
      "type": "major",
      "parent": null,
      "description": "Linear Quadratic Regulator algorithm used for optimal control problems in linear systems."
    },
    {
      "id": "Belief States Update",
      "type": "subnode",
      "parent": "Kalman Filter",
      "description": "Combination of predict and update steps to refine belief states over time."
    },
    {
      "id": "Gaussian Distribution",
      "type": "subnode",
      "parent": "Predict Step",
      "description": "Distribution used in the prediction step for state estimation."
    },
    {
      "id": "State Transition",
      "type": "subnode",
      "parent": "Predict Step",
      "description": "Process of predicting next state from current state and transition matrix A."
    },
    {
      "id": "Kalman Gain",
      "type": "subnode",
      "parent": "Update Step",
      "description": "Matrix used to compute the update step for refining state estimate."
    },
    {
      "id": "Chapter_17_Policy_Gradient_REINFORCE",
      "type": "major",
      "parent": null,
      "description": "Introduces REINFORCE algorithm for model-free reinforcement learning without value functions."
    },
    {
      "id": "REINFORCE_Finite_Horizon_Case",
      "type": "subnode",
      "parent": "Chapter_17_Policy_Gradient_REINFORCE",
      "description": "Explains the application of REINFORCE in finite horizon scenarios."
    },
    {
      "id": "Randomized_Policy",
      "type": "subnode",
      "parent": "REINFORCE_Finite_Horizon_Case",
      "description": "Describes how REINFORCE applies to learning randomized policies."
    },
    {
      "id": "Sampling_Transition_Probabilities",
      "type": "subnode",
      "parent": "Chapter_17_Policy_Gradient_REINFORCE",
      "description": "Explains the requirement of sampling from transition probabilities and querying reward functions."
    },
    {
      "id": "Expected_Total_Payoff_Optimization",
      "type": "subnode",
      "parent": "Chapter_17_Policy_Gradient_REINFORCE",
      "description": "Describes optimization of expected total payoff over policy parameters."
    },
    {
      "id": "Finite_Infinite_Horizon_Difference",
      "type": "subnode",
      "parent": "Expected_Total_Payoff_Optimization",
      "description": "Notes on the difference between finite and infinite horizon settings."
    },
    {
      "id": "Policy_Gradient_Methods",
      "type": "subnode",
      "parent": "Machine_Learning",
      "description": "Methods that optimize policies directly in reinforcement learning settings."
    },
    {
      "id": "Gradient_Ascend",
      "type": "subnode",
      "parent": "Policy_Gradient_Methods",
      "description": "Optimization technique to maximize the expected reward function."
    },
    {
      "id": "Reward_Function",
      "type": "subnode",
      "parent": "Policy_Gradient_METHODS",
      "description": "Function that assigns a scalar value for an action taken in a given state."
    },
    {
      "id": "Transition_Probabilities",
      "type": "subnode",
      "parent": "Policy_Gradient_Methods",
      "description": "Probabilities of moving from one state to another based on actions."
    },
    {
      "id": "Expectation_Value",
      "type": "subnode",
      "parent": "Gradient_Ascend",
      "description": "Expected value of the reward function under a policy."
    },
    {
      "id": "REINFORCE_Algorithm",
      "type": "subnode",
      "parent": "Policy_Gradient_Methods",
      "description": "Algorithm for estimating gradients in reinforcement learning without explicit knowledge of transition probabilities or rewards."
    },
    {
      "id": "PolicyGradientTheorem",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Explanation of the policy gradient theorem in reinforcement learning."
    },
    {
      "id": "ExpectationEstimation",
      "type": "subnode",
      "parent": "PolicyGradientTheorem",
      "description": "Derivation and explanation of expectation estimation using samples."
    },
    {
      "id": "LogProbabilityComputation",
      "type": "subnode",
      "parent": "PolicyGradientTheorem",
      "description": "Details on how to compute the log probability of a trajectory under policy \u0398."
    },
    {
      "id": "Policy_Gradient_Theory",
      "type": "major",
      "parent": null,
      "description": "Theoretical foundation of policy gradients in reinforcement learning."
    },
    {
      "id": "Log_Probability_Gradients",
      "type": "subnode",
      "parent": "Policy_Gradient_Theory",
      "description": "Derivation and calculation of gradients for log probability terms."
    },
    {
      "id": "Vanilla_REINFORCE_Algorithm",
      "type": "subnode",
      "parent": "Policy_Gradient_Theory",
      "description": "Basic algorithm that uses policy gradient to update parameters by gradient ascent."
    },
    {
      "id": "Trajectory_Probability_Change",
      "type": "subnode",
      "parent": "Log_Probability_Gradients",
      "description": "Direction of change in trajectory probability with respect to parameter \theta."
    },
    {
      "id": "Empirical_Estimation",
      "type": "subnode",
      "parent": "Vanilla_REINFORCE_Algorithm",
      "description": "Estimating gradients using sample trajectories for unbiased updates."
    },
    {
      "id": "Policy_Gradient_Theorem",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "The theorem that describes how policy gradients can be used to optimize policies in reinforcement learning."
    },
    {
      "id": "Expected_Value_Calculation",
      "type": "subnode",
      "parent": "Policy_Gradient_Theorem",
      "description": "Calculation of expected values for policy gradients under different reward conditions."
    },
    {
      "id": "Trajectory_Probability_Adjustment",
      "type": "subnode",
      "parent": "Expected_Value_Calculation",
      "description": "Adjusting the probability of trajectories based on their rewards."
    },
    {
      "id": "Formula_17.9",
      "type": "subnode",
      "parent": "Expected_Value_Calculation",
      "description": "Derivation and explanation of formula (17.9) which shows that expected gradients are zero under constant reward conditions."
    },
    {
      "id": "Simplification_of_Formalism",
      "type": "subnode",
      "parent": "Policy_Gradient_Theorem",
      "description": "Simplifying the formalism for policy gradient theorem when rewards are considered over time."
    },
    {
      "id": "Machine_Learning_Overview",
      "type": "major",
      "parent": null,
      "description": "General overview of machine learning concepts and applications."
    },
    {
      "id": "Law_of_Total_Expectation",
      "type": "subnode",
      "parent": "Policy_Gradient_Methods",
      "description": "The law used to simplify expectations over multiple random variables."
    },
    {
      "id": "Estimator_Simplification",
      "type": "subnode",
      "parent": "Law_of_Total_Expectation",
      "description": "Simplifying the estimator using the law of total expectation."
    },
    {
      "id": "Baseline_Estimator",
      "type": "subnode",
      "parent": "Policy_Gradient_Methods",
      "description": "Reduces variance in policy gradient estimates by subtracting a baseline value."
    },
    {
      "id": "Algorithm_7_Vanilla_Policy_Gradient_Baseline",
      "type": "subnode",
      "parent": "Baseline_Estimator",
      "description": "Describes the process of updating policies using gradient estimates with baseline adjustments."
    },
    {
      "id": "Algorithm_8_Vanilla_Policy_Gradient_Baseline",
      "type": "subnode",
      "parent": "Baseline_Estimator",
      "description": "Similar to Algorithm 7, describes another version or iteration of the policy gradient algorithm."
    },
    {
      "id": "Machine_Learning_Papers",
      "type": "major",
      "parent": null,
      "description": "Collection of papers related to machine learning concepts and techniques."
    },
    {
      "id": "Double_Descent_Models",
      "type": "subnode",
      "parent": "Machine_Learning_Papers",
      "description": "Models explaining double descent phenomenon for weak features in machine learning."
    },
    {
      "id": "Variational_Inference_Review",
      "type": "subnode",
      "parent": "Machine_Learning_Papers",
      "description": "Overview of variational inference techniques from a statistical perspective."
    },
    {
      "id": "Foundation_Models",
      "type": "subnode",
      "parent": "Machine_Learning_Papers",
      "description": "Analysis of opportunities and risks associated with foundation models in ML."
    },
    {
      "id": "Few_Shot_Learning",
      "type": "subnode",
      "parent": "Machine_Learning_Papers",
      "description": "Demonstration that language models can perform well on few-shot learning tasks."
    },
    {
      "id": "Contrastive_Learning_Visual_Representations",
      "type": "subnode",
      "parent": "Machine_Learning_Papers",
      "description": "Framework for contrastive learning to improve visual representation in machine learning."
    },
    {
      "id": "BERT_Model",
      "type": "subnode",
      "parent": "Machine_Learning_Papers",
      "description": "Introduction of BERT model for pre-training deep bidirectional transformers for language understanding."
    },
    {
      "id": "Implicit_Bias_Noise_Covariance",
      "type": "subnode",
      "parent": "Machine_Learning_Papers",
      "description": "Exploration of implicit bias in machine learning models due to noise covariance shape."
    },
    {
      "id": "High_Dimensional_Statistics",
      "type": "subnode",
      "parent": "Machine_Learning_Papers",
      "description": "Discussion on surprising phenomena in high-dimensional statistics and their implications for ML."
    },
    {
      "id": "Implicit_Bias",
      "type": "subnode",
      "parent": "Machine_Learning_Papers",
      "description": "Research on the implicit bias in high-dimensional models due to noise covariance."
    },
    {
      "id": "High_Dimensional_Ridgeless_Least_Squares_Interpolation",
      "type": "subnode",
      "parent": "Machine_Learning_Papers",
      "description": "Surprises and insights from ridgeless least squares interpolation in high dimensions."
    },
    {
      "id": "Deep_Residual_Learning",
      "type": "subnode",
      "parent": "Machine_Learning_Papers",
      "description": "Techniques for deep residual learning applied to image recognition tasks."
    },
    {
      "id": "Statistical_Learning_Theory",
      "type": "subnode",
      "parent": "Machine_Learning_Papers",
      "description": "Introduction and theoretical foundations of statistical learning methods."
    },
    {
      "id": "Stochastic_Optimization_Methods",
      "type": "subnode",
      "parent": "Machine_Learning_Papers",
      "description": "Advanced optimization techniques for stochastic settings in machine learning."
    },
    {
      "id": "Variational_Bayes",
      "type": "subnode",
      "parent": "Machine_Learning_Papers",
      "description": "Auto-encoding variational Bayes method for approximate inference in probabilistic models."
    },
    {
      "id": "Model-Based_Deep_Reinforcement_Learning",
      "type": "subnode",
      "parent": "Machine_Learning_Papers",
      "description": "Algorithmic framework and theoretical guarantees for model-based deep reinforcement learning."
    },
    {
      "id": "Generalization_Error_Analysis",
      "type": "subnode",
      "parent": "Machine_Learning_Papers",
      "description": "Analysis of the generalization error in random features regression with precise asymptotics."
    },
    {
      "id": "Sample_Size_Effects",
      "type": "subnode",
      "parent": "Machine_Learning_Papers",
      "description": "Impact of sample size on linear regression models and the phenomenon of double descent."
    },
    {
      "id": "Regularization_Techniques",
      "type": "subnode",
      "parent": "Machine_Learning_Papers",
      "description": "Optimal regularization strategies to mitigate the double descent effect in machine learning models."
    },
    {
      "id": "StatisticalMechanicsOfLearning",
      "type": "subnode",
      "parent": "DoubleDescentPhenomenon",
      "description": "Application of statistical mechanics principles to understand the generalization ability of learning algorithms."
    },
    {
      "id": "GeneralizationInLearning",
      "type": "subnode",
      "parent": "StatisticalMechanicsOfLearning",
      "description": "Study on how well a model can generalize from training data to unseen data."
    }
  ],
  "edges": [
    {
      "from": "LossFunction",
      "to": "IntermediateVariables",
      "relationship": "uses"
    },
    {
      "from": "Lagrangian_Formulation",
      "to": "Equation_6.9",
      "relationship": "depends_on"
    },
    {
      "from": "Gaussian Discriminant Analysis (GDA)",
      "to": "Multivariate Normal Distribution",
      "relationship": "subtopic"
    },
    {
      "from": "ParameterEstimation",
      "to": "Prediction",
      "relationship": "depends_on"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Backpropagation",
      "relationship": "contains"
    },
    {
      "from": "Bias-Variance Tradeoff",
      "to": "Test Error Analysis",
      "relationship": "subtopic"
    },
    {
      "from": "LQR, DDP and LQG",
      "to": "Linear Quadratic Regulation (LQR)",
      "relationship": "has_subtopic"
    },
    {
      "from": "MachineLearningConcepts",
      "to": "Conv1DModule",
      "relationship": "has_subtopic"
    },
    {
      "from": "BinaryClassification",
      "to": "EmpiricalRiskMinimization",
      "relationship": "process_of"
    },
    {
      "from": "Support_Vector_Machines",
      "to": "Optimal_Margin_Classifier",
      "relationship": "subtopic"
    },
    {
      "from": "Examples_of_Kernels",
      "to": "Kernel_Matrix",
      "relationship": "related_to"
    },
    {
      "from": "Fitted Value Iteration",
      "to": "Parameter Update",
      "relationship": "has_subtopic"
    },
    {
      "from": "Training Set",
      "to": "Decision Boundary",
      "relationship": "depends_on"
    },
    {
      "from": "Markov_Decision_Processes_MDPs",
      "to": "Value_Iteration",
      "relationship": "related_to"
    },
    {
      "from": "Stochastic Gradient Descent (SGD)",
      "to": "Mini-batch Stochastic Gradient Descent",
      "relationship": "variant_of"
    },
    {
      "from": "MatrixNotation",
      "to": "Vectorization",
      "relationship": "subtopic"
    },
    {
      "from": "Reward Function",
      "to": "Finite-State MDPs",
      "relationship": "depends_on"
    },
    {
      "from": "ContinuousLatentVariables",
      "to": "GaussianDistributionQ_i",
      "relationship": "depends_on"
    },
    {
      "from": "LinearModelOnTopOfFeatureMap",
      "to": "FeatureMaps",
      "relationship": "depends_on"
    },
    {
      "from": "Chain_Rule_Applications",
      "to": "Gradient_Computation",
      "relationship": "depends_on"
    },
    {
      "from": "MLPModel",
      "to": "ModulesInMLP",
      "relationship": "contains"
    },
    {
      "from": "Kernels_in_Machine_Learning",
      "to": "Kernel_Function_K",
      "relationship": "related_to"
    },
    {
      "from": "Fitted Value Iteration",
      "to": "Value Function Approximation",
      "relationship": "subtopic"
    },
    {
      "from": "Naive_Bayes_Assumption",
      "to": "Conditional_Independence",
      "relationship": "explains"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Bias-Variance Tradeoff",
      "relationship": "subtopic"
    },
    {
      "from": "Clustering",
      "to": "KMeansAlgorithm",
      "relationship": "subtopic"
    },
    {
      "from": "StatisticalMechanicsOfLearning",
      "to": "GeneralizationInLearning",
      "relationship": "related_to"
    },
    {
      "from": "ExpectedTestError",
      "to": "BiasVarianceTradeoff",
      "relationship": "explains"
    },
    {
      "from": "MLPArchitecture",
      "to": "NonlinearActivationModule",
      "relationship": "depends_on"
    },
    {
      "from": "Continuous state MDPs",
      "to": "Value function approximation",
      "relationship": "has_subtopic"
    },
    {
      "from": "PrimalProblem",
      "to": "GeneralizedLagrangian",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningBasics",
      "to": "LMSAlgorithm",
      "relationship": "depends_on"
    },
    {
      "from": "Machine_Learning",
      "to": "Contrastive_Learning",
      "relationship": "has_subtopic"
    },
    {
      "from": "Reinforcement learning",
      "to": "Value iteration and policy iteration",
      "relationship": "subtopic"
    },
    {
      "from": "ProbabilisticModeling",
      "to": "IndependenceAssumption",
      "relationship": "depends_on"
    },
    {
      "from": "Optimization Problem",
      "to": "Convex Quadratic Objective",
      "relationship": "has_subtopic"
    },
    {
      "from": "Dual_Problem_Formulation",
      "to": "Lagrange_Multipliers",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Algorithms",
      "to": "Hyperparameters",
      "relationship": "depends_on"
    },
    {
      "from": "Naive_Bayes_Classifier",
      "to": "Spam_Filtering",
      "relationship": "depends_on"
    },
    {
      "from": "NeuralNetworkParameters",
      "to": "BiologicalInspiration",
      "relationship": "related_to"
    },
    {
      "from": "Machine_Learning",
      "to": "Hypothesis_Class",
      "relationship": "related_to"
    },
    {
      "from": "Step2OptimalPolicy",
      "to": "DynamicProgramming",
      "relationship": "subtopic"
    },
    {
      "from": "Backward_Propagation_Equation",
      "to": "Gradient_Computation",
      "relationship": "related_to"
    },
    {
      "from": "Reinforcement_Learning",
      "to": "Value_Function_Approximation",
      "relationship": "depends_on"
    },
    {
      "from": "EvidenceLowerBoundELBO",
      "to": "PosteriorDistribution",
      "relationship": "related_to"
    },
    {
      "from": "Machine_Learning",
      "to": "Gradient_Descent",
      "relationship": "depends_on"
    },
    {
      "from": "Discretization in MDPs",
      "to": "Discretization Limitations",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Locally Weighted Linear Regression",
      "relationship": "contains"
    },
    {
      "from": "SupportVectors",
      "to": "DecisionBoundary",
      "relationship": "depends_on"
    },
    {
      "from": "Value Function Approximation",
      "to": "Max Action Selection",
      "relationship": "has_subtopic"
    },
    {
      "from": "Few_Shot_Learning",
      "to": "Machine_Learning_Papers",
      "relationship": "subtopic"
    },
    {
      "from": "Cross Validation",
      "to": "Infinite Model Space",
      "relationship": "subtopic"
    },
    {
      "from": "Support Vector Machines (SVM)",
      "to": "Functional Margin",
      "relationship": "has_subtopic"
    },
    {
      "from": "Modern Neural Networks",
      "to": "Modules in Modern Neural Networks",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Adaptation",
      "to": "Pretraining_Methods",
      "relationship": "related_to"
    },
    {
      "from": "LinearModelOnTopOfFeatureMap",
      "to": "DeepLearning",
      "relationship": "subtopic"
    },
    {
      "from": "TemperatureParameter",
      "to": "TextGenerationModels",
      "relationship": "depends_on"
    },
    {
      "from": "Principal Component Analysis (PCA)",
      "to": "Eigenfaces Method",
      "relationship": "subtopic_of"
    },
    {
      "from": "Target Vector",
      "to": "Least Squares Revisited",
      "relationship": "subtopic"
    },
    {
      "from": "dStarAndPStarEquality",
      "to": "ConvexityAssumptions",
      "relationship": "depends_on"
    },
    {
      "from": "MachineLearningOverview",
      "to": "LocallyWeightedLinearRegression",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningOptimization",
      "to": "BatchGradientDescent",
      "relationship": "has_subtopic"
    },
    {
      "from": "Sample_Size_Effects",
      "to": "Machine_Learning_Papers",
      "relationship": "subtopic"
    },
    {
      "from": "From non-linear dynamics to LQR",
      "to": "Linearization of dynamics",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Overfitting_Underfitting",
      "relationship": "contains"
    },
    {
      "from": "GeneralizedLinearModelsGLMs",
      "to": "LogisticRegression",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearning",
      "to": "ReinforcementLearning",
      "relationship": "has_subtopic"
    },
    {
      "from": "Optimizers and Generalization",
      "to": "Implicit Regularization",
      "relationship": "depends_on"
    },
    {
      "from": "GLMFormulation",
      "to": "OtherDistributions",
      "relationship": "related_to"
    },
    {
      "from": "MachineLearningOptimization",
      "to": "StochasticGradientDescent",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine_Learning_Topics",
      "to": "Convolutional_Neural_Networks",
      "relationship": "has_subtopic"
    },
    {
      "from": "EMAlgorithm",
      "to": "MStep",
      "relationship": "subtopic"
    },
    {
      "from": "Policy_Gradient_Theorem",
      "to": "Expected_Value_Calculation",
      "relationship": "subtopic_of"
    },
    {
      "from": "DiscriminativeAlgorithms",
      "to": "LogisticRegression",
      "relationship": "has_subtopic"
    },
    {
      "from": "DataNormalization",
      "to": "PCAAlgorithm",
      "relationship": "depends_on"
    },
    {
      "from": "ICA Ambiguities",
      "to": "Sign Changes",
      "relationship": "depends_on"
    },
    {
      "from": "Overfitting",
      "to": "MachineLearningConcepts",
      "relationship": "subtopic"
    },
    {
      "from": "Policy_Gradient_Methods",
      "to": "Law_of_Total_Expectation",
      "relationship": "depends_on"
    },
    {
      "from": "Optimization Problem",
      "to": "Lagrangian Function",
      "relationship": "related_to"
    },
    {
      "from": "Markov Decision Processes (MDP)",
      "to": "Finite Horizon Setting",
      "relationship": "has_subtopic"
    },
    {
      "from": "Support Vector Machines (SVM)",
      "to": "Machine Learning Overview",
      "relationship": "depends_on"
    },
    {
      "from": "Supervised Learning with Non-Linear Models",
      "to": "Deep Learning Introduction",
      "relationship": "subtopic"
    },
    {
      "from": "JensensInequality",
      "to": "EqualityCondition",
      "relationship": "depends_on"
    },
    {
      "from": "DifferentialDynamicProgramming",
      "to": "LinearizationOfDynamics",
      "relationship": "subtopic"
    },
    {
      "from": "Loss Function",
      "to": "Average Loss",
      "relationship": "subtopic"
    },
    {
      "from": "LQR Extension",
      "to": "Observation Model",
      "relationship": "has_subtopic"
    },
    {
      "from": "PrimalDualProblem",
      "to": "KKTConditions",
      "relationship": "related_to"
    },
    {
      "from": "ReLUFunction",
      "to": "LeakyReLUFunction",
      "relationship": "depends_on"
    },
    {
      "from": "Machine_Learning_Theory",
      "to": "Union_Bound",
      "relationship": "depends_on"
    },
    {
      "from": "Expectation-Maximization Algorithm",
      "to": "E-step",
      "relationship": "has_subtopic"
    },
    {
      "from": "Dimensionality Reduction",
      "to": "Data Visualization",
      "relationship": "subtopic"
    },
    {
      "from": "Chapter_17_Policy_Gradient_REINFORCE",
      "to": "Expected_Total_Payoff_Optimization",
      "relationship": "subtopic"
    },
    {
      "from": "General Case",
      "to": "Density Transformation",
      "relationship": "subtopic"
    },
    {
      "from": "FeatureEngineering",
      "to": "FeatureMaps",
      "relationship": "related_to"
    },
    {
      "from": "VE Procedure",
      "to": "Option 1 Initialization",
      "relationship": "subtopic"
    },
    {
      "from": "Joint Distribution",
      "to": "Mixture of Gaussians Model",
      "relationship": "subtopic"
    },
    {
      "from": "ParallelTrainingExamples",
      "to": "VectorizedForwardPass",
      "relationship": "subtopic"
    },
    {
      "from": "KernelTrickIntroduction",
      "to": "ThetaVectorInitialization",
      "relationship": "subtopic"
    },
    {
      "from": "LQR, DDP and LQG",
      "to": "From non-linear dynamics to LQR",
      "relationship": "has_subtopic"
    },
    {
      "from": "Implicit Regularization",
      "to": "Global Minima and Generalization",
      "relationship": "depends_on"
    },
    {
      "from": "GeneralizationErrorGuarantees",
      "to": "MachineLearningConcepts",
      "relationship": "subtopic"
    },
    {
      "from": "5 Kernel methods",
      "to": "5.2 LMS (least mean squares) with features",
      "relationship": "has_subtopic"
    },
    {
      "from": "Stacking Neurons",
      "to": "Complex Neural Network Example",
      "relationship": "example_of"
    },
    {
      "from": "PilotSurveyExample",
      "to": "RedundancyDetection",
      "relationship": "related_to"
    },
    {
      "from": "Learning a model for an MDP",
      "to": "Continuous state MDPs",
      "relationship": "has_subtopic"
    },
    {
      "from": "State Transition Probabilities",
      "to": "Finite-State MDPs",
      "relationship": "depends_on"
    },
    {
      "from": "JensensInequality",
      "to": "ConvexFunctionDefinition",
      "relationship": "depends_on"
    },
    {
      "from": "DiscriminativeAlgorithms",
      "to": "PerceptronAlgorithm",
      "relationship": "has_subtopic"
    },
    {
      "from": "MachineLearningOverview",
      "to": "KernelTrickIntroduction",
      "relationship": "contains"
    },
    {
      "from": "FunctionalMargin",
      "to": "FunctionMarginTrainingSet",
      "relationship": "subtopic"
    },
    {
      "from": "Reinforcement learning",
      "to": "Connections between Policy and Value Iteration (Optional)",
      "relationship": "subtopic"
    },
    {
      "from": "LinearCombinationRepresentation",
      "to": "InductiveProofOfRepresentation",
      "relationship": "depends_on"
    },
    {
      "from": "EM_Algorithm",
      "to": "M_Step",
      "relationship": "depends_on"
    },
    {
      "from": "OptimizationMethods",
      "to": "FisherScoring",
      "relationship": "related_to"
    },
    {
      "from": "Zero-Shot_Adaptation",
      "to": "Machine_Learning_Adaptation",
      "relationship": "subtopic"
    },
    {
      "from": "Independent Component Analysis (ICA)",
      "to": "Cocktail Party Problem",
      "relationship": "has_example"
    },
    {
      "from": "Bernoulli Distribution",
      "to": "Exponential Family Distributions",
      "relationship": "subtopic"
    },
    {
      "from": "Regularization_and_Kernels",
      "to": "Support_Vector_Machines",
      "relationship": "related_to"
    },
    {
      "from": "Valid_Kernels_Conditions",
      "to": "Kernel_Matrix",
      "relationship": "depends_on"
    },
    {
      "from": "Transformer_Model",
      "to": "Training_Transformer",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningModels",
      "to": "RobustnessToAssumptions",
      "relationship": "contains"
    },
    {
      "from": "FiniteHorizonMDP",
      "to": "OptimalPolicyFiniteHorizon",
      "relationship": "has_subtopic"
    },
    {
      "from": "Sample-wise Double Descent",
      "to": "Double Descent Phenomenon",
      "relationship": "subtopic"
    },
    {
      "from": "Bias-Variance Tradeoff",
      "to": "Double Descent Phenomenon",
      "relationship": "related_to"
    },
    {
      "from": "LMSAlgorithmWithFeatures",
      "to": "BatchGradientDescentUpdate",
      "relationship": "subtopic"
    },
    {
      "from": "ReLUFunction",
      "to": "GELUFunction",
      "relationship": "depends_on"
    },
    {
      "from": "SMO_Algorithm",
      "to": "Constraint_Satisfaction",
      "relationship": "contains"
    },
    {
      "from": "Discretization Limitations",
      "to": "Curse of Dimensionality",
      "relationship": "subtopic"
    },
    {
      "from": "BackpropagationAlgorithm",
      "to": "IntermediateValuesStorage",
      "relationship": "related_to"
    },
    {
      "from": "Linear Quadratic Regulation (LQR)",
      "to": "Continuous Setting",
      "relationship": "defines"
    },
    {
      "from": "MachineLearningOverview",
      "to": "DoubleDescentPhenomenon",
      "relationship": "contains"
    },
    {
      "from": "Vanilla_REINFORCE_Algorithm",
      "to": "Empirical_Estimation",
      "relationship": "has_subtopic"
    },
    {
      "from": "1DConvolution",
      "to": "ConvolutionalLayers",
      "relationship": "related_to"
    },
    {
      "from": "Adaptation_Phase",
      "to": "Labeled_Dataset_Task",
      "relationship": "depends_on"
    },
    {
      "from": "Training_Dataset",
      "to": "Empirical_Distribution",
      "relationship": "subtopic"
    },
    {
      "from": "LeakyReLUFunction",
      "to": "ReLUFunction",
      "relationship": "subtopic"
    },
    {
      "from": "Optimal_Parameter_Finding",
      "to": "Support_Vector_Machines",
      "relationship": "depends_on"
    },
    {
      "from": "Softmax Function",
      "to": "2.3 Multi-class classification",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Backward_Propagation",
      "to": "Jacobian_Matrix",
      "relationship": "related_to"
    },
    {
      "from": "Dimensionality Reduction",
      "to": "Compression",
      "relationship": "subtopic"
    },
    {
      "from": "Computational_Graph_Illustration",
      "to": "Gradient_Computation",
      "relationship": "subtopic"
    },
    {
      "from": "Optimizers and Generalization",
      "to": "Learning Rate Schedules",
      "relationship": "subtopic"
    },
    {
      "from": "NeuralNetworks",
      "to": "WeightMatrices",
      "relationship": "contains"
    },
    {
      "from": "Modern Neural Networks",
      "to": "Vectorization over training examples",
      "relationship": "subtopic"
    },
    {
      "from": "Coordinate_Ascend_Method",
      "to": "Quadratic_Function_Optimization",
      "relationship": "depends_on"
    },
    {
      "from": "Linearization_of_Dynamics",
      "to": "LQR_Assumptions",
      "relationship": "similar_to"
    },
    {
      "from": "High_Dimensional_Statistics",
      "to": "Machine_Learning_Papers",
      "relationship": "subtopic"
    },
    {
      "from": "Data Augmentation",
      "to": "Positive Pair",
      "relationship": "creates"
    },
    {
      "from": "KernelTrick",
      "to": "KernelsAsSimilarityMetrics",
      "relationship": "depends_on"
    },
    {
      "from": "Step1Estimation",
      "to": "ValueApproximation",
      "relationship": "contains"
    },
    {
      "from": "Continuous Latent Variables",
      "to": "Variational Inference",
      "relationship": "subtopic"
    },
    {
      "from": "LikelihoodFunction",
      "to": "LogLikelihoodFunction",
      "relationship": "depends_on"
    },
    {
      "from": "Expected_Value_Calculation",
      "to": "Trajectory_Probability_Adjustment",
      "relationship": "depends_on"
    },
    {
      "from": "High_Dimensional_Ridgeless_Least_Squares_Interpolation",
      "to": "Machine_Learning_Papers",
      "relationship": "related_to"
    },
    {
      "from": "Chain_Rule_Applications",
      "to": "Complex_Functions",
      "relationship": "related_to"
    },
    {
      "from": "KKT_Conditions",
      "to": "Dual_Complementarity",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Cross_Entropy_Loss",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine_Learning",
      "to": "Variational_Autoencoder",
      "relationship": "related_to"
    },
    {
      "from": "LQR, DDP and LQG",
      "to": "Linear Quadratic Gaussian (LQG)",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine_Learning_Topics",
      "to": "Kernel_Method",
      "relationship": "related_to"
    },
    {
      "from": "KL_Divergence",
      "to": "ELBO",
      "relationship": "depends_on"
    },
    {
      "from": "Machine_Learning_Algorithms",
      "to": "Support_Vector_Machines_SVMs",
      "relationship": "has_subtopic"
    },
    {
      "from": "Kernels_in_Machine_Learning",
      "to": "Feature_Map_Phi",
      "relationship": "depends_on"
    },
    {
      "from": "MachineLearningOverview",
      "to": "MaximumLikelihoodEstimation",
      "relationship": "related_to"
    },
    {
      "from": "Contrastive_Learning_Visual_Representations",
      "to": "Machine_Learning_Papers",
      "relationship": "subtopic"
    },
    {
      "from": "IterativeUpdateRule",
      "to": "LinearCombinationRepresentation",
      "relationship": "contains"
    },
    {
      "from": "Hold_Out_Cross_Validation",
      "to": "Validation_Error",
      "relationship": "related_to"
    },
    {
      "from": "FeatureMap",
      "to": "CubicFunctionRepresentation",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Algorithms",
      "to": "Policy Iteration",
      "relationship": "has_subtopic"
    },
    {
      "from": "Sample_Complexity",
      "to": "Empirical_Risk_Minimization",
      "relationship": "related_to"
    },
    {
      "from": "MachineLearningOverview",
      "to": "ModelComplexityMeasures",
      "relationship": "subtopic"
    },
    {
      "from": "Continuous state MDPs",
      "to": "Discretization",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningChallenges",
      "to": "kFoldCrossValidation",
      "relationship": "has_subtopic"
    },
    {
      "from": "Supervised Learning vs Reinforcement Learning",
      "to": "Reinforcement Learning",
      "relationship": "subtopic"
    },
    {
      "from": "HoldoutCrossValidation",
      "to": "ModelSelection",
      "relationship": "subtopic_of"
    },
    {
      "from": "LMS_Rule",
      "to": "Single_Training_Example",
      "relationship": "subtopic"
    },
    {
      "from": "ProbabilisticModeling",
      "to": "DesignMatrixX",
      "relationship": "related_to"
    },
    {
      "from": "Double_Descent_Phenomenon",
      "to": "Model_Wise_Double_Descent",
      "relationship": "subtopic"
    },
    {
      "from": "Gaussian Distributions",
      "to": "Step 1",
      "relationship": "related_to"
    },
    {
      "from": "InfiniteHorizonMDP",
      "to": "DiscountFactor",
      "relationship": "includes_concept"
    },
    {
      "from": "OptimizationInML",
      "to": "PrimalDualProblem",
      "relationship": "subtopic"
    },
    {
      "from": "Independent components analysis",
      "to": "ICA algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Convergence_Guarantees",
      "to": "EM_Algorithm",
      "relationship": "analyzes"
    },
    {
      "from": "Convolutional_Neural_Networks",
      "to": "2D_Convolution",
      "relationship": "has_subtopic"
    },
    {
      "from": "DeepLearning",
      "to": "MachineLearningOverview",
      "relationship": "subtopic"
    },
    {
      "from": "Constraints_Satisfaction",
      "to": "KKT_Conditions",
      "relationship": "has_subtopic"
    },
    {
      "from": "MachineLearningOverview",
      "to": "LeastSquaresRegression",
      "relationship": "related_to"
    },
    {
      "from": "Model Selection",
      "to": "Bias-Variance Tradeoff",
      "relationship": "related_to"
    },
    {
      "from": "Dynamics_and_Control_Systems",
      "to": "Inverted_Pendulum",
      "relationship": "illustrates"
    },
    {
      "from": "ELBO",
      "to": "KL_Divergence",
      "relationship": "depends_on"
    },
    {
      "from": "Loss_Functions",
      "to": "Mean_Squared_Error",
      "relationship": "related_to"
    },
    {
      "from": "SupervisedLearning",
      "to": "FittedValueIteration",
      "relationship": "related_to"
    },
    {
      "from": "SIMCLR",
      "to": "Augmentation_Techniques",
      "relationship": "depends_on"
    },
    {
      "from": "Normal_Distribution",
      "to": "Density_Properties",
      "relationship": "contains"
    },
    {
      "from": "MachineLearningConcepts",
      "to": "Conv2DModule",
      "relationship": "has_subtopic"
    },
    {
      "from": "2 Classification and logistic regression",
      "to": "2.4 Another algorithm for maximizing \\(\\ell(\\theta)\\)",
      "relationship": "has_subtopic"
    },
    {
      "from": "VariationalInference",
      "to": "GradientComputation",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Markov Decision Processes (MDP)",
      "relationship": "contains"
    },
    {
      "from": "Policy_Iteration",
      "to": "Machine_Learning_Algorithms",
      "relationship": "subtopic"
    },
    {
      "from": "LayerNormalization",
      "to": "LN-SModule",
      "relationship": "subtopic_of"
    },
    {
      "from": "LogLikelihoodBound",
      "to": "ProbabilityDistributions",
      "relationship": "related_to"
    },
    {
      "from": "VariationalAutoEncoder",
      "to": "EncoderDecoderConcepts",
      "relationship": "contains"
    },
    {
      "from": "Predict Step",
      "to": "State Transition",
      "relationship": "related_to"
    },
    {
      "from": "PCA",
      "to": "Dimensionality Reduction",
      "relationship": "subtopic"
    },
    {
      "from": "ParameterizationOfH",
      "to": "LinearClassifiers",
      "relationship": "depends_on"
    },
    {
      "from": "Logistic_Regression",
      "to": "Logistic_Function",
      "relationship": "uses"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "PCA_Methods",
      "relationship": "related_to"
    },
    {
      "from": "MachineLearningArchitectures",
      "to": "LayerNormalization",
      "relationship": "contains"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "M-step_Update_for_phi_j",
      "relationship": "contains"
    },
    {
      "from": "MachineLearningModels",
      "to": "LogisticRegression",
      "relationship": "contains"
    },
    {
      "from": "Regularization",
      "to": "TrainingLoss",
      "relationship": "modifies"
    },
    {
      "from": "GaussianDiscriminantAnalysis",
      "to": "AsymptoticEfficiency",
      "relationship": "has_property"
    },
    {
      "from": "Value Function Approximation",
      "to": "Action Evaluation",
      "relationship": "has_subtopic"
    },
    {
      "from": "Expectation_Calculation",
      "to": "Sampling_Methods",
      "relationship": "subtopic"
    },
    {
      "from": "Expected_Value_Calculation",
      "to": "Formula_17.9",
      "relationship": "contains"
    },
    {
      "from": "Loss_Functions",
      "to": "Regularization",
      "relationship": "has_subtopic"
    },
    {
      "from": "Loss Function",
      "to": "Negative Log-Likelihood",
      "relationship": "sum_of"
    },
    {
      "from": "Exponential Family Distributions",
      "to": "Canonical Response Function",
      "relationship": "has_subnode"
    },
    {
      "from": "Double_Descent_Phenomenon",
      "to": "Sample_Wise_Double_Descent",
      "relationship": "subtopic"
    },
    {
      "from": "Chapter16",
      "to": "FiniteHorizonMDPs",
      "relationship": "contains"
    },
    {
      "from": "MachineLearningBasics",
      "to": "BinaryClassification",
      "relationship": "contains"
    },
    {
      "from": "MachineLearningOverview",
      "to": "EMAlgorithm",
      "relationship": "depends_on"
    },
    {
      "from": "Pretraining_Phase",
      "to": "Model_Parameterization",
      "relationship": "subtopic"
    },
    {
      "from": "SingleNeuronNN",
      "to": "ReLUActivationFunction",
      "relationship": "uses"
    },
    {
      "from": "Fitted Value Iteration",
      "to": "Value Function Approximation",
      "relationship": "has_subtopic"
    },
    {
      "from": "IterativeUpdateRule",
      "to": "UpdateRuleForCoefficients",
      "relationship": "subtopic"
    },
    {
      "from": "ICAAlgorithm",
      "to": "MixingMatrix",
      "relationship": "depends_on"
    },
    {
      "from": "ModelEvaluation",
      "to": "GeneralizationErrorEstimation",
      "relationship": "subtopic_of"
    },
    {
      "from": "Support_Vector_Machines_SMO",
      "to": "Dual_Problem_Formulation",
      "relationship": "has_subtopic"
    },
    {
      "from": "NeuralNetworks",
      "to": "Vectorization",
      "relationship": "subtopic"
    },
    {
      "from": "Bayesian Statistics",
      "to": "Frequentist View",
      "relationship": "contrasts_with"
    },
    {
      "from": "Kalman Filter",
      "to": "Step 1",
      "relationship": "depends_on"
    },
    {
      "from": "GeneralizedLinearModelsGLMs",
      "to": "OrdinaryLeastSquaresOLS",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Bias_Variance_Tradeoff",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Overview",
      "to": "Discretization in MDPs",
      "relationship": "depends_on"
    },
    {
      "from": "ErrorTermAssumption",
      "to": "ProbabilisticInterpretation",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Techniques",
      "to": "Principal Component Analysis (PCA)",
      "relationship": "has_subtopic"
    },
    {
      "from": "Model Creation Methods",
      "to": "Off-the-Shelf Physics Software",
      "relationship": "has_subtopic"
    },
    {
      "from": "LogisticRegression",
      "to": "ProbabilityPrediction",
      "relationship": "has_subtopic"
    },
    {
      "from": "MaximumLikelihoodEstimation",
      "to": "LogLikelihoodFunction",
      "relationship": "related_to"
    },
    {
      "from": "ParameterEstimation",
      "to": "NaiveBayesAlgorithm",
      "relationship": "subtopic"
    },
    {
      "from": "distortion_function",
      "to": "k-means_algorithm",
      "relationship": "related_to"
    },
    {
      "from": "Convergence Rate",
      "to": "Newton's Method",
      "relationship": "depends_on"
    },
    {
      "from": "Necessary Conditions for Valid Kernels",
      "to": "Positive Semi-Definite Property",
      "relationship": "subtopic"
    },
    {
      "from": "Posterior Distribution on Parameters",
      "to": "Prediction on New Data",
      "relationship": "depends_on"
    },
    {
      "from": "Reinforcement learning",
      "to": "Continuous state MDPs",
      "relationship": "subtopic"
    },
    {
      "from": "Classification Problem",
      "to": "Binary Classification",
      "relationship": "contains"
    },
    {
      "from": "Value Iteration Algorithm",
      "to": "Value Iteration and Policy Iteration",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningOverview",
      "to": "OutputParameterization",
      "relationship": "contains"
    },
    {
      "from": "MachineLearningOverview",
      "to": "VariationalAutoEncoder",
      "relationship": "contains"
    },
    {
      "from": "BERT_Model",
      "to": "Machine_Learning_Papers",
      "relationship": "subtopic"
    },
    {
      "from": "Objective_Function",
      "to": "Quadratic_Formulation",
      "relationship": "subtopic"
    },
    {
      "from": "MLP Backpropagation",
      "to": "Gradient Computation",
      "relationship": "depends_on"
    },
    {
      "from": "Markov Decision Processes (MDP)",
      "to": "Policy in MDPs",
      "relationship": "has_subtopic"
    },
    {
      "from": "Chapter_17_Policy_Gradient_REINFORCE",
      "to": "REINFORCE_Finite_Horizon_Case",
      "relationship": "subtopic"
    },
    {
      "from": "DistributionQ",
      "to": "LogLikelihoodExpression",
      "relationship": "subtopic"
    },
    {
      "from": "LossFunction",
      "to": "BackwardPass",
      "relationship": "leads_to"
    },
    {
      "from": "1 Linear regression",
      "to": "1.2 The normal equations",
      "relationship": "has_subtopic"
    },
    {
      "from": "ConstrainedOptimization",
      "to": "PrimalProblem",
      "relationship": "subtopic"
    },
    {
      "from": "BinaryClassificationProblem",
      "to": "MLPModel",
      "relationship": "uses"
    },
    {
      "from": "Backpropagation",
      "to": "Modules Composition",
      "relationship": "subtopic"
    },
    {
      "from": "Deep Learning Regularizers",
      "to": "Lipschitzness Regularization",
      "relationship": "contains"
    },
    {
      "from": "GaussianDiscriminantAnalysis",
      "to": "GDAParameters",
      "relationship": "depends_on"
    },
    {
      "from": "NaiveBayesClassifier",
      "to": "ProbabilityEstimation",
      "relationship": "related_to"
    },
    {
      "from": "LinearHypothesis",
      "to": "ParametersWeights",
      "relationship": "subtopic"
    },
    {
      "from": "Maximum_Likelihood_Estimation",
      "to": "State_Transition_Probabilities",
      "relationship": "method"
    },
    {
      "from": "String_Classification",
      "to": "Examples_of_Kernels",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning",
      "to": "Generalization",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Models",
      "to": "MDP Simulators",
      "relationship": "subtopic"
    },
    {
      "from": "Optimal_Margin_Classifiers",
      "to": "KKT_Conditions",
      "relationship": "depends_on"
    },
    {
      "from": "Activation Function",
      "to": "Single Neuron Model",
      "relationship": "applied_to"
    },
    {
      "from": "2 Classification and logistic regression",
      "to": "2.1 Logistic regression",
      "relationship": "has_subtopic"
    },
    {
      "from": "StateTransitionModel",
      "to": "StochasticModel",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine_Learning_Techniques",
      "to": "Kernel_Methods",
      "relationship": "has_subtopic"
    },
    {
      "from": "Loss_Functions",
      "to": "Cross_Entropy_Loss",
      "relationship": "has_subtopic"
    },
    {
      "from": "Representation Function",
      "to": "Negative Pair",
      "relationship": "related_to"
    },
    {
      "from": "MachineLearningOverview",
      "to": "LQRModelAssumptions",
      "relationship": "contains"
    },
    {
      "from": "Hypothesis_Class_Switching",
      "to": "Machine_Learning_Bias_Variance_Tradeoff",
      "relationship": "subtopic"
    },
    {
      "from": "Bias-Variance Tradeoff",
      "to": "Test Error Decomposition",
      "relationship": "subtopic"
    },
    {
      "from": "HoeffdingInequality",
      "to": "GeneralizationErrorGuarantees",
      "relationship": "subtopic"
    },
    {
      "from": "Mixture of Gaussians Model",
      "to": "Latent Variables",
      "relationship": "contains"
    },
    {
      "from": "MachineLearningConcepts",
      "to": "ValidKernelConditions",
      "relationship": "related_to"
    },
    {
      "from": "Model_Learning",
      "to": "Unknown_State_Transitions_and_Rewards",
      "relationship": "contains"
    },
    {
      "from": "RegressionProblems",
      "to": "TestExample",
      "relationship": "depends_on"
    },
    {
      "from": "Test Error Analysis",
      "to": "Linear Regression Example",
      "relationship": "example_of"
    },
    {
      "from": "Parameterized Model",
      "to": "2.3 Multi-class classification",
      "relationship": "subtopic"
    },
    {
      "from": "VarianceMaximization",
      "to": "LagrangeMultipliers",
      "relationship": "related_to"
    },
    {
      "from": "Expected_Total_Payoff_Optimization",
      "to": "Finite_Infinite_Horizon_Difference",
      "relationship": "depends_on"
    },
    {
      "from": "FiniteHorizonMDPs",
      "to": "OptimalPolicyRecovery",
      "relationship": "contains"
    },
    {
      "from": "Machine Learning Algorithms",
      "to": "Naive Bayes Algorithm",
      "relationship": "contains"
    },
    {
      "from": "Predict Step",
      "to": "Gaussian Distribution",
      "relationship": "depends_on"
    },
    {
      "from": "Generalization and regularization",
      "to": "Generalization",
      "relationship": "subtopic"
    },
    {
      "from": "PolynomialFitting",
      "to": "Variance",
      "relationship": "related_to"
    },
    {
      "from": "Supervised Learning Algorithm",
      "to": "Linear Regression",
      "relationship": "depends_on"
    },
    {
      "from": "DoubleDescentPhenomenon",
      "to": "DeepNeuralNetworks",
      "relationship": "depends_on"
    },
    {
      "from": "VC Dimension",
      "to": "Shattering",
      "relationship": "defines"
    },
    {
      "from": "Neural Networks",
      "to": "Bias Term",
      "relationship": "has_component"
    },
    {
      "from": "GaussianDistributionQ_i",
      "to": "MeanAndVarianceFunctions",
      "relationship": "defines"
    },
    {
      "from": "kFoldCrossValidation",
      "to": "LeaveOneOutCV",
      "relationship": "subtopic_of"
    },
    {
      "from": "StochasticGradientDescent",
      "to": "GradientDescentAlgorithm",
      "relationship": "subtopic"
    },
    {
      "from": "BackpropagationAlgorithm",
      "to": "GradientComputation",
      "relationship": "depends_on"
    },
    {
      "from": "Self-Supervised Learning",
      "to": "Data Augmentation",
      "relationship": "depends_on"
    },
    {
      "from": "Value Function Approximation",
      "to": "Feature Mapping",
      "relationship": "depends_on"
    },
    {
      "from": "LinearRegressionOptimization",
      "to": "GradientDescentConvergence",
      "relationship": "has_subtopic"
    },
    {
      "from": "FullyConnectedNN",
      "to": "Parameterization",
      "relationship": "depends_on"
    },
    {
      "from": "Discretization in MDPs",
      "to": "Policy Iteration",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningOverview",
      "to": "PoissonDistribution",
      "relationship": "related_to"
    },
    {
      "from": "MachineLearningOverview",
      "to": "ConvolutionalLayers",
      "relationship": "has_subtopic"
    },
    {
      "from": "Model-wise Double Descent",
      "to": "Implicit Regularization",
      "relationship": "subtopic"
    },
    {
      "from": "Decision Boundary",
      "to": "Functional Margins",
      "relationship": "related_to"
    },
    {
      "from": "FeatureSelection",
      "to": "LinearRegression",
      "relationship": "depends_on"
    },
    {
      "from": "MachineLearningOverview",
      "to": "PolicyGradientTheorem",
      "relationship": "contains"
    },
    {
      "from": "Backpropagation",
      "to": "Chain Rule",
      "relationship": "depends_on"
    },
    {
      "from": "Independent components analysis",
      "to": "Densities and linear transformations",
      "relationship": "subtopic"
    },
    {
      "from": "NeuralNetworkParameters",
      "to": "FeatureMaps",
      "relationship": "depends_on"
    },
    {
      "from": "Machine_Learning",
      "to": "Policy_Gradient_Methods",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Algorithms",
      "to": "Stochastic Gradient Descent (SGD)",
      "relationship": "has_subtopic"
    },
    {
      "from": "Support_Vector_Machines_SVMs",
      "to": "SMO_Algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "DifferentialDynamicProgramming",
      "to": "RewardLinearization",
      "relationship": "subtopic"
    },
    {
      "from": "Inverted_Pendulum_Problem",
      "to": "Model_Learning_for_MDPs",
      "relationship": "example"
    },
    {
      "from": "ExpectedTestError",
      "to": "MSEDecomposition",
      "relationship": "subtopic"
    },
    {
      "from": "Self-Supervised Learning",
      "to": "Representation Function",
      "relationship": "depends_on"
    },
    {
      "from": "Value_Iteration",
      "to": "Machine_Learning_Algorithms",
      "relationship": "subtopic"
    },
    {
      "from": "Lagrangian_Formulation",
      "to": "Equation_6.11",
      "relationship": "results_in"
    },
    {
      "from": "Cross Validation Techniques",
      "to": "Leave-One-Out Cross Validation",
      "relationship": "has_subtopic"
    },
    {
      "from": "Markov Decision Processes (MDP)",
      "to": "Value Function in MDPs",
      "relationship": "has_subtopic"
    },
    {
      "from": "Update Step",
      "to": "Kalman Gain",
      "relationship": "subtopic"
    },
    {
      "from": "NonLinearModels",
      "to": "TrainingExamples",
      "relationship": "depends_on"
    },
    {
      "from": "MachineLearningOverview",
      "to": "GeneralizedLinearModelsGLMs",
      "relationship": "contains"
    },
    {
      "from": "Variational_Autoencoder",
      "to": "Variational_Inference",
      "relationship": "uses"
    },
    {
      "from": "MajorAxisOfVariation",
      "to": "ProjectionObjective",
      "relationship": "subtopic"
    },
    {
      "from": "Self-supervised Learning",
      "to": "Foundation Models",
      "relationship": "has_subtopic"
    },
    {
      "from": "Spam_Filtering",
      "to": "Training_Set",
      "relationship": "depends_on"
    },
    {
      "from": "PolicyGradientTheorem",
      "to": "LogProbabilityComputation",
      "relationship": "subtopic"
    },
    {
      "from": "Optimal Value Function",
      "to": "Bellman's Equation",
      "relationship": "related_to"
    },
    {
      "from": "Transformer_Model",
      "to": "Autoregressive_Transformer",
      "relationship": "subtopic"
    },
    {
      "from": "Multinomial Distribution",
      "to": "2.3 Multi-class classification",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Loss Function J(theta)",
      "relationship": "depends_on"
    },
    {
      "from": "Implicit_Bias_Noise_Covariance",
      "to": "Machine_Learning_Papers",
      "relationship": "subtopic"
    },
    {
      "from": "initialization_step",
      "to": "k-means_algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Optimal Policy",
      "to": "Machine Learning Concepts",
      "relationship": "depends_on"
    },
    {
      "from": "PolynomialFeatures",
      "to": "Overfitting",
      "relationship": "related_to"
    },
    {
      "from": "LogLikelihoodBound",
      "to": "JensensInequality",
      "relationship": "uses"
    },
    {
      "from": "Bias-Variance Tradeoff",
      "to": "Training Dataset Example",
      "relationship": "related_to"
    },
    {
      "from": "MachineLearningOverview",
      "to": "LocallyWeightedLinearRegression",
      "relationship": "related_to"
    },
    {
      "from": "Policy_Iteration",
      "to": "Greedy_Policy_With_Respect_To_V",
      "relationship": "has_subtopic"
    },
    {
      "from": "Union_Bound_Application",
      "to": "Uniform_Convergence",
      "relationship": "depends_on"
    },
    {
      "from": "Machine_Learning",
      "to": "Text_Classification",
      "relationship": "contains"
    },
    {
      "from": "GaussianDiscriminantAnalysis",
      "to": "DecisionBoundary",
      "relationship": "subtopic"
    },
    {
      "from": "EmpiricalRiskMinimization",
      "to": "MachineLearningConcepts",
      "relationship": "subtopic"
    },
    {
      "from": "TrainingExample",
      "to": "DecisionBoundary",
      "relationship": "related_to"
    },
    {
      "from": "Kernel Functions",
      "to": "Necessary Conditions for Valid Kernels",
      "relationship": "depends_on"
    },
    {
      "from": "Value Function",
      "to": "Policy Execution",
      "relationship": "depends_on"
    },
    {
      "from": "MachineLearningBasics",
      "to": "MatrixNotation",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningBasics",
      "to": "GeneralizationToLayers",
      "relationship": "subtopic"
    },
    {
      "from": "ICA Ambiguities",
      "to": "Scaling Ambiguity",
      "relationship": "depends_on"
    },
    {
      "from": "Independent Component Analysis (ICA)",
      "to": "Unmixing Matrix (W)",
      "relationship": "related_to"
    },
    {
      "from": "StateTransitionModel",
      "to": "DeterministicModel",
      "relationship": "has_subtopic"
    },
    {
      "from": "MachineLearningConcepts",
      "to": "MaximumLikelihoodEstimation",
      "relationship": "depends_on"
    },
    {
      "from": "TrainingSetSampling",
      "to": "GeneralizationErrorGuarantees",
      "relationship": "subtopic"
    },
    {
      "from": "Continuous state MDPs",
      "to": "Discretization",
      "relationship": "has_subtopic"
    },
    {
      "from": "BernoulliDistribution",
      "to": "NaturalParameterOfBernoulli",
      "relationship": "subtopic"
    },
    {
      "from": "Bellman Equations",
      "to": "Value Function",
      "relationship": "subtopic"
    },
    {
      "from": "Log_Probability_Gradients",
      "to": "Trajectory_Probability_Change",
      "relationship": "has_subtopic"
    },
    {
      "from": "PrincipalComponentAnalysis",
      "to": "EigenvectorsEigenvalues",
      "relationship": "has_subtopic"
    },
    {
      "from": "Feature_Vectors",
      "to": "String_Features",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine_Learning",
      "to": "Markov_Decision_Processes_MDPs",
      "relationship": "contains"
    },
    {
      "from": "Feature_Maps",
      "to": "Cubic_Features",
      "relationship": "example_of"
    },
    {
      "from": "Self-supervised learning and foundation models",
      "to": "Pretraining and adaptation",
      "relationship": "subtopic"
    },
    {
      "from": "ProbabilisticInterpretation",
      "to": "LinearRegression",
      "relationship": "related_to"
    },
    {
      "from": "Baseline_Estimator",
      "to": "Algorithm_8_Vanilla_Policy_Gradient_Baseline",
      "relationship": "related_to"
    },
    {
      "from": "MarkovDecisionProcesses",
      "to": "Actions",
      "relationship": "component_of"
    },
    {
      "from": "ReinforcementLearning",
      "to": "MarkovDecisionProcesses",
      "relationship": "defines_formalism"
    },
    {
      "from": "MachineLearningConcepts",
      "to": "MSEDecomposition",
      "relationship": "has_subtopic"
    },
    {
      "from": "EM algorithms",
      "to": "General EM algorithms",
      "relationship": "subtopic"
    },
    {
      "from": "Overfitting_Underfitting",
      "to": "Generalization_Error",
      "relationship": "defines"
    },
    {
      "from": "Scaling_Signal",
      "to": "ICA_Ambiguities",
      "relationship": "depends_on"
    },
    {
      "from": "Dual_Form_Optimization",
      "to": "Support_Vector_Machines",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningConcepts",
      "to": "KernelFunctions",
      "relationship": "has_subtopic"
    },
    {
      "from": "LinearTransformations",
      "to": "DensityCalculation",
      "relationship": "subtopic"
    },
    {
      "from": "Logistic_Function",
      "to": "Sigmoid_Function",
      "relationship": "same_as"
    },
    {
      "from": "NeuralNetworks",
      "to": "FullyConnectedNN",
      "relationship": "subtopic"
    },
    {
      "from": "Characterization_of_Valid_Kernels",
      "to": "Concrete_Examples_of_Kernels",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningOverview",
      "to": "LinearQuadraticRegulatorLQR",
      "relationship": "related_to"
    },
    {
      "from": "5 Kernel methods",
      "to": "5.3 LMS with the kernel trick",
      "relationship": "has_subtopic"
    },
    {
      "from": "MachineLearningOverview",
      "to": "Parameterization",
      "relationship": "related_to"
    },
    {
      "from": "Kalman Filter",
      "to": "Forward Pass",
      "relationship": "related_to"
    },
    {
      "from": "Independent Component Analysis (ICA)",
      "to": "Mixing Matrix (A)",
      "relationship": "depends_on"
    },
    {
      "from": "Kalman Filter",
      "to": "Belief States Update",
      "relationship": "depends_on"
    },
    {
      "from": "Loss Function",
      "to": "Optimizers",
      "relationship": "related_to"
    },
    {
      "from": "StochasticGradientAscent",
      "to": "ConvergenceAndRecovery",
      "relationship": "leads_to"
    },
    {
      "from": "Optimization Problem",
      "to": "Optimal Margin Classifier",
      "relationship": "results_in"
    },
    {
      "from": "Machine Learning Techniques",
      "to": "Independent Components Analysis (ICA)",
      "relationship": "has_subtopic"
    },
    {
      "from": "MixingMatrix",
      "to": "RotationalSymmetry",
      "relationship": "related_to"
    },
    {
      "from": "Markov_Decision_Processes_MDPs",
      "to": "Reward_Functions",
      "relationship": "depends_on"
    },
    {
      "from": "Generalization",
      "to": "Bias-variance tradeoff",
      "relationship": "subtopic"
    },
    {
      "from": "EM_Algorithm",
      "to": "Local_Optima",
      "relationship": "susceptible_to"
    },
    {
      "from": "Pretrained large language models",
      "to": "Zero-shot learning and in-context learning",
      "relationship": "subtopic"
    },
    {
      "from": "Exponential Family Distributions",
      "to": "Canonical Link Function",
      "relationship": "has_subnode"
    },
    {
      "from": "Mini-batch Stochastic Gradient Descent",
      "to": "Batch Size (B)",
      "relationship": "depends_on"
    },
    {
      "from": "M-step",
      "to": "Update Rule for \\(\\phi_j\\)",
      "relationship": "related_to"
    },
    {
      "from": "MSEDecomposition",
      "to": "Claim811",
      "relationship": "uses"
    },
    {
      "from": "NeuralNetworks",
      "to": "SingleNeuronNN",
      "relationship": "subtopic"
    },
    {
      "from": "MDP (Markov Decision Process)",
      "to": "Machine Learning Concepts",
      "relationship": "related_to"
    },
    {
      "from": "Loss_Functions",
      "to": "Training_Loss",
      "relationship": "subtopic"
    },
    {
      "from": "Linear Regression",
      "to": "Housing Example",
      "relationship": "has_example"
    },
    {
      "from": "Value_Iteration",
      "to": "Convergence_to_Optimal_Value",
      "relationship": "subtopic"
    },
    {
      "from": "GeneralizedLinearModel",
      "to": "GLMAssumptions",
      "relationship": "subtopic"
    },
    {
      "from": "VE Procedure",
      "to": "Option 2 Initialization",
      "relationship": "subtopic"
    },
    {
      "from": "Backpropagation",
      "to": "Differentiable_Circuit",
      "relationship": "defines"
    },
    {
      "from": "Binary Classification",
      "to": "2.3 Multi-class classification",
      "relationship": "related_to"
    },
    {
      "from": "Neural Networks",
      "to": "Parametrization (h_\u03b8(x))",
      "relationship": "has_subtopic"
    },
    {
      "from": "MachineLearningOverview",
      "to": "ResidualConnections",
      "relationship": "subtopic"
    },
    {
      "from": "Eigenvectors",
      "to": "Principal Components",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningConcepts",
      "to": "KMeansAlgorithm",
      "relationship": "contains"
    },
    {
      "from": "Gradient_Descent",
      "to": "LMS_Update_Rule",
      "relationship": "has_subtopic"
    },
    {
      "from": "Supervised Learning Problem",
      "to": "Hypothesis",
      "relationship": "has_subtopic"
    },
    {
      "from": "EMAlgorithm",
      "to": "ConvergenceGuarantees",
      "relationship": "subtopic"
    },
    {
      "from": "Regularization and model selection",
      "to": "Bayesian statistics and regularization",
      "relationship": "subtopic"
    },
    {
      "from": "Support Vector Machines (SVMs)",
      "to": "Notation for SVMs",
      "relationship": "subtopic"
    },
    {
      "from": "Deep Learning Introduction",
      "to": "Machine Learning Overview",
      "relationship": "related_to"
    },
    {
      "from": "Finding Roots",
      "to": "Newton's Method",
      "relationship": "depends_on"
    },
    {
      "from": "Machine_Learning_Topics",
      "to": "Differential_Dynamic_Programming",
      "relationship": "contains"
    },
    {
      "from": "Kernel_Methods",
      "to": "Feature_Maps",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningOverview",
      "to": "RegressionProblem",
      "relationship": "subtopic"
    },
    {
      "from": "dStarAndPStarEquality",
      "to": "FeasibilityConstraints",
      "relationship": "depends_on"
    },
    {
      "from": "I Supervised learning",
      "to": "5 Kernel methods",
      "relationship": "has_subtopic"
    },
    {
      "from": "PCA",
      "to": "Eigenvectors",
      "relationship": "depends_on"
    },
    {
      "from": "Variational_Autoencoder",
      "to": "EM_Algorithms",
      "relationship": "extends"
    },
    {
      "from": "KernelTrick",
      "to": "MachineLearningOverview",
      "relationship": "related_to"
    },
    {
      "from": "State_Transition_Probabilities",
      "to": "Model_Learning_for_MDPs",
      "relationship": "subtopic"
    },
    {
      "from": "HypothesisSpace",
      "to": "GeneralizationError",
      "relationship": "related_to"
    },
    {
      "from": "2 Classification and logistic regression",
      "to": "2.2 Digression: the perceptron learning algorithm",
      "relationship": "has_subtopic"
    },
    {
      "from": "Self-supervised learning and foundation models",
      "to": "Pretraining methods in computer vision",
      "relationship": "subtopic"
    },
    {
      "from": "Value_Function_Approximation",
      "to": "Deterministic_Simulator",
      "relationship": "related_to"
    },
    {
      "from": "WeightsCalculation",
      "to": "BandwidthParameter",
      "relationship": "depends_on"
    },
    {
      "from": "5 Kernel methods",
      "to": "5.1 Feature maps",
      "relationship": "has_subtopic"
    },
    {
      "from": "MachineLearningConcepts",
      "to": "PartialDerivatives",
      "relationship": "depends_on"
    },
    {
      "from": "Infinite_Hypothesis_Classes",
      "to": "Machine_Learning_Bias_Variance_Tradeoff",
      "relationship": "subtopic"
    },
    {
      "from": "PolynomialKernels",
      "to": "ComputationalEfficiency",
      "relationship": "depends_on"
    },
    {
      "from": "LMS_Rule",
      "to": "Widrow_Hoff_Learning_Rule",
      "relationship": "related_to"
    },
    {
      "from": "Value_Functions",
      "to": "Optimal_Value_Function",
      "relationship": "subtopic"
    },
    {
      "from": "Regularization and model selection",
      "to": "Model selection via cross validation",
      "relationship": "subtopic"
    },
    {
      "from": "Feature Discovery",
      "to": "Black Box Nature",
      "relationship": "related_to"
    },
    {
      "from": "Deep Learning Regularizers",
      "to": "Weight Decay",
      "relationship": "contains"
    },
    {
      "from": "MachineLearningModels",
      "to": "GaussianDiscriminantAnalysis",
      "relationship": "contains"
    },
    {
      "from": "Gradient_Descent",
      "to": "Partial_Derivative_Calculation",
      "relationship": "has_subtopic"
    },
    {
      "from": "MulticlassClassification",
      "to": "LogitsInMultiClass",
      "relationship": "has_subtopic"
    },
    {
      "from": "Optimal_Control",
      "to": "Value_Functions",
      "relationship": "subtopic"
    },
    {
      "from": "UnderfittingLinearModel",
      "to": "BiasInModels",
      "relationship": "related_to"
    },
    {
      "from": "Vapnik's Theorem",
      "to": "Hypothesis Class",
      "relationship": "depends_on"
    },
    {
      "from": "DataNormalization",
      "to": "MeanRemoval",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Backpropagation Algorithm",
      "relationship": "related_to"
    },
    {
      "from": "LinearQuadraticRegulatorLQR",
      "to": "LQG",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Policy_Gradient_Theorem",
      "relationship": "contains"
    },
    {
      "from": "PCA",
      "to": "Approximation Error Minimization",
      "relationship": "related_to"
    },
    {
      "from": "BiologicalInspiration",
      "to": "TwoLayerNetworks",
      "relationship": "depends_on"
    },
    {
      "from": "Model Specification",
      "to": "Logistic Regression Example",
      "relationship": "subtopic"
    },
    {
      "from": "Jensen's_Inequality",
      "to": "Convergence",
      "relationship": "subtopic"
    },
    {
      "from": "CostFunction",
      "to": "OrdinaryLeastSquares",
      "relationship": "related_to"
    },
    {
      "from": "Machine_Learning_Theory",
      "to": "Sample_Complexity",
      "relationship": "subtopic"
    },
    {
      "from": "Sample-wise Double Descent",
      "to": "Optimization Algorithms",
      "relationship": "depends_on"
    },
    {
      "from": "MachineLearningAlgorithms",
      "to": "DiscriminativeAlgorithms",
      "relationship": "has_subtopic"
    },
    {
      "from": "MachineLearningConcepts",
      "to": "ShatteringConcept",
      "relationship": "related_to"
    },
    {
      "from": "Foundation_Models",
      "to": "Machine_Learning_Papers",
      "relationship": "subtopic"
    },
    {
      "from": "OverfittingFifthDegree",
      "to": "GeneralizationFailure",
      "relationship": "depends_on"
    },
    {
      "from": "E-step",
      "to": "Q-function",
      "relationship": "depends_on"
    },
    {
      "from": "Frequentist View",
      "to": "Maximum Likelihood Estimation (MLE)",
      "relationship": "includes_method"
    },
    {
      "from": "Support_Vector_Machines",
      "to": "Lagrange_Duality",
      "relationship": "subtopic"
    },
    {
      "from": "Deep Learning Regularizers",
      "to": "Dropout",
      "relationship": "contains"
    },
    {
      "from": "MachineLearningValidationTechniques",
      "to": "kFoldCrossValidation",
      "relationship": "has_subtopic"
    },
    {
      "from": "Value Iteration",
      "to": "Bellman Update",
      "relationship": "depends_on"
    },
    {
      "from": "IntermediateVariables",
      "to": "ReLUActivationFunction",
      "relationship": "uses"
    },
    {
      "from": "Activation Function Backward Function",
      "to": "Activation Derivative Matrix",
      "relationship": "has_subtopic"
    },
    {
      "from": "ConstrainedOptimization",
      "to": "LagrangeMultipliers",
      "relationship": "depends_on"
    },
    {
      "from": "Necessary Conditions for Valid Kernels",
      "to": "Kernel Matrix",
      "relationship": "subtopic"
    },
    {
      "from": "Model Selection",
      "to": "SVM and Regularization",
      "relationship": "related_to"
    },
    {
      "from": "Unsupervised learning",
      "to": "Principal components analysis",
      "relationship": "subtopic"
    },
    {
      "from": "ELBO_Optimization",
      "to": "Efficient_Evaluation_ELBO",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningOverview",
      "to": "NeuralNetworks",
      "relationship": "contains"
    },
    {
      "from": "GradientComputation",
      "to": "ReparameterizationTrick",
      "relationship": "related_to"
    },
    {
      "from": "ProjectionDirection",
      "to": "VarianceMaximization",
      "relationship": "has_subtopic"
    },
    {
      "from": "TrainingSetExamples",
      "to": "LayerActivations",
      "relationship": "depends_on"
    },
    {
      "from": "Non_Gaussian_Sources",
      "to": "ICA_Ambiguities",
      "relationship": "related_to"
    },
    {
      "from": "RegressionProblems",
      "to": "MeanSquareCostFunction",
      "relationship": "subtopic"
    },
    {
      "from": "MultipleExamplesExtension",
      "to": "ELBOFormula",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningOverview",
      "to": "NeuralNetworksComposition",
      "relationship": "has_subtopic"
    },
    {
      "from": "Multivariate Normal Distribution",
      "to": "Covariance Matrix",
      "relationship": "related_to"
    },
    {
      "from": "Implementation Details",
      "to": "Conversion Rules",
      "relationship": "subtopic"
    },
    {
      "from": "Test Error Decomposition",
      "to": "Quadratic Model Example",
      "relationship": "example_of"
    },
    {
      "from": "Partial Derivatives",
      "to": "Scalar Variable J",
      "relationship": "depends_on"
    },
    {
      "from": "GLMFormulation",
      "to": "BernoulliDistribution",
      "relationship": "subtopic"
    },
    {
      "from": "Pretraining_Phase",
      "to": "Pretraining_Loss_Function",
      "relationship": "subtopic"
    },
    {
      "from": "GeometricMargin",
      "to": "DecisionBoundary",
      "relationship": "subtopic"
    },
    {
      "from": "Loss_Functions",
      "to": "Test_Error",
      "relationship": "subtopic"
    },
    {
      "from": "Regularization in Machine Learning",
      "to": "Sparsity Regularization",
      "relationship": "contains"
    },
    {
      "from": "Sample complexity bounds (optional readings)",
      "to": "The case of finite H",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningOverview",
      "to": "MaximumLikelihoodEstimation",
      "relationship": "depends_on"
    },
    {
      "from": "Bias-Variance Tradeoff",
      "to": "Overfitting",
      "relationship": "related_to"
    },
    {
      "from": "Machine_Learning_Theory",
      "to": "Hypothesis_Class_Size",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningConcepts",
      "to": "CumulativeDistributionFunction",
      "relationship": "related_to"
    },
    {
      "from": "Machine_Learning_Topic",
      "to": "Text_Classification",
      "relationship": "has_subtopic"
    },
    {
      "from": "EM_Algorithm",
      "to": "E_Step",
      "relationship": "depends_on"
    },
    {
      "from": "VariationalInference",
      "to": "ELBO",
      "relationship": "depends_on"
    },
    {
      "from": "LagrangianFormulation",
      "to": "DualProblem",
      "relationship": "subtopic"
    },
    {
      "from": "1D_Convolution",
      "to": "Bias_Scalar",
      "relationship": "has_component"
    },
    {
      "from": "Machine_Learning",
      "to": "Policy_Gradient_Methods",
      "relationship": "has_subtopic"
    },
    {
      "from": "5 Kernel methods",
      "to": "5.4 Properties of kernels",
      "relationship": "has_subtopic"
    },
    {
      "from": "MachineLearningModels",
      "to": "DecisionBoundaries",
      "relationship": "contains"
    },
    {
      "from": "Finetuning_Pretrained_Models",
      "to": "Optimization_Objective",
      "relationship": "has_subtopic"
    },
    {
      "from": "Law_of_Total_Expectation",
      "to": "Estimator_Simplification",
      "relationship": "related_to"
    },
    {
      "from": "Adaptation Algorithms",
      "to": "Finetuning",
      "relationship": "subtopic"
    },
    {
      "from": "FunctionRepresentation",
      "to": "LinearHypothesis",
      "relationship": "subtopic"
    },
    {
      "from": "Self-Supervised Learning",
      "to": "Negative Pair",
      "relationship": "subtopic"
    },
    {
      "from": "Matrix Multiplication Module",
      "to": "MLP Composition",
      "relationship": "subtopic"
    },
    {
      "from": "Value Function",
      "to": "Optimal Value Function",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "ELBO_Optimization",
      "relationship": "contains"
    },
    {
      "from": "Classification Confidence",
      "to": "Functional Margins",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningModels",
      "to": "GDA",
      "relationship": "contains"
    },
    {
      "from": "EMAlgorithm",
      "to": "EStep",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningOverview",
      "to": "VariationalInference",
      "relationship": "contains"
    },
    {
      "from": "VariationalAutoEncoder",
      "to": "LatentVariableModel",
      "relationship": "contains"
    },
    {
      "from": "Sample Complexity Bounds",
      "to": "Model Selection Methods",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningOverview",
      "to": "LeastSquaresRegression",
      "relationship": "depends_on"
    },
    {
      "from": "Model Parameters",
      "to": "Likelihood Function",
      "relationship": "subtopic"
    },
    {
      "from": "Batch_Gradient_Descent",
      "to": "Gradient_Descent",
      "relationship": "related_to"
    },
    {
      "from": "Machine_Learning",
      "to": "Training_Dataset",
      "relationship": "depends_on"
    },
    {
      "from": "FeatureMapping",
      "to": "GradientDescentAlgorithm",
      "relationship": "depends_on"
    },
    {
      "from": "BinaryClassification",
      "to": "TrainingError",
      "relationship": "defines"
    },
    {
      "from": "Support_Vector_Machines",
      "to": "SMO_Algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "1 Linear regression",
      "to": "1.4 Locally weighted linear regression (optional reading)",
      "relationship": "has_subtopic"
    },
    {
      "from": "MachineLearningOverview",
      "to": "LinearRegression",
      "relationship": "related_to"
    },
    {
      "from": "LayerNormalization",
      "to": "MachineLearningConcepts",
      "relationship": "subtopic"
    },
    {
      "from": "Training_Set_Issues",
      "to": "Spam_Filtering",
      "relationship": "related_to"
    },
    {
      "from": "PCA_Methods",
      "to": "Computational_Efficiency",
      "relationship": "subtopic"
    },
    {
      "from": "Immediate Reward",
      "to": "Bellman Equations",
      "relationship": "related_to"
    },
    {
      "from": "Parameter_Scaling_Invariance",
      "to": "Geometric_Margin",
      "relationship": "related_to"
    },
    {
      "from": "Optimizers and Generalization",
      "to": "Global Minima and Generalization",
      "relationship": "related_to"
    },
    {
      "from": "I Supervised learning",
      "to": "1 Linear regression",
      "relationship": "has_subtopic"
    },
    {
      "from": "Sample complexity bounds (optional readings)",
      "to": "The case of infinite H",
      "relationship": "subtopic"
    },
    {
      "from": "Partially Observable MDPs (POMDP)",
      "to": "Belief State",
      "relationship": "has_subtopic"
    },
    {
      "from": "Expectation_Calculation",
      "to": "Gaussian_Noise_Model",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning",
      "to": "Support_Vector_Machines_SVM",
      "relationship": "has_subtopic"
    },
    {
      "from": "SupportVectors",
      "to": "AlphaCoefficients",
      "relationship": "related_to"
    },
    {
      "from": "Mean Field Assumption",
      "to": "Variational Inference",
      "relationship": "subtopic"
    },
    {
      "from": "LQR Algorithm",
      "to": "Step 3",
      "relationship": "related_to"
    },
    {
      "from": "FunctionalMargin",
      "to": "ScalingImpact",
      "relationship": "subtopic"
    },
    {
      "from": "Markov_Decision_Processes_(MDP)",
      "to": "State_Transitions",
      "relationship": "depends_on"
    },
    {
      "from": "BatchNormalization",
      "to": "OtherNormalizationLayers",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningOverview",
      "to": "LossFunctionOptimization",
      "relationship": "contains"
    },
    {
      "from": "MachineLearningConcepts",
      "to": "BiasVsVarianceTradeoff",
      "relationship": "subtopic"
    },
    {
      "from": "Reinforcement_Learning",
      "to": "Policy_Iteration",
      "relationship": "depends_on"
    },
    {
      "from": "Geometric_Margin",
      "to": "Optimal_Margin_Classifier",
      "relationship": "depends_on"
    },
    {
      "from": "Sample Complexity Bounds",
      "to": "Bias-Variance Tradeoff",
      "relationship": "subtopic"
    },
    {
      "from": "3 Generalized linear models",
      "to": "3.1 The exponential family",
      "relationship": "has_subtopic"
    },
    {
      "from": "LocallyWeightedLinearRegression",
      "to": "MachineLearningConcepts",
      "relationship": "subtopic"
    },
    {
      "from": "Neural_Network_Parameterization",
      "to": "Variational_Autoencoder",
      "relationship": "parameterizes"
    },
    {
      "from": "ELBO_Optimization",
      "to": "Form_of_Q_i",
      "relationship": "subtopic"
    },
    {
      "from": "Jensen's Inequality",
      "to": "Convex Functions",
      "relationship": "depends_on"
    },
    {
      "from": "Implicit_Bias",
      "to": "Machine_Learning_Papers",
      "relationship": "depends_on"
    },
    {
      "from": "MachineLearningConcepts",
      "to": "ZeroShotLearning",
      "relationship": "has_subtopic"
    },
    {
      "from": "Decision Boundary",
      "to": "Geometric Margins",
      "relationship": "related_to"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Objective_Function",
      "relationship": "related_to"
    },
    {
      "from": "Value Iteration",
      "to": "Theorem",
      "relationship": "proves"
    },
    {
      "from": "BernoulliDistribution",
      "to": "SufficientStatisticForBernoulli",
      "relationship": "subtopic"
    },
    {
      "from": "Chapter_17_Policy_Gradient_REINFORCE",
      "to": "Randomized_Policy",
      "relationship": "subtopic"
    },
    {
      "from": "EM algorithms",
      "to": "Jensen's inequality",
      "relationship": "subtopic"
    },
    {
      "from": "Policy_Gradient_Methods",
      "to": "Baseline_Estimator",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine_Learning",
      "to": "Logistic_Regression",
      "relationship": "contains"
    },
    {
      "from": "System_Dynamics",
      "to": "Linearization_of_Dynamics",
      "relationship": "subtopic"
    },
    {
      "from": "Alpha_Parameters",
      "to": "Constraint_Equation",
      "relationship": "depends_on"
    },
    {
      "from": "MachineLearningOverview",
      "to": "DoubleDescentPhenomenon",
      "relationship": "depends_on"
    },
    {
      "from": "FeatureEngineering",
      "to": "MachineLearningOverview",
      "relationship": "subtopic"
    },
    {
      "from": "DifferentialDynamicProgramming",
      "to": "NominalTrajectoryGeneration",
      "relationship": "subtopic"
    },
    {
      "from": "Supervised Learning Problem",
      "to": "Regression",
      "relationship": "has_subtopic"
    },
    {
      "from": "Linear_Separability",
      "to": "Maximize_Geometric_Margin_Optimization",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningConcepts",
      "to": "JensensInequality",
      "relationship": "depends_on"
    },
    {
      "from": "Lagrange Duality",
      "to": "Lagrange Multipliers",
      "relationship": "uses"
    },
    {
      "from": "Kernel Functions",
      "to": "Feature Mapping",
      "relationship": "related_to"
    },
    {
      "from": "Parameter_Estimation",
      "to": "Laplace_Smoothing",
      "relationship": "related_to"
    },
    {
      "from": "BackwardFunctionsBasics",
      "to": "ActivationFunctions",
      "relationship": "related_to"
    },
    {
      "from": "ExpectationRewriting",
      "to": "StateActionDependentRewards",
      "relationship": "has_subtopic"
    },
    {
      "from": "Cross_Validation",
      "to": "Hold_Out_Cross_Validation",
      "relationship": "subtopic"
    },
    {
      "from": "UniformConvergence",
      "to": "GeneralizationError",
      "relationship": "depends_on"
    },
    {
      "from": "StateTransitionModel",
      "to": "NonLinearModels",
      "relationship": "has_subtopic"
    },
    {
      "from": "NeuralNetworks",
      "to": "BiasVectors",
      "relationship": "contains"
    },
    {
      "from": "Model Selection",
      "to": "Cross Validation",
      "relationship": "depends_on"
    },
    {
      "from": "Markov_Decision_Processes_MDPs",
      "to": "State_Transition_Probabilities",
      "relationship": "depends_on"
    },
    {
      "from": "MulticlassClassification",
      "to": "SoftmaxFunction",
      "relationship": "has_subtopic"
    },
    {
      "from": "OptimizationInML",
      "to": "HessianMatrix",
      "relationship": "depends_on"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Training_Test_Distributions",
      "relationship": "contains"
    },
    {
      "from": "NeuralNetworks",
      "to": "ActivationFunctions",
      "relationship": "subtopic"
    },
    {
      "from": "Support Vector Machines (SVMs)",
      "to": "Functional Margin",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningConcepts",
      "to": "GeometricMargins",
      "relationship": "subtopic"
    },
    {
      "from": "kFoldCrossValidation",
      "to": "DataSplitting",
      "relationship": "depends_on"
    },
    {
      "from": "Backpropagation Strategy",
      "to": "Concrete Backprop Algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "TrainingLoss",
      "to": "Regularizer",
      "relationship": "contains"
    },
    {
      "from": "ProbabilisticModeling",
      "to": "ConditionalProbabilityDistribution",
      "relationship": "depends_on"
    },
    {
      "from": "MSEDecomposition",
      "to": "AverageModel",
      "relationship": "subtopic_of"
    },
    {
      "from": "MachineLearningBasics",
      "to": "TrainingSetExamples",
      "relationship": "depends_on"
    },
    {
      "from": "Regularization in Machine Learning",
      "to": "Deep Learning Regularizers",
      "relationship": "contains"
    },
    {
      "from": "Parameter_Scaling_Invariance",
      "to": "Machine_Learning_Concepts",
      "relationship": "subtopic"
    },
    {
      "from": "Value_Functions",
      "to": "Bellman_Equation",
      "relationship": "contains"
    },
    {
      "from": "Machine Learning Algorithms",
      "to": "Algorithm 6",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Nonlinear_Dynamics_to_LQR",
      "relationship": "subtopic"
    },
    {
      "from": "MaximumLikelihoodEstimation",
      "to": "GradientAscentUpdateRule",
      "relationship": "subtopic"
    },
    {
      "from": "LogisticRegression",
      "to": "GradientAscentRule",
      "relationship": "subtopic"
    },
    {
      "from": "KMeansAlgorithm",
      "to": "ConvergenceProperties",
      "relationship": "related_to"
    },
    {
      "from": "MachineLearningOverview",
      "to": "GradientDescentOptimizer",
      "relationship": "depends_on"
    },
    {
      "from": "GeneralizedLinearModelsGLMs",
      "to": "AssumptionsOfGLMs",
      "relationship": "depends_on"
    },
    {
      "from": "BiasVarianceTradeoff",
      "to": "HypothesisSpace",
      "relationship": "subtopic"
    },
    {
      "from": "6 Support vector machines",
      "to": "6.1 Margins: intuition",
      "relationship": "has_subtopic"
    },
    {
      "from": "MachineLearningConcepts",
      "to": "EvidenceLowerBoundELBO",
      "relationship": "related_to"
    },
    {
      "from": "Backward Function Overview",
      "to": "Matrix Multiplication Backward Function",
      "relationship": "has_subtopic"
    },
    {
      "from": "SigmoidFunction",
      "to": "ActivationFunctions",
      "relationship": "subtopic"
    },
    {
      "from": "GLMFormulation",
      "to": "GaussianDistribution",
      "relationship": "subtopic"
    },
    {
      "from": "Probability_Error",
      "to": "Training_Error_Generalization_Error",
      "relationship": "subtopic"
    },
    {
      "from": "Regularization",
      "to": "L2_Regularization",
      "relationship": "subtopic_of"
    },
    {
      "from": "Gradient_Estimation",
      "to": "Reparameterization_Trick",
      "relationship": "subtopic"
    },
    {
      "from": "Uniform_Convergence_Bound",
      "to": "Hypothesis_Space_Size",
      "relationship": "related_to"
    },
    {
      "from": "Bayesian Machine Learning",
      "to": "Prediction on New Data",
      "relationship": "related_to"
    },
    {
      "from": "Gradient Descent (GD)",
      "to": "Learning Rate",
      "relationship": "depends_on"
    },
    {
      "from": "NaiveBayes",
      "to": "EmailSpamFiltering",
      "relationship": "contains"
    },
    {
      "from": "Jensen's Inequality",
      "to": "Concave Functions",
      "relationship": "related_to"
    },
    {
      "from": "BackpropagationDiscussion",
      "to": "BackwardFunctionsBasics",
      "relationship": "has_subtopic"
    },
    {
      "from": "Contrastive_Learning",
      "to": "SIMCLR",
      "relationship": "example_of"
    },
    {
      "from": "Naive_Bayes_Classifier",
      "to": "Parameter_Estimation",
      "relationship": "depends_on"
    },
    {
      "from": "Model_Learning_for_MDPs",
      "to": "Machine_Learning_Algorithms",
      "relationship": "related_to"
    },
    {
      "from": "Feature_Vector",
      "to": "Vocabulary",
      "relationship": "based_on"
    },
    {
      "from": "Chain_Rule_Application",
      "to": "Gradient_Computation",
      "relationship": "depends_on"
    },
    {
      "from": "KMeansAlgorithm",
      "to": "DistortionFunctionJ",
      "relationship": "depends_on"
    },
    {
      "from": "1 Linear regression",
      "to": "1.1 LMS algorithm",
      "relationship": "has_subtopic"
    },
    {
      "from": "I Supervised learning",
      "to": "3 Generalized linear models",
      "relationship": "has_subtopic"
    },
    {
      "from": "Regularization in Deep Learning",
      "to": "Implicit Regularization Effect",
      "relationship": "has_subtopic"
    },
    {
      "from": "Joint Distribution",
      "to": "Model Parameters",
      "relationship": "depends_on"
    },
    {
      "from": "Policy_Gradient_Methods",
      "to": "Transition_Probabilities",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Partially Observable MDPs (POMDP)",
      "relationship": "contains"
    },
    {
      "from": "1D Example",
      "to": "Density Transformation",
      "relationship": "subtopic"
    },
    {
      "from": "Dual_Problem_Formulation",
      "to": "KKT_Conditions",
      "relationship": "related_to"
    },
    {
      "from": "Optimal Value Function",
      "to": "Optimal Policy",
      "relationship": "defines"
    },
    {
      "from": "NeurIPS_Conference",
      "to": "Training_Set_Issues",
      "relationship": "related_to"
    },
    {
      "from": "Reinforcement learning",
      "to": "Learning a model for an MDP",
      "relationship": "subtopic"
    },
    {
      "from": "Dynamics_and_Control_Systems",
      "to": "System_Dynamics",
      "relationship": "includes"
    },
    {
      "from": "Regularization",
      "to": "Overfitting",
      "relationship": "addresses"
    },
    {
      "from": "MachineLearningConcepts",
      "to": "EMAlgorithms",
      "relationship": "contains"
    },
    {
      "from": "DoubleDescentPhenomenon",
      "to": "StatisticalMechanicsOfLearning",
      "relationship": "depends_on"
    },
    {
      "from": "NaiveBayes",
      "to": "DiscreteFeatures",
      "relationship": "contains"
    },
    {
      "from": "Chapter9",
      "to": "Regularization",
      "relationship": "contains"
    },
    {
      "from": "GaussianDistribution",
      "to": "ErrorTermAssumption",
      "relationship": "related_to"
    },
    {
      "from": "Conditional_Probability",
      "to": "Softmax_Function",
      "relationship": "uses"
    },
    {
      "from": "BackwardFunctionsBasics",
      "to": "MatrixMultiplicationModule",
      "relationship": "has_subtopic"
    },
    {
      "from": "Training_Test_Distributions",
      "to": "Domain_Shift",
      "relationship": "related_to"
    },
    {
      "from": "Bayesian Machine Learning",
      "to": "Computational Challenges",
      "relationship": "related_to"
    },
    {
      "from": "Neural Networks",
      "to": "Stacking Neurons",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Linear Regression Model",
      "relationship": "contains"
    },
    {
      "from": "MachineLearningAlgorithm",
      "to": "BatchGradientDescent",
      "relationship": "contains"
    },
    {
      "from": "ScaleInvariantProperty",
      "to": "LayerNormalization",
      "relationship": "related_to"
    },
    {
      "from": "Machine_Learning_Topics",
      "to": "Dynamics_and_Control_Systems",
      "relationship": "contains"
    },
    {
      "from": "LinearRegression",
      "to": "CostFunctionJ",
      "relationship": "depends_on"
    },
    {
      "from": "MachineLearningOverview",
      "to": "GeneralizedLinearModel",
      "relationship": "subtopic"
    },
    {
      "from": "Pretraining_Methods",
      "to": "Contrastive_Learning",
      "relationship": "has_subtopic"
    },
    {
      "from": "Baseline_Estimator",
      "to": "Algorithm_7_Vanilla_Policy_Gradient_Baseline",
      "relationship": "related_to"
    },
    {
      "from": "NeuralNetworkParameters",
      "to": "DeepLearning",
      "relationship": "subtopic"
    },
    {
      "from": "Back-propagation Algorithm",
      "to": "MLP Backpropagation",
      "relationship": "has_subtopic"
    },
    {
      "from": "In-Context_Learning",
      "to": "Machine_Learning_Adaptation",
      "relationship": "subtopic"
    },
    {
      "from": "LayerNormalization",
      "to": "LearnableParameters",
      "relationship": "has"
    },
    {
      "from": "Backpropagation",
      "to": "Gradient_Computation",
      "relationship": "leads_to"
    },
    {
      "from": "Learning a model for an MDP",
      "to": "Connections between Policy and Value Iteration (Optional)",
      "relationship": "has_subtopic"
    },
    {
      "from": "Pretraining_Phase",
      "to": "Unlabeled_Dataset",
      "relationship": "depends_on"
    },
    {
      "from": "Asynchronous Updates",
      "to": "Value Iteration Algorithm",
      "relationship": "related_to"
    },
    {
      "from": "Value Function",
      "to": "Policy Evaluation",
      "relationship": "has_subtopic"
    },
    {
      "from": "Reinforcement_Learning",
      "to": "Policy",
      "relationship": "subtopic"
    },
    {
      "from": "BLASOptimization",
      "to": "VectorizationInNN",
      "relationship": "subtopic"
    },
    {
      "from": "Logistic_Regression",
      "to": "Newton_Method",
      "relationship": "related_to"
    },
    {
      "from": "M-step_Update_for_phi_j",
      "to": "Lagrangian_Method",
      "relationship": "uses"
    },
    {
      "from": "MachineLearningOverview",
      "to": "RegressionProblems",
      "relationship": "subtopic"
    },
    {
      "from": "Design Matrix",
      "to": "Least Squares Revisited",
      "relationship": "subtopic"
    },
    {
      "from": "Logistic Regression Derivation",
      "to": "Perceptron Learning Algorithm",
      "relationship": "related_to"
    },
    {
      "from": "Policy_Gradient_Theory",
      "to": "Vanilla_REINFORCE_Algorithm",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine Learning Algorithms",
      "to": "Neural Networks",
      "relationship": "related_to"
    },
    {
      "from": "BackwardFunctionsBasics",
      "to": "LossFunctionBackward",
      "relationship": "related_to"
    },
    {
      "from": "REINFORCE_Finite_Horizon_Case",
      "to": "Randomized_Policy",
      "relationship": "depends_on"
    },
    {
      "from": "Transformer_Model",
      "to": "Conditional_Probability",
      "relationship": "depends_on"
    },
    {
      "from": "Conditional Probabilistic Model",
      "to": "Exponential Family Distribution",
      "relationship": "depends_on"
    },
    {
      "from": "Dual_Optimization_Problem",
      "to": "Equation_6.12",
      "relationship": "defines"
    },
    {
      "from": "MachineLearningConcepts",
      "to": "ICAIndependenceAssumption",
      "relationship": "related_to"
    },
    {
      "from": "Self-supervised learning and foundation models",
      "to": "Pretrained large language models",
      "relationship": "subtopic"
    },
    {
      "from": "HousingPricesModel",
      "to": "DerivedFeatures",
      "relationship": "depends_on"
    },
    {
      "from": "I Supervised learning",
      "to": "2 Classification and logistic regression",
      "relationship": "has_subtopic"
    },
    {
      "from": "Backward Function Overview",
      "to": "Activation Function Backward Function",
      "relationship": "has_subtopic"
    },
    {
      "from": "Naive Bayes Algorithm",
      "to": "Spam Classification",
      "relationship": "application"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Random_Variables",
      "relationship": "contains"
    },
    {
      "from": "MachineLearningValidationTechniques",
      "to": "HoldoutCrossValidation",
      "relationship": "has_subtopic"
    },
    {
      "from": "Naive Bayes Algorithm",
      "to": "Multinomial Distribution",
      "relationship": "subtopic_of"
    },
    {
      "from": "MachineLearningModeling",
      "to": "StateTransitionModel",
      "relationship": "has_subtopic"
    },
    {
      "from": "NormalEquationsMethod",
      "to": "MatrixDerivatives",
      "relationship": "has_subtopic"
    },
    {
      "from": "Support Vector Machines (SVMs)",
      "to": "Geometric Margin",
      "relationship": "subtopic"
    },
    {
      "from": "Bias Term",
      "to": "Variance Term",
      "relationship": "related_to"
    },
    {
      "from": "ExponentialFamilyDistributions",
      "to": "LogPartitionFunction",
      "relationship": "has"
    },
    {
      "from": "ICAIndependenceAssumption",
      "to": "JointDistributionModeling",
      "relationship": "subtopic"
    },
    {
      "from": "Vectorized Notation",
      "to": "Equation 7.65",
      "relationship": "defines"
    },
    {
      "from": "LQR Extension",
      "to": "Kalman Filter",
      "relationship": "depends_on"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Optimal_Margin_Classifiers",
      "relationship": "contains"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "KKT_Conditions",
      "relationship": "related_to"
    },
    {
      "from": "Generative_Modeling",
      "to": "Naive_Bayes_Assumption",
      "relationship": "assumes"
    },
    {
      "from": "ProbabilisticModeling",
      "to": "LikelihoodFunction",
      "relationship": "subtopic"
    },
    {
      "from": "BatchGradientDescent",
      "to": "ThetaRepresentation",
      "relationship": "related_to"
    },
    {
      "from": "Unsupervised learning",
      "to": "Clustering and the k-means algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "SigmoidFunction",
      "to": "TanhFunction",
      "relationship": "related_to"
    },
    {
      "from": "KernelTrickIntroduction",
      "to": "PhiFunctionExpansion",
      "relationship": "subtopic"
    },
    {
      "from": "Generalization",
      "to": "The double descent phenomenon",
      "relationship": "subtopic"
    },
    {
      "from": "PretrainedLanguageModels",
      "to": "DocumentProbabilityModeling",
      "relationship": "contains"
    },
    {
      "from": "Off-the-Shelf Physics Software",
      "to": "Open Dynamics Engine",
      "relationship": "example_of"
    },
    {
      "from": "Foundation Models",
      "to": "Pretraining and Adaptation",
      "relationship": "has_subtopic"
    },
    {
      "from": "Regularization",
      "to": "Inductive_Biases",
      "relationship": "subtopic_of"
    },
    {
      "from": "MSEDecomposition",
      "to": "VarianceTerm",
      "relationship": "subtopic_of"
    },
    {
      "from": "Dual Problem",
      "to": "Primal Problem",
      "relationship": "related_to"
    },
    {
      "from": "Unsupervised learning",
      "to": "Self-supervised learning and foundation models",
      "relationship": "related_to"
    },
    {
      "from": "Maximizing Functions",
      "to": "Newton's Method",
      "relationship": "subtopic"
    },
    {
      "from": "I Supervised learning",
      "to": "6 Support vector machines",
      "relationship": "has_subtopic"
    },
    {
      "from": "MachineLearningBasics",
      "to": "CostFunction",
      "relationship": "subtopic"
    },
    {
      "from": "Perceptron Learning Algorithm",
      "to": "Multi-class Classification",
      "relationship": "depends_on"
    },
    {
      "from": "Policy_Gradient_Methods",
      "to": "Value_Functions",
      "relationship": "has_subtopic"
    },
    {
      "from": "SummationNotEssential",
      "to": "SingleExampleLikelihood",
      "relationship": "subtopic"
    },
    {
      "from": "Inner_Products_Calculation",
      "to": "Dual_Form_Optimization",
      "relationship": "subtopic"
    },
    {
      "from": "FeatureMap",
      "to": "LinearFunctionOverFeatures",
      "relationship": "related_to"
    },
    {
      "from": "Action Evaluation",
      "to": "Reward Estimation",
      "relationship": "depends_on"
    },
    {
      "from": "Gradient_Descent",
      "to": "Cost_Function_J_theta",
      "relationship": "related_to"
    },
    {
      "from": "EMAlgorithm",
      "to": "LogLikelihoodOptimization",
      "relationship": "subtopic"
    },
    {
      "from": "Cross_Entropy_Loss",
      "to": "Gradient_Descent",
      "relationship": "related_to"
    },
    {
      "from": "Support_Vector_Machines_SVMs",
      "to": "SVM_Dual_Optimization_Problem",
      "relationship": "subtopic"
    },
    {
      "from": "Vapnik's Theorem",
      "to": "Uniform Convergence",
      "relationship": "implies"
    },
    {
      "from": "EMAlgorithm",
      "to": "MultipleExamplesExtension",
      "relationship": "subtopic"
    },
    {
      "from": "BinaryClassification",
      "to": "TrainingSet",
      "relationship": "depends_on"
    },
    {
      "from": "Sequential Minimal Optimization (SMO) Algorithm",
      "to": "Support Vector Machines (SVM)",
      "relationship": "subtopic"
    },
    {
      "from": "Kernel_Methods",
      "to": "Kernel_Tricks",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine_Learning_Backward_Propagation",
      "to": "Chain_Rule_Applications",
      "relationship": "subtopic"
    },
    {
      "from": "Policy_Gradient_Methods",
      "to": "REINFORCE_Algorithm",
      "relationship": "related_to"
    },
    {
      "from": "Markov_Decision_Processes_MDPs",
      "to": "Model_Learning",
      "relationship": "contains"
    },
    {
      "from": "Linearization_of_Dynamics",
      "to": "Taylor_Expansion",
      "relationship": "uses"
    },
    {
      "from": "Supervised Learning",
      "to": "Linear Regression",
      "relationship": "subtopic"
    },
    {
      "from": "EM Algorithm",
      "to": "Latent Variable Models",
      "relationship": "subtopic"
    },
    {
      "from": "Cocktail Party Problem",
      "to": "Mixing Matrix (A)",
      "relationship": "uses"
    },
    {
      "from": "Convolutional_Neural_Networks",
      "to": "1D_Convolution",
      "relationship": "has_subtopic"
    },
    {
      "from": "HoldoutCrossValidation",
      "to": "RetrainingModel",
      "relationship": "subtopic_of"
    },
    {
      "from": "Bias-variance tradeoff",
      "to": "A mathematical decomposition (for regression)",
      "relationship": "subtopic"
    },
    {
      "from": "Independent components analysis",
      "to": "ICA ambiguities",
      "relationship": "subtopic"
    },
    {
      "from": "Generalization",
      "to": "Training Loss Function",
      "relationship": "subtopic"
    },
    {
      "from": "Value_Functions",
      "to": "Quadratic_Value_Functions",
      "relationship": "special_case_of"
    },
    {
      "from": "MachineLearningOverview",
      "to": "FeatureMapsAndKernels",
      "relationship": "contains"
    },
    {
      "from": "RegressionProblem",
      "to": "ProbabilisticInterpretation",
      "relationship": "subtopic"
    },
    {
      "from": "FeatureMapsAndKernels",
      "to": "EfficientComputationOfKernels",
      "relationship": "contains"
    },
    {
      "from": "FeatureMapping",
      "to": "KernelsAsSimilarityMetrics",
      "relationship": "related_to"
    },
    {
      "from": "Machine_Learning",
      "to": "Linear_Regression",
      "relationship": "related_to"
    },
    {
      "from": "Stochastic_Optimization_Methods",
      "to": "Machine_Learning_Papers",
      "relationship": "related_to"
    },
    {
      "from": "ICAAlgorithm",
      "to": "NonGaussianDataRecovery",
      "relationship": "subtopic"
    },
    {
      "from": "Optimal_Policy_Derivation",
      "to": "Linear_Optimal_Policy",
      "relationship": "result"
    },
    {
      "from": "Machine_Learning_Pretraining_Adaptation",
      "to": "Adaptation_Phase",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Deep Learning Packages",
      "relationship": "related_to"
    },
    {
      "from": "Linear Regression",
      "to": "Feature Selection",
      "relationship": "depends_on"
    },
    {
      "from": "SingleExampleLikelihood",
      "to": "EMAlgorithm",
      "relationship": "subtopic"
    },
    {
      "from": "ELBO_Optimization",
      "to": "Gradient_Ascend_ELBO",
      "relationship": "subtopic"
    },
    {
      "from": "Policy in MDPs",
      "to": "Time-Dependent Policy",
      "relationship": "subtopic_of"
    },
    {
      "from": "Policy_Iteration",
      "to": "Policy_Improvement",
      "relationship": "subtopic"
    },
    {
      "from": "M_Step",
      "to": "Gaussian_Distribution",
      "relationship": "uses"
    },
    {
      "from": "PolynomialFitting",
      "to": "Overfitting",
      "relationship": "depends_on"
    },
    {
      "from": "Optimal_Policy_Linear_Systems",
      "to": "Discrete_Ricatti_Equations",
      "relationship": "depends_on"
    },
    {
      "from": "ExponentialFamilyDistributions",
      "to": "SufficientStatistic",
      "relationship": "has"
    },
    {
      "from": "ConvolutionalLayers",
      "to": "ParameterSharing",
      "relationship": "has_subtopic"
    },
    {
      "from": "EM_Algorithm",
      "to": "K_Means_Clustering",
      "relationship": "related_to"
    },
    {
      "from": "VarianceScaling",
      "to": "FeatureComparison",
      "relationship": "related_to"
    },
    {
      "from": "BatchGradientDescent",
      "to": "InnerProductEfficiency",
      "relationship": "contains"
    },
    {
      "from": "BayesianInference",
      "to": "MAPEstimation",
      "relationship": "subtopic"
    },
    {
      "from": "UnsupervisedLearning",
      "to": "Clustering",
      "relationship": "contains"
    },
    {
      "from": "Text_Classification",
      "to": "Spam_Filtering",
      "relationship": "example_of"
    },
    {
      "from": "Hyperparameters",
      "to": "Number of Iterations (n_iter)",
      "relationship": "has_subtopic"
    },
    {
      "from": "ELBO",
      "to": "MarginalDistribution",
      "relationship": "related_to"
    },
    {
      "from": "GDA",
      "to": "RobustnessToAssumptions",
      "relationship": "related_to"
    },
    {
      "from": "MachineLearningConcepts",
      "to": "FunctionalMargin",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning",
      "to": "Empirical_Risk_Minimization",
      "relationship": "depends_on"
    },
    {
      "from": "Optimization Problem",
      "to": "Scaling Constraint",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Double Descent Phenomenon",
      "relationship": "subtopic"
    },
    {
      "from": "LogisticLossFunction",
      "to": "Logit",
      "relationship": "defines"
    },
    {
      "from": "Parameter_Estimation",
      "to": "Likelihood_Function",
      "relationship": "depends_on"
    },
    {
      "from": "Reinforcement_Learning",
      "to": "Dynamic_Programming",
      "relationship": "subtopic"
    },
    {
      "from": "Algorithm 6",
      "to": "Update Rule (15.12)",
      "relationship": "depends_on"
    },
    {
      "from": "LinearRegressionOptimization",
      "to": "BatchGradientDescentExample",
      "relationship": "has_subtopic"
    },
    {
      "from": "BiasInModels",
      "to": "MachineLearningIssues",
      "relationship": "subtopic"
    },
    {
      "from": "Discrete Latent Variables",
      "to": "Mean Field Assumption",
      "relationship": "related_to"
    },
    {
      "from": "Machine_Learning_Theory",
      "to": "Empirical_Risk_Minimization",
      "relationship": "subtopic"
    },
    {
      "from": "Constraints_Satisfaction",
      "to": "Convergence_Tolerance",
      "relationship": "has_subtopic"
    },
    {
      "from": "Regularization",
      "to": "Regularized_Loss",
      "relationship": "subtopic_of"
    },
    {
      "from": "Policy_Gradient_Methods",
      "to": "Reward_Function",
      "relationship": "depends_on"
    },
    {
      "from": "MachineLearningAlgorithms",
      "to": "GenerativeAlgorithms",
      "relationship": "has_subtopic"
    },
    {
      "from": "Bayes Rule Application",
      "to": "Class Priors",
      "relationship": "depends_on"
    },
    {
      "from": "Algorithm 6",
      "to": "VE Procedure",
      "relationship": "depends_on"
    },
    {
      "from": "MachineLearningModels",
      "to": "ModelAssumptions",
      "relationship": "contains"
    },
    {
      "from": "Primal-Dual Relationship",
      "to": "Dual Problem",
      "relationship": "related_to"
    },
    {
      "from": "DataNormalization",
      "to": "VarianceScaling",
      "relationship": "depends_on"
    },
    {
      "from": "Generalization_Error",
      "to": "Training_Error_vs_Generalization_Error",
      "relationship": "depends_on"
    },
    {
      "from": "Kalman Filter",
      "to": "Update Step",
      "relationship": "subtopic"
    },
    {
      "from": "Spam_Filtering",
      "to": "Feature_Vector",
      "relationship": "uses"
    },
    {
      "from": "Regularization_Techniques",
      "to": "Machine_Learning_Papers",
      "relationship": "depends_on"
    },
    {
      "from": "Activation Function",
      "to": "ReLU",
      "relationship": "subtopic"
    },
    {
      "from": "Model-Based_Deep_Reinforcement_Learning",
      "to": "Machine_Learning_Papers",
      "relationship": "depends_on"
    },
    {
      "from": "EM algorithms",
      "to": "Mixture of Gaussians revisited",
      "relationship": "subtopic"
    },
    {
      "from": "Double_Descent_Models",
      "to": "Machine_Learning_Papers",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Loss Functions",
      "to": "Logistic Loss Function",
      "relationship": "has_subtopic"
    },
    {
      "from": "Sample_Complexity_Bound",
      "to": "Machine_Learning_Bias_Variance_Tradeoff",
      "relationship": "subtopic"
    },
    {
      "from": "E_Step",
      "to": "Bayes_Rule",
      "relationship": "depends_on"
    },
    {
      "from": "Policy Evaluation",
      "to": "Bellman's Equation",
      "relationship": "depends_on"
    },
    {
      "from": "MachineLearningOverview",
      "to": "BinaryClassification",
      "relationship": "subtopic"
    },
    {
      "from": "Representation Function",
      "to": "Positive Pair",
      "relationship": "related_to"
    },
    {
      "from": "Deep Learning Regularizers",
      "to": "Data Augmentation",
      "relationship": "contains"
    },
    {
      "from": "Algorithm_Independence_of_Phi",
      "to": "Characterization_of_Valid_Kernels",
      "relationship": "subtopic"
    },
    {
      "from": "Finetuning_Pretrained_Models",
      "to": "Prediction_Model_Structure",
      "relationship": "has_subtopic"
    },
    {
      "from": "Loss Function",
      "to": "Cross-Entropy Loss",
      "relationship": "alternative_definition"
    },
    {
      "from": "MachineLearningOverview",
      "to": "ClassificationProblem",
      "relationship": "subtopic"
    },
    {
      "from": "Policy_Gradient_Theory",
      "to": "Log_Probability_Gradients",
      "relationship": "has_subtopic"
    },
    {
      "from": "MachineLearningOverview",
      "to": "OverparameterizationRegime",
      "relationship": "related_to"
    },
    {
      "from": "Backpropagation",
      "to": "Gradient Computation",
      "relationship": "related_to"
    },
    {
      "from": "FunctionalMargin",
      "to": "ConfidenceAndCorrectness",
      "relationship": "depends_on"
    },
    {
      "from": "OptimizationMethods",
      "to": "HessianMatrix",
      "relationship": "depends_on"
    },
    {
      "from": "ICAIndependenceAssumption",
      "to": "DensityFunctionTransformation",
      "relationship": "subtopic"
    },
    {
      "from": "Optimal_Control",
      "to": "Optimal_Policy_Derivation",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningBasics",
      "to": "LinearClassification",
      "relationship": "contains"
    },
    {
      "from": "Cross_Entropy_Loss_Backward",
      "to": "Vectorized_Notation_Backward",
      "relationship": "subtopic"
    },
    {
      "from": "GeneralizedLinearModels",
      "to": "ExponentialFamilyDistributions",
      "relationship": "subtopic"
    },
    {
      "from": "MaximumLikelihoodEstimation",
      "to": "ClassificationModelAssumptions",
      "relationship": "subtopic"
    },
    {
      "from": "Pretrained large language models",
      "to": "Open up the blackbox of Transformers",
      "relationship": "subtopic"
    },
    {
      "from": "Linear Quadratic Regulation (LQR)",
      "to": "Linear Transitions",
      "relationship": "includes"
    },
    {
      "from": "Softmax Function",
      "to": "Probability Vector",
      "relationship": "transforms_to"
    },
    {
      "from": "LogisticRegression",
      "to": "LMSUpdateRule",
      "relationship": "related_to"
    },
    {
      "from": "PrimalProblem",
      "to": "ThetaP",
      "relationship": "subtopic"
    },
    {
      "from": "Generalization_Error",
      "to": "Uniform_Convergence_Bound",
      "relationship": "depends_on"
    },
    {
      "from": "BernoulliDistribution",
      "to": "LogPartitionFunctionOfBernoulli",
      "relationship": "subtopic"
    },
    {
      "from": "BatchGradientDescentExample",
      "to": "StochasticGradientDescent",
      "relationship": "contrasts_with"
    },
    {
      "from": "Hyperparameters",
      "to": "Learning Rate (\u03b1)",
      "relationship": "has_subtopic"
    },
    {
      "from": "SoftplusFunction",
      "to": "ActivationFunctions",
      "relationship": "subtopic"
    },
    {
      "from": "Non-Separable Case",
      "to": "Optimization Problem",
      "relationship": "depends_on"
    },
    {
      "from": "Machine_Learning_Algorithms",
      "to": "Empirical_Risk_Minimization",
      "relationship": "depends_on"
    },
    {
      "from": "DensityEstimation",
      "to": "GaussianMixtureModels",
      "relationship": "depends_on"
    },
    {
      "from": "Implicit Regularization Effect",
      "to": "Optimizer Bias",
      "relationship": "has_subtopic"
    },
    {
      "from": "Hoeffding_Inequality",
      "to": "Chernoff_Bound",
      "relationship": "related_to"
    },
    {
      "from": "MachineLearningModels",
      "to": "RelationshipToLogisticRegression",
      "relationship": "subtopic"
    },
    {
      "from": "Text_Classification",
      "to": "Feature_Vector_Selection",
      "relationship": "depends_on"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Dual_Optimization_Problem",
      "relationship": "leads_to"
    },
    {
      "from": "RegressionProblems",
      "to": "ModelTraining",
      "relationship": "depends_on"
    },
    {
      "from": "BinaryClassification",
      "to": "HypothesisFunction",
      "relationship": "has_component"
    },
    {
      "from": "DistanceToBoundary",
      "to": "DecisionBoundary",
      "relationship": "subtopic"
    },
    {
      "from": "Continuous_State_MDPs",
      "to": "Discretization_Method",
      "relationship": "subtopic"
    },
    {
      "from": "Discretization_Methods",
      "to": "Curse_of_Dimensionality",
      "relationship": "depends_on"
    },
    {
      "from": "Gaussian_Data_Issue",
      "to": "ICA_Ambiguities",
      "relationship": "subtopic"
    },
    {
      "from": "FunctionalMargin",
      "to": "NormalizationCondition",
      "relationship": "related_to"
    },
    {
      "from": "Chapter16",
      "to": "GeneralSettingEquations",
      "relationship": "contains"
    },
    {
      "from": "Bias Backward Function",
      "to": "Equation 7.66",
      "relationship": "derives_from"
    },
    {
      "from": "ProjectionDirection",
      "to": "EmpiricalCovarianceMatrix",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Models",
      "to": "Logistic Regression",
      "relationship": "subtopic"
    },
    {
      "from": "Self-Supervised Learning",
      "to": "Positive Pair",
      "relationship": "subtopic"
    },
    {
      "from": "EfficiencyConcerns",
      "to": "VectorizationInNN",
      "relationship": "depends_on"
    },
    {
      "from": "Policy_Gradient_Theorem",
      "to": "Simplification_of_Formalism",
      "relationship": "subtopic_of"
    },
    {
      "from": "Optimization Problem",
      "to": "Non-Convex Constraint",
      "relationship": "depends_on"
    },
    {
      "from": "Regularization in Machine Learning",
      "to": "L1 Norm (LASSO)",
      "relationship": "contains"
    },
    {
      "from": "SupervisedLearning",
      "to": "LinearRegression",
      "relationship": "depends_on"
    },
    {
      "from": "LagrangianFormulation",
      "to": "DerivativeWithRespectToW",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Gradient Computation",
      "relationship": "subtopic"
    },
    {
      "from": "Pretraining_Methods",
      "to": "Supervised_Pretraining",
      "relationship": "has_subtopic"
    },
    {
      "from": "Policy_Iteration",
      "to": "Policy_Evaluation",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningConcepts",
      "to": "Backpropagation",
      "relationship": "related_to"
    },
    {
      "from": "LogisticRegression",
      "to": "RobustnessToAssumptions",
      "relationship": "related_to"
    },
    {
      "from": "Hypothesis_Class_Size",
      "to": "Floating_Point_Precision",
      "relationship": "depends_on"
    },
    {
      "from": "MachineLearningOverview",
      "to": "LinearRegression",
      "relationship": "contains"
    },
    {
      "from": "E-step",
      "to": "Gaussian Mixture Model (GMM)",
      "relationship": "related_to"
    },
    {
      "from": "Value_Function_Approximation",
      "to": "Expectation_Calculation",
      "relationship": "related_to"
    },
    {
      "from": "MachineLearningConcepts",
      "to": "EMAlgorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Bayes Rule Application",
      "to": "Conditional Probability p(x|y)",
      "relationship": "depends_on"
    },
    {
      "from": "MachineLearningModels",
      "to": "ConstructingGLMs",
      "relationship": "next_topic"
    },
    {
      "from": "MachineLearningOverview",
      "to": "LikelihoodFunction",
      "relationship": "depends_on"
    },
    {
      "from": "Discretization in MDPs",
      "to": "Value Iteration",
      "relationship": "subtopic"
    },
    {
      "from": "ELBO",
      "to": "Variational Inference",
      "relationship": "depends_on"
    },
    {
      "from": "Corollary",
      "to": "VC Dimension",
      "relationship": "related_to"
    },
    {
      "from": "SMO_Algorithm",
      "to": "Efficient_Update_Mechanism",
      "relationship": "related_to"
    },
    {
      "from": "1D_Convolution",
      "to": "Filter_Vector",
      "relationship": "has_component"
    },
    {
      "from": "ComputationalComplexity",
      "to": "FeatureMapping",
      "relationship": "depends_on"
    },
    {
      "from": "MachineLearningOverview",
      "to": "LinearRegression",
      "relationship": "depends_on"
    },
    {
      "from": "Sample complexity bounds (optional readings)",
      "to": "Preliminaries",
      "relationship": "subtopic"
    },
    {
      "from": "LocallyWeightedLinearRegression",
      "to": "WeightsCalculation",
      "relationship": "subtopic"
    },
    {
      "from": "Squared_Loss_Backward",
      "to": "Vectorized_Notation_Backward",
      "relationship": "subtopic"
    },
    {
      "from": "Support Vector Machines (SVM)",
      "to": "Non-Separable Case",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Naive_Bayes_Classifier",
      "relationship": "subtopic"
    },
    {
      "from": "LMS_Rule",
      "to": "Error_Term",
      "relationship": "depends_on"
    },
    {
      "from": "Machine_Learning_Topics",
      "to": "Value_Function_Approximation",
      "relationship": "related_to"
    },
    {
      "from": "Regularization",
      "to": "L1 Regularization",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning",
      "to": "Test_Dataset",
      "relationship": "depends_on"
    },
    {
      "from": "ConditionalProbability",
      "to": "ParameterEstimation",
      "relationship": "depends_on"
    },
    {
      "from": "Regularization",
      "to": "ModelComplexity",
      "relationship": "depends_on"
    },
    {
      "from": "Gradient_Ascend",
      "to": "Expectation_Value",
      "relationship": "related_to"
    },
    {
      "from": "CovarianceMatrixEffects",
      "to": "MachineLearningConcepts",
      "relationship": "subtopic"
    },
    {
      "from": "Multivariate Normal Distribution",
      "to": "Mean Vector",
      "relationship": "related_to"
    },
    {
      "from": "Model Creation Methods",
      "to": "Learning from Data",
      "relationship": "has_subtopic"
    },
    {
      "from": "Necessary Conditions for Valid Kernels",
      "to": "Symmetry Property",
      "relationship": "subtopic"
    },
    {
      "from": "Gradient_Estimation",
      "to": "ELBO_Optimization",
      "relationship": "subtopic"
    },
    {
      "from": "KernelsAsSimilarityMetrics",
      "to": "GaussianKernel",
      "relationship": "subtopic"
    },
    {
      "from": "MLP Backpropagation",
      "to": "Forward Pass",
      "relationship": "related_to"
    },
    {
      "from": "Backward Function Overview",
      "to": "Bias Backward Function",
      "relationship": "has_subtopic"
    },
    {
      "from": "MSEDecomposition",
      "to": "BiasTerm",
      "relationship": "subtopic_of"
    },
    {
      "from": "Data Augmentation",
      "to": "Negative Pair",
      "relationship": "creates"
    },
    {
      "from": "Efficiency_of_Backward_Pass",
      "to": "Machine_Learning_Backward_Functions",
      "relationship": "subtopic"
    },
    {
      "from": "Testing_Valid_Kernel",
      "to": "Mercer_Theorem",
      "relationship": "subtopic"
    },
    {
      "from": "Finding_w_and_b",
      "to": "Dual_Optimization_Problem",
      "relationship": "follows_from"
    },
    {
      "from": "Probabilistic Model",
      "to": "Negative Log-Likelihood",
      "relationship": "subtopic"
    },
    {
      "from": "Activation Function Backward Function",
      "to": "Element-wise Operation",
      "relationship": "follows_from"
    },
    {
      "from": "LearnedFeaturesRepresentations",
      "to": "DeepLearning",
      "relationship": "subtopic"
    },
    {
      "from": "Optimal_Margin_Classifiers",
      "to": "Dual_Form",
      "relationship": "contains"
    },
    {
      "from": "Bias-Variance Tradeoff",
      "to": "Model Complexity",
      "relationship": "depends_on"
    },
    {
      "from": "Naive Bayes Algorithm",
      "to": "Binary Features",
      "relationship": "subtopic_of"
    },
    {
      "from": "Unsupervised learning",
      "to": "EM algorithms",
      "relationship": "subtopic"
    },
    {
      "from": "BinaryClassification",
      "to": "GeneralizationError",
      "relationship": "defines"
    },
    {
      "from": "kFoldCrossValidation",
      "to": "ModelEvaluation",
      "relationship": "has_subtopic"
    },
    {
      "from": "Principal Component Analysis (PCA)",
      "to": "Noise Reduction",
      "relationship": "subtopic_of"
    },
    {
      "from": "PartialDerivatives",
      "to": "ScalarFunctionDerivatives",
      "relationship": "related_to"
    },
    {
      "from": "InfiniteHorizonMDP",
      "to": "FiniteHorizonMDP",
      "relationship": "contrasts_with"
    },
    {
      "from": "Kalman Filter",
      "to": "Backward Pass",
      "relationship": "related_to"
    },
    {
      "from": "Cross Validation",
      "to": "Finite Model Set",
      "relationship": "subtopic"
    },
    {
      "from": "Value_Function_Approximation",
      "to": "Model_or_Simulator",
      "relationship": "subtopic"
    },
    {
      "from": "Primal-Dual Relationship",
      "to": "Primal Problem",
      "relationship": "related_to"
    },
    {
      "from": "Supervised Learning Problem",
      "to": "Classification",
      "relationship": "has_subtopic"
    },
    {
      "from": "LQRModelAssumptions",
      "to": "Step2OptimalPolicy",
      "relationship": "subtopic"
    },
    {
      "from": "Fitted Value Iteration",
      "to": "Discrete Action Space",
      "relationship": "related_to"
    },
    {
      "from": "Neural Networks",
      "to": "Weight Vector",
      "relationship": "has_component"
    },
    {
      "from": "Machine_Learning_Theory",
      "to": "Hoeffding_Inequality",
      "relationship": "depends_on"
    },
    {
      "from": "RegressionProblems",
      "to": "LeastSquareCostFunction",
      "relationship": "subtopic"
    },
    {
      "from": "Regularization in Machine Learning",
      "to": "L2 Norm Regularization",
      "relationship": "contains"
    },
    {
      "from": "Negative Log-Likelihood",
      "to": "Conditional Probabilistic Model",
      "relationship": "related_to"
    },
    {
      "from": "Expectation-Maximization Algorithm",
      "to": "M-step",
      "relationship": "has_subtopic"
    },
    {
      "from": "LogLikelihood",
      "to": "MaximizingLogLikelihood",
      "relationship": "subtopic"
    },
    {
      "from": "Objective Function Primal",
      "to": "Primal Problem",
      "relationship": "depends_on"
    },
    {
      "from": "E_Step",
      "to": "ELBO",
      "relationship": "related_to"
    },
    {
      "from": "Model-wise Double Descent",
      "to": "Double Descent Phenomenon",
      "relationship": "subtopic"
    },
    {
      "from": "NaturalParameterOfBernoulli",
      "to": "ExponentialFamilyDistributions",
      "relationship": "related_to"
    },
    {
      "from": "ActivationFunctions",
      "to": "ReLUFunction",
      "relationship": "subtopic"
    },
    {
      "from": "PCA_Methods",
      "to": "Data_Subspace_Identification",
      "relationship": "subtopic"
    },
    {
      "from": "M-step",
      "to": "Update Rule for \\(\\mu_j\\)",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningArchitectures",
      "to": "ResNetArchitecture",
      "relationship": "contains"
    },
    {
      "from": "Training Set",
      "to": "Posterior Distribution on Parameters",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Support Vector Machines (SVMs)",
      "relationship": "contains"
    },
    {
      "from": "GaussianDiscriminantAnalysis",
      "to": "MachineLearningConcepts",
      "relationship": "subtopic"
    },
    {
      "from": "TwoLayerNetwork",
      "to": "VectorizationInNN",
      "relationship": "subtopic"
    },
    {
      "from": "Support Vector Machines (SVM)",
      "to": "Optimization Problem",
      "relationship": "has_subtopic"
    },
    {
      "from": "MachineLearningOverview",
      "to": "BinaryClassificationProblem",
      "relationship": "contains"
    },
    {
      "from": "Coordinate_Ascend_Algorithm",
      "to": "Unconstrained_Optimization_Problem",
      "relationship": "has_subtopic"
    },
    {
      "from": "Policy_Iteration",
      "to": "Value_Evaluation_Procedure",
      "relationship": "depends_on"
    },
    {
      "from": "Kalman Filter",
      "to": "Predict Step",
      "relationship": "subtopic"
    },
    {
      "from": "VectorW",
      "to": "DecisionBoundary",
      "relationship": "related_to"
    },
    {
      "from": "Training_Set",
      "to": "Stop_Words",
      "relationship": "excludes"
    },
    {
      "from": "TrainingLoss",
      "to": "LambdaParameter",
      "relationship": "depends_on"
    },
    {
      "from": "inner_loop_steps",
      "to": "k-means_algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Gradient_Estimation",
      "relationship": "depends_on"
    },
    {
      "from": "LogisticRegression",
      "to": "LogisticLossFunction",
      "relationship": "contains"
    },
    {
      "from": "Volume Mapping",
      "to": "Density Transformation",
      "relationship": "subtopic"
    },
    {
      "from": "KernelTrick",
      "to": "InnerProduct",
      "relationship": "subtopic"
    },
    {
      "from": "Deep Learning Regularizers",
      "to": "Spectral Norm Regularization",
      "relationship": "contains"
    },
    {
      "from": "Regularization in Deep Learning",
      "to": "Explicit Regularization Techniques",
      "relationship": "has_subtopic"
    },
    {
      "from": "Bias-Variance Tradeoff",
      "to": "Underfitting",
      "relationship": "related_to"
    },
    {
      "from": "NeuralNetworks",
      "to": "WeightMatricesBiases",
      "relationship": "depends_on"
    },
    {
      "from": "StateTransitionModel",
      "to": "LinearModel",
      "relationship": "has_subtopic"
    },
    {
      "from": "Model Creation Methods",
      "to": "Physics Simulation",
      "relationship": "has_subtopic"
    },
    {
      "from": "Mixing_Matrix_Rotation",
      "to": "Gaussian_Data_Issue",
      "relationship": "depends_on"
    },
    {
      "from": "GradientDescentOptimizer",
      "to": "MinimumNormSolution",
      "relationship": "subtopic"
    },
    {
      "from": "Gradient_Descent",
      "to": "Learning_Rate",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Loss Functions",
      "to": "Cross-Entropy Loss Function",
      "relationship": "has_subtopic"
    },
    {
      "from": "Efficiency_of_Modules",
      "to": "Machine_Learning_Backpropagation",
      "relationship": "subtopic"
    },
    {
      "from": "Contrastive_Learning",
      "to": "Negative_Pairs",
      "relationship": "has_concept"
    },
    {
      "from": "Support_Vector_Machines_SMO",
      "to": "SMO_Algorithm",
      "relationship": "has_subtopic"
    },
    {
      "from": "Gradient Computation",
      "to": "Loss Function J",
      "relationship": "depends_on"
    },
    {
      "from": "Classification Problem",
      "to": "Logistic Regression",
      "relationship": "contains"
    },
    {
      "from": "Machine Learning Tasks",
      "to": "Adaptation Algorithms",
      "relationship": "depends_on"
    },
    {
      "from": "Cross Validation",
      "to": "Polynomial Regression Models",
      "relationship": "subtopic"
    },
    {
      "from": "Transformer_Model",
      "to": "Autoregressive_Decoding",
      "relationship": "related_to"
    },
    {
      "from": "General EM algorithms",
      "to": "Other interpretation of ELBO",
      "relationship": "subtopic"
    },
    {
      "from": "ShatteringConcept",
      "to": "VCDimension",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Double Descent Phenomenon",
      "relationship": "contains"
    },
    {
      "from": "SingleNeuronNN",
      "to": "HousingPricePrediction",
      "relationship": "example_of"
    },
    {
      "from": "Reinforcement learning",
      "to": "Markov decision processes",
      "relationship": "subtopic"
    },
    {
      "from": "Principal Component Analysis (PCA)",
      "to": "Data Visualization",
      "relationship": "subtopic_of"
    },
    {
      "from": "Machine Learning Algorithms",
      "to": "Laplace Smoothing",
      "relationship": "contains"
    },
    {
      "from": "EventModelsForTextClassification",
      "to": "MultinomialEventModel",
      "relationship": "has_subtopic"
    },
    {
      "from": "MLPArchitecture",
      "to": "MatrixMultiplicationModule",
      "relationship": "depends_on"
    },
    {
      "from": "Generalization",
      "to": "Sample complexity bounds (optional readings)",
      "relationship": "subtopic"
    },
    {
      "from": "2 Classification and logistic regression",
      "to": "2.3 Multi-class classification",
      "relationship": "has_subtopic"
    },
    {
      "from": "Pretraining_Phase",
      "to": "Optimizers",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "LQR_Algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Regularization and model selection",
      "to": "Regularization",
      "relationship": "subtopic"
    },
    {
      "from": "FeatureMaps",
      "to": "DeepLearning",
      "relationship": "subtopic"
    },
    {
      "from": "EM algorithms",
      "to": "EM for mixture of Gaussians",
      "relationship": "subtopic"
    },
    {
      "from": "MarkovDecisionProcesses",
      "to": "States",
      "relationship": "component_of"
    },
    {
      "from": "FeatureMapsAndKernels",
      "to": "KernelFunctionDefinition",
      "relationship": "contains"
    },
    {
      "from": "Principal Component Analysis (PCA)",
      "to": "Overfitting Prevention",
      "relationship": "subtopic_of"
    },
    {
      "from": "Sample Complexity Bounds",
      "to": "Generalization Error",
      "relationship": "contains"
    },
    {
      "from": "SIMCLR",
      "to": "Loss_Function",
      "relationship": "uses"
    },
    {
      "from": "FiniteHorizonMDPs",
      "to": "ValueIterationPolicyIteration",
      "relationship": "contains"
    },
    {
      "from": "LogLikelihood",
      "to": "StochasticGradientAscent",
      "relationship": "subtopic"
    },
    {
      "from": "Prediction on New Data",
      "to": "Fully Bayesian Prediction",
      "relationship": "subtopic"
    },
    {
      "from": "TanhFunction",
      "to": "ActivationFunctions",
      "relationship": "subtopic"
    },
    {
      "from": "Value Function",
      "to": "Bellman's Equation",
      "relationship": "has_subtopic"
    },
    {
      "from": "GroupNormalization",
      "to": "OtherNormalizationLayers",
      "relationship": "subtopic"
    },
    {
      "from": "BinaryClassificationProblem",
      "to": "LossFunction",
      "relationship": "depends_on"
    },
    {
      "from": "RegressionProblems",
      "to": "TrainingDataset",
      "relationship": "depends_on"
    },
    {
      "from": "LogisticLossFunction",
      "to": "NegativeLogLikelihood",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Theory",
      "to": "Binary_Classification",
      "relationship": "subtopic"
    },
    {
      "from": "BatchGradientDescent",
      "to": "BetaUpdateEquation",
      "relationship": "depends_on"
    },
    {
      "from": "FeatureMapsAndKernels",
      "to": "PredictionUsingKernelFunction",
      "relationship": "contains"
    },
    {
      "from": "BayesianInference",
      "to": "PriorDistributions",
      "relationship": "subtopic"
    },
    {
      "from": "Mercer_Theorem",
      "to": "Valid_Kernels_Conditions",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Models",
      "to": "Transformer_Model",
      "relationship": "has_subtopic"
    },
    {
      "from": "Chain Rule",
      "to": "Basic Chain Rule Perspective",
      "relationship": "subtopic"
    },
    {
      "from": "Backpropagation Strategy",
      "to": "Backward Function Computation",
      "relationship": "subtopic"
    },
    {
      "from": "Normal_Distribution",
      "to": "Mean",
      "relationship": "depends_on"
    },
    {
      "from": "MachineLearningBasics",
      "to": "LogisticRegression",
      "relationship": "contains"
    },
    {
      "from": "Machine Learning Models",
      "to": "Locally Weighted Linear Regression",
      "relationship": "related_to"
    },
    {
      "from": "LogisticRegression",
      "to": "LogitFunction",
      "relationship": "has_subtopic"
    },
    {
      "from": "Matrix Multiplication Backward Function",
      "to": "Equation 7.64",
      "relationship": "derives_from"
    },
    {
      "from": "MarkovDecisionProcesses",
      "to": "DiscountFactor",
      "relationship": "component_of"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Overfitting and Underfitting",
      "relationship": "related_to"
    },
    {
      "from": "Vectorization",
      "to": "Broadcasting",
      "relationship": "related_to"
    },
    {
      "from": "Training_Error_Generalization_Error",
      "to": "Uniform_Convergence",
      "relationship": "subtopic"
    },
    {
      "from": "Logistic Regression",
      "to": "Newton's Method",
      "relationship": "subtopic"
    },
    {
      "from": "Overfitting",
      "to": "Variance",
      "relationship": "causes"
    },
    {
      "from": "4 Generative learning algorithms",
      "to": "4.2 Naive bayes (Option Reading)",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "LQR Extension",
      "relationship": "contains"
    },
    {
      "from": "Step1Estimation",
      "to": "GaussianDiscriminantAnalysis",
      "relationship": "depends_on"
    },
    {
      "from": "Regularization and model selection",
      "to": "Implicit regularization effect (optional reading)",
      "relationship": "subtopic"
    },
    {
      "from": "Hessian Matrix",
      "to": "Logistic Regression",
      "relationship": "related_to"
    },
    {
      "from": "ConvolutionalLayers",
      "to": "EfficiencyOfConvolution",
      "relationship": "has_subtopic"
    },
    {
      "from": "Model_Wise_Double_Descent",
      "to": "Sample_Wise_Double_Descent",
      "relationship": "related_to"
    },
    {
      "from": "Machine_Learning_Algorithms",
      "to": "Policy_Iteration",
      "relationship": "has_subtopic"
    },
    {
      "from": "Modern Neural Networks",
      "to": "Backpropagation",
      "relationship": "subtopic"
    },
    {
      "from": "LogisticRegression",
      "to": "NegativeLikelihoodLoss",
      "relationship": "has_subtopic"
    },
    {
      "from": "OptimizationChallenges",
      "to": "EMAlgorithm",
      "relationship": "related_to"
    },
    {
      "from": "FifthDegreePolynomial",
      "to": "MachineLearningIssues",
      "relationship": "subtopic"
    },
    {
      "from": "Conditional Distribution Modeling",
      "to": "Hypothesis Function",
      "relationship": "related_to"
    },
    {
      "from": "Support_Vector_Machines_SVMs",
      "to": "Sequential_Minimal_Optimization_SMO",
      "relationship": "depends_on"
    },
    {
      "from": "ReLUFunction",
      "to": "ActivationFunctions",
      "relationship": "subtopic"
    },
    {
      "from": "LQR, DDP and LQG",
      "to": "Finite-horizon MDPs",
      "relationship": "subtopic"
    },
    {
      "from": "ProbabilityDistributions",
      "to": "JensensInequality",
      "relationship": "depends_on"
    },
    {
      "from": "Partially Observable MDPs (POMDP)",
      "to": "Observation Layer",
      "relationship": "has_subtopic"
    },
    {
      "from": "Objective Function Dual",
      "to": "Dual Problem",
      "relationship": "depends_on"
    },
    {
      "from": "ConvolutionalLayers",
      "to": "MachineLearningConcepts",
      "relationship": "subtopic"
    },
    {
      "from": "BinaryClassification",
      "to": "LogisticFunction",
      "relationship": "subtopic"
    },
    {
      "from": "ConditionalProbability",
      "to": "NaiveBayesAlgorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Models",
      "to": "Parameter_Estimation",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine_Learning_Theory",
      "to": "Optimal_Hypothesis_Selection",
      "relationship": "subtopic"
    },
    {
      "from": "InvertibleMatrixAssumption",
      "to": "LinearRegression",
      "relationship": "depends_on"
    },
    {
      "from": "CostFunctionJ",
      "to": "NormalEquations",
      "relationship": "leads_to"
    },
    {
      "from": "PrincipalComponentAnalysis",
      "to": "kDimensionalSubspace",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Overparameterized_Models",
      "relationship": "related_to"
    },
    {
      "from": "Naive Bayes Algorithm",
      "to": "Feature Discretization",
      "relationship": "subtopic_of"
    },
    {
      "from": "ICA Ambiguities",
      "to": "Permutation Matrix",
      "relationship": "depends_on"
    },
    {
      "from": "MachineLearningOverview",
      "to": "EMAlgorithm",
      "relationship": "contains"
    },
    {
      "from": "Classification Confidence",
      "to": "Geometric Margins",
      "relationship": "subtopic"
    },
    {
      "from": "KernelFunctions",
      "to": "FeatureMapping",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Stochastic Gradient Ascent",
      "relationship": "has_subtopic"
    },
    {
      "from": "Principal Component Analysis (PCA)",
      "to": "Computational Efficiency",
      "relationship": "subtopic_of"
    },
    {
      "from": "Foundation Models",
      "to": "Machine Learning Overview",
      "relationship": "subtopic"
    },
    {
      "from": "GaussianDiscriminantAnalysis",
      "to": "MLEstimation",
      "relationship": "subtopic"
    },
    {
      "from": "NeuralNetworksComposition",
      "to": "BackpropagationDiscussion",
      "relationship": "has_subtopic"
    },
    {
      "from": "Neural Networks",
      "to": "Activation Function",
      "relationship": "depends_on"
    },
    {
      "from": "Machine_Learning_Optimization",
      "to": "Coordinate_Ascend_Method",
      "relationship": "contains"
    },
    {
      "from": "MarkovDecisionProcesses",
      "to": "RewardFunction",
      "relationship": "component_of"
    },
    {
      "from": "Machine_Learning_Algorithms",
      "to": "Gradient_Descent",
      "relationship": "has_subtopic"
    },
    {
      "from": "MLP Backpropagation",
      "to": "Backward Pass",
      "relationship": "depends_on"
    },
    {
      "from": "Machine_Learning_Algorithms",
      "to": "Support_Vector_Machines",
      "relationship": "related_to"
    },
    {
      "from": "MachineLearningConcepts",
      "to": "ChainRule",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Techniques",
      "to": "Feature_Vectors",
      "relationship": "has_subtopic"
    },
    {
      "from": "Matrix Multiplication Backward Function",
      "to": "Vectorized Notation",
      "relationship": "follows_from"
    },
    {
      "from": "Variational_Bayes",
      "to": "Machine_Learning_Papers",
      "relationship": "subtopic"
    },
    {
      "from": "PolynomialFeatures",
      "to": "Underfitting",
      "relationship": "related_to"
    },
    {
      "from": "Lagrange Duality",
      "to": "Dual Formulation",
      "relationship": "leads_to"
    },
    {
      "from": "DataNormalization",
      "to": "NormalizedDatasetExample",
      "relationship": "leads_to"
    },
    {
      "from": "Reinforcement_Learning",
      "to": "Value_Function",
      "relationship": "subtopic"
    },
    {
      "from": "Adaptation Algorithms",
      "to": "Linear Probe",
      "relationship": "subtopic"
    },
    {
      "from": "Optimization Problem",
      "to": "Linear Constraints",
      "relationship": "has_subtopic"
    },
    {
      "from": "Softmax Function",
      "to": "Logits",
      "relationship": "depends_on"
    },
    {
      "from": "FeatureMapsAndKernels",
      "to": "UpdateRepresentationBeta",
      "relationship": "contains"
    },
    {
      "from": "CarAttributesExample",
      "to": "RedundancyDetection",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Adaptation",
      "to": "Finetuning_Pretrained_Models",
      "relationship": "has_subtopic"
    },
    {
      "from": "UnitVectorW",
      "to": "DistanceToBoundary",
      "relationship": "depends_on"
    },
    {
      "from": "Bias_Variance_Tradeoff",
      "to": "Test_Error_Influence",
      "relationship": "depends_on"
    },
    {
      "from": "MachineLearningOverview",
      "to": "NeuralNetworkStructure",
      "relationship": "contains"
    },
    {
      "from": "GeneralizedLinearModel",
      "to": "ExponentialFamilyDistributions",
      "relationship": "depends_on"
    },
    {
      "from": "E_Step",
      "to": "Soft_Guesses",
      "relationship": "produces"
    },
    {
      "from": "OtherNormalizationLayers",
      "to": "MachineLearningConcepts",
      "relationship": "subtopic"
    },
    {
      "from": "Efficient_Update_Mechanism",
      "to": "Alpha_Updating_Process",
      "relationship": "depends_on"
    },
    {
      "from": "MachineLearningBasics",
      "to": "MulticlassClassification",
      "relationship": "contains"
    },
    {
      "from": "Underfitting",
      "to": "MachineLearningConcepts",
      "relationship": "subtopic"
    },
    {
      "from": "Algorithm 6",
      "to": "Policy Update Rule (15.13)",
      "relationship": "depends_on"
    },
    {
      "from": "Empirical_Risk_Minimization",
      "to": "Training_Error",
      "relationship": "related_to"
    },
    {
      "from": "LayerNormalization",
      "to": "AffineTransformation",
      "relationship": "follows"
    },
    {
      "from": "Statistical_Learning_Theory",
      "to": "Machine_Learning_Papers",
      "relationship": "depends_on"
    },
    {
      "from": "Synchronous Updates",
      "to": "Value Iteration Algorithm",
      "relationship": "related_to"
    },
    {
      "from": "Sparsity Regularization",
      "to": "L0 Norm",
      "relationship": "contains"
    },
    {
      "from": "Fitted Value Iteration",
      "to": "Continuous State Space",
      "relationship": "depends_on"
    },
    {
      "from": "LogLikelihoodExpression",
      "to": "SummationNotEssential",
      "relationship": "subtopic"
    },
    {
      "from": "NaiveBayesAlgorithm",
      "to": "BernoulliEventModel",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Tasks",
      "to": "Zero-Shot Learning",
      "relationship": "subtopic"
    },
    {
      "from": "Continuous state MDPs",
      "to": "Value function approximation",
      "relationship": "subtopic"
    },
    {
      "from": "Log-Likelihood",
      "to": "EM_Algorithm",
      "relationship": "depends_on"
    },
    {
      "from": "Logistic_Loss_Backward",
      "to": "Vectorized_Notation_Backward",
      "relationship": "subtopic"
    },
    {
      "from": "Reinforcement_Learning",
      "to": "Discounted_Rewards",
      "relationship": "related_to"
    },
    {
      "from": "ExponentialFamilyDistributions",
      "to": "NaturalParameter",
      "relationship": "has"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Update_Rule",
      "relationship": "contains"
    },
    {
      "from": "Vectorized_Notation_Backward",
      "to": "Machine_Learning_Backward_Functions",
      "relationship": "subtopic"
    },
    {
      "from": "LikelihoodFunction",
      "to": "LogLikelihood",
      "relationship": "subtopic"
    },
    {
      "from": "3 Generalized linear models",
      "to": "3.2 Constructing GLMs",
      "relationship": "has_subtopic"
    },
    {
      "from": "Fitted Value Iteration",
      "to": "State Sample",
      "relationship": "uses"
    },
    {
      "from": "Gaussian_Mixture_Models",
      "to": "Posterior_Distribution",
      "relationship": "computes"
    },
    {
      "from": "Machine_Learning_Topics",
      "to": "Markov_Decision_Processes",
      "relationship": "related_to"
    },
    {
      "from": "Reinforcement_Learning",
      "to": "Bellman_Equation",
      "relationship": "related_to"
    },
    {
      "from": "Cocktail Party Problem",
      "to": "Unmixing Matrix (W)",
      "relationship": "aims_to_recover"
    },
    {
      "from": "Unsupervised Learning",
      "to": "Joint Distribution",
      "relationship": "depends_on"
    },
    {
      "from": "ICAIndependenceAssumption",
      "to": "SigmoidFunctionChoice",
      "relationship": "subtopic"
    },
    {
      "from": "LayerNormalization",
      "to": "AffineTransformation",
      "relationship": "related_to"
    },
    {
      "from": "MachineLearningOverview",
      "to": "MLPArchitecture",
      "relationship": "subtopic"
    },
    {
      "from": "Independent Components Analysis (ICA)",
      "to": "Cocktail Party Problem",
      "relationship": "motivational_example"
    },
    {
      "from": "Digit_Recognition_Problem",
      "to": "Examples_of_Kernels",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Algorithms",
      "to": "Initialization",
      "relationship": "depends_on"
    },
    {
      "from": "LQRModelAssumptions",
      "to": "Step1Estimation",
      "relationship": "subtopic"
    },
    {
      "from": "Value_Iteration",
      "to": "Optimal_Policy_Finding",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Algorithms",
      "to": "Bellman_Equations",
      "relationship": "related_to"
    },
    {
      "from": "ActivationFunctions",
      "to": "SigmoidTanh",
      "relationship": "related_to"
    },
    {
      "from": "Support Vector Machines (SVM)",
      "to": "Geometric Margin",
      "relationship": "has_subtopic"
    },
    {
      "from": "LinearRegressionOptimization",
      "to": "StochasticGradientDescent",
      "relationship": "has_subtopic"
    },
    {
      "from": "Policy_Gradient_Methods",
      "to": "Gradient_Ascend",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Linear Regression",
      "relationship": "contains"
    },
    {
      "from": "Machine Learning Algorithms",
      "to": "Value Iteration",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine_Learning_Algorithms",
      "to": "Kernel_Tricks",
      "relationship": "depends_on"
    },
    {
      "from": "LagrangianFormulation",
      "to": "DerivativeWithRespectToB",
      "relationship": "depends_on"
    },
    {
      "from": "Variational_Inference",
      "to": "Variational_Autoencoder",
      "relationship": "extends_to"
    },
    {
      "from": "GELUFunction",
      "to": "ReLUFunction",
      "relationship": "subtopic"
    },
    {
      "from": "Markov Decision Processes (MDP)",
      "to": "Time Dependent Dynamics",
      "relationship": "has_subtopic"
    },
    {
      "from": "MachineLearningOverview",
      "to": "IndependenceAssumption",
      "relationship": "related_to"
    },
    {
      "from": "LinearModelLimitations",
      "to": "UnderfittingLinearModel",
      "relationship": "depends_on"
    },
    {
      "from": "Alpha Updates",
      "to": "Sequential Minimal Optimization (SMO) Algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Lagrangian_Formulation",
      "to": "Equation_6.10",
      "relationship": "depends_on"
    },
    {
      "from": "Machine_Learning_Pretraining_Adaptation",
      "to": "Downstream_Tasks",
      "relationship": "subtopic"
    },
    {
      "from": "Optimal_Margin_Classifiers",
      "to": "Primal_Problem",
      "relationship": "contains"
    },
    {
      "from": "DifferentialDynamicProgramming",
      "to": "RewritingDynamics",
      "relationship": "subtopic"
    },
    {
      "from": "IdentityFunction",
      "to": "ActivationFunctions",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningModels",
      "to": "LogisticRegression",
      "relationship": "related_to"
    },
    {
      "from": "Linear Quadratic Regulation (LQR)",
      "to": "Quadratic Rewards",
      "relationship": "includes"
    },
    {
      "from": "Machine_Learning_Models",
      "to": "Embeddings_and_Representations",
      "relationship": "has_subtopic"
    },
    {
      "from": "MachineLearningOverview",
      "to": "BayesianInference",
      "relationship": "contains"
    },
    {
      "from": "GenerativeAlgorithms",
      "to": "BayesRule",
      "relationship": "uses"
    },
    {
      "from": "Machine_Learning_Algorithms",
      "to": "Value_Iteration",
      "relationship": "has_subtopic"
    },
    {
      "from": "I Supervised learning",
      "to": "4 Generative learning algorithms",
      "relationship": "has_subtopic"
    },
    {
      "from": "NeuralNetworks",
      "to": "LayerStructure",
      "relationship": "contains"
    },
    {
      "from": "Cross_Entropy_Loss",
      "to": "Gradient_Calculation",
      "relationship": "related_to"
    },
    {
      "from": "Optimizers",
      "to": "Gradient Descent (GD)",
      "relationship": "subtopic"
    },
    {
      "from": "PartialDerivatives",
      "to": "MathematicalNotationChallenges",
      "relationship": "depends_on"
    },
    {
      "from": "DataPreprocessing",
      "to": "LogisticFunction",
      "relationship": "depends_on"
    },
    {
      "from": "PrincipalComponentAnalysis",
      "to": "ProjectionDirection",
      "relationship": "has_subtopic"
    },
    {
      "from": "Random_Variables",
      "to": "Normal_Distribution",
      "relationship": "subtopic"
    },
    {
      "from": "Self-Supervised Learning",
      "to": "Supervised Contrastive Algorithms",
      "relationship": "related_to"
    },
    {
      "from": "Binary_Classification",
      "to": "Training_Set",
      "relationship": "depends_on"
    },
    {
      "from": "Regularized_Loss",
      "to": "Lambda_Parameter",
      "relationship": "has_subcomponent"
    },
    {
      "from": "MachineLearningOverview",
      "to": "HousingPricesModel",
      "relationship": "contains"
    },
    {
      "from": "Lagrange Duality",
      "to": "Lagrangian Function",
      "relationship": "defines"
    },
    {
      "from": "Machine_Learning_Models",
      "to": "Multinomial_Event_Model",
      "relationship": "has_subtopic"
    },
    {
      "from": "ScalingInvariantProperty",
      "to": "LN-S",
      "relationship": "depends_on"
    },
    {
      "from": "Finite-State MDPs",
      "to": "Value Iteration and Policy Iteration",
      "relationship": "subtopic"
    },
    {
      "from": "Prediction",
      "to": "NaiveBayesAlgorithm",
      "relationship": "subtopic"
    },
    {
      "from": "SMO_Algorithm",
      "to": "Constraints_Satisfaction",
      "relationship": "depends_on"
    },
    {
      "from": "PretrainedLanguageModels",
      "to": "NaturalLanguageProcessing",
      "relationship": "contains"
    },
    {
      "from": "Chapter_17_Policy_Gradient_REINFORCE",
      "to": "Sampling_Transition_Probabilities",
      "relationship": "subtopic"
    },
    {
      "from": "Optimizers",
      "to": "Stochastic Gradient Descent (SGD)",
      "relationship": "subtopic"
    },
    {
      "from": "Reinforcement_Learning",
      "to": "Value_Functions",
      "relationship": "subtopic"
    },
    {
      "from": "GaussianMixtureModels",
      "to": "EMAlgorithm",
      "relationship": "subtopic"
    },
    {
      "from": "MarkovDecisionProcesses",
      "to": "StateTransitionProbabilities",
      "relationship": "component_of"
    },
    {
      "from": "II Deep learning",
      "to": "7 Deep learning",
      "relationship": "has_subtopic"
    },
    {
      "from": "NeuralNetworks",
      "to": "ActivationFunctions",
      "relationship": "contains"
    },
    {
      "from": "Sigmoid_Function",
      "to": "Derivative_of_Sigmoid",
      "relationship": "has"
    },
    {
      "from": "Machine Learning",
      "to": "Matricization Approach",
      "relationship": "depends_on"
    },
    {
      "from": "JensensInequality",
      "to": "TheoremStatement",
      "relationship": "contains"
    },
    {
      "from": "PolicyGradientTheorem",
      "to": "ExpectationEstimation",
      "relationship": "subtopic"
    },
    {
      "from": "4 Generative learning algorithms",
      "to": "4.1 Gaussian discriminant analysis",
      "relationship": "has_subtopic"
    },
    {
      "from": "EventModelsForTextClassification",
      "to": "BernoulliEventModel",
      "relationship": "has_subtopic"
    },
    {
      "from": "LayerNormalization",
      "to": "LN-S",
      "relationship": "depends_on"
    },
    {
      "from": "Deep_Residual_Learning",
      "to": "Machine_Learning_Papers",
      "relationship": "subtopic"
    },
    {
      "from": "GenerativeAlgorithms",
      "to": "ClassPriors",
      "relationship": "depends_on"
    },
    {
      "from": "Normal_Distribution",
      "to": "Standard_Normal_Distribution",
      "relationship": "subtopic"
    },
    {
      "from": "Partially Observable MDPs (POMDP)",
      "to": "Policy Mapping",
      "relationship": "has_subtopic"
    },
    {
      "from": "Reinforcement Learning and Control",
      "to": "Reinforcement learning",
      "relationship": "subtopic"
    },
    {
      "from": "Loss Function",
      "to": "Negative Log-Likelihood",
      "relationship": "depends_on"
    },
    {
      "from": "House Price Prediction Example",
      "to": "Feature Discovery",
      "relationship": "depends_on"
    },
    {
      "from": "Sample_Size_Calculation",
      "to": "Training_Error_Generalization_Error",
      "relationship": "subtopic"
    },
    {
      "from": "E_Step",
      "to": "Hard_Assignments",
      "relationship": "contrasts_with"
    },
    {
      "from": "ParameterMatrixW",
      "to": "LogLikelihood",
      "relationship": "related_to"
    },
    {
      "from": "MachineLearningOptimization",
      "to": "NormalEquationsMethod",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine_Learning_Models",
      "to": "Conditional_Probability_Modeling",
      "relationship": "has_subtopic"
    },
    {
      "from": "Bellman_Equation",
      "to": "Value_Iteration",
      "relationship": "depends_on"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Loss_Functions",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine Learning Tasks",
      "to": "Few-Shot Learning",
      "relationship": "subtopic"
    },
    {
      "from": "Unknown_State_Transitions_and_Rewards",
      "to": "Efficient_Experience_Use",
      "relationship": "depends_on"
    },
    {
      "from": "convergence_property",
      "to": "k-means_algorithm",
      "relationship": "related_to"
    },
    {
      "from": "Support_Vector_Machines",
      "to": "Margins_Concept",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Models",
      "to": "Non-Linear Feature Mappings",
      "relationship": "depends_on"
    },
    {
      "from": "1 Linear regression",
      "to": "1.3 Probabilistic interpretation",
      "relationship": "has_subtopic"
    },
    {
      "from": "LQR, DDP and LQG",
      "to": "Finite-horizon MDPs",
      "relationship": "has_subtopic"
    },
    {
      "from": "PartialDerivatives",
      "to": "ComputationalComplexity",
      "relationship": "related_to"
    },
    {
      "from": "HoldoutCrossValidation",
      "to": "ValidationSetSize",
      "relationship": "subtopic_of"
    },
    {
      "from": "From non-linear dynamics to LQR",
      "to": "Differential Dynamic Programming (DDP)",
      "relationship": "has_subtopic"
    },
    {
      "from": "Finite Horizon Setting",
      "to": "Optimal Policy in Finite Horizon",
      "relationship": "subtopic_of"
    },
    {
      "from": "Cross_Entropy_Loss",
      "to": "Softmax_Function",
      "relationship": "depends_on"
    },
    {
      "from": "MultinomialRandomVariable",
      "to": "MaximumLikelihoodEstimates",
      "relationship": "depends_on"
    },
    {
      "from": "Variational_Autoencoder",
      "to": "Reparametrization_Trick",
      "relationship": "uses"
    },
    {
      "from": "MachineLearningModels",
      "to": "GLMFormulation",
      "relationship": "contains"
    },
    {
      "from": "ConditionalLikelihood",
      "to": "MixtureOfGaussians",
      "relationship": "subtopic"
    },
    {
      "from": "Double Descent Phenomenon",
      "to": "Regularization Techniques",
      "relationship": "related_to"
    },
    {
      "from": "BinaryClassification",
      "to": "PACAssumptions",
      "relationship": "related_to"
    },
    {
      "from": "Variational_Inference_Review",
      "to": "Machine_Learning_Papers",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningOverview",
      "to": "IntermediateVariables",
      "relationship": "contains"
    },
    {
      "from": "L2_Regularization",
      "to": "Weight_Decay",
      "relationship": "explains_concept"
    },
    {
      "from": "ConvolutionalLayers",
      "to": "ChannelConcepts",
      "relationship": "has_subtopic"
    },
    {
      "from": "MaximumLikelihoodEstimation",
      "to": "LikelihoodFunction",
      "relationship": "subtopic"
    },
    {
      "from": "MatrixAlgebra",
      "to": "VectorizationInNN",
      "relationship": "subtopic"
    },
    {
      "from": "BernoulliRandomVariableZ",
      "to": "GeneralizationErrorGuarantees",
      "relationship": "subtopic"
    },
    {
      "from": "Implementation Details",
      "to": "Data Representation",
      "relationship": "subtopic"
    },
    {
      "from": "Text_Classification",
      "to": "Generative_Modeling",
      "relationship": "follows"
    },
    {
      "from": "Bias_Variance_Tradeoff",
      "to": "Machine_Learning_Papers",
      "relationship": "subtopic"
    },
    {
      "from": "Unsupervised learning",
      "to": "Independent components analysis",
      "relationship": "subtopic"
    },
    {
      "from": "Gradient Calculation",
      "to": "Matrix Derivatives",
      "relationship": "depends_on"
    },
    {
      "from": "Matricization Approach",
      "to": "Implementation Details",
      "relationship": "subtopic"
    },
    {
      "from": "Empirical_Risk_Minimization",
      "to": "Finite_Hypothesis_Class",
      "relationship": "subtopic"
    },
    {
      "from": "FullyConnectedNN",
      "to": "IntermediateVariables",
      "relationship": "depends_on"
    },
    {
      "from": "Conditional Distribution Modeling",
      "to": "Bernoulli Distribution",
      "relationship": "depends_on"
    },
    {
      "from": "LatentVariableModel",
      "to": "ContinuousLatentVariables",
      "relationship": "contains"
    },
    {
      "from": "Machine_Learning_Overview",
      "to": "Policy_Gradient_Methods",
      "relationship": "has_subtopic"
    },
    {
      "from": "FiniteHorizonMDPs",
      "to": "OptimalBellmanEquation",
      "relationship": "contains"
    },
    {
      "from": "ProbabilityEstimation",
      "to": "EventModelsTextClassification",
      "relationship": "subtopic"
    },
    {
      "from": "Multinomial_Event_Model",
      "to": "Spam_Detection",
      "relationship": "depends_on"
    },
    {
      "from": "Normal_Distribution",
      "to": "Covariance_Matrix",
      "relationship": "related_to"
    },
    {
      "from": "ModelComplexityMeasures",
      "to": "NormAsComplexityMeasure",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Overview",
      "to": "Supervised Learning",
      "relationship": "related_to"
    },
    {
      "from": "Support_Vector_Machines",
      "to": "Kernels_in_SVM",
      "relationship": "subtopic"
    },
    {
      "from": "Support Vector Machines (SVM)",
      "to": "Regularization",
      "relationship": "depends_on"
    },
    {
      "from": "Policy_Gradient_Methods",
      "to": "Value_Function",
      "relationship": "depends_on"
    },
    {
      "from": "KernelFunctions",
      "to": "PolynomialKernels",
      "relationship": "has_subtopic"
    },
    {
      "from": "FittedValueIteration",
      "to": "DeterministicSimulator",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Optimal_Control",
      "relationship": "contains"
    },
    {
      "from": "LikelihoodFunction",
      "to": "OptimizationChallenges",
      "relationship": "depends_on"
    },
    {
      "from": "ProbabilityEstimation",
      "to": "LaplaceSmoothing",
      "relationship": "subtopic"
    },
    {
      "from": "Markov_Decision_Processes_MDPs",
      "to": "Policy_Iteration",
      "relationship": "related_to"
    },
    {
      "from": "Generalization_Error_Analysis",
      "to": "Machine_Learning_Papers",
      "relationship": "related_to"
    },
    {
      "from": "Machine_Learning_Algorithms",
      "to": "SMO_Algorithm",
      "relationship": "has_subtopic"
    },
    {
      "from": "KernelTrickIntroduction",
      "to": "IterativeUpdateRule",
      "relationship": "subtopic"
    },
    {
      "from": "GaussianDiscriminantAnalysis",
      "to": "LogLikelihood",
      "relationship": "subtopic"
    },
    {
      "from": "EMAlgorithms",
      "to": "MixtureOfGaussians",
      "relationship": "contains"
    }
  ]
}