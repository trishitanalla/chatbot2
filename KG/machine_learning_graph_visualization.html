<html>
    <head>
        <meta charset="utf-8">
        
            <script src="lib/bindings/utils.js"></script>
            <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/vis-network/9.1.2/dist/dist/vis-network.min.css" integrity="sha512-WgxfT5LWjfszlPHXRmBWHkV2eceiWTOBvrKCNbdgDYTHrT2AeLCGbF4sZlZw3UMN3WtL0tGUoIAKsu8mllg/XA==" crossorigin="anonymous" referrerpolicy="no-referrer" />
            <script src="https://cdnjs.cloudflare.com/ajax/libs/vis-network/9.1.2/dist/vis-network.min.js" integrity="sha512-LnvoEWDFrqGHlHmDD2101OrLcbsfkrzoSpvtSQtxK3RMnRV0eOkhhBN2dXHKRrUU8p2DGRTk35n4O8nWSVe1mQ==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
            
        
<center>
<h1>Machine Learning Knowledge Graph</h1>
</center>

<!-- <link rel="stylesheet" href="../node_modules/vis/dist/vis.min.css" type="text/css" />
<script type="text/javascript" src="../node_modules/vis/dist/vis.js"> </script>-->
        <link
          href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/css/bootstrap.min.css"
          rel="stylesheet"
          integrity="sha384-eOJMYsd53ii+scO/bJGFsiCZc+5NDVN2yr8+0RDqr0Ql0h+rP48ckxlpbzKgwra6"
          crossorigin="anonymous"
        />
        <script
          src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/js/bootstrap.bundle.min.js"
          integrity="sha384-JEW9xMcG8R+pH31jmWH6WWP0WintQrMb4s7ZOdauHnUtxwoG2vI5DkLtS3qm9Ekf"
          crossorigin="anonymous"
        ></script>


        <center>
          <h1>Machine Learning Knowledge Graph</h1>
        </center>
        <style type="text/css">

             #mynetwork {
                 width: 100%;
                 height: 95vh;
                 background-color: #ffffff;
                 border: 1px solid lightgray;
                 position: relative;
                 float: left;
             }

             
             #loadingBar {
                 position:absolute;
                 top:0px;
                 left:0px;
                 width: 100%;
                 height: 95vh;
                 background-color:rgba(200,200,200,0.8);
                 -webkit-transition: all 0.5s ease;
                 -moz-transition: all 0.5s ease;
                 -ms-transition: all 0.5s ease;
                 -o-transition: all 0.5s ease;
                 transition: all 0.5s ease;
                 opacity:1;
             }

             #bar {
                 position:absolute;
                 top:0px;
                 left:0px;
                 width:20px;
                 height:20px;
                 margin:auto auto auto auto;
                 border-radius:11px;
                 border:2px solid rgba(30,30,30,0.05);
                 background: rgb(0, 173, 246); /* Old browsers */
                 box-shadow: 2px 0px 4px rgba(0,0,0,0.4);
             }

             #border {
                 position:absolute;
                 top:10px;
                 left:10px;
                 width:500px;
                 height:23px;
                 margin:auto auto auto auto;
                 box-shadow: 0px 0px 4px rgba(0,0,0,0.2);
                 border-radius:10px;
             }

             #text {
                 position:absolute;
                 top:8px;
                 left:530px;
                 width:30px;
                 height:50px;
                 margin:auto auto auto auto;
                 font-size:22px;
                 color: #000000;
             }

             div.outerBorder {
                 position:relative;
                 top:400px;
                 width:600px;
                 height:44px;
                 margin:auto auto auto auto;
                 border:8px solid rgba(0,0,0,0.1);
                 background: rgb(252,252,252); /* Old browsers */
                 background: -moz-linear-gradient(top,  rgba(252,252,252,1) 0%, rgba(237,237,237,1) 100%); /* FF3.6+ */
                 background: -webkit-gradient(linear, left top, left bottom, color-stop(0%,rgba(252,252,252,1)), color-stop(100%,rgba(237,237,237,1))); /* Chrome,Safari4+ */
                 background: -webkit-linear-gradient(top,  rgba(252,252,252,1) 0%,rgba(237,237,237,1) 100%); /* Chrome10+,Safari5.1+ */
                 background: -o-linear-gradient(top,  rgba(252,252,252,1) 0%,rgba(237,237,237,1) 100%); /* Opera 11.10+ */
                 background: -ms-linear-gradient(top,  rgba(252,252,252,1) 0%,rgba(237,237,237,1) 100%); /* IE10+ */
                 background: linear-gradient(to bottom,  rgba(252,252,252,1) 0%,rgba(237,237,237,1) 100%); /* W3C */
                 filter: progid:DXImageTransform.Microsoft.gradient( startColorstr='#fcfcfc', endColorstr='#ededed',GradientType=0 ); /* IE6-9 */
                 border-radius:72px;
                 box-shadow: 0px 0px 10px rgba(0,0,0,0.2);
             }
             

             

             
        </style>
    </head>


    <body>
        <div class="card" style="width: 100%">
            
            
            <div id="mynetwork" class="card-body"></div>
        </div>

        
            <div id="loadingBar">
              <div class="outerBorder">
                <div id="text">0%</div>
                <div id="border">
                  <div id="bar"></div>
                </div>
              </div>
            </div>
        
        

        <script type="text/javascript">

              // initialize global variables.
              var edges;
              var nodes;
              var allNodes;
              var allEdges;
              var nodeColors;
              var originalNodes;
              var network;
              var container;
              var options, data;
              var filter = {
                  item : '',
                  property : '',
                  value : []
              };

              

              

              // This method is responsible for drawing the graph, returns the drawn network
              function drawGraph() {
                  var container = document.getElementById('mynetwork');

                  

                  // parsing and collecting nodes and edges from the python
                  nodes = new vis.DataSet([{"color": "#aa7e54", "id": "I Supervised learning", "label": "I Supervised learning", "shape": "star", "size": 25, "title": "Overview of supervised learning techniques in machine learning."}, {"color": "#a149ef", "id": "1 Linear regression", "label": "1 Linear regression", "shape": "dot", "size": 10, "title": "Introduction to linear regression and its applications."}, {"color": "#8dce63", "id": "1.1 LMS algorithm", "label": "1.1 LMS algorithm", "shape": "dot", "size": 10, "title": "Least Mean Squares (LMS) algorithm for updating weights in linear models."}, {"color": "#428321", "id": "1.2 The normal equations", "label": "1.2 The normal equations", "shape": "dot", "size": 10, "title": "Derivation and use of the normal equations to solve linear regression problems."}, {"color": "#573083", "id": "1.2.1 Matrix derivatives", "label": "1.2.1 Matrix derivatives", "shape": "dot", "size": 10, "title": "Calculation of matrix derivatives relevant for solving normal equations."}, {"color": "#efde46", "id": "1.2.2 Least squares revisited", "label": "1.2.2 Least squares revisited", "shape": "dot", "size": 10, "title": "Revisiting least squares method in the context of linear regression."}, {"color": "#12d668", "id": "1.3 Probabilistic interpretation", "label": "1.3 Probabilistic interpretation", "shape": "dot", "size": 10, "title": "Probabilistic view of linear regression models and assumptions."}, {"color": "#3df968", "id": "1.4 Locally weighted linear regression (optional reading)", "label": "1.4 Locally weighted linear regression (optional reading)", "shape": "dot", "size": 10, "title": "Advanced topic: locally weighted linear regression for non-stationary data."}, {"color": "#9f6522", "id": "2 Classification and logistic regression", "label": "2 Classification and logistic regression", "shape": "dot", "size": 10, "title": "Introduction to classification problems and logistic regression models."}, {"color": "#2f1d5f", "id": "2.1 Logistic regression", "label": "2.1 Logistic regression", "shape": "dot", "size": 10, "title": "Logistic regression model for binary classification tasks."}, {"color": "#be6ff6", "id": "2.2 Digression: the perceptron learning algorithm", "label": "2.2 Digression: the perceptron learning algorithm", "shape": "dot", "size": 10, "title": "Discussion on the perceptron learning algorithm as a precursor to modern classifiers."}, {"color": "#d8be6b", "id": "2.3 Multi-class classification", "label": "2.3 Multi-class classification", "shape": "dot", "size": 10, "title": "Techniques for extending binary classifiers to multi-class problems."}, {"color": "#b0bbe0", "id": "2.4 Another algorithm for maximizing \\(\\ell(\\theta)\\)", "label": "2.4 Another algorithm for maximizing \\(\\ell(\\theta)\\)", "shape": "dot", "size": 10, "title": "Alternative methods for optimizing the likelihood function in classification models."}, {"color": "#eb1afd", "id": "3 Generalized linear models", "label": "3 Generalized linear models", "shape": "dot", "size": 10, "title": "Introduction to generalized linear models (GLMs) extending beyond linear and logistic regression."}, {"color": "#8400c2", "id": "3.1 The exponential family", "label": "3.1 The exponential family", "shape": "dot", "size": 10, "title": "Overview of the exponential family distributions used in GLMs."}, {"color": "#0b8412", "id": "3.2 Constructing GLMs", "label": "3.2 Constructing GLMs", "shape": "dot", "size": 10, "title": "Steps and methods for constructing generalized linear models."}, {"color": "#69f968", "id": "3.2.1 Ordinary least squares", "label": "3.2.1 Ordinary least squares", "shape": "dot", "size": 10, "title": "Ordinary least squares method in the context of GLMs."}, {"color": "#7fb196", "id": "3.2.2 Logistic regression", "label": "3.2.2 Logistic regression", "shape": "dot", "size": 10, "title": "Logistic regression as a specific case of generalized linear models."}, {"color": "#f85869", "id": "4 Generative learning algorithms", "label": "4 Generative learning algorithms", "shape": "dot", "size": 10, "title": "Introduction to generative approaches in machine learning for classification tasks."}, {"color": "#a06075", "id": "4.1 Gaussian discriminant analysis", "label": "4.1 Gaussian discriminant analysis", "shape": "dot", "size": 10, "title": "Gaussian Discriminant Analysis (GDA) model for binary and multi-class classification."}, {"color": "#303372", "id": "4.1.1 The multivariate normal distribution", "label": "4.1.1 The multivariate normal distribution", "shape": "dot", "size": 10, "title": "Properties of the multivariate normal distribution used in GDA."}, {"color": "#8f4a6a", "id": "4.1.2 The Gaussian discriminant analysis model", "label": "4.1.2 The Gaussian discriminant analysis model", "shape": "dot", "size": 10, "title": "Formulation and application of the Gaussian Discriminant Analysis model."}, {"color": "#87c196", "id": "4.1.3 Discussion: GDA and logistic regression", "label": "4.1.3 Discussion: GDA and logistic regression", "shape": "dot", "size": 10, "title": "Comparison between GDA and logistic regression models."}, {"color": "#0b08b1", "id": "4.2 Naive bayes (Option Reading)", "label": "4.2 Naive bayes (Option Reading)", "shape": "dot", "size": 10, "title": "Introduction to the naive Bayes classifier for text classification tasks."}, {"color": "#9a98fb", "id": "4.2.1 Laplace smoothing", "label": "4.2.1 Laplace smoothing", "shape": "dot", "size": 10, "title": "Technique of Laplace smoothing in naive Bayes classifiers."}, {"color": "#e281c6", "id": "4.2.2 Event models for text classification", "label": "4.2.2 Event models for text classification", "shape": "dot", "size": 10, "title": "Event models used in naive Bayes for text classification tasks."}, {"color": "#e46cfb", "id": "5 Kernel methods", "label": "5 Kernel methods", "shape": "dot", "size": 10, "title": "Introduction to kernel methods and their applications in machine learning."}, {"color": "#d400fe", "id": "5.1 Feature maps", "label": "5.1 Feature maps", "shape": "dot", "size": 10, "title": "Concept of feature mapping in the context of kernel methods."}, {"color": "#5ca983", "id": "5.2 LMS (least mean squares) with features", "label": "5.2 LMS (least mean squares) with features", "shape": "dot", "size": 10, "title": "Application of LMS algorithm using feature maps."}, {"color": "#fb21be", "id": "5.3 LMS with the kernel trick", "label": "5.3 LMS with the kernel trick", "shape": "dot", "size": 10, "title": "Use of the kernel trick in LMS for non-linear data."}, {"color": "#acabff", "id": "5.4 Properties of kernels", "label": "5.4 Properties of kernels", "shape": "dot", "size": 10, "title": "Properties and characteristics of valid kernels in machine learning."}, {"color": "#ddd3ec", "id": "6 Support vector machines", "label": "6 Support vector machines", "shape": "dot", "size": 10, "title": "Introduction to support vector machines (SVMs) for classification tasks."}, {"color": "#b2ce12", "id": "6.1 Margins: intuition", "label": "6.1 Margins: intuition", "shape": "dot", "size": 10, "title": "Intuitive explanation of margins in SVMs."}, {"color": "#4906c2", "id": "II Deep learning", "label": "II Deep learning", "shape": "star", "size": 25, "title": "Overview of deep learning concepts and techniques."}, {"color": "#3ae1b7", "id": "7 Deep learning", "label": "7 Deep learning", "shape": "dot", "size": 10, "title": "Introduction to the field of deep learning including neural networks and backpropagation."}, {"color": "#fc231b", "id": "Modern Neural Networks", "label": "Modern Neural Networks", "shape": "star", "size": 25, "title": "Overview of modern neural network concepts and techniques."}, {"color": "#a9abd2", "id": "Modules in Modern Neural Networks", "label": "Modules in Modern Neural Networks", "shape": "dot", "size": 10, "title": "Discussion on the components that make up contemporary neural networks."}, {"color": "#72c8f7", "id": "Backpropagation", "label": "Backpropagation", "shape": "dot", "size": 10, "title": "Introduction to backpropagation for efficient gradient computation in neural networks."}, {"color": "#8efdf7", "id": "Preliminaries on partial derivatives", "label": "Preliminaries on partial derivatives", "shape": "dot", "size": 10, "title": "Introduction to the mathematical concepts needed for backpropagation."}, {"color": "#4de6bd", "id": "General strategy of backpropagation", "label": "General strategy of backpropagation", "shape": "dot", "size": 10, "title": "Overview of the algorithmic steps in backpropagation."}, {"color": "#83a87a", "id": "Backward functions for basic modules", "label": "Backward functions for basic modules", "shape": "dot", "size": 10, "title": "Details on how to implement backward functions for simple network components."}, {"color": "#2b57a5", "id": "Back-propagation for MLPs", "label": "Back-propagation for MLPs", "shape": "dot", "size": 10, "title": "Specific implementation of backpropagation in multi-layer perceptrons."}, {"color": "#e1ece5", "id": "Vectorization over training examples", "label": "Vectorization over training examples", "shape": "dot", "size": 10, "title": "Techniques for efficient computation using vector operations."}, {"color": "#4a06ec", "id": "Generalization and regularization", "label": "Generalization and regularization", "shape": "star", "size": 25, "title": "Focuses on methods to improve model performance on unseen data."}, {"color": "#3998d7", "id": "Generalization", "label": "Generalization", "shape": "dot", "size": 10, "title": "Exploration of concepts that enhance a model\u0027s ability to generalize from training data."}, {"color": "#e81324", "id": "Bias-variance tradeoff", "label": "Bias-variance tradeoff", "shape": "dot", "size": 10, "title": "Analysis of the balance between model complexity and error due to variance or bias."}, {"color": "#50db30", "id": "A mathematical decomposition (for regression)", "label": "A mathematical decomposition (for regression)", "shape": "dot", "size": 10, "title": "Mathematical breakdown for understanding regression models\u0027 performance."}, {"color": "#4aa8fe", "id": "The double descent phenomenon", "label": "The double descent phenomenon", "shape": "dot", "size": 10, "title": "Observation of model performance as complexity increases, showing a second peak and decline in error."}, {"color": "#215c81", "id": "Sample complexity bounds (optional readings)", "label": "Sample complexity bounds (optional readings)", "shape": "dot", "size": 10, "title": "Theoretical limits on the number of samples needed for learning tasks."}, {"color": "#00fb52", "id": "Preliminaries", "label": "Preliminaries", "shape": "dot", "size": 10, "title": "Introduction to necessary concepts and definitions."}, {"color": "#59402c", "id": "The case of finite H", "label": "The case of finite H", "shape": "dot", "size": 10, "title": "Analysis when the hypothesis space is limited in size."}, {"color": "#0d1d89", "id": "The case of infinite H", "label": "The case of infinite H", "shape": "dot", "size": 10, "title": "Discussion on scenarios where the hypothesis space is unbounded."}, {"color": "#058882", "id": "Regularization and model selection", "label": "Regularization and model selection", "shape": "star", "size": 25, "title": "Techniques to prevent overfitting by penalizing overly complex models or selecting optimal ones."}, {"color": "#1854c0", "id": "Regularization", "label": "Regularization", "shape": "dot", "size": 10, "title": "Techniques that add penalty terms to loss functions to prevent overfitting."}, {"color": "#0ee16d", "id": "Implicit regularization effect (optional reading)", "label": "Implicit regularization effect (optional reading)", "shape": "dot", "size": 10, "title": "Exploration of how certain algorithms inherently regularize models."}, {"color": "#58e444", "id": "Model selection via cross validation", "label": "Model selection via cross validation", "shape": "dot", "size": 10, "title": "Use of cross-validation to choose the best model from a set of candidates."}, {"color": "#0b29e4", "id": "Bayesian statistics and regularization", "label": "Bayesian statistics and regularization", "shape": "dot", "size": 10, "title": "Application of Bayesian principles in regularizing models."}, {"color": "#7a36f0", "id": "Unsupervised learning", "label": "Unsupervised learning", "shape": "star", "size": 25, "title": "Techniques for learning from data without labeled responses."}, {"color": "#25bd8f", "id": "Clustering and the k-means algorithm", "label": "Clustering and the k-means algorithm", "shape": "dot", "size": 10, "title": "Introduction to clustering techniques with a focus on the k-means method."}, {"color": "#ec89ec", "id": "EM algorithms", "label": "EM algorithms", "shape": "dot", "size": 10, "title": "Exploration of Expectation-Maximization algorithms for probabilistic modeling."}, {"color": "#ef3da0", "id": "EM for mixture of Gaussians", "label": "EM for mixture of Gaussians", "shape": "dot", "size": 10, "title": "Application of EM to model data with a mixture of Gaussian distributions."}, {"color": "#49d7b0", "id": "Jensen\u0027s inequality", "label": "Jensen\u0027s inequality", "shape": "dot", "size": 10, "title": "Mathematical principle used in the derivation and understanding of EM algorithms."}, {"color": "#dc8520", "id": "General EM algorithms", "label": "General EM algorithms", "shape": "dot", "size": 10, "title": "Extension of basic EM to more complex scenarios, including variational inference."}, {"color": "#405a94", "id": "Other interpretation of ELBO", "label": "Other interpretation of ELBO", "shape": "dot", "size": 10, "title": "Alternative perspectives on the Evidence Lower Bound (ELBO) in variational inference."}, {"color": "#41f219", "id": "Mixture of Gaussians revisited", "label": "Mixture of Gaussians revisited", "shape": "dot", "size": 10, "title": "Re-examination and advanced topics related to Gaussian mixture models using EM."}, {"color": "#269603", "id": "Variational inference and variational auto-encoder (optional reading)", "label": "Variational inference and variational auto-encoder (optional reading)", "shape": "dot", "size": 10, "title": "Advanced topic on probabilistic modeling with variational methods and neural networks."}, {"color": "#083725", "id": "Principal components analysis", "label": "Principal components analysis", "shape": "dot", "size": 10, "title": "Dimensionality reduction technique that identifies principal components in data."}, {"color": "#c6480d", "id": "Independent components analysis", "label": "Independent components analysis", "shape": "dot", "size": 10, "title": "Technique for separating mixed signals into independent sources."}, {"color": "#ed3881", "id": "ICA ambiguities", "label": "ICA ambiguities", "shape": "dot", "size": 10, "title": "Discussion on the inherent limitations and issues in ICA solutions."}, {"color": "#5b13a4", "id": "Densities and linear transformations", "label": "Densities and linear transformations", "shape": "dot", "size": 10, "title": "Analysis of how densities change under linear transformations relevant to ICA."}, {"color": "#b1550c", "id": "ICA algorithm", "label": "ICA algorithm", "shape": "dot", "size": 10, "title": "Detailed explanation of the Independent Components Analysis procedure."}, {"color": "#acdaf7", "id": "Self-supervised learning and foundation models", "label": "Self-supervised learning and foundation models", "shape": "star", "size": 25, "title": "Approaches to training models with self-generated supervision signals and foundational architectures."}, {"color": "#81e33f", "id": "Pretraining and adaptation", "label": "Pretraining and adaptation", "shape": "dot", "size": 10, "title": "Overview of the process of pre-training large models followed by fine-tuning on specific tasks."}, {"color": "#14f4ff", "id": "Pretraining methods in computer vision", "label": "Pretraining methods in computer vision", "shape": "dot", "size": 10, "title": "Exploration of self-supervised techniques for image data."}, {"color": "#962a0f", "id": "Pretrained large language models", "label": "Pretrained large language models", "shape": "dot", "size": 10, "title": "Discussion on the development and applications of pre-trained language models."}, {"color": "#27632f", "id": "Open up the blackbox of Transformers", "label": "Open up the blackbox of Transformers", "shape": "dot", "size": 10, "title": "Insight into the architecture and workings of Transformer-based models."}, {"color": "#cecfe6", "id": "Zero-shot learning and in-context learning", "label": "Zero-shot learning and in-context learning", "shape": "dot", "size": 10, "title": "Exploration of capabilities for learning without explicit training data or with minimal context."}, {"color": "#fb57be", "id": "Reinforcement Learning and Control", "label": "Reinforcement Learning and Control", "shape": "star", "size": 25, "title": "Techniques for agents to learn optimal actions through interaction in an environment."}, {"color": "#491392", "id": "Reinforcement learning", "label": "Reinforcement learning", "shape": "dot", "size": 10, "title": "Introduction to the principles of reinforcement learning and its applications."}, {"color": "#e80319", "id": "Markov decision processes", "label": "Markov decision processes", "shape": "dot", "size": 10, "title": "Foundation for understanding state transitions and rewards in RL problems."}, {"color": "#a13f3b", "id": "Value iteration and policy iteration", "label": "Value iteration and policy iteration", "shape": "dot", "size": 10, "title": "Algorithms that compute optimal policies or value functions through iterations."}, {"color": "#f81338", "id": "Learning a model for an MDP", "label": "Learning a model for an MDP", "shape": "dot", "size": 10, "title": "Techniques to infer the dynamics of an environment from observations."}, {"color": "#76c58e", "id": "Continuous state MDPs", "label": "Continuous state MDPs", "shape": "dot", "size": 10, "title": "Handling environments with a continuum of states rather than discrete ones."}, {"color": "#108efa", "id": "Discretization", "label": "Discretization", "shape": "dot", "size": 10, "title": "Method to approximate continuous state spaces by dividing them into discrete bins."}, {"color": "#b7f47a", "id": "Value function approximation", "label": "Value function approximation", "shape": "dot", "size": 10, "title": "Techniques for approximating value functions in large or continuous state spaces"}, {"color": "#418d9c", "id": "Connections between Policy and Value Iteration (Optional)", "label": "Connections between Policy and Value Iteration (Optional)", "shape": "dot", "size": 10, "title": "Analysis of the interplay and equivalences between policy and value iteration methods."}, {"color": "#676102", "id": "LQR, DDP and LQG", "label": "LQR, DDP and LQG", "shape": "star", "size": 25, "title": "Advanced control techniques for linear systems with quadratic costs."}, {"color": "#9e20a7", "id": "Finite-horizon MDPs", "label": "Finite-horizon MDPs", "shape": "dot", "size": 10, "title": "Formulation of decision-making problems in environments with a limited time horizon."}, {"color": "#12102a", "id": "Linear Quadratic Regulation (LQR)", "label": "Linear Quadratic Regulation (LQR)", "shape": "dot", "size": 10, "title": "Special case of finite-horizon setting with tractable exact solutions used in robotics."}, {"color": "#297baa", "id": "From non-linear dynamics to LQR", "label": "From non-linear dynamics to LQR", "shape": "dot", "size": 10, "title": "Approach to converting non-linear dynamics into a form suitable for LQR analysis"}, {"color": "#479816", "id": "Linearization of dynamics", "label": "Linearization of dynamics", "shape": "dot", "size": 10, "title": "Process of approximating non-linear systems with linear models"}, {"color": "#20d0f0", "id": "Differential Dynamic Programming (DDP)", "label": "Differential Dynamic Programming (DDP)", "shape": "dot", "size": 10, "title": "Optimization technique for nonlinear control problems"}, {"color": "#cb8631", "id": "Linear Quadratic Gaussian (LQG)", "label": "Linear Quadratic Gaussian (LQG)", "shape": "dot", "size": 10, "title": "Control theory framework combining LQR with stochastic noise models"}, {"color": "#8730b8", "id": "Policy Gradient (REINFORCE)", "label": "Policy Gradient (REINFORCE)", "shape": "star", "size": 25, "title": "Method for learning policies in reinforcement learning using gradient ascent on the expected reward"}, {"color": "#d6a47d", "id": "Supervised Learning Examples", "label": "Supervised Learning Examples", "shape": "star", "size": 25, "title": "Introduction to supervised learning through examples of predicting house prices based on living area"}, {"color": "#defd54", "id": "Supervised Learning Problem", "label": "Supervised Learning Problem", "shape": "star", "size": 25, "title": "Formal description of the goal in supervised learning"}, {"color": "#6a77c9", "id": "Regression", "label": "Regression", "shape": "dot", "size": 10, "title": "Learning problem with continuous target variable"}, {"color": "#15bb3c", "id": "Classification", "label": "Classification", "shape": "dot", "size": 10, "title": "Learning problem with discrete target variables"}, {"color": "#fb5895", "id": "Hypothesis", "label": "Hypothesis", "shape": "dot", "size": 10, "title": "Function learned by the model to predict y from x"}, {"color": "#e99772", "id": "Linear Regression", "label": "Linear Regression", "shape": "star", "size": 25, "title": "Basic supervised learning method for predicting continuous values based on input features."}, {"color": "#46ca44", "id": "Housing Example", "label": "Housing Example", "shape": "dot", "size": 10, "title": "Example dataset with living area and number of bedrooms as features"}, {"color": "#dcc53a", "id": "Feature Selection", "label": "Feature Selection", "shape": "dot", "size": 10, "title": "Process of choosing which features to include in the model"}, {"color": "#6134e2", "id": "MachineLearningBasics", "label": "MachineLearningBasics", "shape": "star", "size": 25, "title": "Fundamental concepts in machine learning including models and loss functions."}, {"color": "#ee6d01", "id": "FunctionRepresentation", "label": "FunctionRepresentation", "shape": "dot", "size": 10, "title": "How functions and hypotheses are represented in machine learning models."}, {"color": "#d0bf17", "id": "LinearHypothesis", "label": "LinearHypothesis", "shape": "dot", "size": 10, "title": "Introduction to linear hypothesis functions used in regression problems."}, {"color": "#6e4052", "id": "ParametersWeights", "label": "ParametersWeights", "shape": "dot", "size": 10, "title": "Explanation of parameters and weights in the context of linear models."}, {"color": "#4809b3", "id": "CostFunction", "label": "CostFunction", "shape": "dot", "size": 10, "title": "Definition and purpose of a cost function in machine learning."}, {"color": "#99494e", "id": "OrdinaryLeastSquares", "label": "OrdinaryLeastSquares", "shape": "dot", "size": 10, "title": "Explanation of ordinary least squares regression as an example of minimizing the cost function."}, {"color": "#30daee", "id": "LMSAlgorithm", "label": "LMSAlgorithm", "shape": "star", "size": 25, "title": "Introduction to the Least Mean Squares algorithm for parameter optimization."}, {"color": "#2fe3fc", "id": "Machine_Learning_Algorithms", "label": "Machine_Learning_Algorithms", "shape": "star", "size": 25, "title": "Collection of algorithms used in machine learning for prediction and decision making."}, {"color": "#1cb18d", "id": "Gradient_Descent", "label": "Gradient_Descent", "shape": "dot", "size": 10, "title": "Optimization algorithm that minimizes a function by iteratively moving towards the minimum value of that function."}, {"color": "#042d34", "id": "Learning_Rate", "label": "Learning_Rate", "shape": "dot", "size": 10, "title": "Hyperparameter in gradient descent determining the step size at each iteration."}, {"color": "#fb48d3", "id": "Cost_Function_J_theta", "label": "Cost_Function_J_theta", "shape": "dot", "size": 10, "title": "Function that measures the performance of a machine learning model for given data and parameters."}, {"color": "#6037f2", "id": "Partial_Derivative_Calculation", "label": "Partial_Derivative_Calculation", "shape": "dot", "size": 10, "title": "Calculation of partial derivatives to update parameters in gradient descent."}, {"color": "#6f58a2", "id": "LMS_Update_Rule", "label": "LMS_Update_Rule", "shape": "dot", "size": 10, "title": "Rule for updating model parameters based on the least mean squares method."}, {"color": "#563e7b", "id": "LMS_Rule", "label": "LMS_Rule", "shape": "star", "size": 25, "title": "Least Mean Squares update rule for adjusting parameters in machine learning."}, {"color": "#134e08", "id": "Widrow_Hoff_Learning_Rule", "label": "Widrow_Hoff_Learning_Rule", "shape": "dot", "size": 10, "title": "Alternative name for the LMS update rule."}, {"color": "#c5849e", "id": "Error_Term", "label": "Error_Term", "shape": "dot", "size": 10, "title": "The difference between actual and predicted values, guiding parameter updates."}, {"color": "#de1630", "id": "Single_Training_Example", "label": "Single_Training_Example", "shape": "dot", "size": 10, "title": "Derivation of LMS rule for a single training example scenario."}, {"color": "#4f76d1", "id": "Batch_Gradient_Descent", "label": "Batch_Gradient_Descent", "shape": "star", "size": 25, "title": "Method that uses the entire dataset to update parameters in each iteration."}, {"color": "#fa7526", "id": "LinearRegressionOptimization", "label": "LinearRegressionOptimization", "shape": "star", "size": 25, "title": "Exploration of the optimization problem in linear regression."}, {"color": "#903b7b", "id": "GradientDescentConvergence", "label": "GradientDescentConvergence", "shape": "dot", "size": 10, "title": "Discussion on how gradient descent converges to a global minimum for convex functions."}, {"color": "#dbf7ab", "id": "BatchGradientDescentExample", "label": "BatchGradientDescentExample", "shape": "dot", "size": 10, "title": "Illustration of batch gradient descent with specific parameters and results."}, {"color": "#55e61a", "id": "StochasticGradientDescent", "label": "StochasticGradientDescent", "shape": "dot", "size": 10, "title": "Variant of gradient descent that uses a single or subset of data points to update the model parameters."}, {"color": "#f957b1", "id": "MachineLearningOptimization", "label": "MachineLearningOptimization", "shape": "star", "size": 25, "title": "Methods for minimizing cost functions in machine learning."}, {"color": "#da8954", "id": "BatchGradientDescent", "label": "BatchGradientDescent", "shape": "dot", "size": 10, "title": "Description of the batch gradient descent algorithm in machine learning."}, {"color": "#8fe36f", "id": "NormalEquationsMethod", "label": "NormalEquationsMethod", "shape": "dot", "size": 10, "title": "Explicit minimization of cost function without iterative algorithm."}, {"color": "#fa1506", "id": "MatrixDerivatives", "label": "MatrixDerivatives", "shape": "dot", "size": 10, "title": "Notation for calculus with matrices in machine learning context."}, {"color": "#805541", "id": "Matrix Derivatives", "label": "Matrix Derivatives", "shape": "star", "size": 25, "title": "Derivation of matrix derivatives for functions mapping matrices to real numbers."}, {"color": "#b0ea3e", "id": "Gradient Calculation", "label": "Gradient Calculation", "shape": "dot", "size": 10, "title": "Calculation of the gradient for a given function and matrix."}, {"color": "#614a89", "id": "Least Squares Revisited", "label": "Least Squares Revisited", "shape": "star", "size": 25, "title": "Revisiting least squares using matrix derivatives."}, {"color": "#501282", "id": "Design Matrix", "label": "Design Matrix", "shape": "dot", "size": 10, "title": "Matrix containing training examples\u0027 input values in its rows."}, {"color": "#c0f1e3", "id": "Target Vector", "label": "Target Vector", "shape": "dot", "size": 10, "title": "Vector containing target values from the training set."}, {"color": "#7e171b", "id": "MachineLearningOverview", "label": "MachineLearningOverview", "shape": "star", "size": 25, "title": "Introduction to machine learning concepts and EM algorithm overview."}, {"color": "#7600a2", "id": "LinearRegression", "label": "LinearRegression", "shape": "dot", "size": 10, "title": "Algorithm for modeling the relationship between a scalar dependent variable and one or more explanatory variables."}, {"color": "#d86421", "id": "CostFunctionJ", "label": "CostFunctionJ", "shape": "dot", "size": 10, "title": "Definition and minimization process of the cost function J(theta)."}, {"color": "#fee14e", "id": "NormalEquations", "label": "NormalEquations", "shape": "dot", "size": 10, "title": "Derivation and explanation of normal equations for finding theta that minimizes J(theta)."}, {"color": "#d6b8a3", "id": "InvertibleMatrixAssumption", "label": "InvertibleMatrixAssumption", "shape": "dot", "size": 10, "title": "Assumption that $X^{T}X$ is invertible for solving linear equations."}, {"color": "#78b52c", "id": "ProbabilisticInterpretation", "label": "ProbabilisticInterpretation", "shape": "star", "size": 25, "title": "Explanation of the probabilistic assumptions underlying least-squares regression."}, {"color": "#e0e47c", "id": "RegressionProblem", "label": "RegressionProblem", "shape": "dot", "size": 10, "title": "Discussion on why linear regression is a reasonable choice for regression problems."}, {"color": "#1de8dc", "id": "ErrorTermAssumption", "label": "ErrorTermAssumption", "shape": "dot", "size": 10, "title": "Assumption that error terms are IID and follow a Gaussian distribution with mean zero."}, {"color": "#c10c31", "id": "GaussianDistribution", "label": "GaussianDistribution", "shape": "dot", "size": 10, "title": "Description of the Gaussian distribution used for modeling errors in regression analysis."}, {"color": "#c09e57", "id": "ProbabilisticModeling", "label": "ProbabilisticModeling", "shape": "dot", "size": 10, "title": "Models that use probability distributions to describe data generation processes."}, {"color": "#089e6e", "id": "ConditionalProbabilityDistribution", "label": "ConditionalProbabilityDistribution", "shape": "dot", "size": 10, "title": "Describes the distribution of one random variable given another."}, {"color": "#30f8b0", "id": "DesignMatrixX", "label": "DesignMatrixX", "shape": "dot", "size": 10, "title": "Contains all input data vectors for a model."}, {"color": "#a88809", "id": "LikelihoodFunction", "label": "LikelihoodFunction", "shape": "dot", "size": 10, "title": "Probability of observed data given parameters, viewed as function of parameters."}, {"color": "#8d7289", "id": "IndependenceAssumption", "label": "IndependenceAssumption", "shape": "dot", "size": 10, "title": "Simplifying assumption that training examples are independent, though this may not hold for time series or correlated data."}, {"color": "#b40540", "id": "MaximumLikelihoodEstimation", "label": "MaximumLikelihoodEstimation", "shape": "dot", "size": 10, "title": "Justification of least squares regression using maximum likelihood estimation under probabilistic assumptions."}, {"color": "#13d442", "id": "LogLikelihoodFunction", "label": "LogLikelihoodFunction", "shape": "dot", "size": 10, "title": "Natural logarithm of the likelihood function, simplifies calculations."}, {"color": "#c23bc1", "id": "LogLikelihood", "label": "LogLikelihood", "shape": "dot", "size": 10, "title": "Measure of how likely a given set of parameters is to produce observed data, crucial for model fitting."}, {"color": "#14b4b4", "id": "MaximizingLogLikelihood", "label": "MaximizingLogLikelihood", "shape": "dot", "size": 10, "title": "Process of maximizing the likelihood function to estimate model parameters."}, {"color": "#01a5b9", "id": "LeastSquaresRegression", "label": "LeastSquaresRegression", "shape": "dot", "size": 10, "title": "Linear regression technique that minimizes the sum of squared residuals."}, {"color": "#76a4dc", "id": "LocallyWeightedLinearRegression", "label": "LocallyWeightedLinearRegression", "shape": "star", "size": 25, "title": "Algorithm that assigns weights to training examples based on proximity to the query point."}, {"color": "#fcfd17", "id": "MachineLearningConcepts", "label": "MachineLearningConcepts", "shape": "star", "size": 25, "title": "Overview of machine learning concepts including convolutional neural networks and backpropagation."}, {"color": "#cfa536", "id": "PolynomialFeatures", "label": "PolynomialFeatures", "shape": "dot", "size": 10, "title": "Adding polynomial terms to improve model fit."}, {"color": "#0d09d7", "id": "Underfitting", "label": "Underfitting", "shape": "dot", "size": 10, "title": "Occurs when a model is too simple to capture underlying patterns in data."}, {"color": "#03a09f", "id": "Overfitting", "label": "Overfitting", "shape": "dot", "size": 10, "title": "Result of using too complex models, needs proper model complexity for optimal bias-variance tradeoff"}, {"color": "#870f8a", "id": "FeatureSelection", "label": "FeatureSelection", "shape": "dot", "size": 10, "title": "Choosing relevant features to improve model performance."}, {"color": "#e62132", "id": "WeightsCalculation", "label": "WeightsCalculation", "shape": "dot", "size": 10, "title": "Method for calculating weights using a Gaussian-like function of distance from the query point."}, {"color": "#4375dc", "id": "BandwidthParameter", "label": "BandwidthParameter", "shape": "dot", "size": 10, "title": "Controls how quickly weight decreases with distance, affecting model flexibility and overfitting."}, {"color": "#6265d8", "id": "Machine Learning Concepts", "label": "Machine Learning Concepts", "shape": "star", "size": 25, "title": "General concepts in machine learning including training and performance considerations."}, {"color": "#a9770a", "id": "Locally Weighted Linear Regression", "label": "Locally Weighted Linear Regression", "shape": "dot", "size": 10, "title": "A non-linear learning algorithm for estimating future states based on current state and action."}, {"color": "#bfbe0c", "id": "Classification Problem", "label": "Classification Problem", "shape": "star", "size": 25, "title": "Task of predicting discrete values for input data."}, {"color": "#d189e9", "id": "Binary Classification", "label": "Binary Classification", "shape": "dot", "size": 10, "title": "Example of classifying emails into two categories (spam or not-spam)"}, {"color": "#57f204", "id": "Logistic Regression", "label": "Logistic Regression", "shape": "dot", "size": 10, "title": "Generalizing Newton\u0027s method for multidimensional settings in logistic regression."}, {"color": "#1336ab", "id": "Machine_Learning", "label": "Machine_Learning", "shape": "star", "size": 25, "title": "Study of algorithms and statistical models for computer systems to perform tasks without explicit instructions."}, {"color": "#9ebc67", "id": "Logistic_Regression", "label": "Logistic_Regression", "shape": "dot", "size": 10, "title": "Statistical method for binary classification problems using the logistic function."}, {"color": "#eeecb5", "id": "Linear_Regression", "label": "Linear_Regression", "shape": "dot", "size": 10, "title": "Method for modeling relationships between a scalar response and one or more explanatory variables."}, {"color": "#fc541d", "id": "Logistic_Function", "label": "Logistic_Function", "shape": "dot", "size": 10, "title": "S-shaped curve function used in logistic regression to model probabilities."}, {"color": "#ac19fe", "id": "Sigmoid_Function", "label": "Sigmoid_Function", "shape": "dot", "size": 10, "title": "Alternative name for the logistic function, named due to its S-shape."}, {"color": "#eddec4", "id": "Derivative_of_Sigmoid", "label": "Derivative_of_Sigmoid", "shape": "dot", "size": 10, "title": "Mathematical expression describing how the sigmoid changes with input."}, {"color": "#d21694", "id": "ClassificationModelAssumptions", "label": "ClassificationModelAssumptions", "shape": "dot", "size": 10, "title": "Probabilistic assumptions for classification models in machine learning."}, {"color": "#ba8515", "id": "LogisticRegressionFunction", "label": "LogisticRegressionFunction", "shape": "dot", "size": 10, "title": "Mathematical function representing the probability of class membership given input features."}, {"color": "#0c4e31", "id": "GradientAscentUpdateRule", "label": "GradientAscentUpdateRule", "shape": "dot", "size": 10, "title": "Algorithm for updating model parameters in the direction that increases likelihood."}, {"color": "#40c1d6", "id": "LogisticRegression", "label": "LogisticRegression", "shape": "dot", "size": 10, "title": "Discriminative model used to predict the probability of a binary outcome based on input variables."}, {"color": "#d16830", "id": "GradientAscentRule", "label": "GradientAscentRule", "shape": "dot", "size": 10, "title": "Update rule for parameters in logistic regression."}, {"color": "#01bd11", "id": "LMSUpdateRule", "label": "LMSUpdateRule", "shape": "dot", "size": 10, "title": "Least Mean Squares update rule comparison with logistic gradient ascent."}, {"color": "#030580", "id": "LogisticLossFunction", "label": "LogisticLossFunction", "shape": "dot", "size": 10, "title": "Definition and properties of the logistic loss function."}, {"color": "#92e300", "id": "NegativeLogLikelihood", "label": "NegativeLogLikelihood", "shape": "dot", "size": 10, "title": "Connection between negative log-likelihood and logistic loss."}, {"color": "#a6a18f", "id": "Logit", "label": "Logit", "shape": "dot", "size": 10, "title": "Definition of the logit in relation to logistic regression."}, {"color": "#a094bc", "id": "Logistic Regression Derivation", "label": "Logistic Regression Derivation", "shape": "dot", "size": 10, "title": "Derivation and interpretation of logistic regression equations."}, {"color": "#ebf89a", "id": "Perceptron Learning Algorithm", "label": "Perceptron Learning Algorithm", "shape": "dot", "size": 10, "title": "Historical algorithm for binary classification with a threshold function."}, {"color": "#1ab6df", "id": "Multi-class Classification", "label": "Multi-class Classification", "shape": "dot", "size": 10, "title": "Classification problems where the response variable can take on multiple values."}, {"color": "#1d28fc", "id": "Multinomial Distribution", "label": "Multinomial Distribution", "shape": "dot", "size": 10, "title": "Probability distribution over non-negative integers used for modeling features with multiple categories"}, {"color": "#436614", "id": "Parameterized Model", "label": "Parameterized Model", "shape": "dot", "size": 10, "title": "Model outputs probabilities for each of the k outcomes given input x"}, {"color": "#d78b58", "id": "Softmax Function", "label": "Softmax Function", "shape": "dot", "size": 10, "title": "Function that transforms a vector of real numbers into a probability distribution"}, {"color": "#3a5718", "id": "Logits", "label": "Logits", "shape": "dot", "size": 10, "title": "Inputs to the softmax function before transformation"}, {"color": "#f9983a", "id": "Probability Vector", "label": "Probability Vector", "shape": "dot", "size": 10, "title": "Output of softmax, a vector with nonnegative entries summing to 1"}, {"color": "#7eeb3c", "id": "Probabilistic Model", "label": "Probabilistic Model", "shape": "star", "size": 25, "title": "Model using softmax outputs as probabilities for different outcomes given input x and parameters \u03b8"}, {"color": "#575ba9", "id": "Negative Log-Likelihood", "label": "Negative Log-Likelihood", "shape": "dot", "size": 10, "title": "Measure of model\u0027s performance on a single example (x,y)"}, {"color": "#67994e", "id": "Loss Function", "label": "Loss Function", "shape": "star", "size": 25, "title": "Function measuring discrepancy between predicted and actual values."}, {"color": "#aa40a5", "id": "Cross-Entropy Loss", "label": "Cross-Entropy Loss", "shape": "dot", "size": 10, "title": "Alternative definition for the loss function using cross-entropy"}, {"color": "#ac87cc", "id": "Machine_Learning_Concepts", "label": "Machine_Learning_Concepts", "shape": "star", "size": 25, "title": "Overview of concepts in machine learning including loss functions and optimization."}, {"color": "#646aae", "id": "Cross_Entropy_Loss", "label": "Cross_Entropy_Loss", "shape": "dot", "size": 10, "title": "Function used to measure the performance of a classification model whose output is a probability value between 0 and 1."}, {"color": "#2ef438", "id": "Softmax_Function", "label": "Softmax_Function", "shape": "dot", "size": 10, "title": "Transformation that converts raw scores into probabilities for each class in multi-class classification problems."}, {"color": "#4e6a65", "id": "Gradient_Calculation", "label": "Gradient_Calculation", "shape": "dot", "size": 10, "title": "Derivation of gradients used in backpropagation to update model parameters during training."}, {"color": "#d9756d", "id": "Loss_Functions", "label": "Loss_Functions", "shape": "dot", "size": 10, "title": "Quantitative measures used to evaluate the performance of a model during training."}, {"color": "#c8cf9c", "id": "Newton_Method", "label": "Newton_Method", "shape": "dot", "size": 10, "title": "Optimization algorithm that uses second-order derivatives to find the minimum of a function."}, {"color": "#032ff7", "id": "Newton\u0027s Method", "label": "Newton\u0027s Method", "shape": "star", "size": 25, "title": "An iterative method to find the roots of a real-valued function."}, {"color": "#9988c0", "id": "Finding Roots", "label": "Finding Roots", "shape": "dot", "size": 10, "title": "Using Newton\u0027s method to solve for \u03b8 where f(\u03b8) = 0."}, {"color": "#160643", "id": "Maximizing Functions", "label": "Maximizing Functions", "shape": "dot", "size": 10, "title": "Applying Newton\u0027s method to maximize a function by finding its critical points."}, {"color": "#89ffea", "id": "Hessian Matrix", "label": "Hessian Matrix", "shape": "dot", "size": 10, "title": "Matrix of second-order partial derivatives used to generalize Newton\u0027s method."}, {"color": "#cf8866", "id": "Convergence Rate", "label": "Convergence Rate", "shape": "dot", "size": 10, "title": "Newton\u0027s method typically converges faster than gradient descent, requiring fewer iterations."}, {"color": "#76d318", "id": "OptimizationMethods", "label": "OptimizationMethods", "shape": "dot", "size": 10, "title": "Techniques for finding optimal parameters in models such as Newton\u0027s method."}, {"color": "#a32454", "id": "HessianMatrix", "label": "HessianMatrix", "shape": "dot", "size": 10, "title": "Second-order derivative matrix used in optimization methods like Newton\u0027s method."}, {"color": "#8edccd", "id": "FisherScoring", "label": "FisherScoring", "shape": "dot", "size": 10, "title": "Application of Newton\u0027s method to maximize logistic regression likelihood."}, {"color": "#41a0a5", "id": "GeneralizedLinearModels", "label": "GeneralizedLinearModels", "shape": "star", "size": 25, "title": "Family of models that includes both regression and classification methods."}, {"color": "#ee811e", "id": "ExponentialFamilyDistributions", "label": "ExponentialFamilyDistributions", "shape": "dot", "size": 10, "title": "A class of probability distributions that includes many common distributions such as Poisson, binomial, etc."}, {"color": "#a2a678", "id": "NaturalParameter", "label": "NaturalParameter", "shape": "dot", "size": 10, "title": "The parameter \u03b7 that characterizes the distribution in exponential form."}, {"color": "#0a902b", "id": "SufficientStatistic", "label": "SufficientStatistic", "shape": "dot", "size": 10, "title": "A function T(y) summarizing data relevant to parameter estimation."}, {"color": "#de5434", "id": "LogPartitionFunction", "label": "LogPartitionFunction", "shape": "dot", "size": 10, "title": "The function a(\u03b7) ensuring the distribution sums/integrates to 1."}, {"color": "#1be2c7", "id": "BernoulliDistribution", "label": "BernoulliDistribution", "shape": "star", "size": 25, "title": "A binary random variable with parameter \u03c6 representing success probability."}, {"color": "#4f7c2f", "id": "NaturalParameterOfBernoulli", "label": "NaturalParameterOfBernoulli", "shape": "dot", "size": 10, "title": "\u03b7 = log(\u03c6/(1-\u03c6)), also known as the sigmoid function."}, {"color": "#dff461", "id": "SufficientStatisticForBernoulli", "label": "SufficientStatisticForBernoulli", "shape": "dot", "size": 10, "title": "T(y) = y, indicating that the statistic is simply the outcome itself."}, {"color": "#25268a", "id": "LogPartitionFunctionOfBernoulli", "label": "LogPartitionFunctionOfBernoulli", "shape": "dot", "size": 10, "title": "a(\u03b7) = log(1 + e^\u03b7), ensuring normalization of the distribution."}, {"color": "#bd5b07", "id": "MachineLearningModels", "label": "MachineLearningModels", "shape": "star", "size": 25, "title": "Overview of machine learning models including GDA and logistic regression."}, {"color": "#18acbb", "id": "GLMFormulation", "label": "GLMFormulation", "shape": "dot", "size": 10, "title": "Formulation of Generalized Linear Models (GLMs) using exponential family distributions."}, {"color": "#71b783", "id": "OtherDistributions", "label": "OtherDistributions", "shape": "dot", "size": 10, "title": "Overview of other distributions in exponential family such as Poisson, Gamma, etc."}, {"color": "#b4cc63", "id": "ConstructingGLMs", "label": "ConstructingGLMs", "shape": "star", "size": 25, "title": "Process for constructing GLMs using various distributions."}, {"color": "#d439ae", "id": "PoissonDistribution", "label": "PoissonDistribution", "shape": "dot", "size": 10, "title": "A statistical distribution used for modeling count data such as website visitors."}, {"color": "#8bb36e", "id": "GeneralizedLinearModel", "label": "GeneralizedLinearModel", "shape": "dot", "size": 10, "title": "Models that extend linear regression to accommodate non-normal error distributions and nonlinear relationships."}, {"color": "#834e0a", "id": "GLMAssumptions", "label": "GLMAssumptions", "shape": "dot", "size": 10, "title": "Three key assumptions made when constructing GLMs: conditional distribution form, prediction goal, and linear relationship between natural parameter and input variables."}, {"color": "#8c7c84", "id": "GeneralizedLinearModelsGLMs", "label": "GeneralizedLinearModelsGLMs", "shape": "dot", "size": 10, "title": "A class of statistical models that includes linear regression, logistic regression, etc."}, {"color": "#49b979", "id": "AssumptionsOfGLMs", "label": "AssumptionsOfGLMs", "shape": "dot", "size": 10, "title": "Three assumptions/design choices for deriving GLM algorithms."}, {"color": "#3cf23e", "id": "OrdinaryLeastSquaresOLS", "label": "OrdinaryLeastSquaresOLS", "shape": "dot", "size": 10, "title": "A special case of GLMs where the target variable is continuous and modeled as Gaussian."}, {"color": "#a35dfe", "id": "Machine Learning Models", "label": "Machine Learning Models", "shape": "star", "size": 25, "title": "Models used in machine learning for prediction and decision-making."}, {"color": "#3c9dd6", "id": "Conditional Distribution Modeling", "label": "Conditional Distribution Modeling", "shape": "dot", "size": 10, "title": "Modeling the distribution of y given x."}, {"color": "#886e46", "id": "Bernoulli Distribution", "label": "Bernoulli Distribution", "shape": "dot", "size": 10, "title": "Probability distribution for binary outcomes."}, {"color": "#ce53c6", "id": "Exponential Family Distributions", "label": "Exponential Family Distributions", "shape": "dot", "size": 10, "title": "Family of distributions including Bernoulli, Gaussian, etc."}, {"color": "#4525e0", "id": "Hypothesis Function", "label": "Hypothesis Function", "shape": "dot", "size": 10, "title": "Function predicting the expected value of y given x."}, {"color": "#349201", "id": "Canonical Response Function", "label": "Canonical Response Function", "shape": "dot", "size": 10, "title": "Function relating natural parameter to distribution mean."}, {"color": "#71a0af", "id": "Canonical Link Function", "label": "Canonical Link Function", "shape": "dot", "size": 10, "title": "Inverse of canonical response function, maps x to natural parameter."}, {"color": "#ccb5af", "id": "MachineLearningAlgorithms", "label": "MachineLearningAlgorithms", "shape": "star", "size": 25, "title": "Overview of different types of machine learning algorithms."}, {"color": "#e98f73", "id": "DiscriminativeAlgorithms", "label": "DiscriminativeAlgorithms", "shape": "dot", "size": 10, "title": "Algorithms that learn p(y|x) directly to distinguish between classes."}, {"color": "#dc366a", "id": "PerceptronAlgorithm", "label": "PerceptronAlgorithm", "shape": "dot", "size": 10, "title": "Linear classifier that separates data with a decision boundary."}, {"color": "#6416f1", "id": "GenerativeAlgorithms", "label": "GenerativeAlgorithms", "shape": "dot", "size": 10, "title": "Model p(x|y) and p(y) to derive posterior distribution on y given x."}, {"color": "#6cd580", "id": "ClassPriors", "label": "ClassPriors", "shape": "dot", "size": 10, "title": "Probability of each class before observing data."}, {"color": "#2cf38a", "id": "BayesRule", "label": "BayesRule", "shape": "dot", "size": 10, "title": "Uses p(x|y) and p(y) to calculate posterior probability p(y|x)."}, {"color": "#da0925", "id": "Bayes Rule Application", "label": "Bayes Rule Application", "shape": "star", "size": 25, "title": "Using Bayes rule to derive posterior distribution on y given x."}, {"color": "#3b564e", "id": "Class Priors", "label": "Class Priors", "shape": "dot", "size": 10, "title": "Probability of each class before observing data."}, {"color": "#44a126", "id": "Conditional Probability p(x|y)", "label": "Conditional Probability p(x|y)", "shape": "dot", "size": 10, "title": "Distribution of features given the class label."}, {"color": "#b5cb1f", "id": "Gaussian Discriminant Analysis (GDA)", "label": "Gaussian Discriminant Analysis (GDA)", "shape": "star", "size": 25, "title": "Generative learning algorithm assuming p(x|y) is multivariate normal distribution."}, {"color": "#edd732", "id": "Multivariate Normal Distribution", "label": "Multivariate Normal Distribution", "shape": "dot", "size": 10, "title": "Distribution parameterized by mean vector and covariance matrix."}, {"color": "#218e57", "id": "Mean Vector", "label": "Mean Vector", "shape": "dot", "size": 10, "title": "Vector representing the expected value of each feature in d-dimensions."}, {"color": "#5ed75e", "id": "Covariance Matrix", "label": "Covariance Matrix", "shape": "dot", "size": 10, "title": "Matrix describing the variance and covariance between features."}, {"color": "#ce764c", "id": "Random_Variables", "label": "Random_Variables", "shape": "dot", "size": 10, "title": "Introduction to random variables and their properties."}, {"color": "#067cf6", "id": "Normal_Distribution", "label": "Normal_Distribution", "shape": "dot", "size": 10, "title": "Properties of the normal distribution in machine learning."}, {"color": "#c2c7a1", "id": "Mean", "label": "Mean", "shape": "dot", "size": 10, "title": "Definition and calculation of mean for a normal distribution."}, {"color": "#b5a6a8", "id": "Covariance_Matrix", "label": "Covariance_Matrix", "shape": "dot", "size": 10, "title": "Explanation of covariance matrix in the context of multivariate distributions."}, {"color": "#3dfccc", "id": "Standard_Normal_Distribution", "label": "Standard_Normal_Distribution", "shape": "dot", "size": 10, "title": "Definition and properties of standard normal distribution."}, {"color": "#a9e5b4", "id": "Density_Properties", "label": "Density_Properties", "shape": "dot", "size": 10, "title": "Properties of density functions for different covariance matrices."}, {"color": "#6c66a5", "id": "CovarianceMatrixEffects", "label": "CovarianceMatrixEffects", "shape": "dot", "size": 10, "title": "Exploration of how varying the covariance matrix affects density contours."}, {"color": "#35fb2d", "id": "GaussianDiscriminantAnalysis", "label": "GaussianDiscriminantAnalysis", "shape": "dot", "size": 10, "title": "Model that assumes data is generated from a multivariate Gaussian distribution."}, {"color": "#f474c3", "id": "GDAParameters", "label": "GDAParameters", "shape": "dot", "size": 10, "title": "Model parameters including phi, mu_0, mu_1, and Sigma."}, {"color": "#317b58", "id": "MLEstimation", "label": "MLEstimation", "shape": "dot", "size": 10, "title": "Process to find maximum likelihood estimates for phi, mu_0, mu_1, and Sigma."}, {"color": "#176606", "id": "DecisionBoundary", "label": "DecisionBoundary", "shape": "dot", "size": 10, "title": "The boundary that separates classes in a classification problem."}, {"color": "#86db0f", "id": "RelationshipToLogisticRegression", "label": "RelationshipToLogisticRegression", "shape": "dot", "size": 10, "title": "Connection between GDA and logistic regression models."}, {"color": "#a5ed40", "id": "DecisionBoundaries", "label": "DecisionBoundaries", "shape": "dot", "size": 10, "title": "Boundaries that separate different classes in feature space."}, {"color": "#c816e9", "id": "ModelAssumptions", "label": "ModelAssumptions", "shape": "dot", "size": 10, "title": "Theoretical assumptions made by models about the data generation process."}, {"color": "#de5e55", "id": "AsymptoticEfficiency", "label": "AsymptoticEfficiency", "shape": "dot", "size": 10, "title": "Property of GDA that ensures optimal performance with large datasets under correct assumptions."}, {"color": "#d8b488", "id": "GDA", "label": "GDA", "shape": "dot", "size": 10, "title": "Generative Discriminative Algorithm with strong modeling assumptions."}, {"color": "#5b8c2e", "id": "RobustnessToAssumptions", "label": "RobustnessToAssumptions", "shape": "dot", "size": 10, "title": "Comparison of GDA and Logistic Regression in terms of assumption sensitivity."}, {"color": "#0170eb", "id": "NaiveBayes", "label": "NaiveBayes", "shape": "star", "size": 25, "title": "Classification algorithm for discrete-valued features (optional reading)."}, {"color": "#7f11d7", "id": "DiscreteFeatures", "label": "DiscreteFeatures", "shape": "dot", "size": 10, "title": "Introduction to Naive Bayes with focus on discrete feature vectors."}, {"color": "#a76d94", "id": "EmailSpamFiltering", "label": "EmailSpamFiltering", "shape": "dot", "size": 10, "title": "Example application of Naive Bayes in email spam detection."}, {"color": "#87f4c3", "id": "Text_Classification", "label": "Text_Classification", "shape": "dot", "size": 10, "title": "Techniques for classifying text into categories such as spam or non-spam emails."}, {"color": "#1eaf46", "id": "Spam_Filtering", "label": "Spam_Filtering", "shape": "dot", "size": 10, "title": "Application of machine learning to distinguish spam from non-spam emails."}, {"color": "#ed6f78", "id": "Training_Set", "label": "Training_Set", "shape": "dot", "size": 10, "title": "Set of labeled examples used to train a machine learning model."}, {"color": "#dc67b5", "id": "Feature_Vector", "label": "Feature_Vector", "shape": "dot", "size": 10, "title": "Vector representation of an email based on its words."}, {"color": "#bb0771", "id": "Vocabulary", "label": "Vocabulary", "shape": "dot", "size": 10, "title": "Set of unique words used to create the feature vector."}, {"color": "#0db812", "id": "Stop_Words", "label": "Stop_Words", "shape": "dot", "size": 10, "title": "Commonly occurring words removed from analysis due to lack of content value."}, {"color": "#c359d7", "id": "Machine_Learning_Topic", "label": "Machine_Learning_Topic", "shape": "star", "size": 25, "title": "Overview of machine learning concepts and techniques."}, {"color": "#4b67aa", "id": "Feature_Vector_Selection", "label": "Feature_Vector_Selection", "shape": "dot", "size": 10, "title": "Process of selecting relevant features from the text, excluding stop words."}, {"color": "#3eb035", "id": "Generative_Modeling", "label": "Generative_Modeling", "shape": "dot", "size": 10, "title": "Building models that generate data similar to the training set, focusing on conditional probabilities."}, {"color": "#42bae2", "id": "Naive_Bayes_Assumption", "label": "Naive_Bayes_Assumption", "shape": "dot", "size": 10, "title": "Assumption of conditional independence between features given a class label in Naive Bayes classifiers."}, {"color": "#9c00fe", "id": "Conditional_Independence", "label": "Conditional_Independence", "shape": "dot", "size": 10, "title": "Concept that variables are independent when conditioned on another variable, specifically the class label in text classification."}, {"color": "#11b5be", "id": "NaiveBayesAlgorithm", "label": "NaiveBayesAlgorithm", "shape": "star", "size": 25, "title": "A probabilistic classifier based on applying Bayes\u0027 theorem with strong independence assumptions between the features."}, {"color": "#efbd0a", "id": "ConditionalProbability", "label": "ConditionalProbability", "shape": "dot", "size": 10, "title": "Calculates probability of features given a class label under Naive Bayes assumption."}, {"color": "#7d1781", "id": "ParameterEstimation", "label": "ParameterEstimation", "shape": "dot", "size": 10, "title": "Involves estimating parameters from training data to maximize likelihood."}, {"color": "#27feb2", "id": "Prediction", "label": "Prediction", "shape": "dot", "size": 10, "title": "Uses estimated parameters and Bayes\u0027 theorem for class prediction on new examples."}, {"color": "#662f57", "id": "Machine Learning Algorithms", "label": "Machine Learning Algorithms", "shape": "star", "size": 25, "title": "Collection of methods for learning from data to make predictions or decisions."}, {"color": "#5f3237", "id": "Naive Bayes Algorithm", "label": "Naive Bayes Algorithm", "shape": "dot", "size": 10, "title": "Probabilistic classifier based on applying Bayes\u0027 theorem with strong independence assumptions between the features"}, {"color": "#d8c908", "id": "Binary Features", "label": "Binary Features", "shape": "dot", "size": 10, "title": "Features that can take only two values, typically 0 or 1"}, {"color": "#9c57d9", "id": "Feature Discretization", "label": "Feature Discretization", "shape": "dot", "size": 10, "title": "Process of converting continuous-valued features into discrete values"}, {"color": "#1dd8cb", "id": "Laplace Smoothing", "label": "Laplace Smoothing", "shape": "dot", "size": 10, "title": "Technique to prevent zero probabilities in Naive Bayes by adding a small constant to counts"}, {"color": "#a407e5", "id": "Spam Classification", "label": "Spam Classification", "shape": "dot", "size": 10, "title": "Application of Naive Bayes for distinguishing between spam and non-spam emails"}, {"color": "#6e7012", "id": "Naive_Bayes_Classifier", "label": "Naive_Bayes_Classifier", "shape": "dot", "size": 10, "title": "A probabilistic classifier based on applying Bayes\u0027 theorem with strong independence assumptions between the features."}, {"color": "#73e3ac", "id": "NeurIPS_Conference", "label": "NeurIPS_Conference", "shape": "star", "size": 25, "title": "One of the top machine learning conferences where researchers submit their work for publication."}, {"color": "#78d57d", "id": "Training_Set_Issues", "label": "Training_Set_Issues", "shape": "dot", "size": 10, "title": "Challenges related to training sets in Naive Bayes classifiers when encountering new words like \u0027neurips\u0027."}, {"color": "#73405c", "id": "ProbabilityEstimation", "label": "ProbabilityEstimation", "shape": "star", "size": 25, "title": "Avoiding estimation of probabilities as zero in finite training sets."}, {"color": "#a93d68", "id": "MultinomialRandomVariable", "label": "MultinomialRandomVariable", "shape": "dot", "size": 10, "title": "Parameterizing a multinomial random variable with probabilities \u03c6_j."}, {"color": "#c96f56", "id": "MaximumLikelihoodEstimates", "label": "MaximumLikelihoodEstimates", "shape": "dot", "size": 10, "title": "Estimating parameters using maximum likelihood from independent observations."}, {"color": "#9c2bcc", "id": "LaplaceSmoothing", "label": "LaplaceSmoothing", "shape": "dot", "size": 10, "title": "Adding a constant to numerator and denominator of probability estimates to avoid zero probabilities."}, {"color": "#c91b86", "id": "NaiveBayesClassifier", "label": "NaiveBayesClassifier", "shape": "star", "size": 25, "title": "Using Laplace smoothing in the Naive Bayes classifier for better parameter estimation."}, {"color": "#532c4a", "id": "EventModelsTextClassification", "label": "EventModelsTextClassification", "shape": "dot", "size": 10, "title": "Application of event models to text classification problems with Laplace smoothing."}, {"color": "#430565", "id": "EventModelsForTextClassification", "label": "EventModelsForTextClassification", "shape": "star", "size": 25, "title": "Overview of event models for text classification in machine learning."}, {"color": "#3b3aaa", "id": "BernoulliEventModel", "label": "BernoulliEventModel", "shape": "dot", "size": 10, "title": "A model where each word is included independently according to probabilities given the class."}, {"color": "#d2dd19", "id": "MultinomialEventModel", "label": "MultinomialEventModel", "shape": "dot", "size": 10, "title": "An alternative model using a different notation and feature set for representing emails."}, {"color": "#9dce5f", "id": "Machine_Learning_Models", "label": "Machine_Learning_Models", "shape": "star", "size": 25, "title": "Models used in machine learning for sequence prediction and natural language processing."}, {"color": "#dc1c6a", "id": "Multinomial_Event_Model", "label": "Multinomial_Event_Model", "shape": "dot", "size": 10, "title": "A model where each event is a draw from a multinomial distribution over words."}, {"color": "#c842e7", "id": "Spam_Detection", "label": "Spam_Detection", "shape": "dot", "size": 10, "title": "Application of the multinomial event model to classify emails as spam or non-spam."}, {"color": "#cd2396", "id": "Parameter_Estimation", "label": "Parameter_Estimation", "shape": "dot", "size": 10, "title": "Process of estimating parameters in machine learning models from training data."}, {"color": "#65bba1", "id": "Likelihood_Function", "label": "Likelihood_Function", "shape": "dot", "size": 10, "title": "Function used to measure how well a set of parameter values fits the observed data."}, {"color": "#01257e", "id": "Laplace_Smoothing", "label": "Laplace_Smoothing", "shape": "dot", "size": 10, "title": "Technique to prevent zero probability estimates by adding a small constant to observed counts."}, {"color": "#6f007f", "id": "Kernel_Methods", "label": "Kernel_Methods", "shape": "star", "size": 25, "title": "Techniques for handling non-linear relationships in data using feature maps and kernels."}, {"color": "#bd4da9", "id": "Feature_Maps", "label": "Feature_Maps", "shape": "dot", "size": 10, "title": "Transformation of input features to a higher-dimensional space to capture non-linear patterns."}, {"color": "#5d6817", "id": "Cubic_Features", "label": "Cubic_Features", "shape": "dot", "size": 10, "title": "Example feature transformation for fitting cubic functions to data."}, {"color": "#4379ec", "id": "FeatureMap", "label": "FeatureMap", "shape": "dot", "size": 10, "title": "Mapping from input attributes to feature variables."}, {"color": "#3d98bd", "id": "CubicFunctionRepresentation", "label": "CubicFunctionRepresentation", "shape": "dot", "size": 10, "title": "Representing a cubic function as a linear function over features."}, {"color": "#d123a0", "id": "LinearFunctionOverFeatures", "label": "LinearFunctionOverFeatures", "shape": "dot", "size": 10, "title": "Rewriting the model using feature variables."}, {"color": "#0d3772", "id": "LMSAlgorithmWithFeatures", "label": "LMSAlgorithmWithFeatures", "shape": "star", "size": 25, "title": "Derivation of LMS algorithm for fitting models with features."}, {"color": "#b05af7", "id": "BatchGradientDescentUpdate", "label": "BatchGradientDescentUpdate", "shape": "dot", "size": 10, "title": "Updating the model parameters using batch gradient descent."}, {"color": "#475d01", "id": "FeatureMapping", "label": "FeatureMapping", "shape": "dot", "size": 10, "title": "Transformation of input data into a higher-dimensional space for better separability."}, {"color": "#b64ee5", "id": "GradientDescentAlgorithm", "label": "GradientDescentAlgorithm", "shape": "dot", "size": 10, "title": "Optimization technique for minimizing cost functions in machine learning models."}, {"color": "#bf738c", "id": "KernelTrick", "label": "KernelTrick", "shape": "dot", "size": 10, "title": "Technique for efficiently computing high-dimensional feature mappings without explicitly calculating them."}, {"color": "#aedec3", "id": "ComputationalComplexity", "label": "ComputationalComplexity", "shape": "dot", "size": 10, "title": "Analysis of the computational cost associated with different machine learning algorithms and techniques."}, {"color": "#c9a778", "id": "KernelTrickIntroduction", "label": "KernelTrickIntroduction", "shape": "dot", "size": 10, "title": "Explanation of the kernel trick in machine learning for efficient computation."}, {"color": "#c1421e", "id": "PhiFunctionExpansion", "label": "PhiFunctionExpansion", "shape": "dot", "size": 10, "title": "Description and expansion of the \u03c6(x) function used in the kernel method."}, {"color": "#6ce0c7", "id": "ThetaVectorInitialization", "label": "ThetaVectorInitialization", "shape": "dot", "size": 10, "title": "Details on initializing the \u03b8 vector to zero for simplicity."}, {"color": "#7fc673", "id": "IterativeUpdateRule", "label": "IterativeUpdateRule", "shape": "dot", "size": 10, "title": "Explanation of iterative update rules in the context of kernel methods."}, {"color": "#b4ce9e", "id": "LinearCombinationRepresentation", "label": "LinearCombinationRepresentation", "shape": "dot", "size": 10, "title": "Description of how \u03b8 can be represented as a linear combination of \u03c6(x) vectors."}, {"color": "#c1a909", "id": "InductiveProofOfRepresentation", "label": "InductiveProofOfRepresentation", "shape": "dot", "size": 10, "title": "Inductive proof showing that \u03b8 remains a linear combination after updates."}, {"color": "#6ce3f6", "id": "UpdateRuleForCoefficients", "label": "UpdateRuleForCoefficients", "shape": "dot", "size": 10, "title": "Derivation of the update rule for coefficients in the linear combination representation."}, {"color": "#a69c50", "id": "MachineLearningAlgorithm", "label": "MachineLearningAlgorithm", "shape": "star", "size": 25, "title": "Overview of machine learning algorithms and their implementations."}, {"color": "#726eba", "id": "BetaUpdateEquation", "label": "BetaUpdateEquation", "shape": "dot", "size": 10, "title": "The equation used to update \u03b2 during each iteration of batch gradient descent."}, {"color": "#ea0770", "id": "ThetaRepresentation", "label": "ThetaRepresentation", "shape": "dot", "size": 10, "title": "Expression for \u03b8 in terms of feature vectors and their inner products."}, {"color": "#e1ca94", "id": "InnerProductEfficiency", "label": "InnerProductEfficiency", "shape": "dot", "size": 10, "title": "Discussion on the efficiency of computing inner products between feature maps."}, {"color": "#187aca", "id": "FeatureMapsAndKernels", "label": "FeatureMapsAndKernels", "shape": "dot", "size": 10, "title": "Discussion on feature maps and their corresponding kernel functions."}, {"color": "#f30d76", "id": "KernelFunctionDefinition", "label": "KernelFunctionDefinition", "shape": "dot", "size": 10, "title": "Definition of the kernel function based on inner products in a transformed space."}, {"color": "#1db7e2", "id": "EfficientComputationOfKernels", "label": "EfficientComputationOfKernels", "shape": "dot", "size": 10, "title": "Algorithm for computing kernel values efficiently using equation (5.9)."}, {"color": "#023616", "id": "UpdateRepresentationBeta", "label": "UpdateRepresentationBeta", "shape": "dot", "size": 10, "title": "Process of updating the representation beta with O(n) time per update."}, {"color": "#c97aca", "id": "PredictionUsingKernelFunction", "label": "PredictionUsingKernelFunction", "shape": "dot", "size": 10, "title": "Explanation on how to predict using the kernel function and representation beta."}, {"color": "#c213df", "id": "Kernels_in_Machine_Learning", "label": "Kernels_in_Machine_Learning", "shape": "dot", "size": 10, "title": "Discussion on kernel functions and their properties in machine learning algorithms."}, {"color": "#74b3b2", "id": "Feature_Map_Phi", "label": "Feature_Map_Phi", "shape": "dot", "size": 10, "title": "Introduction to the feature map \u03c6 that induces a kernel function K(x,z)."}, {"color": "#3b668d", "id": "Kernel_Function_K", "label": "Kernel_Function_K", "shape": "dot", "size": 10, "title": "Definition and properties of the kernel function K(\u22c5,\u22c5) in machine learning."}, {"color": "#7031be", "id": "Algorithm_Independence_of_Phi", "label": "Algorithm_Independence_of_Phi", "shape": "dot", "size": 10, "title": "Explanation that algorithms can operate without explicit feature map \u03c6, relying solely on K(\u22c5,\u22c5)."}, {"color": "#3585dd", "id": "Characterization_of_Valid_Kernels", "label": "Characterization_of_Valid_Kernels", "shape": "dot", "size": 10, "title": "Discussion on how to characterize valid kernel functions that correspond to some feature map \u03c6."}, {"color": "#ce4d0c", "id": "Concrete_Examples_of_Kernels", "label": "Concrete_Examples_of_Kernels", "shape": "dot", "size": 10, "title": "Exploration of specific examples like K(x,z)=(x^Tz)^2 to illustrate valid kernels."}, {"color": "#3c31e7", "id": "KernelFunctions", "label": "KernelFunctions", "shape": "dot", "size": 10, "title": "Functions used in machine learning for pattern analysis."}, {"color": "#73f8fd", "id": "PolynomialKernels", "label": "PolynomialKernels", "shape": "dot", "size": 10, "title": "A type of kernel function that maps input data into a higher-dimensional space."}, {"color": "#ea3ff9", "id": "ComputationalEfficiency", "label": "ComputationalEfficiency", "shape": "dot", "size": 10, "title": "Discussion on the efficiency of calculating kernel functions vs. direct computation in high-dimensional spaces."}, {"color": "#1fd7e8", "id": "KernelsAsSimilarityMetrics", "label": "KernelsAsSimilarityMetrics", "shape": "dot", "size": 10, "title": "Using kernels as a measure of similarity between data points."}, {"color": "#7359bf", "id": "GaussianKernel", "label": "GaussianKernel", "shape": "dot", "size": 10, "title": "A specific kernel function that measures the similarity based on the Gaussian distribution."}, {"color": "#42554a", "id": "ValidKernelConditions", "label": "ValidKernelConditions", "shape": "dot", "size": 10, "title": "Criteria a function must meet to be considered a valid kernel in machine learning."}, {"color": "#2deeb5", "id": "Kernel Functions", "label": "Kernel Functions", "shape": "star", "size": 25, "title": "Functions that satisfy certain properties to be valid kernels."}, {"color": "#084b69", "id": "Necessary Conditions for Valid Kernels", "label": "Necessary Conditions for Valid Kernels", "shape": "dot", "size": 10, "title": "Properties a kernel must have to be considered valid."}, {"color": "#4abef8", "id": "Symmetry Property", "label": "Symmetry Property", "shape": "dot", "size": 10, "title": "A valid kernel matrix is symmetric."}, {"color": "#63711a", "id": "Positive Semi-Definite Property", "label": "Positive Semi-Definite Property", "shape": "dot", "size": 10, "title": "A valid kernel matrix must be positive semi-definite."}, {"color": "#84d479", "id": "Feature Mapping", "label": "Feature Mapping", "shape": "dot", "size": 10, "title": "Mapping from input space to a higher-dimensional feature space."}, {"color": "#3a1ae5", "id": "Kernel Matrix", "label": "Kernel Matrix", "shape": "dot", "size": 10, "title": "Matrix representation of kernel function values between points."}, {"color": "#bd92ec", "id": "Kernel_Matrix", "label": "Kernel_Matrix", "shape": "star", "size": 25, "title": "Symmetric positive semidefinite matrix corresponding to kernel function."}, {"color": "#71128c", "id": "Valid_Kernels_Conditions", "label": "Valid_Kernels_Conditions", "shape": "dot", "size": 10, "title": "Conditions for a function K to be considered a valid Mercer kernel."}, {"color": "#e67ba2", "id": "Mercer_Theorem", "label": "Mercer_Theorem", "shape": "dot", "size": 10, "title": "Necessary and sufficient condition for K to be a valid kernel, involving symmetric positive semidefinite matrices."}, {"color": "#1705e4", "id": "Testing_Valid_Kernel", "label": "Testing_Valid_Kernel", "shape": "dot", "size": 10, "title": "Another way of testing if a given function is a valid kernel using Mercer\u0027s theorem."}, {"color": "#e03ea1", "id": "Examples_of_Kernels", "label": "Examples_of_Kernels", "shape": "star", "size": 25, "title": "Various examples demonstrating the application and effectiveness of different types of kernels in machine learning problems."}, {"color": "#554e01", "id": "Digit_Recognition_Problem", "label": "Digit_Recognition_Problem", "shape": "dot", "size": 10, "title": "Example illustrating the use of polynomial or Gaussian kernel with SVMs for digit recognition tasks."}, {"color": "#97cd09", "id": "String_Classification", "label": "String_Classification", "shape": "dot", "size": 10, "title": "Brief mention of string classification problems, such as classifying amino acid sequences into proteins."}, {"color": "#c17f4d", "id": "Machine_Learning_Techniques", "label": "Machine_Learning_Techniques", "shape": "star", "size": 25, "title": "Techniques used in machine learning for classification and regression."}, {"color": "#a91413", "id": "Feature_Vectors", "label": "Feature_Vectors", "shape": "dot", "size": 10, "title": "Vectors representing features of input data."}, {"color": "#e7db5b", "id": "Support_Vector_Machines", "label": "Support_Vector_Machines", "shape": "star", "size": 25, "title": "Supervised learning algorithm for classification and regression analysis."}, {"color": "#728ece", "id": "String_Features", "label": "String_Features", "shape": "dot", "size": 10, "title": "Features derived from string data such as amino acid sequences."}, {"color": "#1dec96", "id": "Kernel_Tricks", "label": "Kernel_Tricks", "shape": "dot", "size": 10, "title": "Technique to extend linear classifiers into non-linear ones using kernels."}, {"color": "#1d41e0", "id": "Margins_Concept", "label": "Margins_Concept", "shape": "dot", "size": 10, "title": "Introduces the concept of margins in SVMs to separate data with a large gap."}, {"color": "#f960fd", "id": "Optimal_Margin_Classifier", "label": "Optimal_Margin_Classifier", "shape": "dot", "size": 10, "title": "Finding the classifier that maximizes the geometric margin for linearly separable datasets."}, {"color": "#8dde73", "id": "Lagrange_Duality", "label": "Lagrange_Duality", "shape": "dot", "size": 10, "title": "Explains duality theory used in optimization problems of SVMs."}, {"color": "#377868", "id": "Kernels_in_SVM", "label": "Kernels_in_SVM", "shape": "dot", "size": 10, "title": "Describes how kernels enable SVM to work efficiently in high-dimensional spaces."}, {"color": "#573cd4", "id": "SMO_Algorithm", "label": "SMO_Algorithm", "shape": "dot", "size": 10, "title": "Algorithm that updates two alpha values simultaneously to maintain constraint satisfaction."}, {"color": "#820005", "id": "Decision Boundary", "label": "Decision Boundary", "shape": "dot", "size": 10, "title": "A line or hyperplane that separates different classes of data points."}, {"color": "#2b3dd0", "id": "Functional Margins", "label": "Functional Margins", "shape": "dot", "size": 10, "title": "Measure of confidence in classification decisions based on the distance from the decision boundary."}, {"color": "#4c72ec", "id": "Geometric Margins", "label": "Geometric Margins", "shape": "dot", "size": 10, "title": "Distance-based measure to formalize confident predictions far from the separating hyperplane."}, {"color": "#7ee61d", "id": "Training Set", "label": "Training Set", "shape": "dot", "size": 10, "title": "Collection of data points used for training a machine learning model."}, {"color": "#aace9b", "id": "Classification Confidence", "label": "Classification Confidence", "shape": "dot", "size": 10, "title": "Level of certainty in predicting the class label based on distance from decision boundary."}, {"color": "#82fbc1", "id": "Support Vector Machines (SVMs)", "label": "Support Vector Machines (SVMs)", "shape": "dot", "size": 10, "title": "Binary classification algorithm that maximizes the margin between classes."}, {"color": "#cdbc1a", "id": "Notation for SVMs", "label": "Notation for SVMs", "shape": "dot", "size": 10, "title": "Introduction to notation used in discussing SVMs, including parameters w and b."}, {"color": "#0a2b6e", "id": "Functional Margin", "label": "Functional Margin", "shape": "dot", "size": 10, "title": "Definition of functional margin for a training example with respect to classifier parameters."}, {"color": "#ee5d67", "id": "Geometric Margin", "label": "Geometric Margin", "shape": "dot", "size": 10, "title": "Conceptual understanding and formalization of geometric margins in SVM context."}, {"color": "#d73a07", "id": "FunctionalMargin", "label": "FunctionalMargin", "shape": "dot", "size": 10, "title": "Measure indicating the confidence and correctness of predictions for linear classifiers."}, {"color": "#ccbab6", "id": "ConfidenceAndCorrectness", "label": "ConfidenceAndCorrectness", "shape": "dot", "size": 10, "title": "Relationship between functional margin and prediction accuracy."}, {"color": "#5c9738", "id": "ScalingImpact", "label": "ScalingImpact", "shape": "dot", "size": 10, "title": "Effect of scaling weights and bias on the functional margin."}, {"color": "#ae107e", "id": "NormalizationCondition", "label": "NormalizationCondition", "shape": "dot", "size": 10, "title": "Proposed normalization to address issues with scaling in functional margins."}, {"color": "#a95c71", "id": "FunctionMarginTrainingSet", "label": "FunctionMarginTrainingSet", "shape": "dot", "size": 10, "title": "Definition of function margin considering a training set."}, {"color": "#95d316", "id": "GeometricMargins", "label": "GeometricMargins", "shape": "dot", "size": 10, "title": "Introduction to geometric margins in machine learning context."}, {"color": "#fb4f72", "id": "VectorW", "label": "VectorW", "shape": "dot", "size": 10, "title": "A vector orthogonal to the decision boundary."}, {"color": "#3ed88b", "id": "DistanceToBoundary", "label": "DistanceToBoundary", "shape": "dot", "size": 10, "title": "The perpendicular distance from a point to the decision boundary."}, {"color": "#502795", "id": "UnitVectorW", "label": "UnitVectorW", "shape": "dot", "size": 10, "title": "A unit vector in the direction of W."}, {"color": "#573ef0", "id": "TrainingExample", "label": "TrainingExample", "shape": "dot", "size": 10, "title": "An input example with a label used for training."}, {"color": "#4b2d76", "id": "GeometricMargin", "label": "GeometricMargin", "shape": "star", "size": 25, "title": "The perpendicular distance from a data point to the decision boundary scaled by the class label."}, {"color": "#d3a21a", "id": "Parameter_Scaling_Invariance", "label": "Parameter_Scaling_Invariance", "shape": "dot", "size": 10, "title": "Invariance to scaling of parameters w and b."}, {"color": "#dce110", "id": "Geometric_Margin", "label": "Geometric_Margin", "shape": "dot", "size": 10, "title": "Definition and calculation of geometric margin in training data."}, {"color": "#322bfb", "id": "Linear_Separability", "label": "Linear_Separability", "shape": "dot", "size": 10, "title": "Condition where positive and negative examples can be separated by a hyperplane."}, {"color": "#534da5", "id": "Maximize_Geometric_Margin_Optimization", "label": "Maximize_Geometric_Margin_Optimization", "shape": "dot", "size": 10, "title": "Formulation of optimization problem to maximize geometric margin."}, {"color": "#a23a95", "id": "Support Vector Machines (SVM)", "label": "Support Vector Machines (SVM)", "shape": "star", "size": 25, "title": "Machine learning model for classification and regression analysis."}, {"color": "#41199b", "id": "Optimization Problem", "label": "Optimization Problem", "shape": "dot", "size": 10, "title": "Formulation of SVM optimization with quadratic objective and linear constraints."}, {"color": "#0a99b4", "id": "Non-Convex Constraint", "label": "Non-Convex Constraint", "shape": "dot", "size": 10, "title": "Constraint that makes the optimization problem difficult to solve."}, {"color": "#940748", "id": "Scaling Constraint", "label": "Scaling Constraint", "shape": "dot", "size": 10, "title": "Constraint used to simplify the optimization problem by scaling parameters."}, {"color": "#b72e2a", "id": "Convex Quadratic Objective", "label": "Convex Quadratic Objective", "shape": "dot", "size": 10, "title": "Objective function to minimize in the SVM problem."}, {"color": "#cb4ede", "id": "Linear Constraints", "label": "Linear Constraints", "shape": "dot", "size": 10, "title": "Constraints ensuring data points are correctly classified with a margin of at least 1."}, {"color": "#4a96ea", "id": "Optimal Margin Classifier", "label": "Optimal Margin Classifier", "shape": "dot", "size": 10, "title": "Resulting classifier from solving the optimization problem."}, {"color": "#c5fb1a", "id": "Lagrange Duality", "label": "Lagrange Duality", "shape": "star", "size": 25, "title": "Theory explaining how to find dual form of constrained optimization problems."}, {"color": "#c8a242", "id": "Dual Formulation", "label": "Dual Formulation", "shape": "dot", "size": 10, "title": "Alternative formulation allowing efficient computation in high-dimensional spaces."}, {"color": "#80b994", "id": "Lagrangian Function", "label": "Lagrangian Function", "shape": "dot", "size": 10, "title": "Combination of objective function and constraints using Lagrange multipliers."}, {"color": "#4720a2", "id": "Lagrange Multipliers", "label": "Lagrange Multipliers", "shape": "dot", "size": 10, "title": "Coefficients used in the Lagrangian to enforce equality constraints."}, {"color": "#68bc98", "id": "ConstrainedOptimization", "label": "ConstrainedOptimization", "shape": "star", "size": 25, "title": "Generalization of optimization problems with equality and inequality constraints."}, {"color": "#6d415a", "id": "LagrangeMultipliers", "label": "LagrangeMultipliers", "shape": "dot", "size": 10, "title": "Scalars used in Lagrangian to incorporate constraints into the objective function."}, {"color": "#6547a6", "id": "PrimalProblem", "label": "PrimalProblem", "shape": "dot", "size": 10, "title": "Minimizing a function subject to inequality and equality constraints."}, {"color": "#604e1e", "id": "GeneralizedLagrangian", "label": "GeneralizedLagrangian", "shape": "dot", "size": 10, "title": "Combination of the objective function with Lagrange multipliers for constraints."}, {"color": "#b072f4", "id": "ThetaP", "label": "ThetaP", "shape": "dot", "size": 10, "title": "Function that evaluates to f(w) if w satisfies primal constraints, otherwise infinity."}, {"color": "#b4f465", "id": "Primal Problem", "label": "Primal Problem", "shape": "star", "size": 25, "title": "Original optimization problem in machine learning."}, {"color": "#501432", "id": "Objective Function Primal", "label": "Objective Function Primal", "shape": "dot", "size": 10, "title": "Function to be minimized or maximized in the primal problem."}, {"color": "#d9b7cb", "id": "Dual Problem", "label": "Dual Problem", "shape": "star", "size": 25, "title": "Optimization problem derived from the primal by exchanging min and max operations."}, {"color": "#90ac06", "id": "Objective Function Dual", "label": "Objective Function Dual", "shape": "dot", "size": 10, "title": "Function to be maximized in the dual problem."}, {"color": "#1de9ed", "id": "Primal-Dual Relationship", "label": "Primal-Dual Relationship", "shape": "star", "size": 25, "title": "Relationship between primal and dual problems including inequalities between their optimal values."}, {"color": "#e855a2", "id": "OptimizationInML", "label": "OptimizationInML", "shape": "dot", "size": 10, "title": "Techniques for optimizing functions in machine learning models."}, {"color": "#b7aed6", "id": "PrimalDualProblem", "label": "PrimalDualProblem", "shape": "dot", "size": 10, "title": "Explanation of primal and dual problems in the context of machine learning optimization."}, {"color": "#be5dd5", "id": "dStarAndPStarEquality", "label": "dStarAndPStarEquality", "shape": "dot", "size": 10, "title": "Conditions under which d* equals p*, allowing solving the dual problem instead of the primal one."}, {"color": "#47c528", "id": "ConvexityAssumptions", "label": "ConvexityAssumptions", "shape": "dot", "size": 10, "title": "Convex functions and affine constraints assumptions for equality between primal and dual problems."}, {"color": "#f194fe", "id": "FeasibilityConstraints", "label": "FeasibilityConstraints", "shape": "dot", "size": 10, "title": "Strict feasibility conditions ensuring the existence of a solution satisfying all constraints."}, {"color": "#1e446f", "id": "KKTConditions", "label": "KKTConditions", "shape": "dot", "size": 10, "title": "Karush-Kuhn-Tucker (KKT) conditions for optimality in constrained optimization problems."}, {"color": "#216f7d", "id": "Optimal_Margin_Classifiers", "label": "Optimal_Margin_Classifiers", "shape": "dot", "size": 10, "title": "Focus on classifiers that maximize the margin between classes."}, {"color": "#1457d4", "id": "KKT_Conditions", "label": "KKT_Conditions", "shape": "dot", "size": 10, "title": "Conditions that must be satisfied by a solution to an optimization problem with inequality constraints."}, {"color": "#e0acc1", "id": "Dual_Complementarity", "label": "Dual_Complementarity", "shape": "dot", "size": 10, "title": "Condition stating that if alpha is positive, the constraint holds with equality."}, {"color": "#88d87e", "id": "Primal_Problem", "label": "Primal_Problem", "shape": "dot", "size": 10, "title": "Minimization problem for finding optimal margin classifier in primal form."}, {"color": "#625776", "id": "Dual_Form", "label": "Dual_Form", "shape": "dot", "size": 10, "title": "Formulation of the optimization problem in dual space, focusing on alpha values."}, {"color": "#1d66d3", "id": "SupportVectors", "label": "SupportVectors", "shape": "star", "size": 25, "title": "Training examples that define the decision boundary in SVMs."}, {"color": "#c69cb0", "id": "AlphaCoefficients", "label": "AlphaCoefficients", "shape": "dot", "size": 10, "title": "Lagrange multipliers corresponding to support vectors."}, {"color": "#0b61e9", "id": "InnerProduct", "label": "InnerProduct", "shape": "dot", "size": 10, "title": "Expression \u03c4(x^(i), x^(j)) representing similarity between points."}, {"color": "#26a5ff", "id": "LagrangianFormulation", "label": "LagrangianFormulation", "shape": "star", "size": 25, "title": "Optimization problem formulation for SVMs using Lagrange multipliers."}, {"color": "#c5f3cf", "id": "DualProblem", "label": "DualProblem", "shape": "dot", "size": 10, "title": "Transformation of the primal optimization problem into a dual form."}, {"color": "#02e70e", "id": "DerivativeWithRespectToW", "label": "DerivativeWithRespectToW", "shape": "dot", "size": 10, "title": "Calculation to find optimal w in terms of \u03b1 and x."}, {"color": "#7f8d9a", "id": "DerivativeWithRespectToB", "label": "DerivativeWithRespectToB", "shape": "dot", "size": 10, "title": "Calculation showing sum of \u03b1y=0 for all training examples."}, {"color": "#f9c212", "id": "Lagrangian_Formulation", "label": "Lagrangian_Formulation", "shape": "dot", "size": 10, "title": "Formulation involving Lagrangian for optimization problems in machine learning."}, {"color": "#5dae1b", "id": "Equation_6.9", "label": "Equation_6.9", "shape": "dot", "size": 10, "title": "Initial Lagrangian equation used in the formulation."}, {"color": "#ceb602", "id": "Equation_6.10", "label": "Equation_6.10", "shape": "dot", "size": 10, "title": "Definition of w used to simplify the Lagrangian."}, {"color": "#e43f54", "id": "Equation_6.11", "label": "Equation_6.11", "shape": "dot", "size": 10, "title": "Constraint equation that simplifies the Lagrangian expression."}, {"color": "#f198fc", "id": "Dual_Optimization_Problem", "label": "Dual_Optimization_Problem", "shape": "dot", "size": 10, "title": "Formulation of the dual optimization problem derived from the primal problem."}, {"color": "#f7ca50", "id": "Equation_6.12", "label": "Equation_6.12", "shape": "dot", "size": 10, "title": "Maximization problem for finding alpha values in the dual formulation."}, {"color": "#4e9069", "id": "Finding_w_and_b", "label": "Finding_w_and_b", "shape": "dot", "size": 10, "title": "Steps to find optimal w and b after solving the dual problem."}, {"color": "#7cda41", "id": "Optimal_Parameter_Finding", "label": "Optimal_Parameter_Finding", "shape": "dot", "size": 10, "title": "Finding optimal parameters for a model."}, {"color": "#b8865a", "id": "Dual_Form_Optimization", "label": "Dual_Form_Optimization", "shape": "dot", "size": 10, "title": "Optimization problem viewed from a dual perspective."}, {"color": "#381f96", "id": "Inner_Products_Calculation", "label": "Inner_Products_Calculation", "shape": "dot", "size": 10, "title": "Calculating inner products for predictions."}, {"color": "#3bd863", "id": "Regularization_and_Kernels", "label": "Regularization_and_Kernels", "shape": "star", "size": 25, "title": "Handling non-separable data with regularization and kernels."}, {"color": "#826734", "id": "Non-Separable Case", "label": "Non-Separable Case", "shape": "dot", "size": 10, "title": "Handling datasets where data points cannot be separated linearly"}, {"color": "#f045b2", "id": "L1 Regularization", "label": "L1 Regularization", "shape": "dot", "size": 10, "title": "Penalizes the absolute value of coefficients to reduce model complexity"}, {"color": "#535d9a", "id": "Support_Vector_Machines_SVM", "label": "Support_Vector_Machines_SVM", "shape": "dot", "size": 10, "title": "Algorithm for classification and regression analysis based on statistical learning theory."}, {"color": "#8d08d4", "id": "Dual_Problem_Formulation", "label": "Dual_Problem_Formulation", "shape": "dot", "size": 10, "title": "Formulation of SVM optimization problem in dual space to simplify computation."}, {"color": "#18659f", "id": "Lagrange_Multipliers", "label": "Lagrange_Multipliers", "shape": "dot", "size": 10, "title": "Multipliers used in the method of Lagrange multipliers for solving constrained optimization problems."}, {"color": "#4f1b4a", "id": "Support_Vector_Machines_SVMs", "label": "Support_Vector_Machines_SVMs", "shape": "dot", "size": 10, "title": "Overview of SVMs including the derivation and optimization problem."}, {"color": "#e4bb62", "id": "Sequential_Minimal_Optimization_SMO", "label": "Sequential_Minimal_Optimization_SMO", "shape": "dot", "size": 10, "title": "Efficient algorithm for solving the dual problem in SVM derivation."}, {"color": "#3eefa4", "id": "Coordinate_Ascend_Algorithm", "label": "Coordinate_Ascend_Algorithm", "shape": "star", "size": 25, "title": "Optimization technique used to solve unconstrained optimization problems."}, {"color": "#b345b8", "id": "Unconstrained_Optimization_Problem", "label": "Unconstrained_Optimization_Problem", "shape": "dot", "size": 10, "title": "Mathematical problem of finding the maximum value of a function W with respect to parameters \u03b1i\u0027s."}, {"color": "#3ab871", "id": "Machine_Learning_Optimization", "label": "Machine_Learning_Optimization", "shape": "star", "size": 25, "title": "Optimization techniques in machine learning including coordinate ascent and SMO."}, {"color": "#bb9f0f", "id": "Coordinate_Ascend_Method", "label": "Coordinate_Ascend_Method", "shape": "dot", "size": 10, "title": "A method for optimizing functions by moving along one variable at a time."}, {"color": "#aaee66", "id": "Quadratic_Function_Optimization", "label": "Quadratic_Function_Optimization", "shape": "dot", "size": 10, "title": "Optimizing quadratic functions using coordinate ascent starting from (2,-2)."}, {"color": "#e408f6", "id": "SVM_Dual_Optimization_Problem", "label": "SVM_Dual_Optimization_Problem", "shape": "dot", "size": 10, "title": "The dual form of the optimization problem for SVMs with constraints."}, {"color": "#81c970", "id": "Constraint_Satisfaction", "label": "Constraint_Satisfaction", "shape": "dot", "size": 10, "title": "Ensuring constraints are met during optimization by updating multiple alphas."}, {"color": "#b7baf2", "id": "Constraints_Satisfaction", "label": "Constraints_Satisfaction", "shape": "dot", "size": 10, "title": "Ensuring constraints are met during the optimization process."}, {"color": "#409828", "id": "Convergence_Tolerance", "label": "Convergence_Tolerance", "shape": "dot", "size": 10, "title": "Parameter defining the acceptable range for convergence checks."}, {"color": "#92ceca", "id": "Efficient_Update_Mechanism", "label": "Efficient_Update_Mechanism", "shape": "dot", "size": 10, "title": "Methodology for efficiently updating alpha values in SMO."}, {"color": "#396b0e", "id": "Alpha_Updating_Process", "label": "Alpha_Updating_Process", "shape": "dot", "size": 10, "title": "Details on how specific alphas are updated while others remain fixed."}, {"color": "#4c9bae", "id": "Alpha_Parameters", "label": "Alpha_Parameters", "shape": "dot", "size": 10, "title": "Parameters \u03b1 that are constrained within a box [0,C]x[0,C]."}, {"color": "#9ff685", "id": "Constraint_Equation", "label": "Constraint_Equation", "shape": "dot", "size": 10, "title": "Equation defining the relationship between \u03b1 parameters and y values."}, {"color": "#db462d", "id": "Objective_Function", "label": "Objective_Function", "shape": "dot", "size": 10, "title": "Function W(\u03b1) that depends on \u03b1 parameters."}, {"color": "#45c50d", "id": "Quadratic_Formulation", "label": "Quadratic_Formulation", "shape": "dot", "size": 10, "title": "W(\u03b1) expressed as a quadratic function in terms of \u03b12."}, {"color": "#4c8258", "id": "Machine Learning Overview", "label": "Machine Learning Overview", "shape": "star", "size": 25, "title": "General introduction to machine learning concepts and techniques."}, {"color": "#8a4d30", "id": "Sequential Minimal Optimization (SMO) Algorithm", "label": "Sequential Minimal Optimization (SMO) Algorithm", "shape": "dot", "size": 10, "title": "Efficient algorithm for solving the optimization problem in SVM."}, {"color": "#f17e85", "id": "Alpha Updates", "label": "Alpha Updates", "shape": "dot", "size": 10, "title": "Process of updating alpha values during SMO."}, {"color": "#bb17c8", "id": "Deep Learning Introduction", "label": "Deep Learning Introduction", "shape": "star", "size": 25, "title": "Introduction to deep learning concepts and neural networks."}, {"color": "#6fff98", "id": "Supervised Learning with Non-Linear Models", "label": "Supervised Learning with Non-Linear Models", "shape": "dot", "size": 10, "title": "Overview of supervised learning using non-linear models like neural networks."}, {"color": "#1652f7", "id": "NonLinearModels", "label": "NonLinearModels", "shape": "dot", "size": 10, "title": "Uses non-linear functions and feature mappings for predictions"}, {"color": "#226aee", "id": "TrainingExamples", "label": "TrainingExamples", "shape": "dot", "size": 10, "title": "Definition of training examples and their role in defining the cost function."}, {"color": "#ee7126", "id": "RegressionProblems", "label": "RegressionProblems", "shape": "dot", "size": 10, "title": "Exploration of regression problems using real number outputs."}, {"color": "#6bad9a", "id": "LeastSquareCostFunction", "label": "LeastSquareCostFunction", "shape": "dot", "size": 10, "title": "Definition and explanation of the least square cost function for individual examples."}, {"color": "#cba2c7", "id": "MeanSquareCostFunction", "label": "MeanSquareCostFunction", "shape": "dot", "size": 10, "title": "Explanation of mean-square cost function used in regression problems."}, {"color": "#7dc41c", "id": "BinaryClassification", "label": "BinaryClassification", "shape": "dot", "size": 10, "title": "Focuses on binary classification problems where labels are either 0 or 1."}, {"color": "#72db18", "id": "LogisticFunction", "label": "LogisticFunction", "shape": "dot", "size": 10, "title": "Use of logistic function to convert logit into a probability for binary classification."}, {"color": "#f02e5a", "id": "MulticlassClassification", "label": "MulticlassClassification", "shape": "dot", "size": 10, "title": "Extension of binary classification for multiple classes using softmax function."}, {"color": "#9da808", "id": "LogitFunction", "label": "LogitFunction", "shape": "dot", "size": 10, "title": "Linear combination of input features and weights before applying logistic or softmax functions."}, {"color": "#a3494c", "id": "ProbabilityPrediction", "label": "ProbabilityPrediction", "shape": "dot", "size": 10, "title": "Conversion of logit to probability using sigmoid function for binary classification."}, {"color": "#880c85", "id": "NegativeLikelihoodLoss", "label": "NegativeLikelihoodLoss", "shape": "dot", "size": 10, "title": "Loss function used in logistic regression that measures the discrepancy between predicted and actual outcomes."}, {"color": "#90efd8", "id": "SoftmaxFunction", "label": "SoftmaxFunction", "shape": "dot", "size": 10, "title": "Transformation of logits into probabilities for multi-class classification problems."}, {"color": "#79bf90", "id": "LogitsInMultiClass", "label": "LogitsInMultiClass", "shape": "dot", "size": 10, "title": "Vector of predicted values before applying softmax function in multi-class scenarios."}, {"color": "#c05788", "id": "Average Loss", "label": "Average Loss", "shape": "dot", "size": 10, "title": "Overall loss calculated as mean of individual losses."}, {"color": "#60bcd5", "id": "Conditional Probabilistic Model", "label": "Conditional Probabilistic Model", "shape": "dot", "size": 10, "title": "Model where output distribution depends on input features."}, {"color": "#7dd922", "id": "Exponential Family Distribution", "label": "Exponential Family Distribution", "shape": "dot", "size": 10, "title": "Family of distributions with exponential form, used in probabilistic models."}, {"color": "#f2d903", "id": "Optimizers", "label": "Optimizers", "shape": "star", "size": 25, "title": "Optimization algorithms used to minimize the pretraining loss."}, {"color": "#7df1a5", "id": "Gradient Descent (GD)", "label": "Gradient Descent (GD)", "shape": "dot", "size": 10, "title": "Algorithm that iteratively minimizes the cost function by moving in direction of steepest descent."}, {"color": "#427365", "id": "Stochastic Gradient Descent (SGD)", "label": "Stochastic Gradient Descent (SGD)", "shape": "dot", "size": 10, "title": "Variant of gradient descent using a single or subset of data points for each iteration."}, {"color": "#c75f1c", "id": "Learning Rate", "label": "Learning Rate", "shape": "dot", "size": 10, "title": "Hyperparameter controlling the step size during optimization."}, {"color": "#74a999", "id": "Mini-batch Stochastic Gradient Descent", "label": "Mini-batch Stochastic Gradient Descent", "shape": "dot", "size": 10, "title": "Variant of SGD using a small batch of data for each iteration."}, {"color": "#6c6490", "id": "Hyperparameters", "label": "Hyperparameters", "shape": "dot", "size": 10, "title": "Parameters set before the learning process that control the overall behavior of the algorithm."}, {"color": "#7db587", "id": "Learning Rate (\u03b1)", "label": "Learning Rate (\u03b1)", "shape": "dot", "size": 10, "title": "Step size at each iteration while moving toward a minimum of a loss function."}, {"color": "#04788f", "id": "Number of Iterations (n_iter)", "label": "Number of Iterations (n_iter)", "shape": "dot", "size": 10, "title": "Total number of iterations over which to optimize the model."}, {"color": "#b948a0", "id": "Batch Size (B)", "label": "Batch Size (B)", "shape": "dot", "size": 10, "title": "Number of samples per gradient update in mini-batch SGD."}, {"color": "#d22faa", "id": "Initialization", "label": "Initialization", "shape": "dot", "size": 10, "title": "Process of setting initial values for model parameters before training begins."}, {"color": "#bfda0d", "id": "Neural Networks", "label": "Neural Networks", "shape": "star", "size": 25, "title": "Non-linear models using combinations of matrix multiplications and non-linear operations."}, {"color": "#5ec5dc", "id": "Parametrization (h_\u03b8(x))", "label": "Parametrization (h_\u03b8(x))", "shape": "dot", "size": 10, "title": "Function that maps input data to output predictions with parameters \u03b8."}, {"color": "#823b5e", "id": "ClassificationProblem", "label": "ClassificationProblem", "shape": "dot", "size": 10, "title": "Categorizing inputs into discrete classes."}, {"color": "#f10c97", "id": "NeuralNetworks", "label": "NeuralNetworks", "shape": "star", "size": 25, "title": "Introduction to neural networks including fully-connected architectures."}, {"color": "#27ec56", "id": "SingleNeuronNN", "label": "SingleNeuronNN", "shape": "dot", "size": 10, "title": "A simple neural network with a single neuron for regression tasks."}, {"color": "#5ad8f2", "id": "HousingPricePrediction", "label": "HousingPricePrediction", "shape": "dot", "size": 10, "title": "Example of predicting housing prices using a single neuron model."}, {"color": "#a86804", "id": "ReLUActivationFunction", "label": "ReLUActivationFunction", "shape": "dot", "size": 10, "title": "Use of ReLU activation function in computing intermediate variables a1, a2, and a3."}, {"color": "#a787d0", "id": "Parameterization", "label": "Parameterization", "shape": "star", "size": 25, "title": "Discussion on parameterization including weights and biases for neural network layers."}, {"color": "#171227", "id": "Activation Function", "label": "Activation Function", "shape": "dot", "size": 10, "title": "Function that introduces non-linearity to the model\u0027s output."}, {"color": "#81b1a7", "id": "ReLU", "label": "ReLU", "shape": "dot", "size": 10, "title": "Rectified Linear Unit, a popular activation function in neural networks."}, {"color": "#1a662b", "id": "Bias Term", "label": "Bias Term", "shape": "dot", "size": 10, "title": "Adjustment term added to the input before applying the activation function."}, {"color": "#de8777", "id": "Weight Vector", "label": "Weight Vector", "shape": "dot", "size": 10, "title": "Vector containing weights for each feature in the input data."}, {"color": "#d0dfbe", "id": "Single Neuron Model", "label": "Single Neuron Model", "shape": "dot", "size": 10, "title": "A model with a single neuron, including activation function and bias term."}, {"color": "#6483b5", "id": "Stacking Neurons", "label": "Stacking Neurons", "shape": "dot", "size": 10, "title": "Process of combining multiple neurons to form complex neural networks."}, {"color": "#8bc93c", "id": "Complex Neural Network Example", "label": "Complex Neural Network Example", "shape": "dot", "size": 10, "title": "Example using house features and family size prediction."}, {"color": "#4fe8a6", "id": "HousingPricesModel", "label": "HousingPricesModel", "shape": "dot", "size": 10, "title": "A model predicting housing prices based on derived features."}, {"color": "#c8fbc2", "id": "DerivedFeatures", "label": "DerivedFeatures", "shape": "dot", "size": 10, "title": "Family size, walkability, and school quality as key factors in determining house price."}, {"color": "#b27423", "id": "IntermediateVariables", "label": "IntermediateVariables", "shape": "dot", "size": 10, "title": "Hidden units (a1, a2, a3) representing intermediate computations in the neural network."}, {"color": "#ad8a9e", "id": "NeuralNetworkStructure", "label": "NeuralNetworkStructure", "shape": "dot", "size": 10, "title": "Description of how input features are processed through hidden layers to produce output."}, {"color": "#6cdcb6", "id": "OutputParameterization", "label": "OutputParameterization", "shape": "dot", "size": 10, "title": "Final output as a linear combination of hidden units with parameters \u03b89 to \u03b812."}, {"color": "#76d9fb", "id": "NeuralNetworkParameters", "label": "NeuralNetworkParameters", "shape": "dot", "size": 10, "title": "Collection of parameters in neural networks excluding those from the last layer."}, {"color": "#a268cb", "id": "BiologicalInspiration", "label": "BiologicalInspiration", "shape": "dot", "size": 10, "title": "Explanation of the inspiration from biological neural networks."}, {"color": "#534174", "id": "TwoLayerNetworks", "label": "TwoLayerNetworks", "shape": "dot", "size": 10, "title": "Description and construction of two-layer fully-connected neural networks."}, {"color": "#4e44fc", "id": "FullyConnectedNN", "label": "FullyConnectedNN", "shape": "dot", "size": 10, "title": "Description of a two-layer fully connected neural network with m hidden units and d-dimensional input."}, {"color": "#245948", "id": "Vectorization", "label": "Vectorization", "shape": "dot", "size": 10, "title": "Importance of vectorization in simplifying expressions and improving implementation efficiency."}, {"color": "#6f05c8", "id": "VectorizationInNN", "label": "VectorizationInNN", "shape": "star", "size": 25, "title": "Process of converting loops into matrix operations for efficiency."}, {"color": "#c3fe49", "id": "EfficiencyConcerns", "label": "EfficiencyConcerns", "shape": "dot", "size": 10, "title": "Discussion on the inefficiency of using for loops in neural networks."}, {"color": "#25717b", "id": "MatrixAlgebra", "label": "MatrixAlgebra", "shape": "dot", "size": 10, "title": "Use of matrix algebra to speed up computations."}, {"color": "#9b3cb5", "id": "BLASOptimization", "label": "BLASOptimization", "shape": "dot", "size": 10, "title": "Utilizing BLAS for optimized numerical linear algebra operations."}, {"color": "#921ec6", "id": "TwoLayerNetwork", "label": "TwoLayerNetwork", "shape": "dot", "size": 10, "title": "Example of vectorizing a two-layer fully-connected neural network."}, {"color": "#8862b5", "id": "WeightMatrices", "label": "WeightMatrices", "shape": "dot", "size": 10, "title": "Description of weight matrices in neural network models."}, {"color": "#19a2e6", "id": "BiasVectors", "label": "BiasVectors", "shape": "dot", "size": 10, "title": "Explanation of bias vectors and their role in neural networks."}, {"color": "#54fa3d", "id": "ActivationFunctions", "label": "ActivationFunctions", "shape": "dot", "size": 10, "title": "Explanation of activation functions used in neural networks, including ReLU and others."}, {"color": "#7fdd4b", "id": "LayerStructure", "label": "LayerStructure", "shape": "dot", "size": 10, "title": "Description of layer structure in neural networks including hidden layers."}, {"color": "#54dd7a", "id": "WeightMatricesBiases", "label": "WeightMatricesBiases", "shape": "dot", "size": 10, "title": "Description of weight matrices and biases in a neural network."}, {"color": "#d3b692", "id": "ReLUFunction", "label": "ReLUFunction", "shape": "dot", "size": 10, "title": "Description of the Rectified Linear Unit (ReLU) function as an activation function."}, {"color": "#50aeff", "id": "SigmoidTanh", "label": "SigmoidTanh", "shape": "dot", "size": 10, "title": "Explanation of sigmoid and tanh functions as alternative non-linear activation functions."}, {"color": "#12d7da", "id": "SigmoidFunction", "label": "SigmoidFunction", "shape": "dot", "size": 10, "title": "A bounded function that maps real numbers to (0, 1)."}, {"color": "#3941ae", "id": "TanhFunction", "label": "TanhFunction", "shape": "dot", "size": 10, "title": "Similar to sigmoid but ranges from -1 to 1."}, {"color": "#4fb721", "id": "LeakyReLUFunction", "label": "LeakyReLUFunction", "shape": "dot", "size": 10, "title": "A variant of ReLU that allows a small gradient when the unit is not active."}, {"color": "#3f62bf", "id": "GELUFunction", "label": "GELUFunction", "shape": "dot", "size": 10, "title": "A smooth approximation to the rectified linear function used in NLP models."}, {"color": "#bf219c", "id": "SoftplusFunction", "label": "SoftplusFunction", "shape": "dot", "size": 10, "title": "Smoothed version of ReLU with a proper second-order derivative."}, {"color": "#513b05", "id": "IdentityFunction", "label": "IdentityFunction", "shape": "dot", "size": 10, "title": "A linear function where the output is equal to the input."}, {"color": "#843c97", "id": "FeatureEngineering", "label": "FeatureEngineering", "shape": "dot", "size": 10, "title": "Process of selecting and preparing the features for a model."}, {"color": "#f7f051", "id": "DeepLearning", "label": "DeepLearning", "shape": "dot", "size": 10, "title": "Subfield of machine learning that uses neural networks to learn representations from data."}, {"color": "#f4f6af", "id": "FeatureMaps", "label": "FeatureMaps", "shape": "dot", "size": 10, "title": "Mappings created by deep learning models to transform input into useful features."}, {"color": "#86ae8c", "id": "LinearModelOnTopOfFeatureMap", "label": "LinearModelOnTopOfFeatureMap", "shape": "dot", "size": 10, "title": "Using a linear model with features generated by deep learning models."}, {"color": "#baf75b", "id": "LearnedFeaturesRepresentations", "label": "LearnedFeaturesRepresentations", "shape": "dot", "size": 10, "title": "Intermediate outputs from penultimate layers used as learned representations."}, {"color": "#621e73", "id": "Deep Learning Representations", "label": "Deep Learning Representations", "shape": "star", "size": 25, "title": "Discussion on feature representations in deep learning."}, {"color": "#619e08", "id": "House Price Prediction Example", "label": "House Price Prediction Example", "shape": "dot", "size": 10, "title": "Example of using a fully-connected neural network for house price prediction."}, {"color": "#f3892d", "id": "Feature Discovery", "label": "Feature Discovery", "shape": "dot", "size": 10, "title": "Automatic discovery of useful features by neural networks."}, {"color": "#7995f8", "id": "Black Box Nature", "label": "Black Box Nature", "shape": "dot", "size": 10, "title": "Complexity and interpretability issues in neural network feature discovery."}, {"color": "#33e721", "id": "Modern Neural Network Modules", "label": "Modern Neural Network Modules", "shape": "star", "size": 25, "title": "Introduction to building blocks of modern neural networks."}, {"color": "#b9b10f", "id": "Matrix Multiplication Module", "label": "Matrix Multiplication Module", "shape": "dot", "size": 10, "title": "Building block for matrix multiplication operations in neural networks."}, {"color": "#93d65e", "id": "MLP Composition", "label": "MLP Composition", "shape": "dot", "size": 10, "title": "Composition of MLP using multiple matrix multiplication and nonlinear activation modules."}, {"color": "#212c9d", "id": "MLPArchitecture", "label": "MLPArchitecture", "shape": "dot", "size": 10, "title": "Multi-layer perceptron architecture using matrix multiplication and activation functions."}, {"color": "#0dfac0", "id": "MatrixMultiplicationModule", "label": "MatrixMultiplicationModule", "shape": "dot", "size": 10, "title": "Explanation of the backward function for matrix multiplication module (MM)."}, {"color": "#65d172", "id": "NonlinearActivationModule", "label": "NonlinearActivationModule", "shape": "dot", "size": 10, "title": "Component of MLP applying nonlinear transformation to matrix multiplication output."}, {"color": "#abc063", "id": "ResidualConnections", "label": "ResidualConnections", "shape": "dot", "size": 10, "title": "Technique used in ResNet for deep learning architectures to mitigate vanishing gradient problem."}, {"color": "#e7ce0c", "id": "MachineLearningArchitectures", "label": "MachineLearningArchitectures", "shape": "star", "size": 25, "title": "Overview of different architectures in machine learning."}, {"color": "#50a069", "id": "ResNetArchitecture", "label": "ResNetArchitecture", "shape": "dot", "size": 10, "title": "Deep residual network architecture using convolution layers and batch normalization."}, {"color": "#54bcad", "id": "LayerNormalization", "label": "LayerNormalization", "shape": "dot", "size": 10, "title": "Technique to normalize the layer inputs by scaling and shifting them."}, {"color": "#4e6e9f", "id": "LN-SModule", "label": "LN-SModule", "shape": "dot", "size": 10, "title": "Sub-module for layer normalization that normalizes each element of the input vector."}, {"color": "#5d3203", "id": "AffineTransformation", "label": "AffineTransformation", "shape": "dot", "size": 10, "title": "Transforms the output of LN-S to have desired mean and standard deviation using learnable parameters \u03b2 and \u03b3."}, {"color": "#5e0052", "id": "LN-S", "label": "LN-S", "shape": "dot", "size": 10, "title": "Standardized version of LN that normalizes input vectors without learnable parameters."}, {"color": "#0c9c0e", "id": "LearnableParameters", "label": "LearnableParameters", "shape": "dot", "size": 10, "title": "Scalars \u03b2 and \u03b3 that are learned during training to adjust the mean and standard deviation of inputs."}, {"color": "#49494f", "id": "ScalingInvariantProperty", "label": "ScalingInvariantProperty", "shape": "star", "size": 25, "title": "Property ensuring model output remains unchanged under scaling transformations in preceding layers."}, {"color": "#9e62eb", "id": "ScaleInvariantProperty", "label": "ScaleInvariantProperty", "shape": "dot", "size": 10, "title": "Property of modern DL architectures regarding weight scaling."}, {"color": "#ca1ee8", "id": "OtherNormalizationLayers", "label": "OtherNormalizationLayers", "shape": "dot", "size": 10, "title": "Alternative normalization techniques used in neural networks."}, {"color": "#12e8c6", "id": "BatchNormalization", "label": "BatchNormalization", "shape": "dot", "size": 10, "title": "Normalization technique commonly used in computer vision applications."}, {"color": "#c3daa3", "id": "GroupNormalization", "label": "GroupNormalization", "shape": "dot", "size": 10, "title": "Normalization method suitable for grouped channels in neural networks."}, {"color": "#d7f91d", "id": "ConvolutionalLayers", "label": "ConvolutionalLayers", "shape": "dot", "size": 10, "title": "Neural network layers designed for spatial hierarchies in data."}, {"color": "#d26d43", "id": "1DConvolution", "label": "1DConvolution", "shape": "dot", "size": 10, "title": "Simplified version of 1-D convolution layer used in neural networks."}, {"color": "#d5fb96", "id": "Machine_Learning_Topics", "label": "Machine_Learning_Topics", "shape": "star", "size": 25, "title": "Main topics in machine learning."}, {"color": "#f05e97", "id": "Convolutional_Neural_Networks", "label": "Convolutional_Neural_Networks", "shape": "dot", "size": 10, "title": "Neural networks that use convolution operations."}, {"color": "#a1c1d7", "id": "1D_Convolution", "label": "1D_Convolution", "shape": "dot", "size": 10, "title": "One-dimensional convolution used in sequence data processing."}, {"color": "#7fca4f", "id": "2D_Convolution", "label": "2D_Convolution", "shape": "dot", "size": 10, "title": "Two-dimensional convolution typically used for image data."}, {"color": "#931f44", "id": "Filter_Vector", "label": "Filter_Vector", "shape": "dot", "size": 10, "title": "Vector of weights applied to input sequences."}, {"color": "#be5a32", "id": "Bias_Scalar", "label": "Bias_Scalar", "shape": "dot", "size": 10, "title": "Scalar bias added to convolution output."}, {"color": "#141cc6", "id": "Kernel_Method", "label": "Kernel_Method", "shape": "dot", "size": 10, "title": "Technique for pattern analysis using a kernel function."}, {"color": "#9d60cd", "id": "ParameterSharing", "label": "ParameterSharing", "shape": "dot", "size": 10, "title": "Explanation of parameter sharing in convolutional layers."}, {"color": "#6c0e6f", "id": "EfficiencyOfConvolution", "label": "EfficiencyOfConvolution", "shape": "dot", "size": 10, "title": "Comparison of computational efficiency between convolution and generic matrix multiplication."}, {"color": "#f82a6a", "id": "ChannelConcepts", "label": "ChannelConcepts", "shape": "dot", "size": 10, "title": "Introduction to the concept of channels in convolutional layers."}, {"color": "#1c7271", "id": "Conv1DModule", "label": "Conv1DModule", "shape": "dot", "size": 10, "title": "One-dimensional convolution module with multiple channels."}, {"color": "#f4adb8", "id": "Conv2DModule", "label": "Conv2DModule", "shape": "dot", "size": 10, "title": "Two-dimensional convolution module with one channel and its extension to multi-channel inputs."}, {"color": "#0973b4", "id": "Differentiable_Circuit", "label": "Differentiable_Circuit", "shape": "dot", "size": 10, "title": "Composition of arithmetic operations and elementary functions that can be differentiated."}, {"color": "#c1f9ec", "id": "Gradient_Computation", "label": "Gradient_Computation", "shape": "dot", "size": 10, "title": "Process of computing gradients for a real-valued function using backpropagation."}, {"color": "#033903", "id": "Loss Function J(theta)", "label": "Loss Function J(theta)", "shape": "dot", "size": 10, "title": "Function representing the error between predicted values and actual values in a model."}, {"color": "#530a95", "id": "Gradient Computation", "label": "Gradient Computation", "shape": "dot", "size": 10, "title": "Process of calculating gradients to optimize parameters in machine learning models."}, {"color": "#80dc65", "id": "Backpropagation Algorithm", "label": "Backpropagation Algorithm", "shape": "dot", "size": 10, "title": "Method for efficiently computing gradients in neural networks."}, {"color": "#3baa44", "id": "Deep Learning Packages", "label": "Deep Learning Packages", "shape": "dot", "size": 10, "title": "Software libraries like TensorFlow and PyTorch that implement backpropagation."}, {"color": "#5a1efc", "id": "Chain Rule", "label": "Chain Rule", "shape": "star", "size": 25, "title": "Mathematical rule for computing derivatives of composite functions."}, {"color": "#9d2e91", "id": "Basic Chain Rule Perspective", "label": "Basic Chain Rule Perspective", "shape": "dot", "size": 10, "title": "New perspective on the chain rule useful for understanding backpropagation."}, {"color": "#a5b9fc", "id": "Backpropagation Strategy", "label": "Backpropagation Strategy", "shape": "star", "size": 25, "title": "General approach to implementing backpropagation in neural networks."}, {"color": "#8bb354", "id": "Backward Function Computation", "label": "Backward Function Computation", "shape": "dot", "size": 10, "title": "Method for calculating gradients of basic modules used in neural networks."}, {"color": "#d106a0", "id": "Concrete Backprop Algorithm", "label": "Concrete Backprop Algorithm", "shape": "dot", "size": 10, "title": "Specific algorithm for implementing backpropagation in MLPs."}, {"color": "#6eee40", "id": "Partial Derivatives", "label": "Partial Derivatives", "shape": "star", "size": 25, "title": "Concept of partial derivatives and their application in machine learning."}, {"color": "#0e20f2", "id": "Scalar Variable J", "label": "Scalar Variable J", "shape": "dot", "size": 10, "title": "Variable representing the scalar output dependent on other variables z."}, {"color": "#f9ab65", "id": "PartialDerivatives", "label": "PartialDerivatives", "shape": "dot", "size": 10, "title": "Explanation and challenges related to partial derivatives in multi-variate functions."}, {"color": "#89d599", "id": "ChainRule", "label": "ChainRule", "shape": "dot", "size": 10, "title": "Review of the chain rule for auto-differentiation purposes."}, {"color": "#037265", "id": "ScalarFunctionDerivatives", "label": "ScalarFunctionDerivatives", "shape": "dot", "size": 10, "title": "Focus on derivatives of scalar functions with respect to vectors, matrices, or tensors."}, {"color": "#e30619", "id": "MathematicalNotationChallenges", "label": "MathematicalNotationChallenges", "shape": "dot", "size": 10, "title": "Discussion on the complexities in notation for multi-variate partial derivatives."}, {"color": "#0a0276", "id": "Machine_Learning_Backward_Propagation", "label": "Machine_Learning_Backward_Propagation", "shape": "star", "size": 25, "title": "Overview of backward propagation in machine learning"}, {"color": "#cb50cf", "id": "Chain_Rule_Applications", "label": "Chain_Rule_Applications", "shape": "dot", "size": 10, "title": "Using the chain rule to compute gradients"}, {"color": "#d0e492", "id": "Jacobian_Matrix", "label": "Jacobian_Matrix", "shape": "dot", "size": 10, "title": "Introduction to Jacobian matrix in context of machine learning"}, {"color": "#a1571b", "id": "Complex_Functions", "label": "Complex_Functions", "shape": "dot", "size": 10, "title": "Handling complex functions where direct computation is difficult"}, {"color": "#0d7baf", "id": "Modules Composition", "label": "Modules Composition", "shape": "dot", "size": 10, "title": "Viewing neural networks as compositions of simple modules"}, {"color": "#1b5109", "id": "Loss Function J", "label": "Loss Function J", "shape": "dot", "size": 10, "title": "Abstract representation of loss function in terms of module composition"}, {"color": "#9a10fa", "id": "BinaryClassificationProblem", "label": "BinaryClassificationProblem", "shape": "dot", "size": 10, "title": "A specific problem in machine learning involving binary outcomes."}, {"color": "#4d2276", "id": "MLPModel", "label": "MLPModel", "shape": "dot", "size": 10, "title": "Multi-layer perceptron model used for solving the classification problem."}, {"color": "#08a2e5", "id": "LossFunction", "label": "LossFunction", "shape": "dot", "size": 10, "title": "Mathematical function that measures the error between predicted and actual outcomes."}, {"color": "#a70140", "id": "ModulesInMLP", "label": "ModulesInMLP", "shape": "dot", "size": 10, "title": "Components of MLP including linear transformations and activation functions."}, {"color": "#9d6e7d", "id": "BackwardPass", "label": "BackwardPass", "shape": "dot", "size": 10, "title": "Process of computing gradients to update model parameters based on intermediate variables and loss function derivatives."}, {"color": "#039845", "id": "Machine_Learning_Backpropagation", "label": "Machine_Learning_Backpropagation", "shape": "star", "size": 25, "title": "Overview of backpropagation in neural networks."}, {"color": "#ebc782", "id": "Chain_Rule_Application", "label": "Chain_Rule_Application", "shape": "dot", "size": 10, "title": "Application of the chain rule in computing gradients."}, {"color": "#7e6815", "id": "Backward_Propagation_Equation", "label": "Backward_Propagation_Equation", "shape": "dot", "size": 10, "title": "Equations detailing the backward propagation process."}, {"color": "#c3da7b", "id": "Computational_Graph_Illustration", "label": "Computational_Graph_Illustration", "shape": "dot", "size": 10, "title": "Visualization of computational graph for backpropagation."}, {"color": "#de918f", "id": "Efficiency_of_Modules", "label": "Efficiency_of_Modules", "shape": "dot", "size": 10, "title": "Discussion on the efficiency and granularity of neural network modules."}, {"color": "#4fe8fa", "id": "NeuralNetworksComposition", "label": "NeuralNetworksComposition", "shape": "dot", "size": 10, "title": "Understanding neural networks as compositions of basic operations."}, {"color": "#4b2f3b", "id": "BackpropagationDiscussion", "label": "BackpropagationDiscussion", "shape": "dot", "size": 10, "title": "Detailed discussion on backpropagation in the context of neural network training."}, {"color": "#72e83f", "id": "BackwardFunctionsBasics", "label": "BackwardFunctionsBasics", "shape": "dot", "size": 10, "title": "Introduction to backward functions for basic modules used in networks."}, {"color": "#4c9035", "id": "LossFunctionBackward", "label": "LossFunctionBackward", "shape": "dot", "size": 10, "title": "Explanation of the backward function computation for loss functions."}, {"color": "#f95bb1", "id": "Backward Function Overview", "label": "Backward Function Overview", "shape": "star", "size": 25, "title": "Overview of backward functions in machine learning."}, {"color": "#065dc0", "id": "Matrix Multiplication Backward Function", "label": "Matrix Multiplication Backward Function", "shape": "dot", "size": 10, "title": "Explanation and computation efficiency for the matrix multiplication backward function."}, {"color": "#4a2468", "id": "Bias Backward Function", "label": "Bias Backward Function", "shape": "dot", "size": 10, "title": "Derivation of the bias backward function using equation (7.59)."}, {"color": "#cb6c22", "id": "Activation Function Backward Function", "label": "Activation Function Backward Function", "shape": "dot", "size": 10, "title": "Explanation and derivation for element-wise activation functions\u0027 backward computation."}, {"color": "#b1c361", "id": "Equation 7.64", "label": "Equation 7.64", "shape": "dot", "size": 10, "title": "Derivation of the matrix multiplication backward function using equation (7.64)."}, {"color": "#c81583", "id": "Vectorized Notation", "label": "Vectorized Notation", "shape": "dot", "size": 10, "title": "Expression in vectorized notation for the matrix multiplication backward function."}, {"color": "#4c2981", "id": "Equation 7.65", "label": "Equation 7.65", "shape": "dot", "size": 10, "title": "Vectorized form of equation (7.64) with dimensions specified."}, {"color": "#3d1984", "id": "Equation 7.66", "label": "Equation 7.66", "shape": "dot", "size": 10, "title": "Derivation and simplified expression for the bias backward function using equation (7.59)."}, {"color": "#db6dfb", "id": "Activation Derivative Matrix", "label": "Activation Derivative Matrix", "shape": "dot", "size": 10, "title": "Matrix representation of derivative terms in activation functions."}, {"color": "#d5353a", "id": "Element-wise Operation", "label": "Element-wise Operation", "shape": "dot", "size": 10, "title": "Simplified form using element-wise operations for the backward function of activations."}, {"color": "#6a7a80", "id": "Machine_Learning_Backward_Functions", "label": "Machine_Learning_Backward_Functions", "shape": "star", "size": 25, "title": "Overview of backward functions in machine learning modules."}, {"color": "#42d2f2", "id": "Efficiency_of_Backward_Pass", "label": "Efficiency_of_Backward_Pass", "shape": "dot", "size": 10, "title": "Discussion on the efficiency and simplification of the backward pass computation."}, {"color": "#9e153d", "id": "Vectorized_Notation_Backward", "label": "Vectorized_Notation_Backward", "shape": "dot", "size": 10, "title": "Explanation of vectorized notation for backward functions in machine learning modules."}, {"color": "#b2c0f3", "id": "Squared_Loss_Backward", "label": "Squared_Loss_Backward", "shape": "dot", "size": 10, "title": "Derivation and explanation of the backward function for squared loss (MSE)."}, {"color": "#c6a4ec", "id": "Logistic_Loss_Backward", "label": "Logistic_Loss_Backward", "shape": "dot", "size": 10, "title": "Explanation of the backward function for logistic loss."}, {"color": "#af4374", "id": "Cross_Entropy_Loss_Backward", "label": "Cross_Entropy_Loss_Backward", "shape": "dot", "size": 10, "title": "Explanation of the backward function for cross-entropy loss."}, {"color": "#4c657f", "id": "Machine Learning Loss Functions", "label": "Machine Learning Loss Functions", "shape": "star", "size": 25, "title": "Overview of loss functions used in machine learning models."}, {"color": "#feba45", "id": "Logistic Loss Function", "label": "Logistic Loss Function", "shape": "dot", "size": 10, "title": "Function used for binary classification problems."}, {"color": "#3d7e02", "id": "Cross-Entropy Loss Function", "label": "Cross-Entropy Loss Function", "shape": "dot", "size": 10, "title": "Alternative loss function to logistic loss, commonly used in classification tasks."}, {"color": "#452d71", "id": "Back-propagation Algorithm", "label": "Back-propagation Algorithm", "shape": "star", "size": 25, "title": "Algorithm for computing gradients of the loss function with respect to parameters in neural networks."}, {"color": "#876127", "id": "MLP Backpropagation", "label": "MLP Backpropagation", "shape": "dot", "size": 10, "title": "Specific application of backpropagation to Multi-Layer Perceptrons (MLPs)."}, {"color": "#1c4eb5", "id": "Forward Pass", "label": "Forward Pass", "shape": "dot", "size": 10, "title": "Sequence of operations to compute the loss function in a multi-layer perceptron."}, {"color": "#b865a7", "id": "Backward Pass", "label": "Backward Pass", "shape": "dot", "size": 10, "title": "Process of computing gradients by applying backward functions sequentially."}, {"color": "#9ccdc3", "id": "BackpropagationAlgorithm", "label": "BackpropagationAlgorithm", "shape": "dot", "size": 10, "title": "Details on the backpropagation algorithm used in neural networks."}, {"color": "#d3e278", "id": "GradientComputation", "label": "GradientComputation", "shape": "dot", "size": 10, "title": "Computation of gradients for weights and biases using backpropagation equations."}, {"color": "#49138e", "id": "IntermediateValuesStorage", "label": "IntermediateValuesStorage", "shape": "dot", "size": 10, "title": "Storing intermediate values like activations and pre-activations during forward pass."}, {"color": "#067af8", "id": "ParallelTrainingExamples", "label": "ParallelTrainingExamples", "shape": "star", "size": 25, "title": "Discussion on handling multiple training examples in neural networks using vectorization."}, {"color": "#8e4650", "id": "VectorizedForwardPass", "label": "VectorizedForwardPass", "shape": "dot", "size": 10, "title": "Matrix notation for the forward pass of a neural network with multiple examples."}, {"color": "#5c0d17", "id": "TrainingSetExamples", "label": "TrainingSetExamples", "shape": "dot", "size": 10, "title": "Explanation of training set examples and their representation."}, {"color": "#3665eb", "id": "MatrixNotation", "label": "MatrixNotation", "shape": "dot", "size": 10, "title": "Use of matrix notation in representing multiple training examples."}, {"color": "#c784c8", "id": "LayerActivations", "label": "LayerActivations", "shape": "dot", "size": 10, "title": "First-layer activations for each example using matrix operations."}, {"color": "#c5afb7", "id": "Broadcasting", "label": "Broadcasting", "shape": "dot", "size": 10, "title": "Technique used to add bias terms across multiple columns in matrix operations."}, {"color": "#a49976", "id": "GeneralizationToLayers", "label": "GeneralizationToLayers", "shape": "dot", "size": 10, "title": "Discussion on generalizing the matricization approach to multiple layers."}, {"color": "#b8cdc9", "id": "Machine Learning", "label": "Machine Learning", "shape": "star", "size": 25, "title": "Field of study focusing on algorithms that learn from and make predictions on data."}, {"color": "#39e828", "id": "Matricization Approach", "label": "Matricization Approach", "shape": "dot", "size": 10, "title": "Method to represent multi-layer neural networks using matrices."}, {"color": "#dcaf0a", "id": "Implementation Details", "label": "Implementation Details", "shape": "dot", "size": 10, "title": "Specifics of implementing matricization in deep learning packages."}, {"color": "#16b188", "id": "Data Representation", "label": "Data Representation", "shape": "dot", "size": 10, "title": "Discussion on how data points are represented as rows or columns in matrices."}, {"color": "#0a620d", "id": "Conversion Rules", "label": "Conversion Rules", "shape": "dot", "size": 10, "title": "Rules for converting between row and column vector representations."}, {"color": "#60d0e8", "id": "Training Loss Function", "label": "Training Loss Function", "shape": "dot", "size": 10, "title": "Function used to train models by minimizing error on the training dataset."}, {"color": "#6af450", "id": "Training_Loss", "label": "Training_Loss", "shape": "dot", "size": 10, "title": "Measure of how well a model fits the training data, often referred to as empirical loss."}, {"color": "#1ebaea", "id": "Test_Error", "label": "Test_Error", "shape": "dot", "size": 10, "title": "Evaluation metric for a model\u0027s performance on unseen test examples, also known as population error."}, {"color": "#caca77", "id": "Mean_Squared_Error", "label": "Mean_Squared_Error", "shape": "dot", "size": 10, "title": "Common loss function used to quantify the difference between predicted and actual values."}, {"color": "#af7252", "id": "Training_Dataset", "label": "Training_Dataset", "shape": "dot", "size": 10, "title": "Collection of examples used for training a model, drawn from an empirical distribution."}, {"color": "#9a9f64", "id": "Test_Dataset", "label": "Test_Dataset", "shape": "dot", "size": 10, "title": "Set of unseen data points used to evaluate the performance of a trained model."}, {"color": "#9e1a27", "id": "Empirical_Distribution", "label": "Empirical_Distribution", "shape": "dot", "size": 10, "title": "Uniform distribution over training examples, representing an empirical approximation of the true distribution."}, {"color": "#4bd2b3", "id": "Training_Test_Distributions", "label": "Training_Test_Distributions", "shape": "dot", "size": 10, "title": "Explanation of training and test data distributions."}, {"color": "#857d52", "id": "Domain_Shift", "label": "Domain_Shift", "shape": "dot", "size": 10, "title": "Scenario where training and test distributions differ."}, {"color": "#3e71be", "id": "Overfitting_Underfitting", "label": "Overfitting_Underfitting", "shape": "dot", "size": 10, "title": "Conditions leading to overfitting or underfitting of models."}, {"color": "#72d042", "id": "Generalization_Error", "label": "Generalization_Error", "shape": "dot", "size": 10, "title": "Difference between true error and empirical error for a hypothesis in the hypothesis space."}, {"color": "#fa22ed", "id": "Bias_Variance_Tradeoff", "label": "Bias_Variance_Tradeoff", "shape": "star", "size": 25, "title": "Decomposition of test error into bias and variance terms to understand model performance."}, {"color": "#397200", "id": "Test_Error_Influence", "label": "Test_Error_Influence", "shape": "dot", "size": 10, "title": "Factors influencing the test error in machine learning models."}, {"color": "#083ed2", "id": "Double_Descent_Phenomenon", "label": "Double_Descent_Phenomenon", "shape": "star", "size": 25, "title": "Description of the double descent phenomenon in model and sample complexity."}, {"color": "#3a6eb2", "id": "Overfitting and Underfitting", "label": "Overfitting and Underfitting", "shape": "dot", "size": 10, "title": "Discussion on when overfitting and underfitting occur and how to avoid them."}, {"color": "#902208", "id": "Bias-Variance Tradeoff", "label": "Bias-Variance Tradeoff", "shape": "dot", "size": 10, "title": "Balancing model complexity to minimize prediction error by adjusting bias and variance."}, {"color": "#3bdfb7", "id": "Double Descent Phenomenon", "label": "Double Descent Phenomenon", "shape": "dot", "size": 10, "title": "Phenomenon where model performance initially improves then worsens and improves again with increasing complexity."}, {"color": "#e3f16a", "id": "Training Dataset Example", "label": "Training Dataset Example", "shape": "dot", "size": 10, "title": "Example illustrating training data and model fitting issues."}, {"color": "#95c561", "id": "Test Error Analysis", "label": "Test Error Analysis", "shape": "dot", "size": 10, "title": "Analysis of test errors when using different types of models."}, {"color": "#243f04", "id": "Linear Regression Example", "label": "Linear Regression Example", "shape": "dot", "size": 10, "title": "Example showing issues with fitting a linear model to non-linear data."}, {"color": "#1762f9", "id": "MachineLearningIssues", "label": "MachineLearningIssues", "shape": "star", "size": 25, "title": "Discussion of issues in machine learning models."}, {"color": "#a5a463", "id": "LinearModelLimitations", "label": "LinearModelLimitations", "shape": "dot", "size": 10, "title": "Exploration of limitations with linear models."}, {"color": "#5b94d4", "id": "BiasInModels", "label": "BiasInModels", "shape": "dot", "size": 10, "title": "Definition and discussion on model bias."}, {"color": "#d7aab7", "id": "UnderfittingLinearModel", "label": "UnderfittingLinearModel", "shape": "dot", "size": 10, "title": "Analysis of underfitting with linear models."}, {"color": "#12acf8", "id": "FifthDegreePolynomial", "label": "FifthDegreePolynomial", "shape": "dot", "size": 10, "title": "Discussion on fitting a 5th-degree polynomial to data."}, {"color": "#c58169", "id": "OverfittingFifthDegree", "label": "OverfittingFifthDegree", "shape": "dot", "size": 10, "title": "Analysis of overfitting with fifth-degree polynomials."}, {"color": "#17891a", "id": "GeneralizationFailure", "label": "GeneralizationFailure", "shape": "dot", "size": 10, "title": "Discussion on the model\u0027s inability to generalize well."}, {"color": "#e1b925", "id": "PolynomialFitting", "label": "PolynomialFitting", "shape": "dot", "size": 10, "title": "Discussion on fitting polynomials to data sets."}, {"color": "#680269", "id": "Variance", "label": "Variance", "shape": "dot", "size": 10, "title": "Description and impact of variance in model fitting."}, {"color": "#e44faa", "id": "BiasVsVarianceTradeoff", "label": "BiasVsVarianceTradeoff", "shape": "dot", "size": 10, "title": "Overview of the trade-off between bias and variance in models."}, {"color": "#b60b34", "id": "Model Complexity", "label": "Model Complexity", "shape": "dot", "size": 10, "title": "Degree of flexibility and capacity of a model to fit data, impacting bias and variance."}, {"color": "#68c9ff", "id": "Test Error Decomposition", "label": "Test Error Decomposition", "shape": "dot", "size": 10, "title": "Breaking down test error into components of bias and variance."}, {"color": "#769dd9", "id": "Quadratic Model Example", "label": "Quadratic Model Example", "shape": "dot", "size": 10, "title": "Illustration showing how quadratic models balance bias and variance effectively."}, {"color": "#59b16d", "id": "TrainingDataset", "label": "TrainingDataset", "shape": "dot", "size": 10, "title": "Description of a training dataset for regression analysis."}, {"color": "#637747", "id": "ModelTraining", "label": "ModelTraining", "shape": "dot", "size": 10, "title": "Process of training a model on the given dataset."}, {"color": "#eca791", "id": "TestExample", "label": "TestExample", "shape": "dot", "size": 10, "title": "An example used to test the trained model\u0027s performance."}, {"color": "#04d238", "id": "ExpectedTestError", "label": "ExpectedTestError", "shape": "dot", "size": 10, "title": "Calculation of expected error on a test set."}, {"color": "#0f3b1f", "id": "MSEDecomposition", "label": "MSEDecomposition", "shape": "dot", "size": 10, "title": "Breaking down the mean squared error into bias and variance terms."}, {"color": "#f7e227", "id": "Claim811", "label": "Claim811", "shape": "dot", "size": 10, "title": "Mathematical claim used to decompose MSE into bias and variance."}, {"color": "#bc5acb", "id": "BiasVarianceTradeoff", "label": "BiasVarianceTradeoff", "shape": "dot", "size": 10, "title": "Concept describing the trade-off between model complexity and generalization ability."}, {"color": "#5fd02f", "id": "AverageModel", "label": "AverageModel", "shape": "dot", "size": 10, "title": "Definition of the average model as a theoretical construct for analysis"}, {"color": "#fe4358", "id": "BiasTerm", "label": "BiasTerm", "shape": "dot", "size": 10, "title": "Explanation and definition of the bias term in MSE decomposition"}, {"color": "#b1ca11", "id": "VarianceTerm", "label": "VarianceTerm", "shape": "dot", "size": 10, "title": "Explanation and definition of the variance term in MSE decomposition"}, {"color": "#d2dc5e", "id": "Variance Term", "label": "Variance Term", "shape": "dot", "size": 10, "title": "Error due to model sensitivity to training data randomness."}, {"color": "#89db1e", "id": "Model-wise Double Descent", "label": "Model-wise Double Descent", "shape": "dot", "size": 10, "title": "Test error decreases, increases, then decreases again as model complexity grows."}, {"color": "#bcbab4", "id": "Model_Wise_Double_Descent", "label": "Model_Wise_Double_Descent", "shape": "dot", "size": 10, "title": "Explanation of how test error changes with respect to model parameters."}, {"color": "#d993fe", "id": "Sample_Wise_Double_Descent", "label": "Sample_Wise_Double_Descent", "shape": "dot", "size": 10, "title": "Description of the relationship between sample size and test error, similar to model-wise double descent."}, {"color": "#79f3cb", "id": "Overparameterized_Models", "label": "Overparameterized_Models", "shape": "dot", "size": 10, "title": "Discussion on the benefits and implications of using overparameterized models in machine learning."}, {"color": "#023385", "id": "Sample-wise Double Descent", "label": "Sample-wise Double Descent", "shape": "dot", "size": 10, "title": "Peak in test error occurs at n approximately equal to d due to suboptimal training algorithms."}, {"color": "#f50a70", "id": "Optimization Algorithms", "label": "Optimization Algorithms", "shape": "dot", "size": 10, "title": "Current algorithms are suboptimal at n approximately equal to d, leading to peak test error."}, {"color": "#c48a6f", "id": "Regularization Techniques", "label": "Regularization Techniques", "shape": "dot", "size": 10, "title": "Optimally tuned regularization can mitigate the double descent effect."}, {"color": "#8dd5bc", "id": "Implicit Regularization", "label": "Implicit Regularization", "shape": "dot", "size": 10, "title": "Common optimizers like gradient descent provide implicit regularization, improving generalization in overparameterized models."}, {"color": "#058275", "id": "GradientDescentOptimizer", "label": "GradientDescentOptimizer", "shape": "dot", "size": 10, "title": "Optimization technique that finds minimum norm solution in overparameterized models."}, {"color": "#72b831", "id": "MinimumNormSolution", "label": "MinimumNormSolution", "shape": "dot", "size": 10, "title": "Solution found by gradient descent with zero initialization in linear models."}, {"color": "#be87ca", "id": "OverparameterizationRegime", "label": "OverparameterizationRegime", "shape": "dot", "size": 10, "title": "Scenario where the number of parameters exceeds the number of samples."}, {"color": "#91ba25", "id": "DoubleDescentPhenomenon", "label": "DoubleDescentPhenomenon", "shape": "star", "size": 25, "title": "A phenomenon in machine learning where performance improves after initially getting worse as model complexity increases."}, {"color": "#c5f6a6", "id": "ModelComplexityMeasures", "label": "ModelComplexityMeasures", "shape": "dot", "size": 10, "title": "Different measures of model complexity such as number of parameters and norm of the model."}, {"color": "#3f51c7", "id": "NormAsComplexityMeasure", "label": "NormAsComplexityMeasure", "shape": "dot", "size": 10, "title": "Using the norm of the learned model to measure complexity can avoid double descent in certain cases."}, {"color": "#57ddf8", "id": "DeepNeuralNetworks", "label": "DeepNeuralNetworks", "shape": "star", "size": 25, "title": "Discussion on finding correct complexity measures for deep neural networks."}, {"color": "#0c115c", "id": "Linear Regression Model", "label": "Linear Regression Model", "shape": "dot", "size": 10, "title": "Model used for predicting a quantitative response using linear relationships between variables."}, {"color": "#3d6687", "id": "Sample Complexity Bounds", "label": "Sample Complexity Bounds", "shape": "star", "size": 25, "title": "Theoretical analysis of the number of samples needed to ensure good generalization performance."}, {"color": "#a37b18", "id": "Model Selection Methods", "label": "Model Selection Methods", "shape": "dot", "size": 10, "title": "Techniques for choosing the best model from a set of candidates based on training data performance."}, {"color": "#43ee5e", "id": "Generalization Error", "label": "Generalization Error", "shape": "dot", "size": 10, "title": "Error rate of a hypothesis when applied to unseen data, distinct from training error."}, {"color": "#f69a88", "id": "Machine_Learning_Theory", "label": "Machine_Learning_Theory", "shape": "star", "size": 25, "title": "Theoretical foundations of machine learning including concepts like generalization and sample complexity."}, {"color": "#d9f364", "id": "Union_Bound", "label": "Union_Bound", "shape": "dot", "size": 10, "title": "Probability bound for the union of events in probability theory."}, {"color": "#1f49c8", "id": "Hoeffding_Inequality", "label": "Hoeffding_Inequality", "shape": "dot", "size": 10, "title": "Bound on the deviation between sample mean and true mean for Bernoulli random variables."}, {"color": "#b67d89", "id": "Chernoff_Bound", "label": "Chernoff_Bound", "shape": "dot", "size": 10, "title": "Alternative name for Hoeffding inequality in learning theory, emphasizing small probability of large deviations."}, {"color": "#41abf1", "id": "Binary_Classification", "label": "Binary_Classification", "shape": "dot", "size": 10, "title": "Classification problem where labels are binary (0 or 1)."}, {"color": "#a2d080", "id": "TrainingSet", "label": "TrainingSet", "shape": "dot", "size": 10, "title": "A collection of training examples used to train a machine learning model."}, {"color": "#62ce80", "id": "HypothesisFunction", "label": "HypothesisFunction", "shape": "dot", "size": 10, "title": "A function that maps input data to predicted labels based on learned parameters."}, {"color": "#571e8d", "id": "TrainingError", "label": "TrainingError", "shape": "dot", "size": 10, "title": "The fraction of training examples misclassified by the hypothesis function."}, {"color": "#900f8a", "id": "GeneralizationError", "label": "GeneralizationError", "shape": "dot", "size": 10, "title": "Probability that a new example will be misclassified based on learned parameters."}, {"color": "#5d6c30", "id": "PACAssumptions", "label": "PACAssumptions", "shape": "dot", "size": 10, "title": "Probably Approximately Correct framework assumptions including same distribution for training and testing."}, {"color": "#45baf0", "id": "LinearClassification", "label": "LinearClassification", "shape": "dot", "size": 10, "title": "A type of classification where the decision boundary is linear."}, {"color": "#68e757", "id": "EmpiricalRiskMinimization", "label": "EmpiricalRiskMinimization", "shape": "dot", "size": 10, "title": "Process of minimizing training error to find optimal parameters for a hypothesis function."}, {"color": "#cb446a", "id": "Empirical_Risk_Minimization", "label": "Empirical_Risk_Minimization", "shape": "dot", "size": 10, "title": "Process of minimizing training error by selecting parameters that minimize empirical risk."}, {"color": "#93dd50", "id": "Hypothesis_Class", "label": "Hypothesis_Class", "shape": "dot", "size": 10, "title": "Set of all classifiers considered by a learning algorithm, abstracting from specific parameterization."}, {"color": "#2df08d", "id": "Finite_Hypothesis_Class", "label": "Finite_Hypothesis_Class", "shape": "dot", "size": 10, "title": "Case where hypothesis class consists of finite number of hypotheses for empirical risk minimization."}, {"color": "#103e61", "id": "GeneralizationErrorGuarantees", "label": "GeneralizationErrorGuarantees", "shape": "dot", "size": 10, "title": "Strategies to ensure that training performance predicts generalization performance."}, {"color": "#b99a84", "id": "BernoulliRandomVariableZ", "label": "BernoulliRandomVariableZ", "shape": "dot", "size": 10, "title": "A random variable indicating misclassification of a hypothesis on a single example."}, {"color": "#db7dc9", "id": "TrainingSetSampling", "label": "TrainingSetSampling", "shape": "dot", "size": 10, "title": "Process of sampling training examples independently from the distribution D."}, {"color": "#efca4c", "id": "HoeffdingInequality", "label": "HoeffdingInequality", "shape": "dot", "size": 10, "title": "Probability bound on the difference between empirical and true error for a single hypothesis."}, {"color": "#a7897c", "id": "Uniform_Convergence", "label": "Uniform_Convergence", "shape": "star", "size": 25, "title": "A result showing that the empirical error is close to the true error for all hypotheses in a class."}, {"color": "#80ad1c", "id": "Training_Error_Generalization_Error", "label": "Training_Error_Generalization_Error", "shape": "dot", "size": 10, "title": "The relationship between training error and generalization error under uniform convergence."}, {"color": "#76fab6", "id": "Union_Bound_Application", "label": "Union_Bound_Application", "shape": "dot", "size": 10, "title": "Using the union bound to extend a result from one hypothesis to all hypotheses in a class."}, {"color": "#a89a55", "id": "Probability_Error", "label": "Probability_Error", "shape": "dot", "size": 10, "title": "The probability that training error is within a certain range of generalization error for all hypotheses."}, {"color": "#331622", "id": "Sample_Size_Calculation", "label": "Sample_Size_Calculation", "shape": "dot", "size": 10, "title": "Calculating the minimum sample size needed to ensure a given level of confidence in the proximity of training and generalization errors."}, {"color": "#78ac7e", "id": "Sample_Complexity", "label": "Sample_Complexity", "shape": "dot", "size": 10, "title": "Number of training examples required to achieve a certain level of performance with high probability."}, {"color": "#754289", "id": "Uniform_Convergence_Bound", "label": "Uniform_Convergence_Bound", "shape": "dot", "size": 10, "title": "Mathematical bound on the difference between empirical and true error for all hypotheses in the hypothesis space."}, {"color": "#6c9768", "id": "Training_Error_vs_Generalization_Error", "label": "Training_Error_vs_Generalization_Error", "shape": "dot", "size": 10, "title": "Relationship between training set performance and actual model performance on unseen data."}, {"color": "#09e94b", "id": "Hypothesis_Space_Size", "label": "Hypothesis_Space_Size", "shape": "dot", "size": 10, "title": "Impact of the size of hypothesis space on uniform convergence bound."}, {"color": "#1db911", "id": "Optimal_Hypothesis_Selection", "label": "Optimal_Hypothesis_Selection", "shape": "dot", "size": 10, "title": "Process of selecting a hypothesis that minimizes empirical error and its implications for generalization performance."}, {"color": "#84e899", "id": "UniformConvergence", "label": "UniformConvergence", "shape": "dot", "size": 10, "title": "Theoretical concept ensuring that the empirical risk is close to the true risk with high probability."}, {"color": "#552798", "id": "HypothesisSpace", "label": "HypothesisSpace", "shape": "dot", "size": 10, "title": "Set of all possible hypotheses or models considered in a learning problem."}, {"color": "#2ea5bc", "id": "Machine_Learning_Bias_Variance_Tradeoff", "label": "Machine_Learning_Bias_Variance_Tradeoff", "shape": "star", "size": 25, "title": "Exploration of bias-variance tradeoff in machine learning hypothesis classes."}, {"color": "#58e6db", "id": "Hypothesis_Class_Switching", "label": "Hypothesis_Class_Switching", "shape": "dot", "size": 10, "title": "Analysis of switching to a larger hypothesis class and its impact on bias and variance."}, {"color": "#1c0716", "id": "Sample_Complexity_Bound", "label": "Sample_Complexity_Bound", "shape": "dot", "size": 10, "title": "Derivation of sample complexity bound for finite hypothesis classes."}, {"color": "#a6f228", "id": "Infinite_Hypothesis_Classes", "label": "Infinite_Hypothesis_Classes", "shape": "dot", "size": 10, "title": "Discussion on handling infinite hypothesis classes in machine learning theory."}, {"color": "#32d4d7", "id": "Hypothesis_Class_Size", "label": "Hypothesis_Class_Size", "shape": "dot", "size": 10, "title": "Size of the hypothesis class in terms of model parameters."}, {"color": "#88d95f", "id": "Floating_Point_Precision", "label": "Floating_Point_Precision", "shape": "dot", "size": 10, "title": "Impact of floating point precision on the size of hypothesis class."}, {"color": "#829d96", "id": "Linear_Classifiers", "label": "Linear_Classifiers", "shape": "dot", "size": 10, "title": "Class of classifiers that are linear in parameters."}, {"color": "#248291", "id": "ParameterizationOfH", "label": "ParameterizationOfH", "shape": "dot", "size": 10, "title": "Discussion on parameterizing the hypothesis class \u03a9."}, {"color": "#886cd2", "id": "LinearClassifiers", "label": "LinearClassifiers", "shape": "dot", "size": 10, "title": "Explanation of linear classifiers with different parameter sets."}, {"color": "#497e7d", "id": "ShatteringConcept", "label": "ShatteringConcept", "shape": "dot", "size": 10, "title": "Definition and explanation of the shattering concept in hypothesis classes."}, {"color": "#15e247", "id": "VCDimension", "label": "VCDimension", "shape": "dot", "size": 10, "title": "Introduction to Vapnik-Chervonenkis dimension as a measure of complexity for \u03a9."}, {"color": "#1ad281", "id": "VC Dimension", "label": "VC Dimension", "shape": "star", "size": 25, "title": "Measure of the capacity of a statistical classification algorithm"}, {"color": "#c85516", "id": "Shattering", "label": "Shattering", "shape": "dot", "size": 10, "title": "A set is shattered by a hypothesis class if every possible labeling can be achieved"}, {"color": "#a8c90a", "id": "Vapnik\u0027s Theorem", "label": "Vapnik\u0027s Theorem", "shape": "star", "size": 25, "title": "Theorem providing bounds on the difference between empirical and true error rates"}, {"color": "#36123f", "id": "Hypothesis Class", "label": "Hypothesis Class", "shape": "dot", "size": 10, "title": "A collection of hypotheses or models used in learning theory"}, {"color": "#45b64a", "id": "Uniform Convergence", "label": "Uniform Convergence", "shape": "dot", "size": 10, "title": "Property where empirical error approximates true error for all hypotheses as sample size grows"}, {"color": "#904930", "id": "Corollary", "label": "Corollary", "shape": "star", "size": 25, "title": "Result derived from Vapnik\u0027s theorem regarding the number of training examples needed"}, {"color": "#65e474", "id": "Chapter9", "label": "Chapter9", "shape": "star", "size": 25, "title": "Regularization and model selection in machine learning"}, {"color": "#fb5e3d", "id": "ModelComplexity", "label": "ModelComplexity", "shape": "dot", "size": 10, "title": "Measured by number of parameters or function of parameters like \u03b2\u2082 norm"}, {"color": "#f11ae5", "id": "TrainingLoss", "label": "TrainingLoss", "shape": "dot", "size": 10, "title": "Modified with regularizer to prevent overfitting, denoted as J_\u03bb(\u03b8)"}, {"color": "#4f451b", "id": "Regularizer", "label": "Regularizer", "shape": "dot", "size": 10, "title": "Additional term added to training loss, denoted by R(\u03b8), measures model complexity"}, {"color": "#175121", "id": "LambdaParameter", "label": "LambdaParameter", "shape": "dot", "size": 10, "title": "Regularization parameter \u03bb that balances original loss and regularizer"}, {"color": "#f623c2", "id": "Regularized_Loss", "label": "Regularized_Loss", "shape": "dot", "size": 10, "title": "Combination of original loss and regularizer, controlled by parameter lambda."}, {"color": "#7a2b78", "id": "Lambda_Parameter", "label": "Lambda_Parameter", "shape": "dot", "size": 10, "title": "Balances the trade-off between fitting data and model complexity."}, {"color": "#e0feb3", "id": "L2_Regularization", "label": "L2_Regularization", "shape": "dot", "size": 10, "title": "Encourages small L2 norm of parameters, often called weight decay in deep learning."}, {"color": "#cfde38", "id": "Weight_Decay", "label": "Weight_Decay", "shape": "dot", "size": 10, "title": "Gradient descent with regularization shrinks weights by a scalar factor."}, {"color": "#830cfc", "id": "Inductive_Biases", "label": "Inductive_Biases", "shape": "dot", "size": 10, "title": "Regulation can impose prior beliefs or structures on model parameters to guide learning."}, {"color": "#e34976", "id": "Regularization in Machine Learning", "label": "Regularization in Machine Learning", "shape": "star", "size": 25, "title": "Techniques to prevent overfitting by adding constraints or penalties."}, {"color": "#75f00d", "id": "Sparsity Regularization", "label": "Sparsity Regularization", "shape": "dot", "size": 10, "title": "Encourages model parameters to be sparse (many zero values)."}, {"color": "#71c3e2", "id": "L0 Norm", "label": "L0 Norm", "shape": "dot", "size": 10, "title": "Counts the number of non-zero elements."}, {"color": "#fe982a", "id": "L1 Norm (LASSO)", "label": "L1 Norm (LASSO)", "shape": "dot", "size": 10, "title": "Penalizes the sum of absolute values of coefficients."}, {"color": "#bc3b95", "id": "L2 Norm Regularization", "label": "L2 Norm Regularization", "shape": "dot", "size": 10, "title": "Penalizes the sum of squared values of coefficients, commonly used with kernel methods."}, {"color": "#3a7049", "id": "Deep Learning Regularizers", "label": "Deep Learning Regularizers", "shape": "dot", "size": 10, "title": "Techniques specific to deep learning models for regularization."}, {"color": "#20f764", "id": "Weight Decay", "label": "Weight Decay", "shape": "dot", "size": 10, "title": "Equivalent to L2 norm regularization, reduces model complexity."}, {"color": "#c4aeb5", "id": "Dropout", "label": "Dropout", "shape": "dot", "size": 10, "title": "Randomly drops units during training to prevent overfitting."}, {"color": "#8da6a9", "id": "Data Augmentation", "label": "Data Augmentation", "shape": "dot", "size": 10, "title": "Increases diversity of training data by applying transformations."}, {"color": "#fc97f4", "id": "Spectral Norm Regularization", "label": "Spectral Norm Regularization", "shape": "dot", "size": 10, "title": "Penalizes the spectral norm of weight matrices to control model complexity."}, {"color": "#643ad3", "id": "Lipschitzness Regularization", "label": "Lipschitzness Regularization", "shape": "dot", "size": 10, "title": "Regulates the Lipschitz constant to ensure bounded sensitivity to input changes."}, {"color": "#3db804", "id": "Regularization in Deep Learning", "label": "Regularization in Deep Learning", "shape": "star", "size": 25, "title": "Overview of regularization techniques and concepts in deep learning."}, {"color": "#7499e3", "id": "Explicit Regularization Techniques", "label": "Explicit Regularization Techniques", "shape": "dot", "size": 10, "title": "Techniques like weight decay, dropout, data augmentation, spectral norm regularization, and Lipschitz regularizers."}, {"color": "#4f9f5c", "id": "Implicit Regularization Effect", "label": "Implicit Regularization Effect", "shape": "dot", "size": 10, "title": "The impact of optimizers on model generalization beyond explicit regularization."}, {"color": "#a72caa", "id": "Optimizer Bias", "label": "Optimizer Bias", "shape": "dot", "size": 10, "title": "How different optimizers converge to different global minima with varying properties and generalization performance."}, {"color": "#d8e0c7", "id": "Optimizers and Generalization", "label": "Optimizers and Generalization", "shape": "star", "size": 25, "title": "Discusses how optimizers affect model generalization."}, {"color": "#8a05df", "id": "Global Minima and Generalization", "label": "Global Minima and Generalization", "shape": "dot", "size": 10, "title": "Investigates how different global minima affect model performance."}, {"color": "#2eabb9", "id": "Learning Rate Schedules", "label": "Learning Rate Schedules", "shape": "dot", "size": 10, "title": "Examines the impact of learning rate schedules on generalization."}, {"color": "#22206a", "id": "Model Selection via Cross Validation", "label": "Model Selection via Cross Validation", "shape": "star", "size": 25, "title": "Describes model selection techniques using cross-validation."}, {"color": "#02c546", "id": "Model Selection", "label": "Model Selection", "shape": "star", "size": 25, "title": "Process of choosing the best model for a given task."}, {"color": "#f0e97d", "id": "Cross Validation", "label": "Cross Validation", "shape": "dot", "size": 10, "title": "Technique to evaluate models and select the best one based on performance metrics."}, {"color": "#d7ae00", "id": "Polynomial Regression Models", "label": "Polynomial Regression Models", "shape": "dot", "size": 10, "title": "Models with varying degrees of polynomial terms for regression analysis."}, {"color": "#3131b1", "id": "SVM and Regularization", "label": "SVM and Regularization", "shape": "dot", "size": 10, "title": "Support Vector Machines with regularization techniques like L1 to control model complexity."}, {"color": "#896fe6", "id": "Finite Model Set", "label": "Finite Model Set", "shape": "dot", "size": 10, "title": "A predefined set of models from which the best one is selected using cross validation."}, {"color": "#2f1552", "id": "Infinite Model Space", "label": "Infinite Model Space", "shape": "dot", "size": 10, "title": "Techniques to handle continuous model parameters by discretization or optimization in a large space."}, {"color": "#981324", "id": "Cross_Validation", "label": "Cross_Validation", "shape": "star", "size": 25, "title": "Technique to evaluate and select models using a validation set."}, {"color": "#b13cd8", "id": "Hold_Out_Cross_Validation", "label": "Hold_Out_Cross_Validation", "shape": "dot", "size": 10, "title": "Split data into training and validation sets for model selection."}, {"color": "#c283c3", "id": "Training_Error", "label": "Training_Error", "shape": "dot", "size": 10, "title": "Error of a hypothesis on the training set used to train it."}, {"color": "#89cbbe", "id": "Validation_Error", "label": "Validation_Error", "shape": "dot", "size": 10, "title": "Error of a hypothesis on the validation set not seen during training."}, {"color": "#e41048", "id": "MachineLearningValidationTechniques", "label": "MachineLearningValidationTechniques", "shape": "star", "size": 25, "title": "Overview of validation techniques in machine learning."}, {"color": "#fa8a03", "id": "HoldoutCrossValidation", "label": "HoldoutCrossValidation", "shape": "dot", "size": 10, "title": "Method where a portion of data is held out for validation purposes."}, {"color": "#6f05dd", "id": "kFoldCrossValidation", "label": "kFoldCrossValidation", "shape": "dot", "size": 10, "title": "A method to validate models by splitting data into k subsets and training on k-1 while testing on 1."}, {"color": "#945c8b", "id": "ModelSelection", "label": "ModelSelection", "shape": "dot", "size": 10, "title": "Process of selecting a model based on validation error estimates."}, {"color": "#e746dd", "id": "ValidationSetSize", "label": "ValidationSetSize", "shape": "dot", "size": 10, "title": "Discussion on the appropriate size for the validation set in relation to total data availability."}, {"color": "#425fff", "id": "RetrainingModel", "label": "RetrainingModel", "shape": "dot", "size": 10, "title": "Option of retraining selected model with full dataset after cross-validation."}, {"color": "#39bdb6", "id": "MachineLearningChallenges", "label": "MachineLearningChallenges", "shape": "star", "size": 25, "title": "Challenges in machine learning when data is scarce."}, {"color": "#bd4458", "id": "LeaveOneOutCV", "label": "LeaveOneOutCV", "shape": "dot", "size": 10, "title": "A variant of cross validation where one example is held out for testing each time."}, {"color": "#c7505e", "id": "DataSplitting", "label": "DataSplitting", "shape": "dot", "size": 10, "title": "Process of splitting data into k disjoint subsets for training and validation."}, {"color": "#ec4dc6", "id": "ModelEvaluation", "label": "ModelEvaluation", "shape": "dot", "size": 10, "title": "Procedure to evaluate models using cross-validation techniques."}, {"color": "#be85f9", "id": "GeneralizationErrorEstimation", "label": "GeneralizationErrorEstimation", "shape": "dot", "size": 10, "title": "Method to estimate the error of a model\u0027s performance on unseen data."}, {"color": "#c6dd38", "id": "Cross Validation Techniques", "label": "Cross Validation Techniques", "shape": "star", "size": 25, "title": "Techniques for model evaluation and selection."}, {"color": "#56a574", "id": "Leave-One-Out Cross Validation", "label": "Leave-One-Out Cross Validation", "shape": "dot", "size": 10, "title": "Method where one training example is held out at a time."}, {"color": "#cb8fc3", "id": "Bayesian Statistics", "label": "Bayesian Statistics", "shape": "star", "size": 25, "title": "Approach to parameter estimation considering parameters as random variables."}, {"color": "#ea8f19", "id": "Frequentist View", "label": "Frequentist View", "shape": "dot", "size": 10, "title": "View where parameters are constant but unknown values."}, {"color": "#98bef1", "id": "Maximum Likelihood Estimation (MLE)", "label": "Maximum Likelihood Estimation (MLE)", "shape": "dot", "size": 10, "title": "Estimation method maximizing likelihood of observed data under parameter \u03b8."}, {"color": "#74ae6d", "id": "Bayesian Machine Learning", "label": "Bayesian Machine Learning", "shape": "star", "size": 25, "title": "Overview of Bayesian approaches in machine learning."}, {"color": "#a52eac", "id": "Posterior Distribution on Parameters", "label": "Posterior Distribution on Parameters", "shape": "dot", "size": 10, "title": "Distribution of parameters given the data and prior knowledge."}, {"color": "#ad3331", "id": "Model Specification", "label": "Model Specification", "shape": "dot", "size": 10, "title": "Definition of the probabilistic model used for prediction."}, {"color": "#a771e6", "id": "Logistic Regression Example", "label": "Logistic Regression Example", "shape": "dot", "size": 10, "title": "Example using Bayesian logistic regression for binary classification."}, {"color": "#6ff520", "id": "Prediction on New Data", "label": "Prediction on New Data", "shape": "dot", "size": 10, "title": "Process of predicting class labels or expected values given new data and posterior distribution."}, {"color": "#322523", "id": "Fully Bayesian Prediction", "label": "Fully Bayesian Prediction", "shape": "dot", "size": 10, "title": "Method that averages predictions over the posterior distribution of parameters."}, {"color": "#93414d", "id": "Computational Challenges", "label": "Computational Challenges", "shape": "dot", "size": 10, "title": "Difficulties in computing high-dimensional integrals for posterior distributions."}, {"color": "#53c853", "id": "BayesianInference", "label": "BayesianInference", "shape": "dot", "size": 10, "title": "Techniques for estimating parameters using prior knowledge."}, {"color": "#f19f9f", "id": "MAPEstimation", "label": "MAPEstimation", "shape": "dot", "size": 10, "title": "Finding the most probable parameter values given data and priors."}, {"color": "#3d7e31", "id": "PriorDistributions", "label": "PriorDistributions", "shape": "dot", "size": 10, "title": "Selection of prior probabilities for parameters."}, {"color": "#fe0d49", "id": "UnsupervisedLearning", "label": "UnsupervisedLearning", "shape": "star", "size": 25, "title": "Techniques to find hidden structure in unlabeled data."}, {"color": "#e14244", "id": "Clustering", "label": "Clustering", "shape": "dot", "size": 10, "title": "Grouping a set of objects into clusters based on similarity."}, {"color": "#56b998", "id": "KMeansAlgorithm", "label": "KMeansAlgorithm", "shape": "dot", "size": 10, "title": "Clustering algorithm that partitions data into k clusters based on mean distance."}, {"color": "#b7133e", "id": "k-means_algorithm", "label": "k-means_algorithm", "shape": "star", "size": 25, "title": "Clustering algorithm that partitions data into k clusters."}, {"color": "#ae2ef8", "id": "distortion_function", "label": "distortion_function", "shape": "dot", "size": 10, "title": "Measures sum of squared distances between examples and cluster centroids."}, {"color": "#291e29", "id": "initialization_step", "label": "initialization_step", "shape": "dot", "size": 10, "title": "Randomly selects k training examples as initial cluster centroids."}, {"color": "#28f18e", "id": "inner_loop_steps", "label": "inner_loop_steps", "shape": "dot", "size": 10, "title": "Assigns examples to closest centroid and updates centroids based on assigned points."}, {"color": "#4da2fc", "id": "convergence_property", "label": "convergence_property", "shape": "dot", "size": 10, "title": "Guaranteed to converge in a certain sense, minimizing distortion function iteratively."}, {"color": "#54de5e", "id": "DistortionFunctionJ", "label": "DistortionFunctionJ", "shape": "dot", "size": 10, "title": "Non-convex function used to measure the quality of clustering in K-means."}, {"color": "#7ad9d9", "id": "ConvergenceProperties", "label": "ConvergenceProperties", "shape": "dot", "size": 10, "title": "Properties related to convergence and local optima in K-means algorithm."}, {"color": "#bb58c4", "id": "EMAlgorithms", "label": "EMAlgorithms", "shape": "star", "size": 25, "title": "Set of notes discussing EM algorithms for density estimation."}, {"color": "#0ec7e7", "id": "MixtureOfGaussians", "label": "MixtureOfGaussians", "shape": "dot", "size": 10, "title": "Modeling data using a mixture of Gaussian distributions in an unsupervised setting."}, {"color": "#aadcc2", "id": "Unsupervised Learning", "label": "Unsupervised Learning", "shape": "star", "size": 25, "title": "Learning setting without labeled data"}, {"color": "#b702ae", "id": "Joint Distribution", "label": "Joint Distribution", "shape": "dot", "size": 10, "title": "Distribution modeling both observed and latent variables"}, {"color": "#d80173", "id": "Mixture of Gaussians Model", "label": "Mixture of Gaussians Model", "shape": "dot", "size": 10, "title": "Model using multiple Gaussian distributions with hidden variables"}, {"color": "#be3d6a", "id": "Latent Variables", "label": "Latent Variables", "shape": "dot", "size": 10, "title": "Hidden random variables that affect the observed data generation process"}, {"color": "#853bce", "id": "Model Parameters", "label": "Model Parameters", "shape": "dot", "size": 10, "title": "Parameters including \u03c6, \u03bc and \u03a3 for the model estimation"}, {"color": "#3bc2f4", "id": "Likelihood Function", "label": "Likelihood Function", "shape": "dot", "size": 10, "title": "Function to estimate parameters by maximizing likelihood of observed data"}, {"color": "#b43113", "id": "DensityEstimation", "label": "DensityEstimation", "shape": "dot", "size": 10, "title": "Techniques for estimating probability densities from data."}, {"color": "#e6e296", "id": "GaussianMixtureModels", "label": "GaussianMixtureModels", "shape": "dot", "size": 10, "title": "Modeling data with a mixture of Gaussian distributions."}, {"color": "#510ab4", "id": "EMAlgorithm", "label": "EMAlgorithm", "shape": "dot", "size": 10, "title": "Iterative algorithm for finding maximum likelihood estimates in probabilistic models with latent variables."}, {"color": "#3e14d8", "id": "EStep", "label": "EStep", "shape": "dot", "size": 10, "title": "Expectation step where posterior probabilities of hidden variables are calculated."}, {"color": "#de9e26", "id": "MStep", "label": "MStep", "shape": "dot", "size": 10, "title": "Maximization step where model parameters are updated based on the E-step\u0027s results."}, {"color": "#4cd19e", "id": "EM_Algorithm", "label": "EM_Algorithm", "shape": "star", "size": 25, "title": "Iterative method for finding maximum likelihood estimates of parameters in probabilistic models with latent variables."}, {"color": "#bd283b", "id": "E_Step", "label": "E_Step", "shape": "dot", "size": 10, "title": "Estimation step where posterior distribution of latent variables is calculated given observed data and current parameters."}, {"color": "#50ae42", "id": "M_Step", "label": "M_Step", "shape": "dot", "size": 10, "title": "Maximization step where parameters are updated to maximize the expected log-likelihood found in E-step."}, {"color": "#190987", "id": "Bayes_Rule", "label": "Bayes_Rule", "shape": "dot", "size": 10, "title": "Used to calculate posterior probabilities of latent variables given observed data and current parameter estimates."}, {"color": "#5b3140", "id": "Gaussian_Distribution", "label": "Gaussian_Distribution", "shape": "dot", "size": 10, "title": "Probability distribution used for modeling the likelihood of observations in EM algorithm."}, {"color": "#97369f", "id": "Soft_Guesses", "label": "Soft_Guesses", "shape": "dot", "size": 10, "title": "Probabilistic assignments of latent variables, taking values between 0 and 1."}, {"color": "#28161b", "id": "Hard_Assignments", "label": "Hard_Assignments", "shape": "dot", "size": 10, "title": "Binary assignments used in K-means clustering as opposed to probabilistic assignments in EM."}, {"color": "#7c5fcc", "id": "K_Means_Clustering", "label": "K_Means_Clustering", "shape": "star", "size": 25, "title": "Clustering algorithm that assigns data points to clusters based on hard assignments."}, {"color": "#5b3d1c", "id": "Local_Optima", "label": "Local_Optima", "shape": "dot", "size": 10, "title": "Potential issue where EM may converge to suboptimal solutions due to initialization and iterative nature."}, {"color": "#89ec6d", "id": "Convergence_Guarantees", "label": "Convergence_Guarantees", "shape": "dot", "size": 10, "title": "Analysis of conditions under which the EM algorithm converges to a global optimum."}, {"color": "#63dd9b", "id": "ConvergenceGuarantees", "label": "ConvergenceGuarantees", "shape": "dot", "size": 10, "title": "Discussion on convergence guarantees for EM algorithm."}, {"color": "#1b6e2e", "id": "JensensInequality", "label": "JensensInequality", "shape": "star", "size": 25, "title": "Introduction to Jensen\u0027s inequality and its application in machine learning."}, {"color": "#21d788", "id": "ConvexFunctionDefinition", "label": "ConvexFunctionDefinition", "shape": "dot", "size": 10, "title": "Definition of convex functions including strict convexity."}, {"color": "#824ac9", "id": "TheoremStatement", "label": "TheoremStatement", "shape": "dot", "size": 10, "title": "Formal statement of Jensen\u0027s inequality theorem and its implications."}, {"color": "#a66ccf", "id": "Jensen\u0027s Inequality", "label": "Jensen\u0027s Inequality", "shape": "star", "size": 25, "title": "Inequality describing the relationship between expected values and convex functions."}, {"color": "#ac0d66", "id": "Convex Functions", "label": "Convex Functions", "shape": "dot", "size": 10, "title": "Functions where the line segment between any two points on the graph of the function lies above or on the graph."}, {"color": "#666e55", "id": "Concave Functions", "label": "Concave Functions", "shape": "dot", "size": 10, "title": "Negative of convex functions, with reversed inequality in Jensen\u0027s inequality."}, {"color": "#cca7cd", "id": "EM Algorithm", "label": "EM Algorithm", "shape": "star", "size": 25, "title": "Algorithm used for maximum likelihood estimation in models with latent variables."}, {"color": "#9c9169", "id": "Latent Variable Models", "label": "Latent Variable Models", "shape": "dot", "size": 10, "title": "Models where some of the variables are not observed directly but rather inferred from other observable variables."}, {"color": "#e5f375", "id": "OptimizationChallenges", "label": "OptimizationChallenges", "shape": "dot", "size": 10, "title": "Difficulties in optimizing parameters due to non-convex problems."}, {"color": "#df60b9", "id": "SingleExampleLikelihood", "label": "SingleExampleLikelihood", "shape": "dot", "size": 10, "title": "Simplifying exposition by considering likelihood for a single example first."}, {"color": "#2fc8af", "id": "SummationNotEssential", "label": "SummationNotEssential", "shape": "dot", "size": 10, "title": "The summation is not crucial in the initial derivation of EM algorithm."}, {"color": "#68588f", "id": "LogLikelihoodExpression", "label": "LogLikelihoodExpression", "shape": "dot", "size": 10, "title": "Rewriting the likelihood function for a single example as log p(x;\u03b8)."}, {"color": "#1d4337", "id": "DistributionQ", "label": "DistributionQ", "shape": "dot", "size": 10, "title": "Introducing distribution Q over possible values of latent variable z."}, {"color": "#8615ef", "id": "ProbabilityDistributions", "label": "ProbabilityDistributions", "shape": "dot", "size": 10, "title": "Discussion on probability distributions used in ML models."}, {"color": "#ab31dd", "id": "LogLikelihoodBound", "label": "LogLikelihoodBound", "shape": "dot", "size": 10, "title": "Derivation and explanation of the log-likelihood bound using Q distribution."}, {"color": "#720082", "id": "EvidenceLowerBoundELBO", "label": "EvidenceLowerBoundELBO", "shape": "dot", "size": 10, "title": "Definition and significance of the evidence lower bound (ELBO)."}, {"color": "#8579e8", "id": "PosteriorDistribution", "label": "PosteriorDistribution", "shape": "dot", "size": 10, "title": "Explanation of how to set Q(z) as the posterior distribution p(z|x;theta)."}, {"color": "#8c5827", "id": "EqualityCondition", "label": "EqualityCondition", "shape": "dot", "size": 10, "title": "Conditions under which Jensen\u0027s inequality holds with equality."}, {"color": "#04b4f8", "id": "LogLikelihoodOptimization", "label": "LogLikelihoodOptimization", "shape": "dot", "size": 10, "title": "Focuses on optimizing log-likelihood for a single example using EM."}, {"color": "#520782", "id": "MultipleExamplesExtension", "label": "MultipleExamplesExtension", "shape": "dot", "size": 10, "title": "Extends the EM algorithm to handle multiple training examples."}, {"color": "#4320df", "id": "ELBOFormula", "label": "ELBOFormula", "shape": "dot", "size": 10, "title": "Derivation and application of evidence lower bound formula for multiple examples."}, {"color": "#86f467", "id": "Log-Likelihood", "label": "Log-Likelihood", "shape": "star", "size": 25, "title": "Measure of how likely a given set of data is under a specific probability model."}, {"color": "#43939a", "id": "Convergence", "label": "Convergence", "shape": "dot", "size": 10, "title": "Condition where the algorithm stops improving the log-likelihood and reaches an optimal solution."}, {"color": "#e0cfed", "id": "Jensen\u0027s_Inequality", "label": "Jensen\u0027s_Inequality", "shape": "star", "size": 25, "title": "Mathematical result stating that for a convex function f, the value of the function at the expected value of x is less than or equal to the expected value of the function at x."}, {"color": "#609000", "id": "ELBO", "label": "ELBO", "shape": "dot", "size": 10, "title": "Evidence Lower BOund used in variational inference and EM algorithm as a lower bound on log-likelihood."}, {"color": "#07c4f4", "id": "KL_Divergence", "label": "KL_Divergence", "shape": "dot", "size": 10, "title": "Measure of difference between two probability distributions."}, {"color": "#0f57ec", "id": "MarginalDistribution", "label": "MarginalDistribution", "shape": "dot", "size": 10, "title": "Probability distribution of a subset of variables in the context of ELBO."}, {"color": "#5c7abf", "id": "ConditionalLikelihood", "label": "ConditionalLikelihood", "shape": "dot", "size": 10, "title": "Probability of observing data given latent variables in probabilistic models."}, {"color": "#1d0051", "id": "Expectation-Maximization Algorithm", "label": "Expectation-Maximization Algorithm", "shape": "star", "size": 25, "title": "Iterative method for finding maximum likelihood or maximum a posteriori estimates of parameters in statistical models."}, {"color": "#609c46", "id": "E-step", "label": "E-step", "shape": "dot", "size": 10, "title": "Calculates the expected value of the log-likelihood evaluated using the current estimate for the parameters."}, {"color": "#35448e", "id": "M-step", "label": "M-step", "shape": "dot", "size": 10, "title": "Maximizes the expected log-likelihood found in the E step as a function of the parameters."}, {"color": "#616d6e", "id": "Q-function", "label": "Q-function", "shape": "dot", "size": 10, "title": "Function used to calculate the probability of latent variables given observed data and current parameter estimates."}, {"color": "#c568ae", "id": "Update Rule for \\(\\mu_j\\)", "label": "Update Rule for \\(\\mu_j\\)", "shape": "dot", "size": 10, "title": "Rule derived from maximizing Q-function with respect to \\(\\mu_j\\)."}, {"color": "#faf790", "id": "Update Rule for \\(\\phi_j\\)", "label": "Update Rule for \\(\\phi_j\\)", "shape": "dot", "size": 10, "title": "Rule derived from maximizing Q-function with respect to \\(\\phi_j\\), left as an exercise."}, {"color": "#08b99b", "id": "Gaussian Mixture Model (GMM)", "label": "Gaussian Mixture Model (GMM)", "shape": "star", "size": 25, "title": "Statistical model used in machine learning for clustering and density estimation."}, {"color": "#fc2303", "id": "Update_Rule", "label": "Update_Rule", "shape": "dot", "size": 10, "title": "Rule for updating parameters based on weighted inputs."}, {"color": "#98883d", "id": "M-step_Update_for_phi_j", "label": "M-step_Update_for_phi_j", "shape": "dot", "size": 10, "title": "Derivation of the update rule for parameter phi_j in EM algorithm."}, {"color": "#1d059b", "id": "Lagrangian_Method", "label": "Lagrangian_Method", "shape": "dot", "size": 10, "title": "Use of Lagrangian to handle constraints during optimization."}, {"color": "#bddaf3", "id": "Variational_Inference", "label": "Variational_Inference", "shape": "star", "size": 25, "title": "Techniques for approximating posterior distributions in complex models using variational methods and neural networks."}, {"color": "#e6c13f", "id": "Variational_Autoencoder", "label": "Variational_Autoencoder", "shape": "dot", "size": 10, "title": "Model used to learn an efficient encoding of the input data by using an encoder and a decoder network."}, {"color": "#b0bd4f", "id": "EM_Algorithms", "label": "EM_Algorithms", "shape": "dot", "size": 10, "title": "Expectation-Maximization algorithm for finding maximum likelihood estimates in probabilistic models with latent variables."}, {"color": "#dcbb6d", "id": "Reparametrization_Trick", "label": "Reparametrization_Trick", "shape": "dot", "size": 10, "title": "Method enabling the use of gradient descent for variational inference in models with continuous latent variables."}, {"color": "#a8e544", "id": "Gaussian_Mixture_Models", "label": "Gaussian_Mixture_Models", "shape": "dot", "size": 10, "title": "Probabilistic model that assumes all data points are generated from a mixture of several Gaussian distributions with unknown parameters."}, {"color": "#45e41c", "id": "Neural_Network_Parameterization", "label": "Neural_Network_Parameterization", "shape": "dot", "size": 10, "title": "Parameterizing the distribution of latent variables using neural networks."}, {"color": "#286149", "id": "Posterior_Distribution", "label": "Posterior_Distribution", "shape": "dot", "size": 10, "title": "Probability distribution of a random variable given evidence about other related random variables."}, {"color": "#308756", "id": "Variational Inference", "label": "Variational Inference", "shape": "star", "size": 25, "title": "Technique to approximate posterior distributions in Bayesian models."}, {"color": "#5fd4f3", "id": "Mean Field Assumption", "label": "Mean Field Assumption", "shape": "dot", "size": 10, "title": "Assumption that latent variables are independent, simplifying the posterior approximation."}, {"color": "#b0becb", "id": "Discrete Latent Variables", "label": "Discrete Latent Variables", "shape": "dot", "size": 10, "title": "Latent variables with discrete values, often modeled using mean field assumption."}, {"color": "#784167", "id": "Continuous Latent Variables", "label": "Continuous Latent Variables", "shape": "dot", "size": 10, "title": "Latent variables with continuous values requiring more complex approximations than mean field alone."}, {"color": "#487afb", "id": "VariationalAutoEncoder", "label": "VariationalAutoEncoder", "shape": "dot", "size": 10, "title": "Technique for learning latent variable models using neural networks."}, {"color": "#7c5349", "id": "LatentVariableModel", "label": "LatentVariableModel", "shape": "dot", "size": 10, "title": "Models that use unobserved variables to represent hidden structure in data."}, {"color": "#9f648c", "id": "ContinuousLatentVariables", "label": "ContinuousLatentVariables", "shape": "dot", "size": 10, "title": "Use of continuous latent variables in probabilistic models."}, {"color": "#1439aa", "id": "GaussianDistributionQ_i", "label": "GaussianDistributionQ_i", "shape": "dot", "size": 10, "title": "Representation of distribution Q_i as a Gaussian with mean and variance functions."}, {"color": "#d0f821", "id": "MeanAndVarianceFunctions", "label": "MeanAndVarianceFunctions", "shape": "dot", "size": 10, "title": "Description of q(x;phi) for mean and v(x;psi) for variance in the Gaussian model."}, {"color": "#2890df", "id": "EncoderDecoderConcepts", "label": "EncoderDecoderConcepts", "shape": "dot", "size": 10, "title": "Introduction to encoder (q, v) and decoder (g(z;theta)) concepts in VAE."}, {"color": "#1a6006", "id": "ELBO_Optimization", "label": "ELBO_Optimization", "shape": "dot", "size": 10, "title": "Details on optimizing the Evidence Lower Bound (ELBO) in variational inference."}, {"color": "#02d1fb", "id": "Form_of_Q_i", "label": "Form_of_Q_i", "shape": "dot", "size": 10, "title": "Description of the form and requirements for Q_i in ELBO optimization."}, {"color": "#8cdfae", "id": "Efficient_Evaluation_ELBO", "label": "Efficient_Evaluation_ELBO", "shape": "dot", "size": 10, "title": "Conditions under which ELBO can be efficiently evaluated given fixed Q and theta."}, {"color": "#87ba3d", "id": "Gradient_Ascend_ELBO", "label": "Gradient_Ascend_ELBO", "shape": "dot", "size": 10, "title": "Process of gradient ascent optimization for the parameters phi, psi, and theta in ELBO."}, {"color": "#178664", "id": "VariationalInference", "label": "VariationalInference", "shape": "dot", "size": 10, "title": "Technique for approximating probability densities using variational methods."}, {"color": "#eaed70", "id": "ReparameterizationTrick", "label": "ReparameterizationTrick", "shape": "dot", "size": 10, "title": "Technique used to simplify gradient computation by re-parameterizing distributions."}, {"color": "#9a995c", "id": "Gradient_Estimation", "label": "Gradient_Estimation", "shape": "dot", "size": 10, "title": "Estimating gradients for optimization in machine learning models."}, {"color": "#f61bfc", "id": "Reparameterization_Trick", "label": "Reparameterization_Trick", "shape": "dot", "size": 10, "title": "Technique to estimate gradients through sampling."}, {"color": "#fa44ca", "id": "PCA_Methods", "label": "PCA_Methods", "shape": "star", "size": 25, "title": "Principal Components Analysis for dimensionality reduction."}, {"color": "#27def5", "id": "Data_Subspace_Identification", "label": "Data_Subspace_Identification", "shape": "dot", "size": 10, "title": "Identifying the subspace where data approximately lies."}, {"color": "#d2ff1b", "id": "Computational_Efficiency", "label": "Computational_Efficiency", "shape": "dot", "size": 10, "title": "Efficient computation using eigenvector calculations."}, {"color": "#cd31e6", "id": "RedundancyDetection", "label": "RedundancyDetection", "shape": "star", "size": 25, "title": "Detecting and removing redundant attributes in data sets."}, {"color": "#e78289", "id": "CarAttributesExample", "label": "CarAttributesExample", "shape": "dot", "size": 10, "title": "Illustration of redundancy with car maximum speed measurements."}, {"color": "#4f06e2", "id": "PilotSurveyExample", "label": "PilotSurveyExample", "shape": "dot", "size": 10, "title": "Example involving correlation between piloting skill and enjoyment in RC helicopter pilots."}, {"color": "#6d6563", "id": "PCAAlgorithm", "label": "PCAAlgorithm", "shape": "star", "size": 25, "title": "Principal Component Analysis for dimensionality reduction."}, {"color": "#4aa78b", "id": "DataNormalization", "label": "DataNormalization", "shape": "dot", "size": 10, "title": "Preprocessing step to normalize data features before PCA application."}, {"color": "#648c85", "id": "MeanRemoval", "label": "MeanRemoval", "shape": "dot", "size": 10, "title": "Subtracting the mean from each feature to center the data around zero."}, {"color": "#09d7b3", "id": "VarianceScaling", "label": "VarianceScaling", "shape": "dot", "size": 10, "title": "Dividing by standard deviation to ensure unit variance for all features."}, {"color": "#b688a0", "id": "FeatureComparison", "label": "FeatureComparison", "shape": "dot", "size": 10, "title": "Ensures comparability of attributes with different scales."}, {"color": "#0d6a11", "id": "MajorAxisOfVariation", "label": "MajorAxisOfVariation", "shape": "star", "size": 25, "title": "Finding the direction in which data varies most significantly."}, {"color": "#ba8b65", "id": "ProjectionObjective", "label": "ProjectionObjective", "shape": "dot", "size": 10, "title": "Maximizing variance of projected data to find major axis."}, {"color": "#01ecc9", "id": "NormalizedDatasetExample", "label": "NormalizedDatasetExample", "shape": "star", "size": 25, "title": "Illustrative dataset after normalization steps are applied."}, {"color": "#b7d926", "id": "PrincipalComponentAnalysis", "label": "PrincipalComponentAnalysis", "shape": "star", "size": 25, "title": "Technique for reducing dimensionality while preserving variance."}, {"color": "#26bf37", "id": "ProjectionDirection", "label": "ProjectionDirection", "shape": "dot", "size": 10, "title": "Choosing the direction to maximize projection variance."}, {"color": "#2cbc2f", "id": "VarianceMaximization", "label": "VarianceMaximization", "shape": "dot", "size": 10, "title": "Objective is to maximize variance of projections onto a unit vector."}, {"color": "#0ed27a", "id": "EmpiricalCovarianceMatrix", "label": "EmpiricalCovarianceMatrix", "shape": "dot", "size": 10, "title": "Matrix representing the covariance between data points."}, {"color": "#ca69a4", "id": "EigenvectorsEigenvalues", "label": "EigenvectorsEigenvalues", "shape": "dot", "size": 10, "title": "Key components for finding optimal projection directions."}, {"color": "#0c7b33", "id": "kDimensionalSubspace", "label": "kDimensionalSubspace", "shape": "dot", "size": 10, "title": "Projection into a lower-dimensional space using top k eigenvectors."}, {"color": "#7bae0c", "id": "PCA", "label": "PCA", "shape": "star", "size": 25, "title": "Principal Component Analysis for dimensionality reduction"}, {"color": "#2d4f2d", "id": "Eigenvectors", "label": "Eigenvectors", "shape": "dot", "size": 10, "title": "Top k eigenvectors of covariance matrix \u03a3 form new orthogonal basis"}, {"color": "#c537ed", "id": "Dimensionality Reduction", "label": "Dimensionality Reduction", "shape": "dot", "size": 10, "title": "Reduces data from d dimensions to k dimensions"}, {"color": "#6b0e87", "id": "Principal Components", "label": "Principal Components", "shape": "dot", "size": 10, "title": "First k eigenvectors of \u03a3 are principal components"}, {"color": "#39a01f", "id": "Approximation Error Minimization", "label": "Approximation Error Minimization", "shape": "dot", "size": 10, "title": "Minimizes error from projecting data onto k-dimensional subspace"}, {"color": "#cc95b7", "id": "Data Visualization", "label": "Data Visualization", "shape": "dot", "size": 10, "title": "Using PCA to visualize similarities and clusters in high-dimensional data."}, {"color": "#7ae798", "id": "Compression", "label": "Compression", "shape": "dot", "size": 10, "title": "Compresses high-dimensional data to lower dimensions"}, {"color": "#7c0531", "id": "Machine Learning Techniques", "label": "Machine Learning Techniques", "shape": "star", "size": 25, "title": "Overview of various machine learning techniques and applications."}, {"color": "#7101c1", "id": "Principal Component Analysis (PCA)", "label": "Principal Component Analysis (PCA)", "shape": "dot", "size": 10, "title": "Dimensionality reduction technique that transforms data into principal components."}, {"color": "#4e6946", "id": "Computational Efficiency", "label": "Computational Efficiency", "shape": "dot", "size": 10, "title": "Reduces computational load by decreasing the dimensionality of input data."}, {"color": "#9af867", "id": "Overfitting Prevention", "label": "Overfitting Prevention", "shape": "dot", "size": 10, "title": "Helps in avoiding overfitting by simplifying the hypothesis space."}, {"color": "#519660", "id": "Noise Reduction", "label": "Noise Reduction", "shape": "dot", "size": 10, "title": "Estimates intrinsic features from noisy data, like \u0027piloting karma\u0027 from noisy measures."}, {"color": "#34b038", "id": "Eigenfaces Method", "label": "Eigenfaces Method", "shape": "dot", "size": 10, "title": "Face recognition technique using PCA to reduce dimensionality of face images."}, {"color": "#a9073b", "id": "Independent Components Analysis (ICA)", "label": "Independent Components Analysis (ICA)", "shape": "star", "size": 25, "title": "Technique for finding independent components in data, different from PCA\u0027s goal."}, {"color": "#b8efd1", "id": "Cocktail Party Problem", "label": "Cocktail Party Problem", "shape": "dot", "size": 10, "title": "Motivational example for ICA where multiple speakers\u0027 voices are separated using ICA."}, {"color": "#c4be32", "id": "Independent Component Analysis (ICA)", "label": "Independent Component Analysis (ICA)", "shape": "star", "size": 25, "title": "Technique to separate mixed signals into their independent sources."}, {"color": "#25dc82", "id": "Mixing Matrix (A)", "label": "Mixing Matrix (A)", "shape": "dot", "size": 10, "title": "Matrix representing how independent sources mix to form observed data."}, {"color": "#15f1aa", "id": "Unmixing Matrix (W)", "label": "Unmixing Matrix (W)", "shape": "dot", "size": 10, "title": "Inverse of mixing matrix used to recover original source signals from mixed data."}, {"color": "#60f78d", "id": "ICA Ambiguities", "label": "ICA Ambiguities", "shape": "star", "size": 25, "title": "Discussion on the limitations and uncertainties in recovering unmixing matrix W without prior knowledge."}, {"color": "#10f900", "id": "Permutation Matrix", "label": "Permutation Matrix", "shape": "dot", "size": 10, "title": "Describes the role of permutation matrices in ICA ambiguities."}, {"color": "#d0a3fa", "id": "Scaling Ambiguity", "label": "Scaling Ambiguity", "shape": "dot", "size": 10, "title": "Explains how scaling factors affect ICA recovery without changing observed data."}, {"color": "#ce3e21", "id": "Sign Changes", "label": "Sign Changes", "shape": "dot", "size": 10, "title": "Discusses the irrelevance of sign changes in source signals for ICA applications."}, {"color": "#005c54", "id": "ICA_Ambiguities", "label": "ICA_Ambiguities", "shape": "star", "size": 25, "title": "Discusses ambiguities in Independent Component Analysis (ICA)"}, {"color": "#59a069", "id": "Scaling_Signal", "label": "Scaling_Signal", "shape": "dot", "size": 10, "title": "Explains the effect of scaling a signal on ICA"}, {"color": "#19071a", "id": "Non_Gaussian_Sources", "label": "Non_Gaussian_Sources", "shape": "dot", "size": 10, "title": "States that non-Gaussian sources resolve ambiguities in ICA"}, {"color": "#f73bfc", "id": "Gaussian_Data_Issue", "label": "Gaussian_Data_Issue", "shape": "dot", "size": 10, "title": "Describes the problem with Gaussian data in ICA"}, {"color": "#0bcb45", "id": "Mixing_Matrix_Rotation", "label": "Mixing_Matrix_Rotation", "shape": "dot", "size": 10, "title": "Explains how rotation of mixing matrix affects Gaussian data"}, {"color": "#f6f94a", "id": "ICAAlgorithm", "label": "ICAAlgorithm", "shape": "dot", "size": 10, "title": "Derivation and understanding of the Independent Component Analysis algorithm."}, {"color": "#258db2", "id": "MixingMatrix", "label": "MixingMatrix", "shape": "dot", "size": 10, "title": "Explanation of mixing matrices in ICA, including their properties and effects on data distribution."}, {"color": "#2b5410", "id": "RotationalSymmetry", "label": "RotationalSymmetry", "shape": "dot", "size": 10, "title": "Discussion on the impact of rotational symmetry in multivariate standard normal distributions."}, {"color": "#79020c", "id": "NonGaussianDataRecovery", "label": "NonGaussianDataRecovery", "shape": "dot", "size": 10, "title": "Conditions and methods for recovering independent sources from non-Gaussian data."}, {"color": "#12ce21", "id": "LinearTransformations", "label": "LinearTransformations", "shape": "star", "size": 25, "title": "Effect of linear transformations on densities in the context of machine learning."}, {"color": "#5f0369", "id": "DensityCalculation", "label": "DensityCalculation", "shape": "dot", "size": 10, "title": "Correct method for calculating density after a linear transformation involving an inverse matrix."}, {"color": "#d47071", "id": "Density Transformation", "label": "Density Transformation", "shape": "star", "size": 25, "title": "Transformation of density functions under linear mappings"}, {"color": "#55de3d", "id": "1D Example", "label": "1D Example", "shape": "dot", "size": 10, "title": "Example in one dimension with specific values and transformations"}, {"color": "#66aecf", "id": "General Case", "label": "General Case", "shape": "dot", "size": 10, "title": "Extension to vector-valued distributions and general matrices"}, {"color": "#6a1feb", "id": "Volume Mapping", "label": "Volume Mapping", "shape": "dot", "size": 10, "title": "Explanation of volume changes under linear transformations in higher dimensions"}, {"color": "#13a5fc", "id": "ICA Algorithm", "label": "ICA Algorithm", "shape": "star", "size": 25, "title": "Derivation and interpretation of an Independent Component Analysis algorithm"}, {"color": "#f4b538", "id": "ICAIndependenceAssumption", "label": "ICAIndependenceAssumption", "shape": "dot", "size": 10, "title": "Independent Component Analysis (ICA) assumes sources are independent and modeled by product distribution."}, {"color": "#0f606a", "id": "JointDistributionModeling", "label": "JointDistributionModeling", "shape": "dot", "size": 10, "title": "Modeling the joint distribution of sources as a product of marginals to capture independence."}, {"color": "#e27860", "id": "DensityFunctionTransformation", "label": "DensityFunctionTransformation", "shape": "dot", "size": 10, "title": "Transforming density functions based on linear transformation and determinant of mixing matrix."}, {"color": "#91e36b", "id": "CumulativeDistributionFunction", "label": "CumulativeDistributionFunction", "shape": "dot", "size": 10, "title": "Definition and properties of cumulative distribution function (CDF)."}, {"color": "#2b17e0", "id": "SigmoidFunctionChoice", "label": "SigmoidFunctionChoice", "shape": "dot", "size": 10, "title": "Choosing sigmoid function as a default cdf for source densities in ICA."}, {"color": "#0433d6", "id": "DataPreprocessing", "label": "DataPreprocessing", "shape": "dot", "size": 10, "title": "Process of preparing data for analysis by normalizing or transforming it to meet certain criteria."}, {"color": "#4ef88f", "id": "ParameterMatrixW", "label": "ParameterMatrixW", "shape": "dot", "size": 10, "title": "Square matrix parameter W used in the model for data transformation and analysis."}, {"color": "#981396", "id": "StochasticGradientAscent", "label": "StochasticGradientAscent", "shape": "dot", "size": 10, "title": "Optimization technique used in machine learning to iteratively update model parameters based on training examples."}, {"color": "#ae9ed4", "id": "ConvergenceAndRecovery", "label": "ConvergenceAndRecovery", "shape": "dot", "size": 10, "title": "Process of achieving a stable solution and recovering original data sources from transformed data."}, {"color": "#571e8d", "id": "Stochastic Gradient Ascent", "label": "Stochastic Gradient Ascent", "shape": "dot", "size": 10, "title": "Technique for optimizing model parameters using randomly selected data points."}, {"color": "#3b12aa", "id": "Self-supervised Learning", "label": "Self-supervised Learning", "shape": "star", "size": 25, "title": "Learning method that uses the input data itself as a source of supervision to train models."}, {"color": "#a10af9", "id": "Foundation Models", "label": "Foundation Models", "shape": "dot", "size": 10, "title": "Large-scale pre-trained models adaptable to various downstream tasks with limited labeled data."}, {"color": "#03da45", "id": "Pretraining and Adaptation", "label": "Pretraining and Adaptation", "shape": "dot", "size": 10, "title": "Two-phase process involving training on unlabeled data followed by adaptation to specific tasks."}, {"color": "#836fd9", "id": "Machine_Learning_Pretraining_Adaptation", "label": "Machine_Learning_Pretraining_Adaptation", "shape": "star", "size": 25, "title": "Overview of pretraining and adaptation phases in machine learning."}, {"color": "#eca412", "id": "Pretraining_Phase", "label": "Pretraining_Phase", "shape": "dot", "size": 10, "title": "Training a model on unlabeled data to learn general representations."}, {"color": "#6c24c8", "id": "Adaptation_Phase", "label": "Adaptation_Phase", "shape": "dot", "size": 10, "title": "Customizing the pretrained model for specific tasks with labeled data."}, {"color": "#e1a56d", "id": "Unlabeled_Dataset", "label": "Unlabeled_Dataset", "shape": "dot", "size": 10, "title": "Dataset used in pretraining phase, consisting of unlabeled examples."}, {"color": "#04ae29", "id": "Model_Parameterization", "label": "Model_Parameterization", "shape": "dot", "size": 10, "title": "Description of model parameters and input-output mapping."}, {"color": "#bc7e63", "id": "Pretraining_Loss_Function", "label": "Pretraining_Loss_Function", "shape": "dot", "size": 10, "title": "Loss function used to train the model during pretraining phase."}, {"color": "#9c49c5", "id": "Labeled_Dataset_Task", "label": "Labeled_Dataset_Task", "shape": "dot", "size": 10, "title": "Dataset with labeled examples for adaptation phase."}, {"color": "#813792", "id": "Downstream_Tasks", "label": "Downstream_Tasks", "shape": "dot", "size": 10, "title": "Tasks that use pretrained models and adapt them to specific domains."}, {"color": "#d59ea8", "id": "Machine Learning Tasks", "label": "Machine Learning Tasks", "shape": "star", "size": 25, "title": "Tasks in machine learning including zero-shot and few-shot learning."}, {"color": "#dffa70", "id": "Zero-Shot Learning", "label": "Zero-Shot Learning", "shape": "dot", "size": 10, "title": "Learning scenario with no labeled examples for the task."}, {"color": "#5df7c7", "id": "Few-Shot Learning", "label": "Few-Shot Learning", "shape": "dot", "size": 10, "title": "Scenario where only a small number of labeled examples are available."}, {"color": "#1bb5de", "id": "Adaptation Algorithms", "label": "Adaptation Algorithms", "shape": "star", "size": 25, "title": "Methods used to adapt pre-trained models for downstream tasks."}, {"color": "#949948", "id": "Linear Probe", "label": "Linear Probe", "shape": "dot", "size": 10, "title": "Uses a linear head on top of the representation to predict labels without modifying the pretrained model."}, {"color": "#686338", "id": "Finetuning", "label": "Finetuning", "shape": "dot", "size": 10, "title": "Further trains both the pretrained model and the prediction model for downstream tasks."}, {"color": "#0a802f", "id": "Downstream Dataset", "label": "Downstream Dataset", "shape": "star", "size": 25, "title": "Dataset used to adapt a pre-trained model for specific tasks."}, {"color": "#751d25", "id": "Machine_Learning_Adaptation", "label": "Machine_Learning_Adaptation", "shape": "star", "size": 25, "title": "Overview of machine learning adaptation techniques."}, {"color": "#61fe89", "id": "Finetuning_Pretrained_Models", "label": "Finetuning_Pretrained_Models", "shape": "dot", "size": 10, "title": "Process of finetuning pretrained models for downstream tasks."}, {"color": "#2498d2", "id": "Prediction_Model_Structure", "label": "Prediction_Model_Structure", "shape": "dot", "size": 10, "title": "Structure of the prediction model using both pretrained and new parameters."}, {"color": "#01fa24", "id": "Optimization_Objective", "label": "Optimization_Objective", "shape": "dot", "size": 10, "title": "Objective function for optimizing the prediction model\u0027s parameters."}, {"color": "#1c471c", "id": "Pretraining_Methods", "label": "Pretraining_Methods", "shape": "star", "size": 25, "title": "Introduction to pretraining methods in machine learning."}, {"color": "#eaed51", "id": "Supervised_Pretraining", "label": "Supervised_Pretraining", "shape": "dot", "size": 10, "title": "Method of using labeled datasets for supervised training of neural networks."}, {"color": "#9179fc", "id": "Contrastive_Learning", "label": "Contrastive_Learning", "shape": "dot", "size": 10, "title": "Technique for learning representations by contrasting similar and dissimilar instances."}, {"color": "#693f7b", "id": "Self-Supervised Learning", "label": "Self-Supervised Learning", "shape": "star", "size": 25, "title": "Method using unlabeled data for pretraining."}, {"color": "#8cec01", "id": "Representation Function", "label": "Representation Function", "shape": "dot", "size": 10, "title": "Function mapping images to representations."}, {"color": "#eeb057", "id": "Positive Pair", "label": "Positive Pair", "shape": "dot", "size": 10, "title": "Augmented versions of the same image."}, {"color": "#bc3c19", "id": "Negative Pair", "label": "Negative Pair", "shape": "dot", "size": 10, "title": "Randomly selected augmented images from different originals."}, {"color": "#d419aa", "id": "Supervised Contrastive Algorithms", "label": "Supervised Contrastive Algorithms", "shape": "dot", "size": 10, "title": "Algorithms using labeled data for contrastive learning."}, {"color": "#a87fb0", "id": "SIMCLR", "label": "SIMCLR", "shape": "dot", "size": 10, "title": "Algorithm introduced in 2020 that uses contrastive learning to learn image representations."}, {"color": "#478b93", "id": "Loss_Function", "label": "Loss_Function", "shape": "dot", "size": 10, "title": "Function used by SIMCLR to measure the similarity between augmented data pairs."}, {"color": "#4b0e8d", "id": "Augmentation_Techniques", "label": "Augmentation_Techniques", "shape": "dot", "size": 10, "title": "Techniques applied to input data to create diverse training samples."}, {"color": "#a74b9b", "id": "Negative_Pairs", "label": "Negative_Pairs", "shape": "dot", "size": 10, "title": "Pairs of instances that are not similar, used for contrast in learning algorithms."}, {"color": "#4dc54e", "id": "LossFunctionOptimization", "label": "LossFunctionOptimization", "shape": "dot", "size": 10, "title": "Techniques for optimizing loss functions in ML models."}, {"color": "#0cee6e", "id": "PretrainedLanguageModels", "label": "PretrainedLanguageModels", "shape": "star", "size": 25, "title": "Introduction to pretrained large language models in NLP."}, {"color": "#633d53", "id": "NaturalLanguageProcessing", "label": "NaturalLanguageProcessing", "shape": "dot", "size": 10, "title": "Overview of natural language processing and its applications."}, {"color": "#4c5974", "id": "DocumentProbabilityModeling", "label": "DocumentProbabilityModeling", "shape": "dot", "size": 10, "title": "Techniques for modeling the probability distribution of a document in NLP."}, {"color": "#de08e1", "id": "Conditional_Probability_Modeling", "label": "Conditional_Probability_Modeling", "shape": "dot", "size": 10, "title": "Modeling the probability of a word given previous words in a sequence."}, {"color": "#183c58", "id": "Embeddings_and_Representations", "label": "Embeddings_and_Representations", "shape": "dot", "size": 10, "title": "Introduction and use of embeddings for discrete variables such as words."}, {"color": "#40c151", "id": "Transformer_Model", "label": "Transformer_Model", "shape": "dot", "size": 10, "title": "Description of the input-output interface of a Transformer model used in sequence prediction tasks."}, {"color": "#170059", "id": "Autoregressive_Transformer", "label": "Autoregressive_Transformer", "shape": "dot", "size": 10, "title": "Version of the Transformer that ensures each output depends only on previous inputs."}, {"color": "#b5850a", "id": "Conditional_Probability", "label": "Conditional_Probability", "shape": "dot", "size": 10, "title": "Probability of the next input given previous inputs in the context of Transformer models."}, {"color": "#ae4361", "id": "Training_Transformer", "label": "Training_Transformer", "shape": "dot", "size": 10, "title": "Process of minimizing the negative log-likelihood using cross-entropy loss."}, {"color": "#fd620c", "id": "Autoregressive_Decoding", "label": "Autoregressive_Decoding", "shape": "star", "size": 25, "title": "Generating text sequentially from a Transformer model given an initial prefix."}, {"color": "#1c80e9", "id": "TemperatureParameter", "label": "TemperatureParameter", "shape": "dot", "size": 10, "title": "Introduction to the temperature parameter used in softmax functions for adjusting model output sharpness."}, {"color": "#f7f243", "id": "TextGenerationModels", "label": "TextGenerationModels", "shape": "dot", "size": 10, "title": "Discussion on models that generate text based on learned probabilities."}, {"color": "#b132ff", "id": "ZeroShotLearning", "label": "ZeroShotLearning", "shape": "dot", "size": 10, "title": "Exploration of zero-shot learning in the context of adapting pre-trained language models to new tasks without task-specific data."}, {"color": "#44c9e4", "id": "Zero-Shot_Adaptation", "label": "Zero-Shot_Adaptation", "shape": "dot", "size": 10, "title": "Technique where no input-output pairs from downstream tasks are available."}, {"color": "#1e41d2", "id": "In-Context_Learning", "label": "In-Context_Learning", "shape": "dot", "size": 10, "title": "Approach used for few-shot settings with a small number of labeled examples."}, {"color": "#fabc79", "id": "Reinforcement Learning", "label": "Reinforcement Learning", "shape": "star", "size": 25, "title": "Study of algorithms for sequential decision making and control."}, {"color": "#b91c71", "id": "Supervised Learning vs Reinforcement Learning", "label": "Supervised Learning vs Reinforcement Learning", "shape": "dot", "size": 10, "title": "Comparison between supervised learning and reinforcement learning approaches."}, {"color": "#8403b1", "id": "MachineLearning", "label": "MachineLearning", "shape": "star", "size": 25, "title": "Field of study focusing on algorithms that learn from data."}, {"color": "#15afee", "id": "ReinforcementLearning", "label": "ReinforcementLearning", "shape": "dot", "size": 10, "title": "Type of ML where agents learn by interacting with an environment to maximize rewards."}, {"color": "#14c1c3", "id": "MarkovDecisionProcesses", "label": "MarkovDecisionProcesses", "shape": "dot", "size": 10, "title": "Formal framework for modeling decision-making situations in RL."}, {"color": "#b81cb6", "id": "States", "label": "States", "shape": "dot", "size": 10, "title": "Set of all possible conditions or configurations an agent can be in."}, {"color": "#2d3cb7", "id": "Actions", "label": "Actions", "shape": "dot", "size": 10, "title": "Set of all possible actions an agent can take in a given state."}, {"color": "#a2502d", "id": "StateTransitionProbabilities", "label": "StateTransitionProbabilities", "shape": "dot", "size": 10, "title": "Probability distribution over states after taking an action from the current state."}, {"color": "#e08729", "id": "DiscountFactor", "label": "DiscountFactor", "shape": "dot", "size": 10, "title": "Parameter that discounts future rewards, ensuring finite sum of discounted rewards."}, {"color": "#e8de32", "id": "RewardFunction", "label": "RewardFunction", "shape": "dot", "size": 10, "title": "Function mapping state-action pairs to real numbers representing immediate reward."}, {"color": "#b5d559", "id": "Reinforcement_Learning", "label": "Reinforcement_Learning", "shape": "star", "size": 25, "title": "Field of machine learning focusing on decision-making in stochastic environments."}, {"color": "#fc4374", "id": "Markov_Decision_Processes_(MDP)", "label": "Markov_Decision_Processes_(MDP)", "shape": "dot", "size": 10, "title": "Model for sequential decision making under uncertainty."}, {"color": "#6e9487", "id": "State_Transitions", "label": "State_Transitions", "shape": "dot", "size": 10, "title": "Random transitions between states based on actions and transition probabilities."}, {"color": "#b163dd", "id": "Discounted_Rewards", "label": "Discounted_Rewards", "shape": "dot", "size": 10, "title": "Rewards at future timesteps are discounted by a factor of gamma."}, {"color": "#ca68a1", "id": "Policy", "label": "Policy", "shape": "dot", "size": 10, "title": "Function mapping states to actions, guiding decision-making in MDPs."}, {"color": "#b284ca", "id": "Value_Function", "label": "Value_Function", "shape": "dot", "size": 10, "title": "Expected cumulative reward starting from a state under a given policy."}, {"color": "#42afb9", "id": "Policy Execution", "label": "Policy Execution", "shape": "star", "size": 25, "title": "Process of following a policy in state-action mapping."}, {"color": "#573420", "id": "Value Function", "label": "Value Function", "shape": "dot", "size": 10, "title": "Functions that represent the expected return starting from a state under a policy or optimally."}, {"color": "#fc868b", "id": "Bellman Equations", "label": "Bellman Equations", "shape": "dot", "size": 10, "title": "Equations used to solve for the value function in an MDP."}, {"color": "#e6ac15", "id": "Immediate Reward", "label": "Immediate Reward", "shape": "dot", "size": 10, "title": "Reward received immediately upon entering a state."}, {"color": "#74b850", "id": "Policy Evaluation", "label": "Policy Evaluation", "shape": "dot", "size": 10, "title": "Evaluating the value of states under a given policy."}, {"color": "#1ea94a", "id": "Optimal Value Function", "label": "Optimal Value Function", "shape": "dot", "size": 10, "title": "The best possible expected sum of discounted rewards for any state and policy."}, {"color": "#90073e", "id": "Bellman\u0027s Equation", "label": "Bellman\u0027s Equation", "shape": "dot", "size": 10, "title": "Equations that define the value function recursively in terms of successor states."}, {"color": "#4a6606", "id": "Optimal Policy", "label": "Optimal Policy", "shape": "dot", "size": 10, "title": "Policy that maximizes the expected utility from a given state."}, {"color": "#88d783", "id": "MDP (Markov Decision Process)", "label": "MDP (Markov Decision Process)", "shape": "dot", "size": 10, "title": "Framework for modeling decision-making situations in AI."}, {"color": "#c961ef", "id": "Value Iteration and Policy Iteration", "label": "Value Iteration and Policy Iteration", "shape": "star", "size": 25, "title": "Algorithms for solving finite-state MDPs."}, {"color": "#839ed3", "id": "Finite-State MDPs", "label": "Finite-State MDPs", "shape": "dot", "size": 10, "title": "MDPs with a finite number of states and actions."}, {"color": "#cbe6c0", "id": "State Transition Probabilities", "label": "State Transition Probabilities", "shape": "dot", "size": 10, "title": "Probabilities of moving from one state to another in an MDP."}, {"color": "#d58782", "id": "Reward Function", "label": "Reward Function", "shape": "dot", "size": 10, "title": "Function that assigns a numerical value for each state-action pair."}, {"color": "#5234fd", "id": "Value Iteration Algorithm", "label": "Value Iteration Algorithm", "shape": "dot", "size": 10, "title": "Algorithm to iteratively update the estimated value function in an MDP."}, {"color": "#69e110", "id": "Synchronous Updates", "label": "Synchronous Updates", "shape": "dot", "size": 10, "title": "Updating all state values simultaneously before moving on."}, {"color": "#425b4a", "id": "Asynchronous Updates", "label": "Asynchronous Updates", "shape": "dot", "size": 10, "title": "Updating one state value at a time in sequence."}, {"color": "#4dce78", "id": "Value_Iteration", "label": "Value_Iteration", "shape": "dot", "size": 10, "title": "Algorithm that uses the Bellman equation to find optimal policies through iterative computation of value functions."}, {"color": "#0f5846", "id": "Policy_Iteration", "label": "Policy_Iteration", "shape": "dot", "size": 10, "title": "Alternative algorithm to value iteration that directly optimizes the policy function."}, {"color": "#804c5a", "id": "Convergence_to_Optimal_Value", "label": "Convergence_to_Optimal_Value", "shape": "dot", "size": 10, "title": "Process of value iteration converging to optimal state values."}, {"color": "#1cfd62", "id": "Optimal_Policy_Finding", "label": "Optimal_Policy_Finding", "shape": "dot", "size": 10, "title": "Using converged values to determine the best policy."}, {"color": "#81ff52", "id": "Policy_Evaluation", "label": "Policy_Evaluation", "shape": "dot", "size": 10, "title": "Computing value function for current policy in each iteration."}, {"color": "#6ddb74", "id": "Policy_Improvement", "label": "Policy_Improvement", "shape": "dot", "size": 10, "title": "Updating the policy based on the computed value function."}, {"color": "#cc679e", "id": "Bellman_Equations", "label": "Bellman_Equations", "shape": "dot", "size": 10, "title": "Set of equations used to solve for optimal policies in MDPs."}, {"color": "#b017a5", "id": "Greedy_Policy_With_Respect_To_V", "label": "Greedy_Policy_With_Respect_To_V", "shape": "dot", "size": 10, "title": "Policy derived from value function that maximizes immediate reward."}, {"color": "#0e5b83", "id": "Model_Learning_for_MDPs", "label": "Model_Learning_for_MDPs", "shape": "star", "size": 25, "title": "Learning state transition probabilities and rewards from data in MDPs."}, {"color": "#080012", "id": "Inverted_Pendulum_Problem", "label": "Inverted_Pendulum_Problem", "shape": "dot", "size": 10, "title": "Example problem used to illustrate learning models from experience."}, {"color": "#e86ff0", "id": "State_Transition_Probabilities", "label": "State_Transition_Probabilities", "shape": "dot", "size": 10, "title": "Probabilities that describe how an agent transitions between states based on actions taken."}, {"color": "#9edf75", "id": "Maximum_Likelihood_Estimation", "label": "Maximum_Likelihood_Estimation", "shape": "dot", "size": 10, "title": "Method for estimating parameters by maximizing the likelihood function based on observed data."}, {"color": "#9c4543", "id": "Markov_Decision_Processes_MDPs", "label": "Markov_Decision_Processes_MDPs", "shape": "dot", "size": 10, "title": "Framework for modeling decision making in situations where outcomes are partly random and partly under the control of a decision maker."}, {"color": "#c9e2c8", "id": "Reward_Functions", "label": "Reward_Functions", "shape": "dot", "size": 10, "title": "Function assigning a numerical value to each possible state or state-action pair in the MDP."}, {"color": "#3a2fb9", "id": "Model_Learning", "label": "Model_Learning", "shape": "dot", "size": 10, "title": "Process of learning an accurate model of the environment from experience."}, {"color": "#1fa871", "id": "Unknown_State_Transitions_and_Rewards", "label": "Unknown_State_Transitions_and_Rewards", "shape": "dot", "size": 10, "title": "Learning state transitions and rewards when they are not fully known in advance."}, {"color": "#cf2676", "id": "Efficient_Experience_Use", "label": "Efficient_Experience_Use", "shape": "dot", "size": 10, "title": "Techniques for updating estimates based on new experience efficiently."}, {"color": "#f4a831", "id": "Continuous_State_MDPs", "label": "Continuous_State_MDPs", "shape": "star", "size": 25, "title": "MDPs with an infinite number of states, often represented in continuous space."}, {"color": "#034d24", "id": "Discretization_Method", "label": "Discretization_Method", "shape": "dot", "size": 10, "title": "Approach to solving MDPs by converting the state space into a discrete form."}, {"color": "#1741b3", "id": "Discretization in MDPs", "label": "Discretization in MDPs", "shape": "dot", "size": 10, "title": "Process of converting continuous state spaces into discrete ones for easier computation."}, {"color": "#e52091", "id": "Value Iteration", "label": "Value Iteration", "shape": "dot", "size": 10, "title": "Algorithm for finding optimal policies in reinforcement learning through iterative updates."}, {"color": "#6cd02d", "id": "Policy Iteration", "label": "Policy Iteration", "shape": "dot", "size": 10, "title": "An iterative method for finding optimal policies in reinforcement learning."}, {"color": "#5852f5", "id": "Supervised Learning", "label": "Supervised Learning", "shape": "star", "size": 25, "title": "Type of machine learning where models are trained on labeled data to predict outcomes."}, {"color": "#d86002", "id": "Discretization Limitations", "label": "Discretization Limitations", "shape": "star", "size": 25, "title": "Issues arising from using a piecewise constant representation in discretized models."}, {"color": "#3add49", "id": "Curse of Dimensionality", "label": "Curse of Dimensionality", "shape": "dot", "size": 10, "title": "Problem where the volume of the state space increases exponentially with dimensionality, making it difficult to represent and learn from data."}, {"color": "#8df60b", "id": "Discretization_Methods", "label": "Discretization_Methods", "shape": "dot", "size": 10, "title": "Techniques for discretizing continuous state spaces."}, {"color": "#54fa34", "id": "Curse_of_Dimensionality", "label": "Curse_of_Dimensionality", "shape": "dot", "size": 10, "title": "Exponential growth of discrete states with dimensionality."}, {"color": "#c75e2b", "id": "Value_Function_Approximation", "label": "Value_Function_Approximation", "shape": "star", "size": 25, "title": "Alternative method for finding policies in continuous-state MDPs without discretization."}, {"color": "#e9c600", "id": "Model_or_Simulator", "label": "Model_or_Simulator", "shape": "dot", "size": 10, "title": "Black-box that simulates state transitions based on input actions and states."}, {"color": "#1867ad", "id": "Model Creation Methods", "label": "Model Creation Methods", "shape": "star", "size": 25, "title": "Different methods to create a model for state transitions in machine learning."}, {"color": "#539dd2", "id": "Physics Simulation", "label": "Physics Simulation", "shape": "dot", "size": 10, "title": "Using physical laws and parameters to simulate system behavior."}, {"color": "#9f6652", "id": "Off-the-Shelf Physics Software", "label": "Off-the-Shelf Physics Software", "shape": "dot", "size": 10, "title": "Utilizing existing physics simulation software packages."}, {"color": "#2529d8", "id": "Learning from Data", "label": "Learning from Data", "shape": "dot", "size": 10, "title": "Deriving models based on data collected through repeated trials in an MDP."}, {"color": "#2a659e", "id": "Open Dynamics Engine", "label": "Open Dynamics Engine", "shape": "dot", "size": 10, "title": "Free/open-source physics simulator used for simulating mechanical systems like inverted pendulum."}, {"color": "#849f22", "id": "MachineLearningModeling", "label": "MachineLearningModeling", "shape": "star", "size": 25, "title": "Overview of modeling approaches in machine learning"}, {"color": "#894968", "id": "StateTransitionModel", "label": "StateTransitionModel", "shape": "dot", "size": 10, "title": "Models predicting next state given current state and action"}, {"color": "#cebc29", "id": "LinearModel", "label": "LinearModel", "shape": "dot", "size": 10, "title": "Predicts next state using linear functions of current state and action"}, {"color": "#146eb2", "id": "DeterministicModel", "label": "DeterministicModel", "shape": "dot", "size": 10, "title": "Next state is exactly determined by inputs"}, {"color": "#feff3d", "id": "StochasticModel", "label": "StochasticModel", "shape": "dot", "size": 10, "title": "Next state includes a random noise term"}, {"color": "#e79a10", "id": "Non-Linear Feature Mappings", "label": "Non-Linear Feature Mappings", "shape": "dot", "size": 10, "title": "Feature transformations that allow models to capture complex relationships between inputs and outputs."}, {"color": "#a4f3c1", "id": "MDP Simulators", "label": "MDP Simulators", "shape": "dot", "size": 10, "title": "Simulations of Markov Decision Processes using learned models to predict outcomes."}, {"color": "#96065f", "id": "Fitted Value Iteration", "label": "Fitted Value Iteration", "shape": "star", "size": 25, "title": "Algorithm for approximating value functions in continuous state MDPs using supervised learning methods."}, {"color": "#dbd514", "id": "Continuous State Space", "label": "Continuous State Space", "shape": "dot", "size": 10, "title": "The space of all possible states is continuous and multidimensional."}, {"color": "#d04b06", "id": "Discrete Action Space", "label": "Discrete Action Space", "shape": "dot", "size": 10, "title": "A finite set of actions available in the decision-making process."}, {"color": "#a56630", "id": "Value Function Approximation", "label": "Value Function Approximation", "shape": "dot", "size": 10, "title": "Estimating value functions using a supervised learning approach over a sample of states."}, {"color": "#e869e3", "id": "Supervised Learning Algorithm", "label": "Supervised Learning Algorithm", "shape": "dot", "size": 10, "title": "Used to get V(s) close to y^(i)."}, {"color": "#63a75e", "id": "State Sample", "label": "State Sample", "shape": "dot", "size": 10, "title": "Randomly sampled states used for approximation."}, {"color": "#57e739", "id": "Action Evaluation", "label": "Action Evaluation", "shape": "dot", "size": 10, "title": "Evaluates Q(a) for each action a in A."}, {"color": "#ff7f00", "id": "Reward Estimation", "label": "Reward Estimation", "shape": "dot", "size": 10, "title": "Estimates R(s)+gamma*V(s\u0027) using sampled transitions."}, {"color": "#66bd03", "id": "Max Action Selection", "label": "Max Action Selection", "shape": "dot", "size": 10, "title": "Selects action with highest Q(a) as y^(i)."}, {"color": "#ff1f03", "id": "Parameter Update", "label": "Parameter Update", "shape": "dot", "size": 10, "title": "Updates theta to minimize squared error between V(s) and y^(i)."}, {"color": "#ae75d1", "id": "SupervisedLearning", "label": "SupervisedLearning", "shape": "star", "size": 25, "title": "Machine learning technique using labeled data to predict outcomes."}, {"color": "#4d66a2", "id": "FittedValueIteration", "label": "FittedValueIteration", "shape": "dot", "size": 10, "title": "Technique using regression algorithms to approximate value functions in reinforcement learning problems."}, {"color": "#b87090", "id": "DeterministicSimulator", "label": "DeterministicSimulator", "shape": "dot", "size": 10, "title": "Simplified model where a single sample is sufficient for exact computation of expectation."}, {"color": "#ccb635", "id": "Expectation_Calculation", "label": "Expectation_Calculation", "shape": "dot", "size": 10, "title": "Method to calculate expected values under a probability distribution Psa."}, {"color": "#4c1441", "id": "Sampling_Methods", "label": "Sampling_Methods", "shape": "dot", "size": 10, "title": "Techniques for approximating expectations through random sampling."}, {"color": "#7e564f", "id": "Deterministic_Simulator", "label": "Deterministic_Simulator", "shape": "dot", "size": 10, "title": "Scenario where the simulator is deterministic, simplifying expectation calculations."}, {"color": "#cbcfb8", "id": "Gaussian_Noise_Model", "label": "Gaussian_Noise_Model", "shape": "dot", "size": 10, "title": "Model incorporating Gaussian noise for more realistic simulations."}, {"color": "#73e7cb", "id": "Value_Evaluation_Procedure", "label": "Value_Evaluation_Procedure", "shape": "dot", "size": 10, "title": "Procedure for evaluating value functions within policy iteration framework."}, {"color": "#fd2d3d", "id": "Algorithm 6", "label": "Algorithm 6", "shape": "dot", "size": 10, "title": "Variant of policy iteration that uses value evaluation procedure VE."}, {"color": "#17e861", "id": "VE Procedure", "label": "VE Procedure", "shape": "dot", "size": 10, "title": "Procedure for evaluating the value function in reinforcement learning algorithms."}, {"color": "#d61072", "id": "Option 1 Initialization", "label": "Option 1 Initialization", "shape": "dot", "size": 10, "title": "Initialization method where V(s) is set to zero."}, {"color": "#e4ccca", "id": "Option 2 Initialization", "label": "Option 2 Initialization", "shape": "dot", "size": 10, "title": "Initialization method using the current value function from the main algorithm."}, {"color": "#2f187d", "id": "Update Rule (15.12)", "label": "Update Rule (15.12)", "shape": "dot", "size": 10, "title": "Rule for updating state values in reinforcement learning algorithms."}, {"color": "#079b52", "id": "Policy Update Rule (15.13)", "label": "Policy Update Rule (15.13)", "shape": "dot", "size": 10, "title": "Rule for updating the policy based on the value function."}, {"color": "#182c2b", "id": "Chapter16", "label": "Chapter16", "shape": "star", "size": 25, "title": "Introduction to LQR, DDP and LQG in the context of finite-horizon MDPs."}, {"color": "#716466", "id": "FiniteHorizonMDPs", "label": "FiniteHorizonMDPs", "shape": "dot", "size": 10, "title": "Discussion on Markov Decision Processes (MDPs) with a focus on finite horizon scenarios."}, {"color": "#ad39ac", "id": "OptimalBellmanEquation", "label": "OptimalBellmanEquation", "shape": "dot", "size": 10, "title": "Definition and explanation of the optimal Bellman equation for determining the optimal value function."}, {"color": "#2f8653", "id": "ValueIterationPolicyIteration", "label": "ValueIterationPolicyIteration", "shape": "dot", "size": 10, "title": "Explanation of Value Iteration and Policy Iteration methods in solving MDPs."}, {"color": "#53e401", "id": "OptimalPolicyRecovery", "label": "OptimalPolicyRecovery", "shape": "dot", "size": 10, "title": "Description of how to recover the optimal policy from the optimal value function."}, {"color": "#03539a", "id": "GeneralSettingEquations", "label": "GeneralSettingEquations", "shape": "dot", "size": 10, "title": "Introduction to equations that apply in both discrete and continuous settings for MDPs."}, {"color": "#04ec9c", "id": "ExpectationRewriting", "label": "ExpectationRewriting", "shape": "star", "size": 25, "title": "Discusses rewriting expectations in finite and continuous cases."}, {"color": "#012df8", "id": "StateActionDependentRewards", "label": "StateActionDependentRewards", "shape": "dot", "size": 10, "title": "Explains rewards depending on both states and actions."}, {"color": "#16bb7e", "id": "InfiniteHorizonMDP", "label": "InfiniteHorizonMDP", "shape": "star", "size": 25, "title": "Describes the concept of an infinite horizon Markov Decision Process (MDP)."}, {"color": "#017920", "id": "FiniteHorizonMDP", "label": "FiniteHorizonMDP", "shape": "dot", "size": 10, "title": "Introduces finite horizon MDPs and their characteristics."}, {"color": "#f873c2", "id": "OptimalPolicyFiniteHorizon", "label": "OptimalPolicyFiniteHorizon", "shape": "dot", "size": 10, "title": "Discusses optimal policy changes over time in finite horizon settings."}, {"color": "#44710f", "id": "Markov Decision Processes (MDP)", "label": "Markov Decision Processes (MDP)", "shape": "dot", "size": 10, "title": "Models for decision making under uncertainty."}, {"color": "#e681cd", "id": "Policy in MDPs", "label": "Policy in MDPs", "shape": "dot", "size": 10, "title": "A strategy that defines the action to take at each state."}, {"color": "#ec010d", "id": "Time-Dependent Policy", "label": "Time-Dependent Policy", "shape": "dot", "size": 10, "title": "A policy that changes over time, denoted as \\(\\pi^{(t)}\\)."}, {"color": "#85988a", "id": "Finite Horizon Setting", "label": "Finite Horizon Setting", "shape": "dot", "size": 10, "title": "An MDP with a fixed number of steps."}, {"color": "#985dad", "id": "Optimal Policy in Finite Horizon", "label": "Optimal Policy in Finite Horizon", "shape": "dot", "size": 10, "title": "The best strategy that changes based on time and remaining actions."}, {"color": "#b03706", "id": "Time Dependent Dynamics", "label": "Time Dependent Dynamics", "shape": "dot", "size": 10, "title": "Transition probabilities that change over time, denoted as \\(P_{sa}^{(t)}\\)."}, {"color": "#5ce9ce", "id": "Value Function in MDPs", "label": "Value Function in MDPs", "shape": "dot", "size": 10, "title": "The expected utility of a policy starting from state \\(s\\) at time \\(t\\)."}, {"color": "#2c65b4", "id": "Value_Functions", "label": "Value_Functions", "shape": "dot", "size": 10, "title": "Functions representing the expected cumulative reward from a given state under an optimal policy."}, {"color": "#7db483", "id": "Optimal_Value_Function", "label": "Optimal_Value_Function", "shape": "dot", "size": 10, "title": "The value function for an optimal policy that maximizes the cumulative reward."}, {"color": "#61c85f", "id": "Bellman_Equation", "label": "Bellman_Equation", "shape": "dot", "size": 10, "title": "Equation defining the value of being in a state as the maximum expected utility over all possible actions."}, {"color": "#0d72e0", "id": "Dynamic_Programming", "label": "Dynamic_Programming", "shape": "dot", "size": 10, "title": "Methodology for solving complex problems by breaking them down into simpler sub-problems in a recursive manner."}, {"color": "#2882f8", "id": "Bellman Update", "label": "Bellman Update", "shape": "dot", "size": 10, "title": "Operator used to update value functions iteratively towards the optimal solution."}, {"color": "#f3c670", "id": "Theorem", "label": "Theorem", "shape": "dot", "size": 10, "title": "Mathematical proof of geometric convergence in value iteration."}, {"color": "#26c67d", "id": "Continuous Setting", "label": "Continuous Setting", "shape": "dot", "size": 10, "title": "Assumptions about state and action spaces as continuous real vectors."}, {"color": "#dad32e", "id": "Linear Transitions", "label": "Linear Transitions", "shape": "dot", "size": 10, "title": "Model of system dynamics with linear equations and Gaussian noise."}, {"color": "#bfd97d", "id": "Quadratic Rewards", "label": "Quadratic Rewards", "shape": "dot", "size": 10, "title": "Reward function that is quadratic in state and action variables, ensuring negative rewards."}, {"color": "#7f1099", "id": "LQRModelAssumptions", "label": "LQRModelAssumptions", "shape": "dot", "size": 10, "title": "Assumptions made for the Linear Quadratic Regulator (LQR) model in reinforcement learning."}, {"color": "#7f6611", "id": "Step1Estimation", "label": "Step1Estimation", "shape": "dot", "size": 10, "title": "First step of LQR algorithm: estimating unknown matrices using value approximation and linear regression."}, {"color": "#9ee41d", "id": "ValueApproximation", "label": "ValueApproximation", "shape": "dot", "size": 10, "title": "Technique for approximating values in reinforcement learning to estimate model parameters."}, {"color": "#979e5e", "id": "Step2OptimalPolicy", "label": "Step2OptimalPolicy", "shape": "dot", "size": 10, "title": "Second step of LQR algorithm: deriving optimal policy using dynamic programming given known model parameters."}, {"color": "#5ad848", "id": "DynamicProgramming", "label": "DynamicProgramming", "shape": "dot", "size": 10, "title": "Technique for solving the optimization problem to find the optimal value function V_t*."}, {"color": "#f57003", "id": "Optimal_Control", "label": "Optimal_Control", "shape": "dot", "size": 10, "title": "Theoretical framework for finding optimal control policies."}, {"color": "#982a65", "id": "Quadratic_Value_Functions", "label": "Quadratic_Value_Functions", "shape": "dot", "size": 10, "title": "Value functions that are quadratic in terms of state variables."}, {"color": "#c31543", "id": "Optimal_Policy_Derivation", "label": "Optimal_Policy_Derivation", "shape": "dot", "size": 10, "title": "Deriving the optimal policy from value function properties and dynamics model."}, {"color": "#d29c6e", "id": "Linear_Optimal_Policy", "label": "Linear_Optimal_Policy", "shape": "dot", "size": 10, "title": "Result showing that under certain conditions, the optimal policy is linear in state variables."}, {"color": "#7b7aa0", "id": "Optimal_Policy_Linear_Systems", "label": "Optimal_Policy_Linear_Systems", "shape": "dot", "size": 10, "title": "Discussion on optimal policies for linear systems."}, {"color": "#bc2536", "id": "Discrete_Ricatti_Equations", "label": "Discrete_Ricatti_Equations", "shape": "dot", "size": 10, "title": "Equations governing the evolution of \u03a6_t and \u03a8_t in discrete time."}, {"color": "#ad4655", "id": "LQR_Algorithm", "label": "LQR_Algorithm", "shape": "dot", "size": 10, "title": "Algorithm for solving Linear Quadratic Regulator problems."}, {"color": "#2d64d6", "id": "Nonlinear_Dynamics_to_LQR", "label": "Nonlinear_Dynamics_to_LQR", "shape": "dot", "size": 10, "title": "Reduction of nonlinear dynamics to LQR framework."}, {"color": "#cfeef1", "id": "Inverted_Pendulum", "label": "Inverted_Pendulum", "shape": "dot", "size": 10, "title": "A classic control problem used to illustrate system dynamics."}, {"color": "#380383", "id": "System_Dynamics", "label": "System_Dynamics", "shape": "dot", "size": 10, "title": "Study of how systems change over time and respond to inputs."}, {"color": "#ca0b6f", "id": "Linearization_of_Dynamics", "label": "Linearization_of_Dynamics", "shape": "dot", "size": 10, "title": "Process of approximating nonlinear dynamics with linear equations."}, {"color": "#107dad", "id": "Taylor_Expansion", "label": "Taylor_Expansion", "shape": "dot", "size": 10, "title": "Mathematical technique for approximating functions using polynomials."}, {"color": "#cf6637", "id": "LQR_Assumptions", "label": "LQR_Assumptions", "shape": "dot", "size": 10, "title": "Similarities between linearized dynamics and Linear Quadratic Regulator assumptions."}, {"color": "#85e2b9", "id": "Differential_Dynamic_Programming", "label": "Differential_Dynamic_Programming", "shape": "star", "size": 25, "title": "Optimization technique for trajectory planning in nonlinear systems."}, {"color": "#3ee96b", "id": "DifferentialDynamicProgramming", "label": "DifferentialDynamicProgramming", "shape": "star", "size": 25, "title": "Method used to optimize trajectories in dynamic systems."}, {"color": "#292cab", "id": "NominalTrajectoryGeneration", "label": "NominalTrajectoryGeneration", "shape": "dot", "size": 10, "title": "Process of generating an initial trajectory approximation using a naive controller."}, {"color": "#459635", "id": "LinearizationOfDynamics", "label": "LinearizationOfDynamics", "shape": "dot", "size": 10, "title": "Technique for linearizing system dynamics around each trajectory point."}, {"color": "#ccc072", "id": "RewritingDynamics", "label": "RewritingDynamics", "shape": "dot", "size": 10, "title": "Rewriting the dynamics using a linear approximation in non-stationary settings."}, {"color": "#b5c802", "id": "RewardLinearization", "label": "RewardLinearization", "shape": "dot", "size": 10, "title": "Second-order Taylor expansion for approximating rewards around trajectory points."}, {"color": "#755376", "id": "LinearQuadraticRegulatorLQR", "label": "LinearQuadraticRegulatorLQR", "shape": "star", "size": 25, "title": "Control strategy for linear systems with quadratic cost functions."}, {"color": "#50e10f", "id": "LQG", "label": "LQG", "shape": "dot", "size": 10, "title": "Extension of LQR to stochastic environments where the state is not fully observable."}, {"color": "#d7a117", "id": "Partially Observable MDPs (POMDP)", "label": "Partially Observable MDPs (POMDP)", "shape": "dot", "size": 10, "title": "MDPs with an additional observation layer to handle partial observability."}, {"color": "#368567", "id": "Observation Layer", "label": "Observation Layer", "shape": "dot", "size": 10, "title": "Introduces the concept of observations in POMDPs."}, {"color": "#fc4930", "id": "Belief State", "label": "Belief State", "shape": "dot", "size": 10, "title": "Maintains a distribution over states based on observations."}, {"color": "#082388", "id": "Policy Mapping", "label": "Policy Mapping", "shape": "dot", "size": 10, "title": "Maps belief states to actions in POMDPs."}, {"color": "#da7166", "id": "LQR Extension", "label": "LQR Extension", "shape": "dot", "size": 10, "title": "Extension of Linear Quadratic Regulator to partially observable settings."}, {"color": "#7faa00", "id": "Observation Model", "label": "Observation Model", "shape": "dot", "size": 10, "title": "Model describing how observations are generated from states in LQR extension."}, {"color": "#2f4fa4", "id": "Kalman Filter", "label": "Kalman Filter", "shape": "dot", "size": 10, "title": "Algorithm for efficient computation of state estimates in dynamic systems with Gaussian noise."}, {"color": "#327984", "id": "Step 1", "label": "Step 1", "shape": "dot", "size": 10, "title": "Initial step to set up the system dynamics and observation model."}, {"color": "#2ef820", "id": "Gaussian Distributions", "label": "Gaussian Distributions", "shape": "star", "size": 25, "title": "Statistical distributions used in modeling uncertainties in state estimation problems."}, {"color": "#4932e4", "id": "Predict Step", "label": "Predict Step", "shape": "dot", "size": 10, "title": "Step to predict the next state based on current observations and dynamics."}, {"color": "#fe2bee", "id": "Update Step", "label": "Update Step", "shape": "dot", "size": 10, "title": "Step to update the predicted state with new observation data."}, {"color": "#186ae1", "id": "LQR Algorithm", "label": "LQR Algorithm", "shape": "star", "size": 25, "title": "Linear Quadratic Regulator algorithm used for optimal control problems in linear systems."}, {"color": "#d55be4", "id": "Belief States Update", "label": "Belief States Update", "shape": "dot", "size": 10, "title": "Combination of predict and update steps to refine belief states over time."}, {"color": "#f941ad", "id": "Gaussian Distribution", "label": "Gaussian Distribution", "shape": "dot", "size": 10, "title": "Distribution used in the prediction step for state estimation."}, {"color": "#1bc969", "id": "State Transition", "label": "State Transition", "shape": "dot", "size": 10, "title": "Process of predicting next state from current state and transition matrix A."}, {"color": "#90ee83", "id": "Kalman Gain", "label": "Kalman Gain", "shape": "dot", "size": 10, "title": "Matrix used to compute the update step for refining state estimate."}, {"color": "#4c662e", "id": "Chapter_17_Policy_Gradient_REINFORCE", "label": "Chapter_17_Policy_Gradient_REINFORCE", "shape": "star", "size": 25, "title": "Introduces REINFORCE algorithm for model-free reinforcement learning without value functions."}, {"color": "#a4ad7b", "id": "REINFORCE_Finite_Horizon_Case", "label": "REINFORCE_Finite_Horizon_Case", "shape": "dot", "size": 10, "title": "Explains the application of REINFORCE in finite horizon scenarios."}, {"color": "#acc742", "id": "Randomized_Policy", "label": "Randomized_Policy", "shape": "dot", "size": 10, "title": "Describes how REINFORCE applies to learning randomized policies."}, {"color": "#331ae1", "id": "Sampling_Transition_Probabilities", "label": "Sampling_Transition_Probabilities", "shape": "dot", "size": 10, "title": "Explains the requirement of sampling from transition probabilities and querying reward functions."}, {"color": "#92dcde", "id": "Expected_Total_Payoff_Optimization", "label": "Expected_Total_Payoff_Optimization", "shape": "dot", "size": 10, "title": "Describes optimization of expected total payoff over policy parameters."}, {"color": "#68aed0", "id": "Finite_Infinite_Horizon_Difference", "label": "Finite_Infinite_Horizon_Difference", "shape": "dot", "size": 10, "title": "Notes on the difference between finite and infinite horizon settings."}, {"color": "#d25efb", "id": "Policy_Gradient_Methods", "label": "Policy_Gradient_Methods", "shape": "dot", "size": 10, "title": "Methods that optimize policies directly in reinforcement learning settings."}, {"color": "#624819", "id": "Gradient_Ascend", "label": "Gradient_Ascend", "shape": "dot", "size": 10, "title": "Optimization technique to maximize the expected reward function."}, {"color": "#dc9701", "id": "Reward_Function", "label": "Reward_Function", "shape": "dot", "size": 10, "title": "Function that assigns a scalar value for an action taken in a given state."}, {"color": "#ba3e96", "id": "Transition_Probabilities", "label": "Transition_Probabilities", "shape": "dot", "size": 10, "title": "Probabilities of moving from one state to another based on actions."}, {"color": "#ef1ee7", "id": "Expectation_Value", "label": "Expectation_Value", "shape": "dot", "size": 10, "title": "Expected value of the reward function under a policy."}, {"color": "#26fdf2", "id": "REINFORCE_Algorithm", "label": "REINFORCE_Algorithm", "shape": "dot", "size": 10, "title": "Algorithm for estimating gradients in reinforcement learning without explicit knowledge of transition probabilities or rewards."}, {"color": "#cf243f", "id": "PolicyGradientTheorem", "label": "PolicyGradientTheorem", "shape": "dot", "size": 10, "title": "Explanation of the policy gradient theorem in reinforcement learning."}, {"color": "#6d0547", "id": "ExpectationEstimation", "label": "ExpectationEstimation", "shape": "dot", "size": 10, "title": "Derivation and explanation of expectation estimation using samples."}, {"color": "#091700", "id": "LogProbabilityComputation", "label": "LogProbabilityComputation", "shape": "dot", "size": 10, "title": "Details on how to compute the log probability of a trajectory under policy \u0398."}, {"color": "#519f7e", "id": "Policy_Gradient_Theory", "label": "Policy_Gradient_Theory", "shape": "star", "size": 25, "title": "Theoretical foundation of policy gradients in reinforcement learning."}, {"color": "#20599d", "id": "Log_Probability_Gradients", "label": "Log_Probability_Gradients", "shape": "dot", "size": 10, "title": "Derivation and calculation of gradients for log probability terms."}, {"color": "#5f3f5b", "id": "Vanilla_REINFORCE_Algorithm", "label": "Vanilla_REINFORCE_Algorithm", "shape": "dot", "size": 10, "title": "Basic algorithm that uses policy gradient to update parameters by gradient ascent."}, {"color": "#ba98bb", "id": "Trajectory_Probability_Change", "label": "Trajectory_Probability_Change", "shape": "dot", "size": 10, "title": "Direction of change in trajectory probability with respect to parameter \theta."}, {"color": "#ef97f4", "id": "Empirical_Estimation", "label": "Empirical_Estimation", "shape": "dot", "size": 10, "title": "Estimating gradients using sample trajectories for unbiased updates."}, {"color": "#4807d7", "id": "Policy_Gradient_Theorem", "label": "Policy_Gradient_Theorem", "shape": "dot", "size": 10, "title": "The theorem that describes how policy gradients can be used to optimize policies in reinforcement learning."}, {"color": "#f82520", "id": "Expected_Value_Calculation", "label": "Expected_Value_Calculation", "shape": "dot", "size": 10, "title": "Calculation of expected values for policy gradients under different reward conditions."}, {"color": "#8aead0", "id": "Trajectory_Probability_Adjustment", "label": "Trajectory_Probability_Adjustment", "shape": "dot", "size": 10, "title": "Adjusting the probability of trajectories based on their rewards."}, {"color": "#3c72ac", "id": "Formula_17.9", "label": "Formula_17.9", "shape": "dot", "size": 10, "title": "Derivation and explanation of formula (17.9) which shows that expected gradients are zero under constant reward conditions."}, {"color": "#b44b20", "id": "Simplification_of_Formalism", "label": "Simplification_of_Formalism", "shape": "dot", "size": 10, "title": "Simplifying the formalism for policy gradient theorem when rewards are considered over time."}, {"color": "#cc7a45", "id": "Machine_Learning_Overview", "label": "Machine_Learning_Overview", "shape": "star", "size": 25, "title": "General overview of machine learning concepts and applications."}, {"color": "#72baef", "id": "Law_of_Total_Expectation", "label": "Law_of_Total_Expectation", "shape": "dot", "size": 10, "title": "The law used to simplify expectations over multiple random variables."}, {"color": "#73fa9b", "id": "Estimator_Simplification", "label": "Estimator_Simplification", "shape": "dot", "size": 10, "title": "Simplifying the estimator using the law of total expectation."}, {"color": "#e1c37b", "id": "Baseline_Estimator", "label": "Baseline_Estimator", "shape": "dot", "size": 10, "title": "Reduces variance in policy gradient estimates by subtracting a baseline value."}, {"color": "#86f3f5", "id": "Algorithm_7_Vanilla_Policy_Gradient_Baseline", "label": "Algorithm_7_Vanilla_Policy_Gradient_Baseline", "shape": "dot", "size": 10, "title": "Describes the process of updating policies using gradient estimates with baseline adjustments."}, {"color": "#ccc518", "id": "Algorithm_8_Vanilla_Policy_Gradient_Baseline", "label": "Algorithm_8_Vanilla_Policy_Gradient_Baseline", "shape": "dot", "size": 10, "title": "Similar to Algorithm 7, describes another version or iteration of the policy gradient algorithm."}, {"color": "#609e92", "id": "Machine_Learning_Papers", "label": "Machine_Learning_Papers", "shape": "star", "size": 25, "title": "Collection of papers related to machine learning concepts and techniques."}, {"color": "#a4aa4e", "id": "Double_Descent_Models", "label": "Double_Descent_Models", "shape": "dot", "size": 10, "title": "Models explaining double descent phenomenon for weak features in machine learning."}, {"color": "#2103d9", "id": "Variational_Inference_Review", "label": "Variational_Inference_Review", "shape": "dot", "size": 10, "title": "Overview of variational inference techniques from a statistical perspective."}, {"color": "#0944f9", "id": "Foundation_Models", "label": "Foundation_Models", "shape": "dot", "size": 10, "title": "Analysis of opportunities and risks associated with foundation models in ML."}, {"color": "#cb7337", "id": "Few_Shot_Learning", "label": "Few_Shot_Learning", "shape": "dot", "size": 10, "title": "Demonstration that language models can perform well on few-shot learning tasks."}, {"color": "#eae152", "id": "Contrastive_Learning_Visual_Representations", "label": "Contrastive_Learning_Visual_Representations", "shape": "dot", "size": 10, "title": "Framework for contrastive learning to improve visual representation in machine learning."}, {"color": "#5e5d8c", "id": "BERT_Model", "label": "BERT_Model", "shape": "dot", "size": 10, "title": "Introduction of BERT model for pre-training deep bidirectional transformers for language understanding."}, {"color": "#e81f75", "id": "Implicit_Bias_Noise_Covariance", "label": "Implicit_Bias_Noise_Covariance", "shape": "dot", "size": 10, "title": "Exploration of implicit bias in machine learning models due to noise covariance shape."}, {"color": "#64337b", "id": "High_Dimensional_Statistics", "label": "High_Dimensional_Statistics", "shape": "dot", "size": 10, "title": "Discussion on surprising phenomena in high-dimensional statistics and their implications for ML."}, {"color": "#b06142", "id": "Implicit_Bias", "label": "Implicit_Bias", "shape": "dot", "size": 10, "title": "Research on the implicit bias in high-dimensional models due to noise covariance."}, {"color": "#83ae9f", "id": "High_Dimensional_Ridgeless_Least_Squares_Interpolation", "label": "High_Dimensional_Ridgeless_Least_Squares_Interpolation", "shape": "dot", "size": 10, "title": "Surprises and insights from ridgeless least squares interpolation in high dimensions."}, {"color": "#43f188", "id": "Deep_Residual_Learning", "label": "Deep_Residual_Learning", "shape": "dot", "size": 10, "title": "Techniques for deep residual learning applied to image recognition tasks."}, {"color": "#926873", "id": "Statistical_Learning_Theory", "label": "Statistical_Learning_Theory", "shape": "dot", "size": 10, "title": "Introduction and theoretical foundations of statistical learning methods."}, {"color": "#c25989", "id": "Stochastic_Optimization_Methods", "label": "Stochastic_Optimization_Methods", "shape": "dot", "size": 10, "title": "Advanced optimization techniques for stochastic settings in machine learning."}, {"color": "#a921f6", "id": "Variational_Bayes", "label": "Variational_Bayes", "shape": "dot", "size": 10, "title": "Auto-encoding variational Bayes method for approximate inference in probabilistic models."}, {"color": "#8b92c5", "id": "Model-Based_Deep_Reinforcement_Learning", "label": "Model-Based_Deep_Reinforcement_Learning", "shape": "dot", "size": 10, "title": "Algorithmic framework and theoretical guarantees for model-based deep reinforcement learning."}, {"color": "#5883d4", "id": "Generalization_Error_Analysis", "label": "Generalization_Error_Analysis", "shape": "dot", "size": 10, "title": "Analysis of the generalization error in random features regression with precise asymptotics."}, {"color": "#ecf974", "id": "Sample_Size_Effects", "label": "Sample_Size_Effects", "shape": "dot", "size": 10, "title": "Impact of sample size on linear regression models and the phenomenon of double descent."}, {"color": "#7fec3f", "id": "Regularization_Techniques", "label": "Regularization_Techniques", "shape": "dot", "size": 10, "title": "Optimal regularization strategies to mitigate the double descent effect in machine learning models."}, {"color": "#27bdf7", "id": "StatisticalMechanicsOfLearning", "label": "StatisticalMechanicsOfLearning", "shape": "dot", "size": 10, "title": "Application of statistical mechanics principles to understand the generalization ability of learning algorithms."}, {"color": "#2e468d", "id": "GeneralizationInLearning", "label": "GeneralizationInLearning", "shape": "dot", "size": 10, "title": "Study on how well a model can generalize from training data to unseen data."}]);
                  edges = new vis.DataSet([{"arrows": "to", "from": "LossFunction", "title": "uses", "to": "IntermediateVariables"}, {"arrows": "to", "from": "Lagrangian_Formulation", "title": "depends_on", "to": "Equation_6.9"}, {"arrows": "to", "from": "Gaussian Discriminant Analysis (GDA)", "title": "subtopic", "to": "Multivariate Normal Distribution"}, {"arrows": "to", "from": "ParameterEstimation", "title": "depends_on", "to": "Prediction"}, {"arrows": "to", "from": "Machine_Learning_Concepts", "title": "contains", "to": "Backpropagation"}, {"arrows": "to", "from": "Bias-Variance Tradeoff", "title": "subtopic", "to": "Test Error Analysis"}, {"arrows": "to", "from": "LQR, DDP and LQG", "title": "has_subtopic", "to": "Linear Quadratic Regulation (LQR)"}, {"arrows": "to", "from": "MachineLearningConcepts", "title": "has_subtopic", "to": "Conv1DModule"}, {"arrows": "to", "from": "BinaryClassification", "title": "process_of", "to": "EmpiricalRiskMinimization"}, {"arrows": "to", "from": "Support_Vector_Machines", "title": "subtopic", "to": "Optimal_Margin_Classifier"}, {"arrows": "to", "from": "Examples_of_Kernels", "title": "related_to", "to": "Kernel_Matrix"}, {"arrows": "to", "from": "Fitted Value Iteration", "title": "has_subtopic", "to": "Parameter Update"}, {"arrows": "to", "from": "Training Set", "title": "depends_on", "to": "Decision Boundary"}, {"arrows": "to", "from": "Markov_Decision_Processes_MDPs", "title": "related_to", "to": "Value_Iteration"}, {"arrows": "to", "from": "Stochastic Gradient Descent (SGD)", "title": "variant_of", "to": "Mini-batch Stochastic Gradient Descent"}, {"arrows": "to", "from": "MatrixNotation", "title": "subtopic", "to": "Vectorization"}, {"arrows": "to", "from": "Reward Function", "title": "depends_on", "to": "Finite-State MDPs"}, {"arrows": "to", "from": "ContinuousLatentVariables", "title": "depends_on", "to": "GaussianDistributionQ_i"}, {"arrows": "to", "from": "LinearModelOnTopOfFeatureMap", "title": "depends_on", "to": "FeatureMaps"}, {"arrows": "to", "from": "Chain_Rule_Applications", "title": "depends_on", "to": "Gradient_Computation"}, {"arrows": "to", "from": "MLPModel", "title": "contains", "to": "ModulesInMLP"}, {"arrows": "to", "from": "Kernels_in_Machine_Learning", "title": "related_to", "to": "Kernel_Function_K"}, {"arrows": "to", "from": "Fitted Value Iteration", "title": "subtopic", "to": "Value Function Approximation"}, {"arrows": "to", "from": "Naive_Bayes_Assumption", "title": "explains", "to": "Conditional_Independence"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "subtopic", "to": "Bias-Variance Tradeoff"}, {"arrows": "to", "from": "Clustering", "title": "subtopic", "to": "KMeansAlgorithm"}, {"arrows": "to", "from": "StatisticalMechanicsOfLearning", "title": "related_to", "to": "GeneralizationInLearning"}, {"arrows": "to", "from": "ExpectedTestError", "title": "explains", "to": "BiasVarianceTradeoff"}, {"arrows": "to", "from": "MLPArchitecture", "title": "depends_on", "to": "NonlinearActivationModule"}, {"arrows": "to", "from": "Continuous state MDPs", "title": "has_subtopic", "to": "Value function approximation"}, {"arrows": "to", "from": "PrimalProblem", "title": "subtopic", "to": "GeneralizedLagrangian"}, {"arrows": "to", "from": "MachineLearningBasics", "title": "depends_on", "to": "LMSAlgorithm"}, {"arrows": "to", "from": "Machine_Learning", "title": "has_subtopic", "to": "Contrastive_Learning"}, {"arrows": "to", "from": "Reinforcement learning", "title": "subtopic", "to": "Value iteration and policy iteration"}, {"arrows": "to", "from": "ProbabilisticModeling", "title": "depends_on", "to": "IndependenceAssumption"}, {"arrows": "to", "from": "Optimization Problem", "title": "has_subtopic", "to": "Convex Quadratic Objective"}, {"arrows": "to", "from": "Dual_Problem_Formulation", "title": "depends_on", "to": "Lagrange_Multipliers"}, {"arrows": "to", "from": "Machine Learning Algorithms", "title": "depends_on", "to": "Hyperparameters"}, {"arrows": "to", "from": "Naive_Bayes_Classifier", "title": "depends_on", "to": "Spam_Filtering"}, {"arrows": "to", "from": "NeuralNetworkParameters", "title": "related_to", "to": "BiologicalInspiration"}, {"arrows": "to", "from": "Machine_Learning", "title": "related_to", "to": "Hypothesis_Class"}, {"arrows": "to", "from": "Step2OptimalPolicy", "title": "subtopic", "to": "DynamicProgramming"}, {"arrows": "to", "from": "Backward_Propagation_Equation", "title": "related_to", "to": "Gradient_Computation"}, {"arrows": "to", "from": "Reinforcement_Learning", "title": "depends_on", "to": "Value_Function_Approximation"}, {"arrows": "to", "from": "EvidenceLowerBoundELBO", "title": "related_to", "to": "PosteriorDistribution"}, {"arrows": "to", "from": "Machine_Learning", "title": "depends_on", "to": "Gradient_Descent"}, {"arrows": "to", "from": "Discretization in MDPs", "title": "depends_on", "to": "Discretization Limitations"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "contains", "to": "Locally Weighted Linear Regression"}, {"arrows": "to", "from": "SupportVectors", "title": "depends_on", "to": "DecisionBoundary"}, {"arrows": "to", "from": "Value Function Approximation", "title": "has_subtopic", "to": "Max Action Selection"}, {"arrows": "to", "from": "Few_Shot_Learning", "title": "subtopic", "to": "Machine_Learning_Papers"}, {"arrows": "to", "from": "Cross Validation", "title": "subtopic", "to": "Infinite Model Space"}, {"arrows": "to", "from": "Support Vector Machines (SVM)", "title": "has_subtopic", "to": "Functional Margin"}, {"arrows": "to", "from": "Modern Neural Networks", "title": "subtopic", "to": "Modules in Modern Neural Networks"}, {"arrows": "to", "from": "Machine_Learning_Adaptation", "title": "related_to", "to": "Pretraining_Methods"}, {"arrows": "to", "from": "LinearModelOnTopOfFeatureMap", "title": "subtopic", "to": "DeepLearning"}, {"arrows": "to", "from": "TemperatureParameter", "title": "depends_on", "to": "TextGenerationModels"}, {"arrows": "to", "from": "Principal Component Analysis (PCA)", "title": "subtopic_of", "to": "Eigenfaces Method"}, {"arrows": "to", "from": "Target Vector", "title": "subtopic", "to": "Least Squares Revisited"}, {"arrows": "to", "from": "dStarAndPStarEquality", "title": "depends_on", "to": "ConvexityAssumptions"}, {"arrows": "to", "from": "MachineLearningOverview", "title": "subtopic", "to": "LocallyWeightedLinearRegression"}, {"arrows": "to", "from": "MachineLearningOptimization", "title": "has_subtopic", "to": "BatchGradientDescent"}, {"arrows": "to", "from": "Sample_Size_Effects", "title": "subtopic", "to": "Machine_Learning_Papers"}, {"arrows": "to", "from": "From non-linear dynamics to LQR", "title": "has_subtopic", "to": "Linearization of dynamics"}, {"arrows": "to", "from": "Machine_Learning_Concepts", "title": "contains", "to": "Overfitting_Underfitting"}, {"arrows": "to", "from": "GeneralizedLinearModelsGLMs", "title": "subtopic", "to": "LogisticRegression"}, {"arrows": "to", "from": "MachineLearning", "title": "has_subtopic", "to": "ReinforcementLearning"}, {"arrows": "to", "from": "Optimizers and Generalization", "title": "depends_on", "to": "Implicit Regularization"}, {"arrows": "to", "from": "GLMFormulation", "title": "related_to", "to": "OtherDistributions"}, {"arrows": "to", "from": "MachineLearningOptimization", "title": "has_subtopic", "to": "StochasticGradientDescent"}, {"arrows": "to", "from": "Machine_Learning_Topics", "title": "has_subtopic", "to": "Convolutional_Neural_Networks"}, {"arrows": "to", "from": "EMAlgorithm", "title": "subtopic", "to": "MStep"}, {"arrows": "to", "from": "Policy_Gradient_Theorem", "title": "subtopic_of", "to": "Expected_Value_Calculation"}, {"arrows": "to", "from": "DiscriminativeAlgorithms", "title": "has_subtopic", "to": "LogisticRegression"}, {"arrows": "to", "from": "DataNormalization", "title": "depends_on", "to": "PCAAlgorithm"}, {"arrows": "to", "from": "ICA Ambiguities", "title": "depends_on", "to": "Sign Changes"}, {"arrows": "to", "from": "Overfitting", "title": "subtopic", "to": "MachineLearningConcepts"}, {"arrows": "to", "from": "Policy_Gradient_Methods", "title": "depends_on", "to": "Law_of_Total_Expectation"}, {"arrows": "to", "from": "Optimization Problem", "title": "related_to", "to": "Lagrangian Function"}, {"arrows": "to", "from": "Markov Decision Processes (MDP)", "title": "has_subtopic", "to": "Finite Horizon Setting"}, {"arrows": "to", "from": "Support Vector Machines (SVM)", "title": "depends_on", "to": "Machine Learning Overview"}, {"arrows": "to", "from": "Supervised Learning with Non-Linear Models", "title": "subtopic", "to": "Deep Learning Introduction"}, {"arrows": "to", "from": "JensensInequality", "title": "depends_on", "to": "EqualityCondition"}, {"arrows": "to", "from": "DifferentialDynamicProgramming", "title": "subtopic", "to": "LinearizationOfDynamics"}, {"arrows": "to", "from": "Loss Function", "title": "subtopic", "to": "Average Loss"}, {"arrows": "to", "from": "LQR Extension", "title": "has_subtopic", "to": "Observation Model"}, {"arrows": "to", "from": "PrimalDualProblem", "title": "related_to", "to": "KKTConditions"}, {"arrows": "to", "from": "ReLUFunction", "title": "depends_on", "to": "LeakyReLUFunction"}, {"arrows": "to", "from": "Machine_Learning_Theory", "title": "depends_on", "to": "Union_Bound"}, {"arrows": "to", "from": "Expectation-Maximization Algorithm", "title": "has_subtopic", "to": "E-step"}, {"arrows": "to", "from": "Dimensionality Reduction", "title": "subtopic", "to": "Data Visualization"}, {"arrows": "to", "from": "Chapter_17_Policy_Gradient_REINFORCE", "title": "subtopic", "to": "Expected_Total_Payoff_Optimization"}, {"arrows": "to", "from": "General Case", "title": "subtopic", "to": "Density Transformation"}, {"arrows": "to", "from": "FeatureEngineering", "title": "related_to", "to": "FeatureMaps"}, {"arrows": "to", "from": "VE Procedure", "title": "subtopic", "to": "Option 1 Initialization"}, {"arrows": "to", "from": "Joint Distribution", "title": "subtopic", "to": "Mixture of Gaussians Model"}, {"arrows": "to", "from": "ParallelTrainingExamples", "title": "subtopic", "to": "VectorizedForwardPass"}, {"arrows": "to", "from": "KernelTrickIntroduction", "title": "subtopic", "to": "ThetaVectorInitialization"}, {"arrows": "to", "from": "LQR, DDP and LQG", "title": "has_subtopic", "to": "From non-linear dynamics to LQR"}, {"arrows": "to", "from": "Implicit Regularization", "title": "depends_on", "to": "Global Minima and Generalization"}, {"arrows": "to", "from": "GeneralizationErrorGuarantees", "title": "subtopic", "to": "MachineLearningConcepts"}, {"arrows": "to", "from": "5 Kernel methods", "title": "has_subtopic", "to": "5.2 LMS (least mean squares) with features"}, {"arrows": "to", "from": "Stacking Neurons", "title": "example_of", "to": "Complex Neural Network Example"}, {"arrows": "to", "from": "PilotSurveyExample", "title": "related_to", "to": "RedundancyDetection"}, {"arrows": "to", "from": "Learning a model for an MDP", "title": "has_subtopic", "to": "Continuous state MDPs"}, {"arrows": "to", "from": "State Transition Probabilities", "title": "depends_on", "to": "Finite-State MDPs"}, {"arrows": "to", "from": "JensensInequality", "title": "depends_on", "to": "ConvexFunctionDefinition"}, {"arrows": "to", "from": "DiscriminativeAlgorithms", "title": "has_subtopic", "to": "PerceptronAlgorithm"}, {"arrows": "to", "from": "MachineLearningOverview", "title": "contains", "to": "KernelTrickIntroduction"}, {"arrows": "to", "from": "FunctionalMargin", "title": "subtopic", "to": "FunctionMarginTrainingSet"}, {"arrows": "to", "from": "Reinforcement learning", "title": "subtopic", "to": "Connections between Policy and Value Iteration (Optional)"}, {"arrows": "to", "from": "LinearCombinationRepresentation", "title": "depends_on", "to": "InductiveProofOfRepresentation"}, {"arrows": "to", "from": "EM_Algorithm", "title": "depends_on", "to": "M_Step"}, {"arrows": "to", "from": "OptimizationMethods", "title": "related_to", "to": "FisherScoring"}, {"arrows": "to", "from": "Zero-Shot_Adaptation", "title": "subtopic", "to": "Machine_Learning_Adaptation"}, {"arrows": "to", "from": "Independent Component Analysis (ICA)", "title": "has_example", "to": "Cocktail Party Problem"}, {"arrows": "to", "from": "Bernoulli Distribution", "title": "subtopic", "to": "Exponential Family Distributions"}, {"arrows": "to", "from": "Regularization_and_Kernels", "title": "related_to", "to": "Support_Vector_Machines"}, {"arrows": "to", "from": "Valid_Kernels_Conditions", "title": "depends_on", "to": "Kernel_Matrix"}, {"arrows": "to", "from": "Transformer_Model", "title": "subtopic", "to": "Training_Transformer"}, {"arrows": "to", "from": "MachineLearningModels", "title": "contains", "to": "RobustnessToAssumptions"}, {"arrows": "to", "from": "FiniteHorizonMDP", "title": "has_subtopic", "to": "OptimalPolicyFiniteHorizon"}, {"arrows": "to", "from": "Sample-wise Double Descent", "title": "subtopic", "to": "Double Descent Phenomenon"}, {"arrows": "to", "from": "Bias-Variance Tradeoff", "title": "related_to", "to": "Double Descent Phenomenon"}, {"arrows": "to", "from": "LMSAlgorithmWithFeatures", "title": "subtopic", "to": "BatchGradientDescentUpdate"}, {"arrows": "to", "from": "ReLUFunction", "title": "depends_on", "to": "GELUFunction"}, {"arrows": "to", "from": "SMO_Algorithm", "title": "contains", "to": "Constraint_Satisfaction"}, {"arrows": "to", "from": "Discretization Limitations", "title": "subtopic", "to": "Curse of Dimensionality"}, {"arrows": "to", "from": "BackpropagationAlgorithm", "title": "related_to", "to": "IntermediateValuesStorage"}, {"arrows": "to", "from": "Linear Quadratic Regulation (LQR)", "title": "defines", "to": "Continuous Setting"}, {"arrows": "to", "from": "MachineLearningOverview", "title": "contains", "to": "DoubleDescentPhenomenon"}, {"arrows": "to", "from": "Vanilla_REINFORCE_Algorithm", "title": "has_subtopic", "to": "Empirical_Estimation"}, {"arrows": "to", "from": "1DConvolution", "title": "related_to", "to": "ConvolutionalLayers"}, {"arrows": "to", "from": "Adaptation_Phase", "title": "depends_on", "to": "Labeled_Dataset_Task"}, {"arrows": "to", "from": "Training_Dataset", "title": "subtopic", "to": "Empirical_Distribution"}, {"arrows": "to", "from": "Optimal_Parameter_Finding", "title": "depends_on", "to": "Support_Vector_Machines"}, {"arrows": "to", "from": "Softmax Function", "title": "subtopic", "to": "2.3 Multi-class classification"}, {"arrows": "to", "from": "Machine_Learning_Backward_Propagation", "title": "related_to", "to": "Jacobian_Matrix"}, {"arrows": "to", "from": "Dimensionality Reduction", "title": "subtopic", "to": "Compression"}, {"arrows": "to", "from": "Computational_Graph_Illustration", "title": "subtopic", "to": "Gradient_Computation"}, {"arrows": "to", "from": "Optimizers and Generalization", "title": "subtopic", "to": "Learning Rate Schedules"}, {"arrows": "to", "from": "NeuralNetworks", "title": "contains", "to": "WeightMatrices"}, {"arrows": "to", "from": "Modern Neural Networks", "title": "subtopic", "to": "Vectorization over training examples"}, {"arrows": "to", "from": "Coordinate_Ascend_Method", "title": "depends_on", "to": "Quadratic_Function_Optimization"}, {"arrows": "to", "from": "Linearization_of_Dynamics", "title": "similar_to", "to": "LQR_Assumptions"}, {"arrows": "to", "from": "High_Dimensional_Statistics", "title": "subtopic", "to": "Machine_Learning_Papers"}, {"arrows": "to", "from": "Data Augmentation", "title": "creates", "to": "Positive Pair"}, {"arrows": "to", "from": "KernelTrick", "title": "depends_on", "to": "KernelsAsSimilarityMetrics"}, {"arrows": "to", "from": "Step1Estimation", "title": "contains", "to": "ValueApproximation"}, {"arrows": "to", "from": "Continuous Latent Variables", "title": "subtopic", "to": "Variational Inference"}, {"arrows": "to", "from": "LikelihoodFunction", "title": "depends_on", "to": "LogLikelihoodFunction"}, {"arrows": "to", "from": "Expected_Value_Calculation", "title": "depends_on", "to": "Trajectory_Probability_Adjustment"}, {"arrows": "to", "from": "High_Dimensional_Ridgeless_Least_Squares_Interpolation", "title": "related_to", "to": "Machine_Learning_Papers"}, {"arrows": "to", "from": "Chain_Rule_Applications", "title": "related_to", "to": "Complex_Functions"}, {"arrows": "to", "from": "KKT_Conditions", "title": "subtopic", "to": "Dual_Complementarity"}, {"arrows": "to", "from": "Machine_Learning_Concepts", "title": "has_subtopic", "to": "Cross_Entropy_Loss"}, {"arrows": "to", "from": "Machine_Learning", "title": "related_to", "to": "Variational_Autoencoder"}, {"arrows": "to", "from": "LQR, DDP and LQG", "title": "has_subtopic", "to": "Linear Quadratic Gaussian (LQG)"}, {"arrows": "to", "from": "Machine_Learning_Topics", "title": "related_to", "to": "Kernel_Method"}, {"arrows": "to", "from": "KL_Divergence", "title": "depends_on", "to": "ELBO"}, {"arrows": "to", "from": "Machine_Learning_Algorithms", "title": "has_subtopic", "to": "Support_Vector_Machines_SVMs"}, {"arrows": "to", "from": "Kernels_in_Machine_Learning", "title": "depends_on", "to": "Feature_Map_Phi"}, {"arrows": "to", "from": "MachineLearningOverview", "title": "related_to", "to": "MaximumLikelihoodEstimation"}, {"arrows": "to", "from": "Contrastive_Learning_Visual_Representations", "title": "subtopic", "to": "Machine_Learning_Papers"}, {"arrows": "to", "from": "IterativeUpdateRule", "title": "contains", "to": "LinearCombinationRepresentation"}, {"arrows": "to", "from": "Hold_Out_Cross_Validation", "title": "related_to", "to": "Validation_Error"}, {"arrows": "to", "from": "FeatureMap", "title": "depends_on", "to": "CubicFunctionRepresentation"}, {"arrows": "to", "from": "Machine Learning Algorithms", "title": "has_subtopic", "to": "Policy Iteration"}, {"arrows": "to", "from": "Sample_Complexity", "title": "related_to", "to": "Empirical_Risk_Minimization"}, {"arrows": "to", "from": "MachineLearningOverview", "title": "subtopic", "to": "ModelComplexityMeasures"}, {"arrows": "to", "from": "Continuous state MDPs", "title": "subtopic", "to": "Discretization"}, {"arrows": "to", "from": "MachineLearningChallenges", "title": "has_subtopic", "to": "kFoldCrossValidation"}, {"arrows": "to", "from": "Supervised Learning vs Reinforcement Learning", "title": "subtopic", "to": "Reinforcement Learning"}, {"arrows": "to", "from": "HoldoutCrossValidation", "title": "subtopic_of", "to": "ModelSelection"}, {"arrows": "to", "from": "LMS_Rule", "title": "subtopic", "to": "Single_Training_Example"}, {"arrows": "to", "from": "ProbabilisticModeling", "title": "related_to", "to": "DesignMatrixX"}, {"arrows": "to", "from": "Double_Descent_Phenomenon", "title": "subtopic", "to": "Model_Wise_Double_Descent"}, {"arrows": "to", "from": "Gaussian Distributions", "title": "related_to", "to": "Step 1"}, {"arrows": "to", "from": "InfiniteHorizonMDP", "title": "includes_concept", "to": "DiscountFactor"}, {"arrows": "to", "from": "OptimizationInML", "title": "subtopic", "to": "PrimalDualProblem"}, {"arrows": "to", "from": "Independent components analysis", "title": "subtopic", "to": "ICA algorithm"}, {"arrows": "to", "from": "Convergence_Guarantees", "title": "analyzes", "to": "EM_Algorithm"}, {"arrows": "to", "from": "Convolutional_Neural_Networks", "title": "has_subtopic", "to": "2D_Convolution"}, {"arrows": "to", "from": "DeepLearning", "title": "subtopic", "to": "MachineLearningOverview"}, {"arrows": "to", "from": "Constraints_Satisfaction", "title": "has_subtopic", "to": "KKT_Conditions"}, {"arrows": "to", "from": "MachineLearningOverview", "title": "related_to", "to": "LeastSquaresRegression"}, {"arrows": "to", "from": "Model Selection", "title": "related_to", "to": "Bias-Variance Tradeoff"}, {"arrows": "to", "from": "Loss_Functions", "title": "related_to", "to": "Mean_Squared_Error"}, {"arrows": "to", "from": "SupervisedLearning", "title": "related_to", "to": "FittedValueIteration"}, {"arrows": "to", "from": "SIMCLR", "title": "depends_on", "to": "Augmentation_Techniques"}, {"arrows": "to", "from": "Normal_Distribution", "title": "contains", "to": "Density_Properties"}, {"arrows": "to", "from": "MachineLearningConcepts", "title": "has_subtopic", "to": "Conv2DModule"}, {"arrows": "to", "from": "2 Classification and logistic regression", "title": "has_subtopic", "to": "2.4 Another algorithm for maximizing \\(\\ell(\\theta)\\)"}, {"arrows": "to", "from": "VariationalInference", "title": "subtopic", "to": "GradientComputation"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "contains", "to": "Markov Decision Processes (MDP)"}, {"arrows": "to", "from": "Policy_Iteration", "title": "subtopic", "to": "Machine_Learning_Algorithms"}, {"arrows": "to", "from": "LayerNormalization", "title": "subtopic_of", "to": "LN-SModule"}, {"arrows": "to", "from": "LogLikelihoodBound", "title": "related_to", "to": "ProbabilityDistributions"}, {"arrows": "to", "from": "VariationalAutoEncoder", "title": "contains", "to": "EncoderDecoderConcepts"}, {"arrows": "to", "from": "Predict Step", "title": "related_to", "to": "State Transition"}, {"arrows": "to", "from": "PCA", "title": "subtopic", "to": "Dimensionality Reduction"}, {"arrows": "to", "from": "ParameterizationOfH", "title": "depends_on", "to": "LinearClassifiers"}, {"arrows": "to", "from": "Logistic_Regression", "title": "uses", "to": "Logistic_Function"}, {"arrows": "to", "from": "Machine_Learning_Concepts", "title": "related_to", "to": "PCA_Methods"}, {"arrows": "to", "from": "MachineLearningArchitectures", "title": "contains", "to": "LayerNormalization"}, {"arrows": "to", "from": "Machine_Learning_Concepts", "title": "contains", "to": "M-step_Update_for_phi_j"}, {"arrows": "to", "from": "MachineLearningModels", "title": "contains", "to": "LogisticRegression"}, {"arrows": "to", "from": "Regularization", "title": "modifies", "to": "TrainingLoss"}, {"arrows": "to", "from": "GaussianDiscriminantAnalysis", "title": "has_property", "to": "AsymptoticEfficiency"}, {"arrows": "to", "from": "Value Function Approximation", "title": "has_subtopic", "to": "Action Evaluation"}, {"arrows": "to", "from": "Expectation_Calculation", "title": "subtopic", "to": "Sampling_Methods"}, {"arrows": "to", "from": "Expected_Value_Calculation", "title": "contains", "to": "Formula_17.9"}, {"arrows": "to", "from": "Loss_Functions", "title": "has_subtopic", "to": "Regularization"}, {"arrows": "to", "from": "Loss Function", "title": "sum_of", "to": "Negative Log-Likelihood"}, {"arrows": "to", "from": "Exponential Family Distributions", "title": "has_subnode", "to": "Canonical Response Function"}, {"arrows": "to", "from": "Double_Descent_Phenomenon", "title": "subtopic", "to": "Sample_Wise_Double_Descent"}, {"arrows": "to", "from": "Chapter16", "title": "contains", "to": "FiniteHorizonMDPs"}, {"arrows": "to", "from": "MachineLearningBasics", "title": "contains", "to": "BinaryClassification"}, {"arrows": "to", "from": "MachineLearningOverview", "title": "depends_on", "to": "EMAlgorithm"}, {"arrows": "to", "from": "Pretraining_Phase", "title": "subtopic", "to": "Model_Parameterization"}, {"arrows": "to", "from": "SingleNeuronNN", "title": "uses", "to": "ReLUActivationFunction"}, {"arrows": "to", "from": "IterativeUpdateRule", "title": "subtopic", "to": "UpdateRuleForCoefficients"}, {"arrows": "to", "from": "ICAAlgorithm", "title": "depends_on", "to": "MixingMatrix"}, {"arrows": "to", "from": "ModelEvaluation", "title": "subtopic_of", "to": "GeneralizationErrorEstimation"}, {"arrows": "to", "from": "NeuralNetworks", "title": "subtopic", "to": "Vectorization"}, {"arrows": "to", "from": "Bayesian Statistics", "title": "contrasts_with", "to": "Frequentist View"}, {"arrows": "to", "from": "Kalman Filter", "title": "depends_on", "to": "Step 1"}, {"arrows": "to", "from": "GeneralizedLinearModelsGLMs", "title": "subtopic", "to": "OrdinaryLeastSquaresOLS"}, {"arrows": "to", "from": "Machine_Learning_Concepts", "title": "related_to", "to": "Bias_Variance_Tradeoff"}, {"arrows": "to", "from": "Machine Learning Overview", "title": "depends_on", "to": "Discretization in MDPs"}, {"arrows": "to", "from": "ErrorTermAssumption", "title": "subtopic", "to": "ProbabilisticInterpretation"}, {"arrows": "to", "from": "Machine Learning Techniques", "title": "has_subtopic", "to": "Principal Component Analysis (PCA)"}, {"arrows": "to", "from": "Model Creation Methods", "title": "has_subtopic", "to": "Off-the-Shelf Physics Software"}, {"arrows": "to", "from": "LogisticRegression", "title": "has_subtopic", "to": "ProbabilityPrediction"}, {"arrows": "to", "from": "MaximumLikelihoodEstimation", "title": "related_to", "to": "LogLikelihoodFunction"}, {"arrows": "to", "from": "ParameterEstimation", "title": "subtopic", "to": "NaiveBayesAlgorithm"}, {"arrows": "to", "from": "distortion_function", "title": "related_to", "to": "k-means_algorithm"}, {"arrows": "to", "from": "Convergence Rate", "title": "depends_on", "to": "Newton\u0027s Method"}, {"arrows": "to", "from": "Necessary Conditions for Valid Kernels", "title": "subtopic", "to": "Positive Semi-Definite Property"}, {"arrows": "to", "from": "Posterior Distribution on Parameters", "title": "depends_on", "to": "Prediction on New Data"}, {"arrows": "to", "from": "Reinforcement learning", "title": "subtopic", "to": "Continuous state MDPs"}, {"arrows": "to", "from": "Classification Problem", "title": "contains", "to": "Binary Classification"}, {"arrows": "to", "from": "Value Iteration Algorithm", "title": "subtopic", "to": "Value Iteration and Policy Iteration"}, {"arrows": "to", "from": "MachineLearningOverview", "title": "contains", "to": "OutputParameterization"}, {"arrows": "to", "from": "MachineLearningOverview", "title": "contains", "to": "VariationalAutoEncoder"}, {"arrows": "to", "from": "BERT_Model", "title": "subtopic", "to": "Machine_Learning_Papers"}, {"arrows": "to", "from": "Objective_Function", "title": "subtopic", "to": "Quadratic_Formulation"}, {"arrows": "to", "from": "MLP Backpropagation", "title": "depends_on", "to": "Gradient Computation"}, {"arrows": "to", "from": "Markov Decision Processes (MDP)", "title": "has_subtopic", "to": "Policy in MDPs"}, {"arrows": "to", "from": "Chapter_17_Policy_Gradient_REINFORCE", "title": "subtopic", "to": "REINFORCE_Finite_Horizon_Case"}, {"arrows": "to", "from": "DistributionQ", "title": "subtopic", "to": "LogLikelihoodExpression"}, {"arrows": "to", "from": "LossFunction", "title": "leads_to", "to": "BackwardPass"}, {"arrows": "to", "from": "1 Linear regression", "title": "has_subtopic", "to": "1.2 The normal equations"}, {"arrows": "to", "from": "ConstrainedOptimization", "title": "subtopic", "to": "PrimalProblem"}, {"arrows": "to", "from": "BinaryClassificationProblem", "title": "uses", "to": "MLPModel"}, {"arrows": "to", "from": "Backpropagation", "title": "subtopic", "to": "Modules Composition"}, {"arrows": "to", "from": "Deep Learning Regularizers", "title": "contains", "to": "Lipschitzness Regularization"}, {"arrows": "to", "from": "GaussianDiscriminantAnalysis", "title": "depends_on", "to": "GDAParameters"}, {"arrows": "to", "from": "NaiveBayesClassifier", "title": "related_to", "to": "ProbabilityEstimation"}, {"arrows": "to", "from": "LinearHypothesis", "title": "subtopic", "to": "ParametersWeights"}, {"arrows": "to", "from": "Maximum_Likelihood_Estimation", "title": "method", "to": "State_Transition_Probabilities"}, {"arrows": "to", "from": "String_Classification", "title": "subtopic", "to": "Examples_of_Kernels"}, {"arrows": "to", "from": "Machine Learning", "title": "related_to", "to": "Generalization"}, {"arrows": "to", "from": "Machine Learning Models", "title": "subtopic", "to": "MDP Simulators"}, {"arrows": "to", "from": "Optimal_Margin_Classifiers", "title": "depends_on", "to": "KKT_Conditions"}, {"arrows": "to", "from": "Activation Function", "title": "applied_to", "to": "Single Neuron Model"}, {"arrows": "to", "from": "2 Classification and logistic regression", "title": "has_subtopic", "to": "2.1 Logistic regression"}, {"arrows": "to", "from": "StateTransitionModel", "title": "has_subtopic", "to": "StochasticModel"}, {"arrows": "to", "from": "Machine_Learning_Techniques", "title": "has_subtopic", "to": "Kernel_Methods"}, {"arrows": "to", "from": "Loss_Functions", "title": "has_subtopic", "to": "Cross_Entropy_Loss"}, {"arrows": "to", "from": "Representation Function", "title": "related_to", "to": "Negative Pair"}, {"arrows": "to", "from": "MachineLearningOverview", "title": "contains", "to": "LQRModelAssumptions"}, {"arrows": "to", "from": "Hypothesis_Class_Switching", "title": "subtopic", "to": "Machine_Learning_Bias_Variance_Tradeoff"}, {"arrows": "to", "from": "Bias-Variance Tradeoff", "title": "subtopic", "to": "Test Error Decomposition"}, {"arrows": "to", "from": "HoeffdingInequality", "title": "subtopic", "to": "GeneralizationErrorGuarantees"}, {"arrows": "to", "from": "Mixture of Gaussians Model", "title": "contains", "to": "Latent Variables"}, {"arrows": "to", "from": "MachineLearningConcepts", "title": "related_to", "to": "ValidKernelConditions"}, {"arrows": "to", "from": "Model_Learning", "title": "contains", "to": "Unknown_State_Transitions_and_Rewards"}, {"arrows": "to", "from": "RegressionProblems", "title": "depends_on", "to": "TestExample"}, {"arrows": "to", "from": "Test Error Analysis", "title": "example_of", "to": "Linear Regression Example"}, {"arrows": "to", "from": "Parameterized Model", "title": "subtopic", "to": "2.3 Multi-class classification"}, {"arrows": "to", "from": "VarianceMaximization", "title": "related_to", "to": "LagrangeMultipliers"}, {"arrows": "to", "from": "Expected_Total_Payoff_Optimization", "title": "depends_on", "to": "Finite_Infinite_Horizon_Difference"}, {"arrows": "to", "from": "FiniteHorizonMDPs", "title": "contains", "to": "OptimalPolicyRecovery"}, {"arrows": "to", "from": "Machine Learning Algorithms", "title": "contains", "to": "Naive Bayes Algorithm"}, {"arrows": "to", "from": "Predict Step", "title": "depends_on", "to": "Gaussian Distribution"}, {"arrows": "to", "from": "Generalization and regularization", "title": "subtopic", "to": "Generalization"}, {"arrows": "to", "from": "PolynomialFitting", "title": "related_to", "to": "Variance"}, {"arrows": "to", "from": "Supervised Learning Algorithm", "title": "depends_on", "to": "Linear Regression"}, {"arrows": "to", "from": "DoubleDescentPhenomenon", "title": "depends_on", "to": "DeepNeuralNetworks"}, {"arrows": "to", "from": "VC Dimension", "title": "defines", "to": "Shattering"}, {"arrows": "to", "from": "Neural Networks", "title": "has_component", "to": "Bias Term"}, {"arrows": "to", "from": "GaussianDistributionQ_i", "title": "defines", "to": "MeanAndVarianceFunctions"}, {"arrows": "to", "from": "kFoldCrossValidation", "title": "subtopic_of", "to": "LeaveOneOutCV"}, {"arrows": "to", "from": "StochasticGradientDescent", "title": "subtopic", "to": "GradientDescentAlgorithm"}, {"arrows": "to", "from": "BackpropagationAlgorithm", "title": "depends_on", "to": "GradientComputation"}, {"arrows": "to", "from": "Self-Supervised Learning", "title": "depends_on", "to": "Data Augmentation"}, {"arrows": "to", "from": "Value Function Approximation", "title": "depends_on", "to": "Feature Mapping"}, {"arrows": "to", "from": "LinearRegressionOptimization", "title": "has_subtopic", "to": "GradientDescentConvergence"}, {"arrows": "to", "from": "FullyConnectedNN", "title": "depends_on", "to": "Parameterization"}, {"arrows": "to", "from": "Discretization in MDPs", "title": "subtopic", "to": "Policy Iteration"}, {"arrows": "to", "from": "MachineLearningOverview", "title": "related_to", "to": "PoissonDistribution"}, {"arrows": "to", "from": "MachineLearningOverview", "title": "has_subtopic", "to": "ConvolutionalLayers"}, {"arrows": "to", "from": "Model-wise Double Descent", "title": "subtopic", "to": "Implicit Regularization"}, {"arrows": "to", "from": "Decision Boundary", "title": "related_to", "to": "Functional Margins"}, {"arrows": "to", "from": "FeatureSelection", "title": "depends_on", "to": "LinearRegression"}, {"arrows": "to", "from": "MachineLearningOverview", "title": "contains", "to": "PolicyGradientTheorem"}, {"arrows": "to", "from": "Backpropagation", "title": "depends_on", "to": "Chain Rule"}, {"arrows": "to", "from": "Independent components analysis", "title": "subtopic", "to": "Densities and linear transformations"}, {"arrows": "to", "from": "NeuralNetworkParameters", "title": "depends_on", "to": "FeatureMaps"}, {"arrows": "to", "from": "Machine_Learning", "title": "subtopic", "to": "Policy_Gradient_Methods"}, {"arrows": "to", "from": "Machine Learning Algorithms", "title": "has_subtopic", "to": "Stochastic Gradient Descent (SGD)"}, {"arrows": "to", "from": "Support_Vector_Machines_SVMs", "title": "subtopic", "to": "SMO_Algorithm"}, {"arrows": "to", "from": "DifferentialDynamicProgramming", "title": "subtopic", "to": "RewardLinearization"}, {"arrows": "to", "from": "Inverted_Pendulum_Problem", "title": "example", "to": "Model_Learning_for_MDPs"}, {"arrows": "to", "from": "ExpectedTestError", "title": "subtopic", "to": "MSEDecomposition"}, {"arrows": "to", "from": "Self-Supervised Learning", "title": "depends_on", "to": "Representation Function"}, {"arrows": "to", "from": "Value_Iteration", "title": "subtopic", "to": "Machine_Learning_Algorithms"}, {"arrows": "to", "from": "Lagrangian_Formulation", "title": "results_in", "to": "Equation_6.11"}, {"arrows": "to", "from": "Cross Validation Techniques", "title": "has_subtopic", "to": "Leave-One-Out Cross Validation"}, {"arrows": "to", "from": "Markov Decision Processes (MDP)", "title": "has_subtopic", "to": "Value Function in MDPs"}, {"arrows": "to", "from": "Update Step", "title": "subtopic", "to": "Kalman Gain"}, {"arrows": "to", "from": "NonLinearModels", "title": "depends_on", "to": "TrainingExamples"}, {"arrows": "to", "from": "MachineLearningOverview", "title": "contains", "to": "GeneralizedLinearModelsGLMs"}, {"arrows": "to", "from": "Variational_Autoencoder", "title": "uses", "to": "Variational_Inference"}, {"arrows": "to", "from": "MajorAxisOfVariation", "title": "subtopic", "to": "ProjectionObjective"}, {"arrows": "to", "from": "Self-supervised Learning", "title": "has_subtopic", "to": "Foundation Models"}, {"arrows": "to", "from": "Spam_Filtering", "title": "depends_on", "to": "Training_Set"}, {"arrows": "to", "from": "PolicyGradientTheorem", "title": "subtopic", "to": "LogProbabilityComputation"}, {"arrows": "to", "from": "Optimal Value Function", "title": "related_to", "to": "Bellman\u0027s Equation"}, {"arrows": "to", "from": "Transformer_Model", "title": "subtopic", "to": "Autoregressive_Transformer"}, {"arrows": "to", "from": "Multinomial Distribution", "title": "subtopic", "to": "2.3 Multi-class classification"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "depends_on", "to": "Loss Function J(theta)"}, {"arrows": "to", "from": "Implicit_Bias_Noise_Covariance", "title": "subtopic", "to": "Machine_Learning_Papers"}, {"arrows": "to", "from": "initialization_step", "title": "subtopic", "to": "k-means_algorithm"}, {"arrows": "to", "from": "Optimal Policy", "title": "depends_on", "to": "Machine Learning Concepts"}, {"arrows": "to", "from": "PolynomialFeatures", "title": "related_to", "to": "Overfitting"}, {"arrows": "to", "from": "LogLikelihoodBound", "title": "uses", "to": "JensensInequality"}, {"arrows": "to", "from": "Bias-Variance Tradeoff", "title": "related_to", "to": "Training Dataset Example"}, {"arrows": "to", "from": "Policy_Iteration", "title": "has_subtopic", "to": "Greedy_Policy_With_Respect_To_V"}, {"arrows": "to", "from": "Union_Bound_Application", "title": "depends_on", "to": "Uniform_Convergence"}, {"arrows": "to", "from": "Machine_Learning", "title": "contains", "to": "Text_Classification"}, {"arrows": "to", "from": "GaussianDiscriminantAnalysis", "title": "subtopic", "to": "DecisionBoundary"}, {"arrows": "to", "from": "EmpiricalRiskMinimization", "title": "subtopic", "to": "MachineLearningConcepts"}, {"arrows": "to", "from": "TrainingExample", "title": "related_to", "to": "DecisionBoundary"}, {"arrows": "to", "from": "Kernel Functions", "title": "depends_on", "to": "Necessary Conditions for Valid Kernels"}, {"arrows": "to", "from": "Value Function", "title": "depends_on", "to": "Policy Execution"}, {"arrows": "to", "from": "MachineLearningBasics", "title": "subtopic", "to": "MatrixNotation"}, {"arrows": "to", "from": "MachineLearningBasics", "title": "subtopic", "to": "GeneralizationToLayers"}, {"arrows": "to", "from": "ICA Ambiguities", "title": "depends_on", "to": "Scaling Ambiguity"}, {"arrows": "to", "from": "Independent Component Analysis (ICA)", "title": "related_to", "to": "Unmixing Matrix (W)"}, {"arrows": "to", "from": "StateTransitionModel", "title": "has_subtopic", "to": "DeterministicModel"}, {"arrows": "to", "from": "MachineLearningConcepts", "title": "depends_on", "to": "MaximumLikelihoodEstimation"}, {"arrows": "to", "from": "TrainingSetSampling", "title": "subtopic", "to": "GeneralizationErrorGuarantees"}, {"arrows": "to", "from": "BernoulliDistribution", "title": "subtopic", "to": "NaturalParameterOfBernoulli"}, {"arrows": "to", "from": "Bellman Equations", "title": "subtopic", "to": "Value Function"}, {"arrows": "to", "from": "Log_Probability_Gradients", "title": "has_subtopic", "to": "Trajectory_Probability_Change"}, {"arrows": "to", "from": "PrincipalComponentAnalysis", "title": "has_subtopic", "to": "EigenvectorsEigenvalues"}, {"arrows": "to", "from": "Feature_Vectors", "title": "has_subtopic", "to": "String_Features"}, {"arrows": "to", "from": "Machine_Learning", "title": "contains", "to": "Markov_Decision_Processes_MDPs"}, {"arrows": "to", "from": "Feature_Maps", "title": "example_of", "to": "Cubic_Features"}, {"arrows": "to", "from": "Self-supervised learning and foundation models", "title": "subtopic", "to": "Pretraining and adaptation"}, {"arrows": "to", "from": "ProbabilisticInterpretation", "title": "related_to", "to": "LinearRegression"}, {"arrows": "to", "from": "Baseline_Estimator", "title": "related_to", "to": "Algorithm_8_Vanilla_Policy_Gradient_Baseline"}, {"arrows": "to", "from": "MarkovDecisionProcesses", "title": "component_of", "to": "Actions"}, {"arrows": "to", "from": "ReinforcementLearning", "title": "defines_formalism", "to": "MarkovDecisionProcesses"}, {"arrows": "to", "from": "MachineLearningConcepts", "title": "has_subtopic", "to": "MSEDecomposition"}, {"arrows": "to", "from": "EM algorithms", "title": "subtopic", "to": "General EM algorithms"}, {"arrows": "to", "from": "Overfitting_Underfitting", "title": "defines", "to": "Generalization_Error"}, {"arrows": "to", "from": "Scaling_Signal", "title": "depends_on", "to": "ICA_Ambiguities"}, {"arrows": "to", "from": "Dual_Form_Optimization", "title": "subtopic", "to": "Support_Vector_Machines"}, {"arrows": "to", "from": "MachineLearningConcepts", "title": "has_subtopic", "to": "KernelFunctions"}, {"arrows": "to", "from": "LinearTransformations", "title": "subtopic", "to": "DensityCalculation"}, {"arrows": "to", "from": "Logistic_Function", "title": "same_as", "to": "Sigmoid_Function"}, {"arrows": "to", "from": "NeuralNetworks", "title": "subtopic", "to": "FullyConnectedNN"}, {"arrows": "to", "from": "Characterization_of_Valid_Kernels", "title": "subtopic", "to": "Concrete_Examples_of_Kernels"}, {"arrows": "to", "from": "MachineLearningOverview", "title": "related_to", "to": "LinearQuadraticRegulatorLQR"}, {"arrows": "to", "from": "5 Kernel methods", "title": "has_subtopic", "to": "5.3 LMS with the kernel trick"}, {"arrows": "to", "from": "MachineLearningOverview", "title": "related_to", "to": "Parameterization"}, {"arrows": "to", "from": "Kalman Filter", "title": "related_to", "to": "Forward Pass"}, {"arrows": "to", "from": "Independent Component Analysis (ICA)", "title": "depends_on", "to": "Mixing Matrix (A)"}, {"arrows": "to", "from": "Kalman Filter", "title": "depends_on", "to": "Belief States Update"}, {"arrows": "to", "from": "Loss Function", "title": "related_to", "to": "Optimizers"}, {"arrows": "to", "from": "StochasticGradientAscent", "title": "leads_to", "to": "ConvergenceAndRecovery"}, {"arrows": "to", "from": "Optimization Problem", "title": "results_in", "to": "Optimal Margin Classifier"}, {"arrows": "to", "from": "Machine Learning Techniques", "title": "has_subtopic", "to": "Independent Components Analysis (ICA)"}, {"arrows": "to", "from": "MixingMatrix", "title": "related_to", "to": "RotationalSymmetry"}, {"arrows": "to", "from": "Markov_Decision_Processes_MDPs", "title": "depends_on", "to": "Reward_Functions"}, {"arrows": "to", "from": "Generalization", "title": "subtopic", "to": "Bias-variance tradeoff"}, {"arrows": "to", "from": "EM_Algorithm", "title": "susceptible_to", "to": "Local_Optima"}, {"arrows": "to", "from": "Pretrained large language models", "title": "subtopic", "to": "Zero-shot learning and in-context learning"}, {"arrows": "to", "from": "Exponential Family Distributions", "title": "has_subnode", "to": "Canonical Link Function"}, {"arrows": "to", "from": "Mini-batch Stochastic Gradient Descent", "title": "depends_on", "to": "Batch Size (B)"}, {"arrows": "to", "from": "M-step", "title": "related_to", "to": "Update Rule for \\(\\phi_j\\)"}, {"arrows": "to", "from": "MSEDecomposition", "title": "uses", "to": "Claim811"}, {"arrows": "to", "from": "NeuralNetworks", "title": "subtopic", "to": "SingleNeuronNN"}, {"arrows": "to", "from": "MDP (Markov Decision Process)", "title": "related_to", "to": "Machine Learning Concepts"}, {"arrows": "to", "from": "Loss_Functions", "title": "subtopic", "to": "Training_Loss"}, {"arrows": "to", "from": "Linear Regression", "title": "has_example", "to": "Housing Example"}, {"arrows": "to", "from": "Value_Iteration", "title": "subtopic", "to": "Convergence_to_Optimal_Value"}, {"arrows": "to", "from": "GeneralizedLinearModel", "title": "subtopic", "to": "GLMAssumptions"}, {"arrows": "to", "from": "VE Procedure", "title": "subtopic", "to": "Option 2 Initialization"}, {"arrows": "to", "from": "Backpropagation", "title": "defines", "to": "Differentiable_Circuit"}, {"arrows": "to", "from": "Binary Classification", "title": "related_to", "to": "2.3 Multi-class classification"}, {"arrows": "to", "from": "Neural Networks", "title": "has_subtopic", "to": "Parametrization (h_\u03b8(x))"}, {"arrows": "to", "from": "MachineLearningOverview", "title": "subtopic", "to": "ResidualConnections"}, {"arrows": "to", "from": "Eigenvectors", "title": "subtopic", "to": "Principal Components"}, {"arrows": "to", "from": "MachineLearningConcepts", "title": "contains", "to": "KMeansAlgorithm"}, {"arrows": "to", "from": "Gradient_Descent", "title": "has_subtopic", "to": "LMS_Update_Rule"}, {"arrows": "to", "from": "Supervised Learning Problem", "title": "has_subtopic", "to": "Hypothesis"}, {"arrows": "to", "from": "EMAlgorithm", "title": "subtopic", "to": "ConvergenceGuarantees"}, {"arrows": "to", "from": "Regularization and model selection", "title": "subtopic", "to": "Bayesian statistics and regularization"}, {"arrows": "to", "from": "Support Vector Machines (SVMs)", "title": "subtopic", "to": "Notation for SVMs"}, {"arrows": "to", "from": "Deep Learning Introduction", "title": "related_to", "to": "Machine Learning Overview"}, {"arrows": "to", "from": "Finding Roots", "title": "depends_on", "to": "Newton\u0027s Method"}, {"arrows": "to", "from": "Machine_Learning_Topics", "title": "contains", "to": "Differential_Dynamic_Programming"}, {"arrows": "to", "from": "Kernel_Methods", "title": "subtopic", "to": "Feature_Maps"}, {"arrows": "to", "from": "MachineLearningOverview", "title": "subtopic", "to": "RegressionProblem"}, {"arrows": "to", "from": "dStarAndPStarEquality", "title": "depends_on", "to": "FeasibilityConstraints"}, {"arrows": "to", "from": "I Supervised learning", "title": "has_subtopic", "to": "5 Kernel methods"}, {"arrows": "to", "from": "PCA", "title": "depends_on", "to": "Eigenvectors"}, {"arrows": "to", "from": "Variational_Autoencoder", "title": "extends", "to": "EM_Algorithms"}, {"arrows": "to", "from": "KernelTrick", "title": "related_to", "to": "MachineLearningOverview"}, {"arrows": "to", "from": "State_Transition_Probabilities", "title": "subtopic", "to": "Model_Learning_for_MDPs"}, {"arrows": "to", "from": "HypothesisSpace", "title": "related_to", "to": "GeneralizationError"}, {"arrows": "to", "from": "2 Classification and logistic regression", "title": "has_subtopic", "to": "2.2 Digression: the perceptron learning algorithm"}, {"arrows": "to", "from": "Self-supervised learning and foundation models", "title": "subtopic", "to": "Pretraining methods in computer vision"}, {"arrows": "to", "from": "Value_Function_Approximation", "title": "related_to", "to": "Deterministic_Simulator"}, {"arrows": "to", "from": "WeightsCalculation", "title": "depends_on", "to": "BandwidthParameter"}, {"arrows": "to", "from": "5 Kernel methods", "title": "has_subtopic", "to": "5.1 Feature maps"}, {"arrows": "to", "from": "MachineLearningConcepts", "title": "depends_on", "to": "PartialDerivatives"}, {"arrows": "to", "from": "Infinite_Hypothesis_Classes", "title": "subtopic", "to": "Machine_Learning_Bias_Variance_Tradeoff"}, {"arrows": "to", "from": "PolynomialKernels", "title": "depends_on", "to": "ComputationalEfficiency"}, {"arrows": "to", "from": "LMS_Rule", "title": "related_to", "to": "Widrow_Hoff_Learning_Rule"}, {"arrows": "to", "from": "Value_Functions", "title": "subtopic", "to": "Optimal_Value_Function"}, {"arrows": "to", "from": "Regularization and model selection", "title": "subtopic", "to": "Model selection via cross validation"}, {"arrows": "to", "from": "Feature Discovery", "title": "related_to", "to": "Black Box Nature"}, {"arrows": "to", "from": "Deep Learning Regularizers", "title": "contains", "to": "Weight Decay"}, {"arrows": "to", "from": "MachineLearningModels", "title": "contains", "to": "GaussianDiscriminantAnalysis"}, {"arrows": "to", "from": "Gradient_Descent", "title": "has_subtopic", "to": "Partial_Derivative_Calculation"}, {"arrows": "to", "from": "MulticlassClassification", "title": "has_subtopic", "to": "LogitsInMultiClass"}, {"arrows": "to", "from": "Optimal_Control", "title": "subtopic", "to": "Value_Functions"}, {"arrows": "to", "from": "UnderfittingLinearModel", "title": "related_to", "to": "BiasInModels"}, {"arrows": "to", "from": "Vapnik\u0027s Theorem", "title": "depends_on", "to": "Hypothesis Class"}, {"arrows": "to", "from": "DataNormalization", "title": "depends_on", "to": "MeanRemoval"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "related_to", "to": "Backpropagation Algorithm"}, {"arrows": "to", "from": "LinearQuadraticRegulatorLQR", "title": "subtopic", "to": "LQG"}, {"arrows": "to", "from": "Machine_Learning_Concepts", "title": "contains", "to": "Policy_Gradient_Theorem"}, {"arrows": "to", "from": "PCA", "title": "related_to", "to": "Approximation Error Minimization"}, {"arrows": "to", "from": "BiologicalInspiration", "title": "depends_on", "to": "TwoLayerNetworks"}, {"arrows": "to", "from": "Model Specification", "title": "subtopic", "to": "Logistic Regression Example"}, {"arrows": "to", "from": "Jensen\u0027s_Inequality", "title": "subtopic", "to": "Convergence"}, {"arrows": "to", "from": "CostFunction", "title": "related_to", "to": "OrdinaryLeastSquares"}, {"arrows": "to", "from": "Machine_Learning_Theory", "title": "subtopic", "to": "Sample_Complexity"}, {"arrows": "to", "from": "Sample-wise Double Descent", "title": "depends_on", "to": "Optimization Algorithms"}, {"arrows": "to", "from": "MachineLearningAlgorithms", "title": "has_subtopic", "to": "DiscriminativeAlgorithms"}, {"arrows": "to", "from": "MachineLearningConcepts", "title": "related_to", "to": "ShatteringConcept"}, {"arrows": "to", "from": "Foundation_Models", "title": "subtopic", "to": "Machine_Learning_Papers"}, {"arrows": "to", "from": "OverfittingFifthDegree", "title": "depends_on", "to": "GeneralizationFailure"}, {"arrows": "to", "from": "E-step", "title": "depends_on", "to": "Q-function"}, {"arrows": "to", "from": "Frequentist View", "title": "includes_method", "to": "Maximum Likelihood Estimation (MLE)"}, {"arrows": "to", "from": "Support_Vector_Machines", "title": "subtopic", "to": "Lagrange_Duality"}, {"arrows": "to", "from": "Deep Learning Regularizers", "title": "contains", "to": "Dropout"}, {"arrows": "to", "from": "MachineLearningValidationTechniques", "title": "has_subtopic", "to": "kFoldCrossValidation"}, {"arrows": "to", "from": "Value Iteration", "title": "depends_on", "to": "Bellman Update"}, {"arrows": "to", "from": "IntermediateVariables", "title": "uses", "to": "ReLUActivationFunction"}, {"arrows": "to", "from": "Activation Function Backward Function", "title": "has_subtopic", "to": "Activation Derivative Matrix"}, {"arrows": "to", "from": "ConstrainedOptimization", "title": "depends_on", "to": "LagrangeMultipliers"}, {"arrows": "to", "from": "Necessary Conditions for Valid Kernels", "title": "subtopic", "to": "Kernel Matrix"}, {"arrows": "to", "from": "Model Selection", "title": "related_to", "to": "SVM and Regularization"}, {"arrows": "to", "from": "Unsupervised learning", "title": "subtopic", "to": "Principal components analysis"}, {"arrows": "to", "from": "ELBO_Optimization", "title": "subtopic", "to": "Efficient_Evaluation_ELBO"}, {"arrows": "to", "from": "MachineLearningOverview", "title": "contains", "to": "NeuralNetworks"}, {"arrows": "to", "from": "GradientComputation", "title": "related_to", "to": "ReparameterizationTrick"}, {"arrows": "to", "from": "ProjectionDirection", "title": "has_subtopic", "to": "VarianceMaximization"}, {"arrows": "to", "from": "TrainingSetExamples", "title": "depends_on", "to": "LayerActivations"}, {"arrows": "to", "from": "Non_Gaussian_Sources", "title": "related_to", "to": "ICA_Ambiguities"}, {"arrows": "to", "from": "RegressionProblems", "title": "subtopic", "to": "MeanSquareCostFunction"}, {"arrows": "to", "from": "MultipleExamplesExtension", "title": "subtopic", "to": "ELBOFormula"}, {"arrows": "to", "from": "MachineLearningOverview", "title": "has_subtopic", "to": "NeuralNetworksComposition"}, {"arrows": "to", "from": "Multivariate Normal Distribution", "title": "related_to", "to": "Covariance Matrix"}, {"arrows": "to", "from": "Implementation Details", "title": "subtopic", "to": "Conversion Rules"}, {"arrows": "to", "from": "Test Error Decomposition", "title": "example_of", "to": "Quadratic Model Example"}, {"arrows": "to", "from": "Partial Derivatives", "title": "depends_on", "to": "Scalar Variable J"}, {"arrows": "to", "from": "GLMFormulation", "title": "subtopic", "to": "BernoulliDistribution"}, {"arrows": "to", "from": "Pretraining_Phase", "title": "subtopic", "to": "Pretraining_Loss_Function"}, {"arrows": "to", "from": "GeometricMargin", "title": "subtopic", "to": "DecisionBoundary"}, {"arrows": "to", "from": "Loss_Functions", "title": "subtopic", "to": "Test_Error"}, {"arrows": "to", "from": "Regularization in Machine Learning", "title": "contains", "to": "Sparsity Regularization"}, {"arrows": "to", "from": "Sample complexity bounds (optional readings)", "title": "subtopic", "to": "The case of finite H"}, {"arrows": "to", "from": "Bias-Variance Tradeoff", "title": "related_to", "to": "Overfitting"}, {"arrows": "to", "from": "Machine_Learning_Theory", "title": "subtopic", "to": "Hypothesis_Class_Size"}, {"arrows": "to", "from": "MachineLearningConcepts", "title": "related_to", "to": "CumulativeDistributionFunction"}, {"arrows": "to", "from": "Machine_Learning_Topic", "title": "has_subtopic", "to": "Text_Classification"}, {"arrows": "to", "from": "EM_Algorithm", "title": "depends_on", "to": "E_Step"}, {"arrows": "to", "from": "VariationalInference", "title": "depends_on", "to": "ELBO"}, {"arrows": "to", "from": "LagrangianFormulation", "title": "subtopic", "to": "DualProblem"}, {"arrows": "to", "from": "1D_Convolution", "title": "has_component", "to": "Bias_Scalar"}, {"arrows": "to", "from": "5 Kernel methods", "title": "has_subtopic", "to": "5.4 Properties of kernels"}, {"arrows": "to", "from": "MachineLearningModels", "title": "contains", "to": "DecisionBoundaries"}, {"arrows": "to", "from": "Finetuning_Pretrained_Models", "title": "has_subtopic", "to": "Optimization_Objective"}, {"arrows": "to", "from": "Law_of_Total_Expectation", "title": "related_to", "to": "Estimator_Simplification"}, {"arrows": "to", "from": "Adaptation Algorithms", "title": "subtopic", "to": "Finetuning"}, {"arrows": "to", "from": "FunctionRepresentation", "title": "subtopic", "to": "LinearHypothesis"}, {"arrows": "to", "from": "Self-Supervised Learning", "title": "subtopic", "to": "Negative Pair"}, {"arrows": "to", "from": "Matrix Multiplication Module", "title": "subtopic", "to": "MLP Composition"}, {"arrows": "to", "from": "Value Function", "title": "has_subtopic", "to": "Optimal Value Function"}, {"arrows": "to", "from": "Machine_Learning_Concepts", "title": "contains", "to": "ELBO_Optimization"}, {"arrows": "to", "from": "Classification Confidence", "title": "subtopic", "to": "Functional Margins"}, {"arrows": "to", "from": "MachineLearningModels", "title": "contains", "to": "GDA"}, {"arrows": "to", "from": "EMAlgorithm", "title": "subtopic", "to": "EStep"}, {"arrows": "to", "from": "MachineLearningOverview", "title": "contains", "to": "VariationalInference"}, {"arrows": "to", "from": "VariationalAutoEncoder", "title": "contains", "to": "LatentVariableModel"}, {"arrows": "to", "from": "Sample Complexity Bounds", "title": "subtopic", "to": "Model Selection Methods"}, {"arrows": "to", "from": "Model Parameters", "title": "subtopic", "to": "Likelihood Function"}, {"arrows": "to", "from": "Batch_Gradient_Descent", "title": "related_to", "to": "Gradient_Descent"}, {"arrows": "to", "from": "Machine_Learning", "title": "depends_on", "to": "Training_Dataset"}, {"arrows": "to", "from": "FeatureMapping", "title": "depends_on", "to": "GradientDescentAlgorithm"}, {"arrows": "to", "from": "BinaryClassification", "title": "defines", "to": "TrainingError"}, {"arrows": "to", "from": "Support_Vector_Machines", "title": "subtopic", "to": "SMO_Algorithm"}, {"arrows": "to", "from": "1 Linear regression", "title": "has_subtopic", "to": "1.4 Locally weighted linear regression (optional reading)"}, {"arrows": "to", "from": "MachineLearningOverview", "title": "related_to", "to": "LinearRegression"}, {"arrows": "to", "from": "LayerNormalization", "title": "subtopic", "to": "MachineLearningConcepts"}, {"arrows": "to", "from": "Training_Set_Issues", "title": "related_to", "to": "Spam_Filtering"}, {"arrows": "to", "from": "PCA_Methods", "title": "subtopic", "to": "Computational_Efficiency"}, {"arrows": "to", "from": "Immediate Reward", "title": "related_to", "to": "Bellman Equations"}, {"arrows": "to", "from": "Parameter_Scaling_Invariance", "title": "related_to", "to": "Geometric_Margin"}, {"arrows": "to", "from": "Optimizers and Generalization", "title": "related_to", "to": "Global Minima and Generalization"}, {"arrows": "to", "from": "I Supervised learning", "title": "has_subtopic", "to": "1 Linear regression"}, {"arrows": "to", "from": "Sample complexity bounds (optional readings)", "title": "subtopic", "to": "The case of infinite H"}, {"arrows": "to", "from": "Partially Observable MDPs (POMDP)", "title": "has_subtopic", "to": "Belief State"}, {"arrows": "to", "from": "Expectation_Calculation", "title": "subtopic", "to": "Gaussian_Noise_Model"}, {"arrows": "to", "from": "Machine_Learning", "title": "has_subtopic", "to": "Support_Vector_Machines_SVM"}, {"arrows": "to", "from": "SupportVectors", "title": "related_to", "to": "AlphaCoefficients"}, {"arrows": "to", "from": "Mean Field Assumption", "title": "subtopic", "to": "Variational Inference"}, {"arrows": "to", "from": "FunctionalMargin", "title": "subtopic", "to": "ScalingImpact"}, {"arrows": "to", "from": "Markov_Decision_Processes_(MDP)", "title": "depends_on", "to": "State_Transitions"}, {"arrows": "to", "from": "BatchNormalization", "title": "subtopic", "to": "OtherNormalizationLayers"}, {"arrows": "to", "from": "MachineLearningOverview", "title": "contains", "to": "LossFunctionOptimization"}, {"arrows": "to", "from": "MachineLearningConcepts", "title": "subtopic", "to": "BiasVsVarianceTradeoff"}, {"arrows": "to", "from": "Reinforcement_Learning", "title": "depends_on", "to": "Policy_Iteration"}, {"arrows": "to", "from": "Geometric_Margin", "title": "depends_on", "to": "Optimal_Margin_Classifier"}, {"arrows": "to", "from": "Sample Complexity Bounds", "title": "subtopic", "to": "Bias-Variance Tradeoff"}, {"arrows": "to", "from": "3 Generalized linear models", "title": "has_subtopic", "to": "3.1 The exponential family"}, {"arrows": "to", "from": "LocallyWeightedLinearRegression", "title": "subtopic", "to": "MachineLearningConcepts"}, {"arrows": "to", "from": "Neural_Network_Parameterization", "title": "parameterizes", "to": "Variational_Autoencoder"}, {"arrows": "to", "from": "ELBO_Optimization", "title": "subtopic", "to": "Form_of_Q_i"}, {"arrows": "to", "from": "Jensen\u0027s Inequality", "title": "depends_on", "to": "Convex Functions"}, {"arrows": "to", "from": "Implicit_Bias", "title": "depends_on", "to": "Machine_Learning_Papers"}, {"arrows": "to", "from": "MachineLearningConcepts", "title": "has_subtopic", "to": "ZeroShotLearning"}, {"arrows": "to", "from": "Decision Boundary", "title": "related_to", "to": "Geometric Margins"}, {"arrows": "to", "from": "Machine_Learning_Concepts", "title": "related_to", "to": "Objective_Function"}, {"arrows": "to", "from": "Value Iteration", "title": "proves", "to": "Theorem"}, {"arrows": "to", "from": "BernoulliDistribution", "title": "subtopic", "to": "SufficientStatisticForBernoulli"}, {"arrows": "to", "from": "Chapter_17_Policy_Gradient_REINFORCE", "title": "subtopic", "to": "Randomized_Policy"}, {"arrows": "to", "from": "EM algorithms", "title": "subtopic", "to": "Jensen\u0027s inequality"}, {"arrows": "to", "from": "Policy_Gradient_Methods", "title": "has_subtopic", "to": "Baseline_Estimator"}, {"arrows": "to", "from": "Machine_Learning", "title": "contains", "to": "Logistic_Regression"}, {"arrows": "to", "from": "System_Dynamics", "title": "subtopic", "to": "Linearization_of_Dynamics"}, {"arrows": "to", "from": "Alpha_Parameters", "title": "depends_on", "to": "Constraint_Equation"}, {"arrows": "to", "from": "FeatureEngineering", "title": "subtopic", "to": "MachineLearningOverview"}, {"arrows": "to", "from": "DifferentialDynamicProgramming", "title": "subtopic", "to": "NominalTrajectoryGeneration"}, {"arrows": "to", "from": "Supervised Learning Problem", "title": "has_subtopic", "to": "Regression"}, {"arrows": "to", "from": "Linear_Separability", "title": "subtopic", "to": "Maximize_Geometric_Margin_Optimization"}, {"arrows": "to", "from": "MachineLearningConcepts", "title": "depends_on", "to": "JensensInequality"}, {"arrows": "to", "from": "Lagrange Duality", "title": "uses", "to": "Lagrange Multipliers"}, {"arrows": "to", "from": "Kernel Functions", "title": "related_to", "to": "Feature Mapping"}, {"arrows": "to", "from": "Parameter_Estimation", "title": "related_to", "to": "Laplace_Smoothing"}, {"arrows": "to", "from": "BackwardFunctionsBasics", "title": "related_to", "to": "ActivationFunctions"}, {"arrows": "to", "from": "ExpectationRewriting", "title": "has_subtopic", "to": "StateActionDependentRewards"}, {"arrows": "to", "from": "Cross_Validation", "title": "subtopic", "to": "Hold_Out_Cross_Validation"}, {"arrows": "to", "from": "UniformConvergence", "title": "depends_on", "to": "GeneralizationError"}, {"arrows": "to", "from": "StateTransitionModel", "title": "has_subtopic", "to": "NonLinearModels"}, {"arrows": "to", "from": "NeuralNetworks", "title": "contains", "to": "BiasVectors"}, {"arrows": "to", "from": "Model Selection", "title": "depends_on", "to": "Cross Validation"}, {"arrows": "to", "from": "Markov_Decision_Processes_MDPs", "title": "depends_on", "to": "State_Transition_Probabilities"}, {"arrows": "to", "from": "MulticlassClassification", "title": "has_subtopic", "to": "SoftmaxFunction"}, {"arrows": "to", "from": "OptimizationInML", "title": "depends_on", "to": "HessianMatrix"}, {"arrows": "to", "from": "Machine_Learning_Concepts", "title": "contains", "to": "Training_Test_Distributions"}, {"arrows": "to", "from": "NeuralNetworks", "title": "subtopic", "to": "ActivationFunctions"}, {"arrows": "to", "from": "Support Vector Machines (SVMs)", "title": "subtopic", "to": "Functional Margin"}, {"arrows": "to", "from": "MachineLearningConcepts", "title": "subtopic", "to": "GeometricMargins"}, {"arrows": "to", "from": "kFoldCrossValidation", "title": "depends_on", "to": "DataSplitting"}, {"arrows": "to", "from": "Backpropagation Strategy", "title": "subtopic", "to": "Concrete Backprop Algorithm"}, {"arrows": "to", "from": "TrainingLoss", "title": "contains", "to": "Regularizer"}, {"arrows": "to", "from": "ProbabilisticModeling", "title": "depends_on", "to": "ConditionalProbabilityDistribution"}, {"arrows": "to", "from": "MSEDecomposition", "title": "subtopic_of", "to": "AverageModel"}, {"arrows": "to", "from": "MachineLearningBasics", "title": "depends_on", "to": "TrainingSetExamples"}, {"arrows": "to", "from": "Regularization in Machine Learning", "title": "contains", "to": "Deep Learning Regularizers"}, {"arrows": "to", "from": "Parameter_Scaling_Invariance", "title": "subtopic", "to": "Machine_Learning_Concepts"}, {"arrows": "to", "from": "Value_Functions", "title": "contains", "to": "Bellman_Equation"}, {"arrows": "to", "from": "Machine Learning Algorithms", "title": "has_subtopic", "to": "Algorithm 6"}, {"arrows": "to", "from": "Machine_Learning_Concepts", "title": "subtopic", "to": "Nonlinear_Dynamics_to_LQR"}, {"arrows": "to", "from": "MaximumLikelihoodEstimation", "title": "subtopic", "to": "GradientAscentUpdateRule"}, {"arrows": "to", "from": "LogisticRegression", "title": "subtopic", "to": "GradientAscentRule"}, {"arrows": "to", "from": "KMeansAlgorithm", "title": "related_to", "to": "ConvergenceProperties"}, {"arrows": "to", "from": "MachineLearningOverview", "title": "depends_on", "to": "GradientDescentOptimizer"}, {"arrows": "to", "from": "GeneralizedLinearModelsGLMs", "title": "depends_on", "to": "AssumptionsOfGLMs"}, {"arrows": "to", "from": "BiasVarianceTradeoff", "title": "subtopic", "to": "HypothesisSpace"}, {"arrows": "to", "from": "6 Support vector machines", "title": "has_subtopic", "to": "6.1 Margins: intuition"}, {"arrows": "to", "from": "MachineLearningConcepts", "title": "related_to", "to": "EvidenceLowerBoundELBO"}, {"arrows": "to", "from": "Backward Function Overview", "title": "has_subtopic", "to": "Matrix Multiplication Backward Function"}, {"arrows": "to", "from": "SigmoidFunction", "title": "subtopic", "to": "ActivationFunctions"}, {"arrows": "to", "from": "GLMFormulation", "title": "subtopic", "to": "GaussianDistribution"}, {"arrows": "to", "from": "Probability_Error", "title": "subtopic", "to": "Training_Error_Generalization_Error"}, {"arrows": "to", "from": "Regularization", "title": "subtopic_of", "to": "L2_Regularization"}, {"arrows": "to", "from": "Gradient_Estimation", "title": "subtopic", "to": "Reparameterization_Trick"}, {"arrows": "to", "from": "Uniform_Convergence_Bound", "title": "related_to", "to": "Hypothesis_Space_Size"}, {"arrows": "to", "from": "Bayesian Machine Learning", "title": "related_to", "to": "Prediction on New Data"}, {"arrows": "to", "from": "Gradient Descent (GD)", "title": "depends_on", "to": "Learning Rate"}, {"arrows": "to", "from": "NaiveBayes", "title": "contains", "to": "EmailSpamFiltering"}, {"arrows": "to", "from": "Jensen\u0027s Inequality", "title": "related_to", "to": "Concave Functions"}, {"arrows": "to", "from": "BackpropagationDiscussion", "title": "has_subtopic", "to": "BackwardFunctionsBasics"}, {"arrows": "to", "from": "Contrastive_Learning", "title": "example_of", "to": "SIMCLR"}, {"arrows": "to", "from": "Naive_Bayes_Classifier", "title": "depends_on", "to": "Parameter_Estimation"}, {"arrows": "to", "from": "Model_Learning_for_MDPs", "title": "related_to", "to": "Machine_Learning_Algorithms"}, {"arrows": "to", "from": "Feature_Vector", "title": "based_on", "to": "Vocabulary"}, {"arrows": "to", "from": "Chain_Rule_Application", "title": "depends_on", "to": "Gradient_Computation"}, {"arrows": "to", "from": "KMeansAlgorithm", "title": "depends_on", "to": "DistortionFunctionJ"}, {"arrows": "to", "from": "1 Linear regression", "title": "has_subtopic", "to": "1.1 LMS algorithm"}, {"arrows": "to", "from": "I Supervised learning", "title": "has_subtopic", "to": "3 Generalized linear models"}, {"arrows": "to", "from": "Regularization in Deep Learning", "title": "has_subtopic", "to": "Implicit Regularization Effect"}, {"arrows": "to", "from": "Joint Distribution", "title": "depends_on", "to": "Model Parameters"}, {"arrows": "to", "from": "Policy_Gradient_Methods", "title": "depends_on", "to": "Transition_Probabilities"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "contains", "to": "Partially Observable MDPs (POMDP)"}, {"arrows": "to", "from": "1D Example", "title": "subtopic", "to": "Density Transformation"}, {"arrows": "to", "from": "Dual_Problem_Formulation", "title": "related_to", "to": "KKT_Conditions"}, {"arrows": "to", "from": "Optimal Value Function", "title": "defines", "to": "Optimal Policy"}, {"arrows": "to", "from": "NeurIPS_Conference", "title": "related_to", "to": "Training_Set_Issues"}, {"arrows": "to", "from": "Reinforcement learning", "title": "subtopic", "to": "Learning a model for an MDP"}, {"arrows": "to", "from": "Regularization", "title": "addresses", "to": "Overfitting"}, {"arrows": "to", "from": "MachineLearningConcepts", "title": "contains", "to": "EMAlgorithms"}, {"arrows": "to", "from": "DoubleDescentPhenomenon", "title": "depends_on", "to": "StatisticalMechanicsOfLearning"}, {"arrows": "to", "from": "NaiveBayes", "title": "contains", "to": "DiscreteFeatures"}, {"arrows": "to", "from": "Chapter9", "title": "contains", "to": "Regularization"}, {"arrows": "to", "from": "GaussianDistribution", "title": "related_to", "to": "ErrorTermAssumption"}, {"arrows": "to", "from": "Conditional_Probability", "title": "uses", "to": "Softmax_Function"}, {"arrows": "to", "from": "BackwardFunctionsBasics", "title": "has_subtopic", "to": "MatrixMultiplicationModule"}, {"arrows": "to", "from": "Training_Test_Distributions", "title": "related_to", "to": "Domain_Shift"}, {"arrows": "to", "from": "Bayesian Machine Learning", "title": "related_to", "to": "Computational Challenges"}, {"arrows": "to", "from": "Neural Networks", "title": "subtopic", "to": "Stacking Neurons"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "contains", "to": "Linear Regression Model"}, {"arrows": "to", "from": "MachineLearningAlgorithm", "title": "contains", "to": "BatchGradientDescent"}, {"arrows": "to", "from": "ScaleInvariantProperty", "title": "related_to", "to": "LayerNormalization"}, {"arrows": "to", "from": "LinearRegression", "title": "depends_on", "to": "CostFunctionJ"}, {"arrows": "to", "from": "MachineLearningOverview", "title": "subtopic", "to": "GeneralizedLinearModel"}, {"arrows": "to", "from": "Pretraining_Methods", "title": "has_subtopic", "to": "Contrastive_Learning"}, {"arrows": "to", "from": "Baseline_Estimator", "title": "related_to", "to": "Algorithm_7_Vanilla_Policy_Gradient_Baseline"}, {"arrows": "to", "from": "NeuralNetworkParameters", "title": "subtopic", "to": "DeepLearning"}, {"arrows": "to", "from": "Back-propagation Algorithm", "title": "has_subtopic", "to": "MLP Backpropagation"}, {"arrows": "to", "from": "In-Context_Learning", "title": "subtopic", "to": "Machine_Learning_Adaptation"}, {"arrows": "to", "from": "LayerNormalization", "title": "has", "to": "LearnableParameters"}, {"arrows": "to", "from": "Backpropagation", "title": "leads_to", "to": "Gradient_Computation"}, {"arrows": "to", "from": "Learning a model for an MDP", "title": "has_subtopic", "to": "Connections between Policy and Value Iteration (Optional)"}, {"arrows": "to", "from": "Pretraining_Phase", "title": "depends_on", "to": "Unlabeled_Dataset"}, {"arrows": "to", "from": "Asynchronous Updates", "title": "related_to", "to": "Value Iteration Algorithm"}, {"arrows": "to", "from": "Value Function", "title": "has_subtopic", "to": "Policy Evaluation"}, {"arrows": "to", "from": "Reinforcement_Learning", "title": "subtopic", "to": "Policy"}, {"arrows": "to", "from": "BLASOptimization", "title": "subtopic", "to": "VectorizationInNN"}, {"arrows": "to", "from": "Logistic_Regression", "title": "related_to", "to": "Newton_Method"}, {"arrows": "to", "from": "M-step_Update_for_phi_j", "title": "uses", "to": "Lagrangian_Method"}, {"arrows": "to", "from": "MachineLearningOverview", "title": "subtopic", "to": "RegressionProblems"}, {"arrows": "to", "from": "Design Matrix", "title": "subtopic", "to": "Least Squares Revisited"}, {"arrows": "to", "from": "Logistic Regression Derivation", "title": "related_to", "to": "Perceptron Learning Algorithm"}, {"arrows": "to", "from": "Policy_Gradient_Theory", "title": "has_subtopic", "to": "Vanilla_REINFORCE_Algorithm"}, {"arrows": "to", "from": "Machine Learning Algorithms", "title": "related_to", "to": "Neural Networks"}, {"arrows": "to", "from": "BackwardFunctionsBasics", "title": "related_to", "to": "LossFunctionBackward"}, {"arrows": "to", "from": "REINFORCE_Finite_Horizon_Case", "title": "depends_on", "to": "Randomized_Policy"}, {"arrows": "to", "from": "Transformer_Model", "title": "depends_on", "to": "Conditional_Probability"}, {"arrows": "to", "from": "Conditional Probabilistic Model", "title": "depends_on", "to": "Exponential Family Distribution"}, {"arrows": "to", "from": "Dual_Optimization_Problem", "title": "defines", "to": "Equation_6.12"}, {"arrows": "to", "from": "MachineLearningConcepts", "title": "related_to", "to": "ICAIndependenceAssumption"}, {"arrows": "to", "from": "Self-supervised learning and foundation models", "title": "subtopic", "to": "Pretrained large language models"}, {"arrows": "to", "from": "HousingPricesModel", "title": "depends_on", "to": "DerivedFeatures"}, {"arrows": "to", "from": "I Supervised learning", "title": "has_subtopic", "to": "2 Classification and logistic regression"}, {"arrows": "to", "from": "Backward Function Overview", "title": "has_subtopic", "to": "Activation Function Backward Function"}, {"arrows": "to", "from": "Naive Bayes Algorithm", "title": "application", "to": "Spam Classification"}, {"arrows": "to", "from": "Machine_Learning_Concepts", "title": "contains", "to": "Random_Variables"}, {"arrows": "to", "from": "MachineLearningValidationTechniques", "title": "has_subtopic", "to": "HoldoutCrossValidation"}, {"arrows": "to", "from": "Naive Bayes Algorithm", "title": "subtopic_of", "to": "Multinomial Distribution"}, {"arrows": "to", "from": "MachineLearningModeling", "title": "has_subtopic", "to": "StateTransitionModel"}, {"arrows": "to", "from": "NormalEquationsMethod", "title": "has_subtopic", "to": "MatrixDerivatives"}, {"arrows": "to", "from": "Support Vector Machines (SVMs)", "title": "subtopic", "to": "Geometric Margin"}, {"arrows": "to", "from": "Bias Term", "title": "related_to", "to": "Variance Term"}, {"arrows": "to", "from": "ExponentialFamilyDistributions", "title": "has", "to": "LogPartitionFunction"}, {"arrows": "to", "from": "ICAIndependenceAssumption", "title": "subtopic", "to": "JointDistributionModeling"}, {"arrows": "to", "from": "Vectorized Notation", "title": "defines", "to": "Equation 7.65"}, {"arrows": "to", "from": "LQR Extension", "title": "depends_on", "to": "Kalman Filter"}, {"arrows": "to", "from": "Machine_Learning_Concepts", "title": "contains", "to": "Optimal_Margin_Classifiers"}, {"arrows": "to", "from": "Machine_Learning_Concepts", "title": "related_to", "to": "KKT_Conditions"}, {"arrows": "to", "from": "Generative_Modeling", "title": "assumes", "to": "Naive_Bayes_Assumption"}, {"arrows": "to", "from": "ProbabilisticModeling", "title": "subtopic", "to": "LikelihoodFunction"}, {"arrows": "to", "from": "BatchGradientDescent", "title": "related_to", "to": "ThetaRepresentation"}, {"arrows": "to", "from": "Unsupervised learning", "title": "subtopic", "to": "Clustering and the k-means algorithm"}, {"arrows": "to", "from": "SigmoidFunction", "title": "related_to", "to": "TanhFunction"}, {"arrows": "to", "from": "KernelTrickIntroduction", "title": "subtopic", "to": "PhiFunctionExpansion"}, {"arrows": "to", "from": "Generalization", "title": "subtopic", "to": "The double descent phenomenon"}, {"arrows": "to", "from": "PretrainedLanguageModels", "title": "contains", "to": "DocumentProbabilityModeling"}, {"arrows": "to", "from": "Off-the-Shelf Physics Software", "title": "example_of", "to": "Open Dynamics Engine"}, {"arrows": "to", "from": "Foundation Models", "title": "has_subtopic", "to": "Pretraining and Adaptation"}, {"arrows": "to", "from": "Regularization", "title": "subtopic_of", "to": "Inductive_Biases"}, {"arrows": "to", "from": "MSEDecomposition", "title": "subtopic_of", "to": "VarianceTerm"}, {"arrows": "to", "from": "Dual Problem", "title": "related_to", "to": "Primal Problem"}, {"arrows": "to", "from": "Unsupervised learning", "title": "related_to", "to": "Self-supervised learning and foundation models"}, {"arrows": "to", "from": "Maximizing Functions", "title": "subtopic", "to": "Newton\u0027s Method"}, {"arrows": "to", "from": "I Supervised learning", "title": "has_subtopic", "to": "6 Support vector machines"}, {"arrows": "to", "from": "MachineLearningBasics", "title": "subtopic", "to": "CostFunction"}, {"arrows": "to", "from": "Perceptron Learning Algorithm", "title": "depends_on", "to": "Multi-class Classification"}, {"arrows": "to", "from": "Policy_Gradient_Methods", "title": "has_subtopic", "to": "Value_Functions"}, {"arrows": "to", "from": "SummationNotEssential", "title": "subtopic", "to": "SingleExampleLikelihood"}, {"arrows": "to", "from": "Inner_Products_Calculation", "title": "subtopic", "to": "Dual_Form_Optimization"}, {"arrows": "to", "from": "FeatureMap", "title": "related_to", "to": "LinearFunctionOverFeatures"}, {"arrows": "to", "from": "Action Evaluation", "title": "depends_on", "to": "Reward Estimation"}, {"arrows": "to", "from": "Gradient_Descent", "title": "related_to", "to": "Cost_Function_J_theta"}, {"arrows": "to", "from": "EMAlgorithm", "title": "subtopic", "to": "LogLikelihoodOptimization"}, {"arrows": "to", "from": "Cross_Entropy_Loss", "title": "related_to", "to": "Gradient_Descent"}, {"arrows": "to", "from": "Support_Vector_Machines_SVMs", "title": "subtopic", "to": "SVM_Dual_Optimization_Problem"}, {"arrows": "to", "from": "Vapnik\u0027s Theorem", "title": "implies", "to": "Uniform Convergence"}, {"arrows": "to", "from": "EMAlgorithm", "title": "subtopic", "to": "MultipleExamplesExtension"}, {"arrows": "to", "from": "BinaryClassification", "title": "depends_on", "to": "TrainingSet"}, {"arrows": "to", "from": "Sequential Minimal Optimization (SMO) Algorithm", "title": "subtopic", "to": "Support Vector Machines (SVM)"}, {"arrows": "to", "from": "Kernel_Methods", "title": "has_subtopic", "to": "Kernel_Tricks"}, {"arrows": "to", "from": "Machine_Learning_Backward_Propagation", "title": "subtopic", "to": "Chain_Rule_Applications"}, {"arrows": "to", "from": "Policy_Gradient_Methods", "title": "related_to", "to": "REINFORCE_Algorithm"}, {"arrows": "to", "from": "Markov_Decision_Processes_MDPs", "title": "contains", "to": "Model_Learning"}, {"arrows": "to", "from": "Linearization_of_Dynamics", "title": "uses", "to": "Taylor_Expansion"}, {"arrows": "to", "from": "Supervised Learning", "title": "subtopic", "to": "Linear Regression"}, {"arrows": "to", "from": "EM Algorithm", "title": "subtopic", "to": "Latent Variable Models"}, {"arrows": "to", "from": "Cocktail Party Problem", "title": "uses", "to": "Mixing Matrix (A)"}, {"arrows": "to", "from": "Convolutional_Neural_Networks", "title": "has_subtopic", "to": "1D_Convolution"}, {"arrows": "to", "from": "HoldoutCrossValidation", "title": "subtopic_of", "to": "RetrainingModel"}, {"arrows": "to", "from": "Bias-variance tradeoff", "title": "subtopic", "to": "A mathematical decomposition (for regression)"}, {"arrows": "to", "from": "Independent components analysis", "title": "subtopic", "to": "ICA ambiguities"}, {"arrows": "to", "from": "Generalization", "title": "subtopic", "to": "Training Loss Function"}, {"arrows": "to", "from": "Value_Functions", "title": "special_case_of", "to": "Quadratic_Value_Functions"}, {"arrows": "to", "from": "MachineLearningOverview", "title": "contains", "to": "FeatureMapsAndKernels"}, {"arrows": "to", "from": "RegressionProblem", "title": "subtopic", "to": "ProbabilisticInterpretation"}, {"arrows": "to", "from": "FeatureMapsAndKernels", "title": "contains", "to": "EfficientComputationOfKernels"}, {"arrows": "to", "from": "FeatureMapping", "title": "related_to", "to": "KernelsAsSimilarityMetrics"}, {"arrows": "to", "from": "Machine_Learning", "title": "related_to", "to": "Linear_Regression"}, {"arrows": "to", "from": "Stochastic_Optimization_Methods", "title": "related_to", "to": "Machine_Learning_Papers"}, {"arrows": "to", "from": "ICAAlgorithm", "title": "subtopic", "to": "NonGaussianDataRecovery"}, {"arrows": "to", "from": "Optimal_Policy_Derivation", "title": "result", "to": "Linear_Optimal_Policy"}, {"arrows": "to", "from": "Machine_Learning_Pretraining_Adaptation", "title": "related_to", "to": "Adaptation_Phase"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "related_to", "to": "Deep Learning Packages"}, {"arrows": "to", "from": "Linear Regression", "title": "depends_on", "to": "Feature Selection"}, {"arrows": "to", "from": "SingleExampleLikelihood", "title": "subtopic", "to": "EMAlgorithm"}, {"arrows": "to", "from": "ELBO_Optimization", "title": "subtopic", "to": "Gradient_Ascend_ELBO"}, {"arrows": "to", "from": "Policy in MDPs", "title": "subtopic_of", "to": "Time-Dependent Policy"}, {"arrows": "to", "from": "Policy_Iteration", "title": "subtopic", "to": "Policy_Improvement"}, {"arrows": "to", "from": "M_Step", "title": "uses", "to": "Gaussian_Distribution"}, {"arrows": "to", "from": "PolynomialFitting", "title": "depends_on", "to": "Overfitting"}, {"arrows": "to", "from": "Optimal_Policy_Linear_Systems", "title": "depends_on", "to": "Discrete_Ricatti_Equations"}, {"arrows": "to", "from": "ExponentialFamilyDistributions", "title": "has", "to": "SufficientStatistic"}, {"arrows": "to", "from": "ConvolutionalLayers", "title": "has_subtopic", "to": "ParameterSharing"}, {"arrows": "to", "from": "EM_Algorithm", "title": "related_to", "to": "K_Means_Clustering"}, {"arrows": "to", "from": "VarianceScaling", "title": "related_to", "to": "FeatureComparison"}, {"arrows": "to", "from": "BatchGradientDescent", "title": "contains", "to": "InnerProductEfficiency"}, {"arrows": "to", "from": "BayesianInference", "title": "subtopic", "to": "MAPEstimation"}, {"arrows": "to", "from": "UnsupervisedLearning", "title": "contains", "to": "Clustering"}, {"arrows": "to", "from": "Text_Classification", "title": "example_of", "to": "Spam_Filtering"}, {"arrows": "to", "from": "Hyperparameters", "title": "has_subtopic", "to": "Number of Iterations (n_iter)"}, {"arrows": "to", "from": "ELBO", "title": "related_to", "to": "MarginalDistribution"}, {"arrows": "to", "from": "GDA", "title": "related_to", "to": "RobustnessToAssumptions"}, {"arrows": "to", "from": "MachineLearningConcepts", "title": "subtopic", "to": "FunctionalMargin"}, {"arrows": "to", "from": "Machine_Learning", "title": "depends_on", "to": "Empirical_Risk_Minimization"}, {"arrows": "to", "from": "Optimization Problem", "title": "depends_on", "to": "Scaling Constraint"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "subtopic", "to": "Double Descent Phenomenon"}, {"arrows": "to", "from": "LogisticLossFunction", "title": "defines", "to": "Logit"}, {"arrows": "to", "from": "Parameter_Estimation", "title": "depends_on", "to": "Likelihood_Function"}, {"arrows": "to", "from": "Reinforcement_Learning", "title": "subtopic", "to": "Dynamic_Programming"}, {"arrows": "to", "from": "Algorithm 6", "title": "depends_on", "to": "Update Rule (15.12)"}, {"arrows": "to", "from": "LinearRegressionOptimization", "title": "has_subtopic", "to": "BatchGradientDescentExample"}, {"arrows": "to", "from": "BiasInModels", "title": "subtopic", "to": "MachineLearningIssues"}, {"arrows": "to", "from": "Discrete Latent Variables", "title": "related_to", "to": "Mean Field Assumption"}, {"arrows": "to", "from": "Machine_Learning_Theory", "title": "subtopic", "to": "Empirical_Risk_Minimization"}, {"arrows": "to", "from": "Constraints_Satisfaction", "title": "has_subtopic", "to": "Convergence_Tolerance"}, {"arrows": "to", "from": "Regularization", "title": "subtopic_of", "to": "Regularized_Loss"}, {"arrows": "to", "from": "Policy_Gradient_Methods", "title": "depends_on", "to": "Reward_Function"}, {"arrows": "to", "from": "MachineLearningAlgorithms", "title": "has_subtopic", "to": "GenerativeAlgorithms"}, {"arrows": "to", "from": "Bayes Rule Application", "title": "depends_on", "to": "Class Priors"}, {"arrows": "to", "from": "Algorithm 6", "title": "depends_on", "to": "VE Procedure"}, {"arrows": "to", "from": "MachineLearningModels", "title": "contains", "to": "ModelAssumptions"}, {"arrows": "to", "from": "Primal-Dual Relationship", "title": "related_to", "to": "Dual Problem"}, {"arrows": "to", "from": "DataNormalization", "title": "depends_on", "to": "VarianceScaling"}, {"arrows": "to", "from": "Generalization_Error", "title": "depends_on", "to": "Training_Error_vs_Generalization_Error"}, {"arrows": "to", "from": "Kalman Filter", "title": "subtopic", "to": "Update Step"}, {"arrows": "to", "from": "Spam_Filtering", "title": "uses", "to": "Feature_Vector"}, {"arrows": "to", "from": "Regularization_Techniques", "title": "depends_on", "to": "Machine_Learning_Papers"}, {"arrows": "to", "from": "Activation Function", "title": "subtopic", "to": "ReLU"}, {"arrows": "to", "from": "Model-Based_Deep_Reinforcement_Learning", "title": "depends_on", "to": "Machine_Learning_Papers"}, {"arrows": "to", "from": "EM algorithms", "title": "subtopic", "to": "Mixture of Gaussians revisited"}, {"arrows": "to", "from": "Double_Descent_Models", "title": "subtopic", "to": "Machine_Learning_Papers"}, {"arrows": "to", "from": "Machine Learning Loss Functions", "title": "has_subtopic", "to": "Logistic Loss Function"}, {"arrows": "to", "from": "Sample_Complexity_Bound", "title": "subtopic", "to": "Machine_Learning_Bias_Variance_Tradeoff"}, {"arrows": "to", "from": "E_Step", "title": "depends_on", "to": "Bayes_Rule"}, {"arrows": "to", "from": "Policy Evaluation", "title": "depends_on", "to": "Bellman\u0027s Equation"}, {"arrows": "to", "from": "MachineLearningOverview", "title": "subtopic", "to": "BinaryClassification"}, {"arrows": "to", "from": "Representation Function", "title": "related_to", "to": "Positive Pair"}, {"arrows": "to", "from": "Deep Learning Regularizers", "title": "contains", "to": "Data Augmentation"}, {"arrows": "to", "from": "Algorithm_Independence_of_Phi", "title": "subtopic", "to": "Characterization_of_Valid_Kernels"}, {"arrows": "to", "from": "Finetuning_Pretrained_Models", "title": "has_subtopic", "to": "Prediction_Model_Structure"}, {"arrows": "to", "from": "Loss Function", "title": "alternative_definition", "to": "Cross-Entropy Loss"}, {"arrows": "to", "from": "MachineLearningOverview", "title": "subtopic", "to": "ClassificationProblem"}, {"arrows": "to", "from": "Policy_Gradient_Theory", "title": "has_subtopic", "to": "Log_Probability_Gradients"}, {"arrows": "to", "from": "MachineLearningOverview", "title": "related_to", "to": "OverparameterizationRegime"}, {"arrows": "to", "from": "Backpropagation", "title": "related_to", "to": "Gradient Computation"}, {"arrows": "to", "from": "FunctionalMargin", "title": "depends_on", "to": "ConfidenceAndCorrectness"}, {"arrows": "to", "from": "OptimizationMethods", "title": "depends_on", "to": "HessianMatrix"}, {"arrows": "to", "from": "ICAIndependenceAssumption", "title": "subtopic", "to": "DensityFunctionTransformation"}, {"arrows": "to", "from": "Optimal_Control", "title": "subtopic", "to": "Optimal_Policy_Derivation"}, {"arrows": "to", "from": "MachineLearningBasics", "title": "contains", "to": "LinearClassification"}, {"arrows": "to", "from": "Cross_Entropy_Loss_Backward", "title": "subtopic", "to": "Vectorized_Notation_Backward"}, {"arrows": "to", "from": "GeneralizedLinearModels", "title": "subtopic", "to": "ExponentialFamilyDistributions"}, {"arrows": "to", "from": "MaximumLikelihoodEstimation", "title": "subtopic", "to": "ClassificationModelAssumptions"}, {"arrows": "to", "from": "Pretrained large language models", "title": "subtopic", "to": "Open up the blackbox of Transformers"}, {"arrows": "to", "from": "Linear Quadratic Regulation (LQR)", "title": "includes", "to": "Linear Transitions"}, {"arrows": "to", "from": "Softmax Function", "title": "transforms_to", "to": "Probability Vector"}, {"arrows": "to", "from": "LogisticRegression", "title": "related_to", "to": "LMSUpdateRule"}, {"arrows": "to", "from": "PrimalProblem", "title": "subtopic", "to": "ThetaP"}, {"arrows": "to", "from": "Generalization_Error", "title": "depends_on", "to": "Uniform_Convergence_Bound"}, {"arrows": "to", "from": "BernoulliDistribution", "title": "subtopic", "to": "LogPartitionFunctionOfBernoulli"}, {"arrows": "to", "from": "BatchGradientDescentExample", "title": "contrasts_with", "to": "StochasticGradientDescent"}, {"arrows": "to", "from": "Hyperparameters", "title": "has_subtopic", "to": "Learning Rate (\u03b1)"}, {"arrows": "to", "from": "SoftplusFunction", "title": "subtopic", "to": "ActivationFunctions"}, {"arrows": "to", "from": "Non-Separable Case", "title": "depends_on", "to": "Optimization Problem"}, {"arrows": "to", "from": "Machine_Learning_Algorithms", "title": "depends_on", "to": "Empirical_Risk_Minimization"}, {"arrows": "to", "from": "DensityEstimation", "title": "depends_on", "to": "GaussianMixtureModels"}, {"arrows": "to", "from": "Implicit Regularization Effect", "title": "has_subtopic", "to": "Optimizer Bias"}, {"arrows": "to", "from": "Hoeffding_Inequality", "title": "related_to", "to": "Chernoff_Bound"}, {"arrows": "to", "from": "MachineLearningModels", "title": "subtopic", "to": "RelationshipToLogisticRegression"}, {"arrows": "to", "from": "Text_Classification", "title": "depends_on", "to": "Feature_Vector_Selection"}, {"arrows": "to", "from": "Machine_Learning_Concepts", "title": "leads_to", "to": "Dual_Optimization_Problem"}, {"arrows": "to", "from": "RegressionProblems", "title": "depends_on", "to": "ModelTraining"}, {"arrows": "to", "from": "BinaryClassification", "title": "has_component", "to": "HypothesisFunction"}, {"arrows": "to", "from": "DistanceToBoundary", "title": "subtopic", "to": "DecisionBoundary"}, {"arrows": "to", "from": "Continuous_State_MDPs", "title": "subtopic", "to": "Discretization_Method"}, {"arrows": "to", "from": "Discretization_Methods", "title": "depends_on", "to": "Curse_of_Dimensionality"}, {"arrows": "to", "from": "Gaussian_Data_Issue", "title": "subtopic", "to": "ICA_Ambiguities"}, {"arrows": "to", "from": "FunctionalMargin", "title": "related_to", "to": "NormalizationCondition"}, {"arrows": "to", "from": "Chapter16", "title": "contains", "to": "GeneralSettingEquations"}, {"arrows": "to", "from": "Bias Backward Function", "title": "derives_from", "to": "Equation 7.66"}, {"arrows": "to", "from": "ProjectionDirection", "title": "depends_on", "to": "EmpiricalCovarianceMatrix"}, {"arrows": "to", "from": "Machine Learning Models", "title": "subtopic", "to": "Logistic Regression"}, {"arrows": "to", "from": "Self-Supervised Learning", "title": "subtopic", "to": "Positive Pair"}, {"arrows": "to", "from": "EfficiencyConcerns", "title": "depends_on", "to": "VectorizationInNN"}, {"arrows": "to", "from": "Policy_Gradient_Theorem", "title": "subtopic_of", "to": "Simplification_of_Formalism"}, {"arrows": "to", "from": "Optimization Problem", "title": "depends_on", "to": "Non-Convex Constraint"}, {"arrows": "to", "from": "Regularization in Machine Learning", "title": "contains", "to": "L1 Norm (LASSO)"}, {"arrows": "to", "from": "SupervisedLearning", "title": "depends_on", "to": "LinearRegression"}, {"arrows": "to", "from": "LagrangianFormulation", "title": "depends_on", "to": "DerivativeWithRespectToW"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "subtopic", "to": "Gradient Computation"}, {"arrows": "to", "from": "Pretraining_Methods", "title": "has_subtopic", "to": "Supervised_Pretraining"}, {"arrows": "to", "from": "Policy_Iteration", "title": "subtopic", "to": "Policy_Evaluation"}, {"arrows": "to", "from": "MachineLearningConcepts", "title": "related_to", "to": "Backpropagation"}, {"arrows": "to", "from": "LogisticRegression", "title": "related_to", "to": "RobustnessToAssumptions"}, {"arrows": "to", "from": "Hypothesis_Class_Size", "title": "depends_on", "to": "Floating_Point_Precision"}, {"arrows": "to", "from": "E-step", "title": "related_to", "to": "Gaussian Mixture Model (GMM)"}, {"arrows": "to", "from": "Value_Function_Approximation", "title": "related_to", "to": "Expectation_Calculation"}, {"arrows": "to", "from": "MachineLearningConcepts", "title": "subtopic", "to": "EMAlgorithm"}, {"arrows": "to", "from": "Bayes Rule Application", "title": "depends_on", "to": "Conditional Probability p(x|y)"}, {"arrows": "to", "from": "MachineLearningModels", "title": "next_topic", "to": "ConstructingGLMs"}, {"arrows": "to", "from": "MachineLearningOverview", "title": "depends_on", "to": "LikelihoodFunction"}, {"arrows": "to", "from": "Discretization in MDPs", "title": "subtopic", "to": "Value Iteration"}, {"arrows": "to", "from": "ELBO", "title": "depends_on", "to": "Variational Inference"}, {"arrows": "to", "from": "Corollary", "title": "related_to", "to": "VC Dimension"}, {"arrows": "to", "from": "SMO_Algorithm", "title": "related_to", "to": "Efficient_Update_Mechanism"}, {"arrows": "to", "from": "1D_Convolution", "title": "has_component", "to": "Filter_Vector"}, {"arrows": "to", "from": "ComputationalComplexity", "title": "depends_on", "to": "FeatureMapping"}, {"arrows": "to", "from": "Sample complexity bounds (optional readings)", "title": "subtopic", "to": "Preliminaries"}, {"arrows": "to", "from": "LocallyWeightedLinearRegression", "title": "subtopic", "to": "WeightsCalculation"}, {"arrows": "to", "from": "Squared_Loss_Backward", "title": "subtopic", "to": "Vectorized_Notation_Backward"}, {"arrows": "to", "from": "Support Vector Machines (SVM)", "title": "subtopic", "to": "Non-Separable Case"}, {"arrows": "to", "from": "Machine_Learning_Concepts", "title": "subtopic", "to": "Naive_Bayes_Classifier"}, {"arrows": "to", "from": "LMS_Rule", "title": "depends_on", "to": "Error_Term"}, {"arrows": "to", "from": "Machine_Learning_Topics", "title": "related_to", "to": "Value_Function_Approximation"}, {"arrows": "to", "from": "Regularization", "title": "subtopic", "to": "L1 Regularization"}, {"arrows": "to", "from": "Machine_Learning", "title": "depends_on", "to": "Test_Dataset"}, {"arrows": "to", "from": "ConditionalProbability", "title": "depends_on", "to": "ParameterEstimation"}, {"arrows": "to", "from": "Regularization", "title": "depends_on", "to": "ModelComplexity"}, {"arrows": "to", "from": "Gradient_Ascend", "title": "related_to", "to": "Expectation_Value"}, {"arrows": "to", "from": "CovarianceMatrixEffects", "title": "subtopic", "to": "MachineLearningConcepts"}, {"arrows": "to", "from": "Multivariate Normal Distribution", "title": "related_to", "to": "Mean Vector"}, {"arrows": "to", "from": "Model Creation Methods", "title": "has_subtopic", "to": "Learning from Data"}, {"arrows": "to", "from": "Necessary Conditions for Valid Kernels", "title": "subtopic", "to": "Symmetry Property"}, {"arrows": "to", "from": "Gradient_Estimation", "title": "subtopic", "to": "ELBO_Optimization"}, {"arrows": "to", "from": "KernelsAsSimilarityMetrics", "title": "subtopic", "to": "GaussianKernel"}, {"arrows": "to", "from": "MLP Backpropagation", "title": "related_to", "to": "Forward Pass"}, {"arrows": "to", "from": "Backward Function Overview", "title": "has_subtopic", "to": "Bias Backward Function"}, {"arrows": "to", "from": "MSEDecomposition", "title": "subtopic_of", "to": "BiasTerm"}, {"arrows": "to", "from": "Data Augmentation", "title": "creates", "to": "Negative Pair"}, {"arrows": "to", "from": "Efficiency_of_Backward_Pass", "title": "subtopic", "to": "Machine_Learning_Backward_Functions"}, {"arrows": "to", "from": "Testing_Valid_Kernel", "title": "subtopic", "to": "Mercer_Theorem"}, {"arrows": "to", "from": "Finding_w_and_b", "title": "follows_from", "to": "Dual_Optimization_Problem"}, {"arrows": "to", "from": "Probabilistic Model", "title": "subtopic", "to": "Negative Log-Likelihood"}, {"arrows": "to", "from": "Activation Function Backward Function", "title": "follows_from", "to": "Element-wise Operation"}, {"arrows": "to", "from": "LearnedFeaturesRepresentations", "title": "subtopic", "to": "DeepLearning"}, {"arrows": "to", "from": "Optimal_Margin_Classifiers", "title": "contains", "to": "Dual_Form"}, {"arrows": "to", "from": "Bias-Variance Tradeoff", "title": "depends_on", "to": "Model Complexity"}, {"arrows": "to", "from": "Naive Bayes Algorithm", "title": "subtopic_of", "to": "Binary Features"}, {"arrows": "to", "from": "Unsupervised learning", "title": "subtopic", "to": "EM algorithms"}, {"arrows": "to", "from": "BinaryClassification", "title": "defines", "to": "GeneralizationError"}, {"arrows": "to", "from": "kFoldCrossValidation", "title": "has_subtopic", "to": "ModelEvaluation"}, {"arrows": "to", "from": "Principal Component Analysis (PCA)", "title": "subtopic_of", "to": "Noise Reduction"}, {"arrows": "to", "from": "PartialDerivatives", "title": "related_to", "to": "ScalarFunctionDerivatives"}, {"arrows": "to", "from": "InfiniteHorizonMDP", "title": "contrasts_with", "to": "FiniteHorizonMDP"}, {"arrows": "to", "from": "Kalman Filter", "title": "related_to", "to": "Backward Pass"}, {"arrows": "to", "from": "Cross Validation", "title": "subtopic", "to": "Finite Model Set"}, {"arrows": "to", "from": "Value_Function_Approximation", "title": "subtopic", "to": "Model_or_Simulator"}, {"arrows": "to", "from": "Primal-Dual Relationship", "title": "related_to", "to": "Primal Problem"}, {"arrows": "to", "from": "Supervised Learning Problem", "title": "has_subtopic", "to": "Classification"}, {"arrows": "to", "from": "LQRModelAssumptions", "title": "subtopic", "to": "Step2OptimalPolicy"}, {"arrows": "to", "from": "Fitted Value Iteration", "title": "related_to", "to": "Discrete Action Space"}, {"arrows": "to", "from": "Neural Networks", "title": "has_component", "to": "Weight Vector"}, {"arrows": "to", "from": "Machine_Learning_Theory", "title": "depends_on", "to": "Hoeffding_Inequality"}, {"arrows": "to", "from": "RegressionProblems", "title": "subtopic", "to": "LeastSquareCostFunction"}, {"arrows": "to", "from": "Regularization in Machine Learning", "title": "contains", "to": "L2 Norm Regularization"}, {"arrows": "to", "from": "Negative Log-Likelihood", "title": "related_to", "to": "Conditional Probabilistic Model"}, {"arrows": "to", "from": "Expectation-Maximization Algorithm", "title": "has_subtopic", "to": "M-step"}, {"arrows": "to", "from": "LogLikelihood", "title": "subtopic", "to": "MaximizingLogLikelihood"}, {"arrows": "to", "from": "Objective Function Primal", "title": "depends_on", "to": "Primal Problem"}, {"arrows": "to", "from": "E_Step", "title": "related_to", "to": "ELBO"}, {"arrows": "to", "from": "Model-wise Double Descent", "title": "subtopic", "to": "Double Descent Phenomenon"}, {"arrows": "to", "from": "NaturalParameterOfBernoulli", "title": "related_to", "to": "ExponentialFamilyDistributions"}, {"arrows": "to", "from": "ActivationFunctions", "title": "subtopic", "to": "ReLUFunction"}, {"arrows": "to", "from": "PCA_Methods", "title": "subtopic", "to": "Data_Subspace_Identification"}, {"arrows": "to", "from": "M-step", "title": "subtopic", "to": "Update Rule for \\(\\mu_j\\)"}, {"arrows": "to", "from": "MachineLearningArchitectures", "title": "contains", "to": "ResNetArchitecture"}, {"arrows": "to", "from": "Training Set", "title": "depends_on", "to": "Posterior Distribution on Parameters"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "contains", "to": "Support Vector Machines (SVMs)"}, {"arrows": "to", "from": "GaussianDiscriminantAnalysis", "title": "subtopic", "to": "MachineLearningConcepts"}, {"arrows": "to", "from": "TwoLayerNetwork", "title": "subtopic", "to": "VectorizationInNN"}, {"arrows": "to", "from": "Support Vector Machines (SVM)", "title": "has_subtopic", "to": "Optimization Problem"}, {"arrows": "to", "from": "MachineLearningOverview", "title": "contains", "to": "BinaryClassificationProblem"}, {"arrows": "to", "from": "Coordinate_Ascend_Algorithm", "title": "has_subtopic", "to": "Unconstrained_Optimization_Problem"}, {"arrows": "to", "from": "Policy_Iteration", "title": "depends_on", "to": "Value_Evaluation_Procedure"}, {"arrows": "to", "from": "Kalman Filter", "title": "subtopic", "to": "Predict Step"}, {"arrows": "to", "from": "VectorW", "title": "related_to", "to": "DecisionBoundary"}, {"arrows": "to", "from": "Training_Set", "title": "excludes", "to": "Stop_Words"}, {"arrows": "to", "from": "TrainingLoss", "title": "depends_on", "to": "LambdaParameter"}, {"arrows": "to", "from": "inner_loop_steps", "title": "subtopic", "to": "k-means_algorithm"}, {"arrows": "to", "from": "Machine_Learning_Concepts", "title": "depends_on", "to": "Gradient_Estimation"}, {"arrows": "to", "from": "LogisticRegression", "title": "contains", "to": "LogisticLossFunction"}, {"arrows": "to", "from": "Volume Mapping", "title": "subtopic", "to": "Density Transformation"}, {"arrows": "to", "from": "KernelTrick", "title": "subtopic", "to": "InnerProduct"}, {"arrows": "to", "from": "Deep Learning Regularizers", "title": "contains", "to": "Spectral Norm Regularization"}, {"arrows": "to", "from": "Regularization in Deep Learning", "title": "has_subtopic", "to": "Explicit Regularization Techniques"}, {"arrows": "to", "from": "Bias-Variance Tradeoff", "title": "related_to", "to": "Underfitting"}, {"arrows": "to", "from": "NeuralNetworks", "title": "depends_on", "to": "WeightMatricesBiases"}, {"arrows": "to", "from": "StateTransitionModel", "title": "has_subtopic", "to": "LinearModel"}, {"arrows": "to", "from": "Model Creation Methods", "title": "has_subtopic", "to": "Physics Simulation"}, {"arrows": "to", "from": "Mixing_Matrix_Rotation", "title": "depends_on", "to": "Gaussian_Data_Issue"}, {"arrows": "to", "from": "GradientDescentOptimizer", "title": "subtopic", "to": "MinimumNormSolution"}, {"arrows": "to", "from": "Gradient_Descent", "title": "depends_on", "to": "Learning_Rate"}, {"arrows": "to", "from": "Machine Learning Loss Functions", "title": "has_subtopic", "to": "Cross-Entropy Loss Function"}, {"arrows": "to", "from": "Efficiency_of_Modules", "title": "subtopic", "to": "Machine_Learning_Backpropagation"}, {"arrows": "to", "from": "Contrastive_Learning", "title": "has_concept", "to": "Negative_Pairs"}, {"arrows": "to", "from": "Gradient Computation", "title": "depends_on", "to": "Loss Function J"}, {"arrows": "to", "from": "Classification Problem", "title": "contains", "to": "Logistic Regression"}, {"arrows": "to", "from": "Machine Learning Tasks", "title": "depends_on", "to": "Adaptation Algorithms"}, {"arrows": "to", "from": "Cross Validation", "title": "subtopic", "to": "Polynomial Regression Models"}, {"arrows": "to", "from": "Transformer_Model", "title": "related_to", "to": "Autoregressive_Decoding"}, {"arrows": "to", "from": "General EM algorithms", "title": "subtopic", "to": "Other interpretation of ELBO"}, {"arrows": "to", "from": "ShatteringConcept", "title": "subtopic", "to": "VCDimension"}, {"arrows": "to", "from": "SingleNeuronNN", "title": "example_of", "to": "HousingPricePrediction"}, {"arrows": "to", "from": "Reinforcement learning", "title": "subtopic", "to": "Markov decision processes"}, {"arrows": "to", "from": "Principal Component Analysis (PCA)", "title": "subtopic_of", "to": "Data Visualization"}, {"arrows": "to", "from": "Machine Learning Algorithms", "title": "contains", "to": "Laplace Smoothing"}, {"arrows": "to", "from": "EventModelsForTextClassification", "title": "has_subtopic", "to": "MultinomialEventModel"}, {"arrows": "to", "from": "MLPArchitecture", "title": "depends_on", "to": "MatrixMultiplicationModule"}, {"arrows": "to", "from": "Generalization", "title": "subtopic", "to": "Sample complexity bounds (optional readings)"}, {"arrows": "to", "from": "2 Classification and logistic regression", "title": "has_subtopic", "to": "2.3 Multi-class classification"}, {"arrows": "to", "from": "Pretraining_Phase", "title": "subtopic", "to": "Optimizers"}, {"arrows": "to", "from": "Machine_Learning_Concepts", "title": "subtopic", "to": "LQR_Algorithm"}, {"arrows": "to", "from": "Regularization and model selection", "title": "subtopic", "to": "Regularization"}, {"arrows": "to", "from": "FeatureMaps", "title": "subtopic", "to": "DeepLearning"}, {"arrows": "to", "from": "EM algorithms", "title": "subtopic", "to": "EM for mixture of Gaussians"}, {"arrows": "to", "from": "MarkovDecisionProcesses", "title": "component_of", "to": "States"}, {"arrows": "to", "from": "FeatureMapsAndKernels", "title": "contains", "to": "KernelFunctionDefinition"}, {"arrows": "to", "from": "Principal Component Analysis (PCA)", "title": "subtopic_of", "to": "Overfitting Prevention"}, {"arrows": "to", "from": "Sample Complexity Bounds", "title": "contains", "to": "Generalization Error"}, {"arrows": "to", "from": "SIMCLR", "title": "uses", "to": "Loss_Function"}, {"arrows": "to", "from": "FiniteHorizonMDPs", "title": "contains", "to": "ValueIterationPolicyIteration"}, {"arrows": "to", "from": "LogLikelihood", "title": "subtopic", "to": "StochasticGradientAscent"}, {"arrows": "to", "from": "Prediction on New Data", "title": "subtopic", "to": "Fully Bayesian Prediction"}, {"arrows": "to", "from": "TanhFunction", "title": "subtopic", "to": "ActivationFunctions"}, {"arrows": "to", "from": "Value Function", "title": "has_subtopic", "to": "Bellman\u0027s Equation"}, {"arrows": "to", "from": "GroupNormalization", "title": "subtopic", "to": "OtherNormalizationLayers"}, {"arrows": "to", "from": "BinaryClassificationProblem", "title": "depends_on", "to": "LossFunction"}, {"arrows": "to", "from": "RegressionProblems", "title": "depends_on", "to": "TrainingDataset"}, {"arrows": "to", "from": "LogisticLossFunction", "title": "subtopic", "to": "NegativeLogLikelihood"}, {"arrows": "to", "from": "Machine_Learning_Theory", "title": "subtopic", "to": "Binary_Classification"}, {"arrows": "to", "from": "BatchGradientDescent", "title": "depends_on", "to": "BetaUpdateEquation"}, {"arrows": "to", "from": "FeatureMapsAndKernels", "title": "contains", "to": "PredictionUsingKernelFunction"}, {"arrows": "to", "from": "BayesianInference", "title": "subtopic", "to": "PriorDistributions"}, {"arrows": "to", "from": "Mercer_Theorem", "title": "subtopic", "to": "Valid_Kernels_Conditions"}, {"arrows": "to", "from": "Machine_Learning_Models", "title": "has_subtopic", "to": "Transformer_Model"}, {"arrows": "to", "from": "Chain Rule", "title": "subtopic", "to": "Basic Chain Rule Perspective"}, {"arrows": "to", "from": "Backpropagation Strategy", "title": "subtopic", "to": "Backward Function Computation"}, {"arrows": "to", "from": "Normal_Distribution", "title": "depends_on", "to": "Mean"}, {"arrows": "to", "from": "MachineLearningBasics", "title": "contains", "to": "LogisticRegression"}, {"arrows": "to", "from": "Machine Learning Models", "title": "related_to", "to": "Locally Weighted Linear Regression"}, {"arrows": "to", "from": "LogisticRegression", "title": "has_subtopic", "to": "LogitFunction"}, {"arrows": "to", "from": "Matrix Multiplication Backward Function", "title": "derives_from", "to": "Equation 7.64"}, {"arrows": "to", "from": "MarkovDecisionProcesses", "title": "component_of", "to": "DiscountFactor"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "related_to", "to": "Overfitting and Underfitting"}, {"arrows": "to", "from": "Vectorization", "title": "related_to", "to": "Broadcasting"}, {"arrows": "to", "from": "Training_Error_Generalization_Error", "title": "subtopic", "to": "Uniform_Convergence"}, {"arrows": "to", "from": "Logistic Regression", "title": "subtopic", "to": "Newton\u0027s Method"}, {"arrows": "to", "from": "Overfitting", "title": "causes", "to": "Variance"}, {"arrows": "to", "from": "4 Generative learning algorithms", "title": "has_subtopic", "to": "4.2 Naive bayes (Option Reading)"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "contains", "to": "LQR Extension"}, {"arrows": "to", "from": "Step1Estimation", "title": "depends_on", "to": "GaussianDiscriminantAnalysis"}, {"arrows": "to", "from": "Regularization and model selection", "title": "subtopic", "to": "Implicit regularization effect (optional reading)"}, {"arrows": "to", "from": "Hessian Matrix", "title": "related_to", "to": "Logistic Regression"}, {"arrows": "to", "from": "ConvolutionalLayers", "title": "has_subtopic", "to": "EfficiencyOfConvolution"}, {"arrows": "to", "from": "Model_Wise_Double_Descent", "title": "related_to", "to": "Sample_Wise_Double_Descent"}, {"arrows": "to", "from": "Modern Neural Networks", "title": "subtopic", "to": "Backpropagation"}, {"arrows": "to", "from": "LogisticRegression", "title": "has_subtopic", "to": "NegativeLikelihoodLoss"}, {"arrows": "to", "from": "OptimizationChallenges", "title": "related_to", "to": "EMAlgorithm"}, {"arrows": "to", "from": "FifthDegreePolynomial", "title": "subtopic", "to": "MachineLearningIssues"}, {"arrows": "to", "from": "Conditional Distribution Modeling", "title": "related_to", "to": "Hypothesis Function"}, {"arrows": "to", "from": "Support_Vector_Machines_SVMs", "title": "depends_on", "to": "Sequential_Minimal_Optimization_SMO"}, {"arrows": "to", "from": "LQR, DDP and LQG", "title": "subtopic", "to": "Finite-horizon MDPs"}, {"arrows": "to", "from": "ProbabilityDistributions", "title": "depends_on", "to": "JensensInequality"}, {"arrows": "to", "from": "Partially Observable MDPs (POMDP)", "title": "has_subtopic", "to": "Observation Layer"}, {"arrows": "to", "from": "Objective Function Dual", "title": "depends_on", "to": "Dual Problem"}, {"arrows": "to", "from": "ConvolutionalLayers", "title": "subtopic", "to": "MachineLearningConcepts"}, {"arrows": "to", "from": "BinaryClassification", "title": "subtopic", "to": "LogisticFunction"}, {"arrows": "to", "from": "ConditionalProbability", "title": "subtopic", "to": "NaiveBayesAlgorithm"}, {"arrows": "to", "from": "Machine_Learning_Models", "title": "has_subtopic", "to": "Parameter_Estimation"}, {"arrows": "to", "from": "Machine_Learning_Theory", "title": "subtopic", "to": "Optimal_Hypothesis_Selection"}, {"arrows": "to", "from": "InvertibleMatrixAssumption", "title": "depends_on", "to": "LinearRegression"}, {"arrows": "to", "from": "CostFunctionJ", "title": "leads_to", "to": "NormalEquations"}, {"arrows": "to", "from": "PrincipalComponentAnalysis", "title": "has_subtopic", "to": "kDimensionalSubspace"}, {"arrows": "to", "from": "Machine_Learning_Concepts", "title": "related_to", "to": "Overparameterized_Models"}, {"arrows": "to", "from": "Naive Bayes Algorithm", "title": "subtopic_of", "to": "Feature Discretization"}, {"arrows": "to", "from": "ICA Ambiguities", "title": "depends_on", "to": "Permutation Matrix"}, {"arrows": "to", "from": "Classification Confidence", "title": "subtopic", "to": "Geometric Margins"}, {"arrows": "to", "from": "KernelFunctions", "title": "has_subtopic", "to": "FeatureMapping"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "has_subtopic", "to": "Stochastic Gradient Ascent"}, {"arrows": "to", "from": "Principal Component Analysis (PCA)", "title": "subtopic_of", "to": "Computational Efficiency"}, {"arrows": "to", "from": "Foundation Models", "title": "subtopic", "to": "Machine Learning Overview"}, {"arrows": "to", "from": "GaussianDiscriminantAnalysis", "title": "subtopic", "to": "MLEstimation"}, {"arrows": "to", "from": "NeuralNetworksComposition", "title": "has_subtopic", "to": "BackpropagationDiscussion"}, {"arrows": "to", "from": "Neural Networks", "title": "depends_on", "to": "Activation Function"}, {"arrows": "to", "from": "Machine_Learning_Optimization", "title": "contains", "to": "Coordinate_Ascend_Method"}, {"arrows": "to", "from": "MarkovDecisionProcesses", "title": "component_of", "to": "RewardFunction"}, {"arrows": "to", "from": "Machine_Learning_Algorithms", "title": "has_subtopic", "to": "Gradient_Descent"}, {"arrows": "to", "from": "MLP Backpropagation", "title": "depends_on", "to": "Backward Pass"}, {"arrows": "to", "from": "Machine_Learning_Algorithms", "title": "related_to", "to": "Support_Vector_Machines"}, {"arrows": "to", "from": "MachineLearningConcepts", "title": "subtopic", "to": "ChainRule"}, {"arrows": "to", "from": "Machine_Learning_Techniques", "title": "has_subtopic", "to": "Feature_Vectors"}, {"arrows": "to", "from": "Matrix Multiplication Backward Function", "title": "follows_from", "to": "Vectorized Notation"}, {"arrows": "to", "from": "Variational_Bayes", "title": "subtopic", "to": "Machine_Learning_Papers"}, {"arrows": "to", "from": "PolynomialFeatures", "title": "related_to", "to": "Underfitting"}, {"arrows": "to", "from": "Lagrange Duality", "title": "leads_to", "to": "Dual Formulation"}, {"arrows": "to", "from": "DataNormalization", "title": "leads_to", "to": "NormalizedDatasetExample"}, {"arrows": "to", "from": "Reinforcement_Learning", "title": "subtopic", "to": "Value_Function"}, {"arrows": "to", "from": "Adaptation Algorithms", "title": "subtopic", "to": "Linear Probe"}, {"arrows": "to", "from": "Optimization Problem", "title": "has_subtopic", "to": "Linear Constraints"}, {"arrows": "to", "from": "Softmax Function", "title": "depends_on", "to": "Logits"}, {"arrows": "to", "from": "FeatureMapsAndKernels", "title": "contains", "to": "UpdateRepresentationBeta"}, {"arrows": "to", "from": "CarAttributesExample", "title": "subtopic", "to": "RedundancyDetection"}, {"arrows": "to", "from": "Machine_Learning_Adaptation", "title": "has_subtopic", "to": "Finetuning_Pretrained_Models"}, {"arrows": "to", "from": "UnitVectorW", "title": "depends_on", "to": "DistanceToBoundary"}, {"arrows": "to", "from": "Bias_Variance_Tradeoff", "title": "depends_on", "to": "Test_Error_Influence"}, {"arrows": "to", "from": "MachineLearningOverview", "title": "contains", "to": "NeuralNetworkStructure"}, {"arrows": "to", "from": "GeneralizedLinearModel", "title": "depends_on", "to": "ExponentialFamilyDistributions"}, {"arrows": "to", "from": "E_Step", "title": "produces", "to": "Soft_Guesses"}, {"arrows": "to", "from": "OtherNormalizationLayers", "title": "subtopic", "to": "MachineLearningConcepts"}, {"arrows": "to", "from": "Efficient_Update_Mechanism", "title": "depends_on", "to": "Alpha_Updating_Process"}, {"arrows": "to", "from": "MachineLearningBasics", "title": "contains", "to": "MulticlassClassification"}, {"arrows": "to", "from": "Underfitting", "title": "subtopic", "to": "MachineLearningConcepts"}, {"arrows": "to", "from": "Algorithm 6", "title": "depends_on", "to": "Policy Update Rule (15.13)"}, {"arrows": "to", "from": "Empirical_Risk_Minimization", "title": "related_to", "to": "Training_Error"}, {"arrows": "to", "from": "LayerNormalization", "title": "follows", "to": "AffineTransformation"}, {"arrows": "to", "from": "Statistical_Learning_Theory", "title": "depends_on", "to": "Machine_Learning_Papers"}, {"arrows": "to", "from": "Synchronous Updates", "title": "related_to", "to": "Value Iteration Algorithm"}, {"arrows": "to", "from": "Sparsity Regularization", "title": "contains", "to": "L0 Norm"}, {"arrows": "to", "from": "Fitted Value Iteration", "title": "depends_on", "to": "Continuous State Space"}, {"arrows": "to", "from": "LogLikelihoodExpression", "title": "subtopic", "to": "SummationNotEssential"}, {"arrows": "to", "from": "NaiveBayesAlgorithm", "title": "related_to", "to": "BernoulliEventModel"}, {"arrows": "to", "from": "Machine Learning Tasks", "title": "subtopic", "to": "Zero-Shot Learning"}, {"arrows": "to", "from": "Log-Likelihood", "title": "depends_on", "to": "EM_Algorithm"}, {"arrows": "to", "from": "Logistic_Loss_Backward", "title": "subtopic", "to": "Vectorized_Notation_Backward"}, {"arrows": "to", "from": "Reinforcement_Learning", "title": "related_to", "to": "Discounted_Rewards"}, {"arrows": "to", "from": "ExponentialFamilyDistributions", "title": "has", "to": "NaturalParameter"}, {"arrows": "to", "from": "Machine_Learning_Concepts", "title": "contains", "to": "Update_Rule"}, {"arrows": "to", "from": "Vectorized_Notation_Backward", "title": "subtopic", "to": "Machine_Learning_Backward_Functions"}, {"arrows": "to", "from": "LikelihoodFunction", "title": "subtopic", "to": "LogLikelihood"}, {"arrows": "to", "from": "3 Generalized linear models", "title": "has_subtopic", "to": "3.2 Constructing GLMs"}, {"arrows": "to", "from": "Fitted Value Iteration", "title": "uses", "to": "State Sample"}, {"arrows": "to", "from": "Gaussian_Mixture_Models", "title": "computes", "to": "Posterior_Distribution"}, {"arrows": "to", "from": "Reinforcement_Learning", "title": "related_to", "to": "Bellman_Equation"}, {"arrows": "to", "from": "Cocktail Party Problem", "title": "aims_to_recover", "to": "Unmixing Matrix (W)"}, {"arrows": "to", "from": "Unsupervised Learning", "title": "depends_on", "to": "Joint Distribution"}, {"arrows": "to", "from": "ICAIndependenceAssumption", "title": "subtopic", "to": "SigmoidFunctionChoice"}, {"arrows": "to", "from": "MachineLearningOverview", "title": "subtopic", "to": "MLPArchitecture"}, {"arrows": "to", "from": "Independent Components Analysis (ICA)", "title": "motivational_example", "to": "Cocktail Party Problem"}, {"arrows": "to", "from": "Digit_Recognition_Problem", "title": "subtopic", "to": "Examples_of_Kernels"}, {"arrows": "to", "from": "Machine Learning Algorithms", "title": "depends_on", "to": "Initialization"}, {"arrows": "to", "from": "LQRModelAssumptions", "title": "subtopic", "to": "Step1Estimation"}, {"arrows": "to", "from": "Value_Iteration", "title": "subtopic", "to": "Optimal_Policy_Finding"}, {"arrows": "to", "from": "Machine_Learning_Algorithms", "title": "related_to", "to": "Bellman_Equations"}, {"arrows": "to", "from": "ActivationFunctions", "title": "related_to", "to": "SigmoidTanh"}, {"arrows": "to", "from": "Support Vector Machines (SVM)", "title": "has_subtopic", "to": "Geometric Margin"}, {"arrows": "to", "from": "LinearRegressionOptimization", "title": "has_subtopic", "to": "StochasticGradientDescent"}, {"arrows": "to", "from": "Policy_Gradient_Methods", "title": "depends_on", "to": "Gradient_Ascend"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "contains", "to": "Linear Regression"}, {"arrows": "to", "from": "Machine Learning Algorithms", "title": "has_subtopic", "to": "Value Iteration"}, {"arrows": "to", "from": "Machine_Learning_Algorithms", "title": "depends_on", "to": "Kernel_Tricks"}, {"arrows": "to", "from": "LagrangianFormulation", "title": "depends_on", "to": "DerivativeWithRespectToB"}, {"arrows": "to", "from": "Markov Decision Processes (MDP)", "title": "has_subtopic", "to": "Time Dependent Dynamics"}, {"arrows": "to", "from": "MachineLearningOverview", "title": "related_to", "to": "IndependenceAssumption"}, {"arrows": "to", "from": "LinearModelLimitations", "title": "depends_on", "to": "UnderfittingLinearModel"}, {"arrows": "to", "from": "Alpha Updates", "title": "subtopic", "to": "Sequential Minimal Optimization (SMO) Algorithm"}, {"arrows": "to", "from": "Lagrangian_Formulation", "title": "depends_on", "to": "Equation_6.10"}, {"arrows": "to", "from": "Machine_Learning_Pretraining_Adaptation", "title": "subtopic", "to": "Downstream_Tasks"}, {"arrows": "to", "from": "Optimal_Margin_Classifiers", "title": "contains", "to": "Primal_Problem"}, {"arrows": "to", "from": "DifferentialDynamicProgramming", "title": "subtopic", "to": "RewritingDynamics"}, {"arrows": "to", "from": "IdentityFunction", "title": "subtopic", "to": "ActivationFunctions"}, {"arrows": "to", "from": "Linear Quadratic Regulation (LQR)", "title": "includes", "to": "Quadratic Rewards"}, {"arrows": "to", "from": "Machine_Learning_Models", "title": "has_subtopic", "to": "Embeddings_and_Representations"}, {"arrows": "to", "from": "MachineLearningOverview", "title": "contains", "to": "BayesianInference"}, {"arrows": "to", "from": "GenerativeAlgorithms", "title": "uses", "to": "BayesRule"}, {"arrows": "to", "from": "I Supervised learning", "title": "has_subtopic", "to": "4 Generative learning algorithms"}, {"arrows": "to", "from": "NeuralNetworks", "title": "contains", "to": "LayerStructure"}, {"arrows": "to", "from": "Cross_Entropy_Loss", "title": "related_to", "to": "Gradient_Calculation"}, {"arrows": "to", "from": "Optimizers", "title": "subtopic", "to": "Gradient Descent (GD)"}, {"arrows": "to", "from": "PartialDerivatives", "title": "depends_on", "to": "MathematicalNotationChallenges"}, {"arrows": "to", "from": "DataPreprocessing", "title": "depends_on", "to": "LogisticFunction"}, {"arrows": "to", "from": "PrincipalComponentAnalysis", "title": "has_subtopic", "to": "ProjectionDirection"}, {"arrows": "to", "from": "Random_Variables", "title": "subtopic", "to": "Normal_Distribution"}, {"arrows": "to", "from": "Self-Supervised Learning", "title": "related_to", "to": "Supervised Contrastive Algorithms"}, {"arrows": "to", "from": "Binary_Classification", "title": "depends_on", "to": "Training_Set"}, {"arrows": "to", "from": "Regularized_Loss", "title": "has_subcomponent", "to": "Lambda_Parameter"}, {"arrows": "to", "from": "MachineLearningOverview", "title": "contains", "to": "HousingPricesModel"}, {"arrows": "to", "from": "Lagrange Duality", "title": "defines", "to": "Lagrangian Function"}, {"arrows": "to", "from": "Machine_Learning_Models", "title": "has_subtopic", "to": "Multinomial_Event_Model"}, {"arrows": "to", "from": "ScalingInvariantProperty", "title": "depends_on", "to": "LN-S"}, {"arrows": "to", "from": "Finite-State MDPs", "title": "subtopic", "to": "Value Iteration and Policy Iteration"}, {"arrows": "to", "from": "Prediction", "title": "subtopic", "to": "NaiveBayesAlgorithm"}, {"arrows": "to", "from": "SMO_Algorithm", "title": "depends_on", "to": "Constraints_Satisfaction"}, {"arrows": "to", "from": "PretrainedLanguageModels", "title": "contains", "to": "NaturalLanguageProcessing"}, {"arrows": "to", "from": "Chapter_17_Policy_Gradient_REINFORCE", "title": "subtopic", "to": "Sampling_Transition_Probabilities"}, {"arrows": "to", "from": "Optimizers", "title": "subtopic", "to": "Stochastic Gradient Descent (SGD)"}, {"arrows": "to", "from": "Reinforcement_Learning", "title": "subtopic", "to": "Value_Functions"}, {"arrows": "to", "from": "GaussianMixtureModels", "title": "subtopic", "to": "EMAlgorithm"}, {"arrows": "to", "from": "MarkovDecisionProcesses", "title": "component_of", "to": "StateTransitionProbabilities"}, {"arrows": "to", "from": "II Deep learning", "title": "has_subtopic", "to": "7 Deep learning"}, {"arrows": "to", "from": "Sigmoid_Function", "title": "has", "to": "Derivative_of_Sigmoid"}, {"arrows": "to", "from": "Machine Learning", "title": "depends_on", "to": "Matricization Approach"}, {"arrows": "to", "from": "JensensInequality", "title": "contains", "to": "TheoremStatement"}, {"arrows": "to", "from": "PolicyGradientTheorem", "title": "subtopic", "to": "ExpectationEstimation"}, {"arrows": "to", "from": "4 Generative learning algorithms", "title": "has_subtopic", "to": "4.1 Gaussian discriminant analysis"}, {"arrows": "to", "from": "EventModelsForTextClassification", "title": "has_subtopic", "to": "BernoulliEventModel"}, {"arrows": "to", "from": "LayerNormalization", "title": "depends_on", "to": "LN-S"}, {"arrows": "to", "from": "Deep_Residual_Learning", "title": "subtopic", "to": "Machine_Learning_Papers"}, {"arrows": "to", "from": "GenerativeAlgorithms", "title": "depends_on", "to": "ClassPriors"}, {"arrows": "to", "from": "Normal_Distribution", "title": "subtopic", "to": "Standard_Normal_Distribution"}, {"arrows": "to", "from": "Partially Observable MDPs (POMDP)", "title": "has_subtopic", "to": "Policy Mapping"}, {"arrows": "to", "from": "Reinforcement Learning and Control", "title": "subtopic", "to": "Reinforcement learning"}, {"arrows": "to", "from": "House Price Prediction Example", "title": "depends_on", "to": "Feature Discovery"}, {"arrows": "to", "from": "Sample_Size_Calculation", "title": "subtopic", "to": "Training_Error_Generalization_Error"}, {"arrows": "to", "from": "E_Step", "title": "contrasts_with", "to": "Hard_Assignments"}, {"arrows": "to", "from": "ParameterMatrixW", "title": "related_to", "to": "LogLikelihood"}, {"arrows": "to", "from": "MachineLearningOptimization", "title": "has_subtopic", "to": "NormalEquationsMethod"}, {"arrows": "to", "from": "Machine_Learning_Models", "title": "has_subtopic", "to": "Conditional_Probability_Modeling"}, {"arrows": "to", "from": "Bellman_Equation", "title": "depends_on", "to": "Value_Iteration"}, {"arrows": "to", "from": "Machine_Learning_Concepts", "title": "has_subtopic", "to": "Loss_Functions"}, {"arrows": "to", "from": "Machine Learning Tasks", "title": "subtopic", "to": "Few-Shot Learning"}, {"arrows": "to", "from": "Unknown_State_Transitions_and_Rewards", "title": "depends_on", "to": "Efficient_Experience_Use"}, {"arrows": "to", "from": "convergence_property", "title": "related_to", "to": "k-means_algorithm"}, {"arrows": "to", "from": "Support_Vector_Machines", "title": "subtopic", "to": "Margins_Concept"}, {"arrows": "to", "from": "Machine Learning Models", "title": "depends_on", "to": "Non-Linear Feature Mappings"}, {"arrows": "to", "from": "1 Linear regression", "title": "has_subtopic", "to": "1.3 Probabilistic interpretation"}, {"arrows": "to", "from": "PartialDerivatives", "title": "related_to", "to": "ComputationalComplexity"}, {"arrows": "to", "from": "HoldoutCrossValidation", "title": "subtopic_of", "to": "ValidationSetSize"}, {"arrows": "to", "from": "From non-linear dynamics to LQR", "title": "has_subtopic", "to": "Differential Dynamic Programming (DDP)"}, {"arrows": "to", "from": "Finite Horizon Setting", "title": "subtopic_of", "to": "Optimal Policy in Finite Horizon"}, {"arrows": "to", "from": "Cross_Entropy_Loss", "title": "depends_on", "to": "Softmax_Function"}, {"arrows": "to", "from": "MultinomialRandomVariable", "title": "depends_on", "to": "MaximumLikelihoodEstimates"}, {"arrows": "to", "from": "Variational_Autoencoder", "title": "uses", "to": "Reparametrization_Trick"}, {"arrows": "to", "from": "MachineLearningModels", "title": "contains", "to": "GLMFormulation"}, {"arrows": "to", "from": "ConditionalLikelihood", "title": "subtopic", "to": "MixtureOfGaussians"}, {"arrows": "to", "from": "Double Descent Phenomenon", "title": "related_to", "to": "Regularization Techniques"}, {"arrows": "to", "from": "BinaryClassification", "title": "related_to", "to": "PACAssumptions"}, {"arrows": "to", "from": "Variational_Inference_Review", "title": "subtopic", "to": "Machine_Learning_Papers"}, {"arrows": "to", "from": "MachineLearningOverview", "title": "contains", "to": "IntermediateVariables"}, {"arrows": "to", "from": "L2_Regularization", "title": "explains_concept", "to": "Weight_Decay"}, {"arrows": "to", "from": "ConvolutionalLayers", "title": "has_subtopic", "to": "ChannelConcepts"}, {"arrows": "to", "from": "MaximumLikelihoodEstimation", "title": "subtopic", "to": "LikelihoodFunction"}, {"arrows": "to", "from": "MatrixAlgebra", "title": "subtopic", "to": "VectorizationInNN"}, {"arrows": "to", "from": "BernoulliRandomVariableZ", "title": "subtopic", "to": "GeneralizationErrorGuarantees"}, {"arrows": "to", "from": "Implementation Details", "title": "subtopic", "to": "Data Representation"}, {"arrows": "to", "from": "Text_Classification", "title": "follows", "to": "Generative_Modeling"}, {"arrows": "to", "from": "Bias_Variance_Tradeoff", "title": "subtopic", "to": "Machine_Learning_Papers"}, {"arrows": "to", "from": "Unsupervised learning", "title": "subtopic", "to": "Independent components analysis"}, {"arrows": "to", "from": "Gradient Calculation", "title": "depends_on", "to": "Matrix Derivatives"}, {"arrows": "to", "from": "Matricization Approach", "title": "subtopic", "to": "Implementation Details"}, {"arrows": "to", "from": "Empirical_Risk_Minimization", "title": "subtopic", "to": "Finite_Hypothesis_Class"}, {"arrows": "to", "from": "FullyConnectedNN", "title": "depends_on", "to": "IntermediateVariables"}, {"arrows": "to", "from": "Conditional Distribution Modeling", "title": "depends_on", "to": "Bernoulli Distribution"}, {"arrows": "to", "from": "LatentVariableModel", "title": "contains", "to": "ContinuousLatentVariables"}, {"arrows": "to", "from": "Machine_Learning_Overview", "title": "has_subtopic", "to": "Policy_Gradient_Methods"}, {"arrows": "to", "from": "FiniteHorizonMDPs", "title": "contains", "to": "OptimalBellmanEquation"}, {"arrows": "to", "from": "ProbabilityEstimation", "title": "subtopic", "to": "EventModelsTextClassification"}, {"arrows": "to", "from": "Multinomial_Event_Model", "title": "depends_on", "to": "Spam_Detection"}, {"arrows": "to", "from": "Normal_Distribution", "title": "related_to", "to": "Covariance_Matrix"}, {"arrows": "to", "from": "ModelComplexityMeasures", "title": "related_to", "to": "NormAsComplexityMeasure"}, {"arrows": "to", "from": "Machine Learning Overview", "title": "related_to", "to": "Supervised Learning"}, {"arrows": "to", "from": "Support_Vector_Machines", "title": "subtopic", "to": "Kernels_in_SVM"}, {"arrows": "to", "from": "Support Vector Machines (SVM)", "title": "depends_on", "to": "Regularization"}, {"arrows": "to", "from": "Policy_Gradient_Methods", "title": "depends_on", "to": "Value_Function"}, {"arrows": "to", "from": "KernelFunctions", "title": "has_subtopic", "to": "PolynomialKernels"}, {"arrows": "to", "from": "FittedValueIteration", "title": "subtopic", "to": "DeterministicSimulator"}, {"arrows": "to", "from": "Machine_Learning_Concepts", "title": "contains", "to": "Optimal_Control"}, {"arrows": "to", "from": "LikelihoodFunction", "title": "depends_on", "to": "OptimizationChallenges"}, {"arrows": "to", "from": "ProbabilityEstimation", "title": "subtopic", "to": "LaplaceSmoothing"}, {"arrows": "to", "from": "Markov_Decision_Processes_MDPs", "title": "related_to", "to": "Policy_Iteration"}, {"arrows": "to", "from": "Generalization_Error_Analysis", "title": "related_to", "to": "Machine_Learning_Papers"}, {"arrows": "to", "from": "Machine_Learning_Algorithms", "title": "has_subtopic", "to": "SMO_Algorithm"}, {"arrows": "to", "from": "KernelTrickIntroduction", "title": "subtopic", "to": "IterativeUpdateRule"}, {"arrows": "to", "from": "GaussianDiscriminantAnalysis", "title": "subtopic", "to": "LogLikelihood"}, {"arrows": "to", "from": "EMAlgorithms", "title": "contains", "to": "MixtureOfGaussians"}]);

                  nodeColors = {};
                  allNodes = nodes.get({ returnType: "Object" });
                  for (nodeId in allNodes) {
                    nodeColors[nodeId] = allNodes[nodeId].color;
                  }
                  allEdges = edges.get({ returnType: "Object" });
                  // adding nodes and edges to the graph
                  data = {nodes: nodes, edges: edges};

                  var options = {
    "configure": {
        "enabled": false
    },
    "edges": {
        "color": {
            "inherit": true
        },
        "smooth": {
            "enabled": true,
            "type": "dynamic"
        }
    },
    "interaction": {
        "dragNodes": true,
        "hideEdgesOnDrag": false,
        "hideNodesOnDrag": false
    },
    "physics": {
        "enabled": true,
        "repulsion": {
            "centralGravity": 0.1,
            "damping": 0.09,
            "nodeDistance": 200,
            "springConstant": 0.05,
            "springLength": 150
        },
        "solver": "repulsion",
        "stabilization": {
            "enabled": true,
            "fit": true,
            "iterations": 1000,
            "onlyDynamicEdges": false,
            "updateInterval": 50
        }
    }
};

                  


                  

                  network = new vis.Network(container, data, options);

                  

                  

                  


                  
                      network.on("stabilizationProgress", function(params) {
                          document.getElementById('loadingBar').removeAttribute("style");
                          var maxWidth = 496;
                          var minWidth = 20;
                          var widthFactor = params.iterations/params.total;
                          var width = Math.max(minWidth,maxWidth * widthFactor);
                          document.getElementById('bar').style.width = width + 'px';
                          document.getElementById('text').innerHTML = Math.round(widthFactor*100) + '%';
                      });
                      network.once("stabilizationIterationsDone", function() {
                          document.getElementById('text').innerHTML = '100%';
                          document.getElementById('bar').style.width = '496px';
                          document.getElementById('loadingBar').style.opacity = 0;
                          // really clean the dom element
                          setTimeout(function () {document.getElementById('loadingBar').style.display = 'none';}, 500);
                      });
                  

                  return network;

              }
              drawGraph();
        </script>
    </body>
</html>