<html>
    <head>
        <meta charset="utf-8">
        
            <script src="lib/bindings/utils.js"></script>
            <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/vis-network/9.1.2/dist/dist/vis-network.min.css" integrity="sha512-WgxfT5LWjfszlPHXRmBWHkV2eceiWTOBvrKCNbdgDYTHrT2AeLCGbF4sZlZw3UMN3WtL0tGUoIAKsu8mllg/XA==" crossorigin="anonymous" referrerpolicy="no-referrer" />
            <script src="https://cdnjs.cloudflare.com/ajax/libs/vis-network/9.1.2/dist/vis-network.min.js" integrity="sha512-LnvoEWDFrqGHlHmDD2101OrLcbsfkrzoSpvtSQtxK3RMnRV0eOkhhBN2dXHKRrUU8p2DGRTk35n4O8nWSVe1mQ==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
            
        
<center>
<h1>Machine Learning Knowledge Graph</h1>
</center>

<!-- <link rel="stylesheet" href="../node_modules/vis/dist/vis.min.css" type="text/css" />
<script type="text/javascript" src="../node_modules/vis/dist/vis.js"> </script>-->
        <link
          href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/css/bootstrap.min.css"
          rel="stylesheet"
          integrity="sha384-eOJMYsd53ii+scO/bJGFsiCZc+5NDVN2yr8+0RDqr0Ql0h+rP48ckxlpbzKgwra6"
          crossorigin="anonymous"
        />
        <script
          src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/js/bootstrap.bundle.min.js"
          integrity="sha384-JEW9xMcG8R+pH31jmWH6WWP0WintQrMb4s7ZOdauHnUtxwoG2vI5DkLtS3qm9Ekf"
          crossorigin="anonymous"
        ></script>


        <center>
          <h1>Machine Learning Knowledge Graph</h1>
        </center>
        <style type="text/css">

             #mynetwork {
                 width: 100%;
                 height: 95vh;
                 background-color: #ffffff;
                 border: 1px solid lightgray;
                 position: relative;
                 float: left;
             }

             
             #loadingBar {
                 position:absolute;
                 top:0px;
                 left:0px;
                 width: 100%;
                 height: 95vh;
                 background-color:rgba(200,200,200,0.8);
                 -webkit-transition: all 0.5s ease;
                 -moz-transition: all 0.5s ease;
                 -ms-transition: all 0.5s ease;
                 -o-transition: all 0.5s ease;
                 transition: all 0.5s ease;
                 opacity:1;
             }

             #bar {
                 position:absolute;
                 top:0px;
                 left:0px;
                 width:20px;
                 height:20px;
                 margin:auto auto auto auto;
                 border-radius:11px;
                 border:2px solid rgba(30,30,30,0.05);
                 background: rgb(0, 173, 246); /* Old browsers */
                 box-shadow: 2px 0px 4px rgba(0,0,0,0.4);
             }

             #border {
                 position:absolute;
                 top:10px;
                 left:10px;
                 width:500px;
                 height:23px;
                 margin:auto auto auto auto;
                 box-shadow: 0px 0px 4px rgba(0,0,0,0.2);
                 border-radius:10px;
             }

             #text {
                 position:absolute;
                 top:8px;
                 left:530px;
                 width:30px;
                 height:50px;
                 margin:auto auto auto auto;
                 font-size:22px;
                 color: #000000;
             }

             div.outerBorder {
                 position:relative;
                 top:400px;
                 width:600px;
                 height:44px;
                 margin:auto auto auto auto;
                 border:8px solid rgba(0,0,0,0.1);
                 background: rgb(252,252,252); /* Old browsers */
                 background: -moz-linear-gradient(top,  rgba(252,252,252,1) 0%, rgba(237,237,237,1) 100%); /* FF3.6+ */
                 background: -webkit-gradient(linear, left top, left bottom, color-stop(0%,rgba(252,252,252,1)), color-stop(100%,rgba(237,237,237,1))); /* Chrome,Safari4+ */
                 background: -webkit-linear-gradient(top,  rgba(252,252,252,1) 0%,rgba(237,237,237,1) 100%); /* Chrome10+,Safari5.1+ */
                 background: -o-linear-gradient(top,  rgba(252,252,252,1) 0%,rgba(237,237,237,1) 100%); /* Opera 11.10+ */
                 background: -ms-linear-gradient(top,  rgba(252,252,252,1) 0%,rgba(237,237,237,1) 100%); /* IE10+ */
                 background: linear-gradient(to bottom,  rgba(252,252,252,1) 0%,rgba(237,237,237,1) 100%); /* W3C */
                 filter: progid:DXImageTransform.Microsoft.gradient( startColorstr='#fcfcfc', endColorstr='#ededed',GradientType=0 ); /* IE6-9 */
                 border-radius:72px;
                 box-shadow: 0px 0px 10px rgba(0,0,0,0.2);
             }
             

             

             
        </style>
    </head>


    <body>
        <div class="card" style="width: 100%">
            
            
            <div id="mynetwork" class="card-body"></div>
        </div>

        
            <div id="loadingBar">
              <div class="outerBorder">
                <div id="text">0%</div>
                <div id="border">
                  <div id="bar"></div>
                </div>
              </div>
            </div>
        
        

        <script type="text/javascript">

              // initialize global variables.
              var edges;
              var nodes;
              var allNodes;
              var allEdges;
              var nodeColors;
              var originalNodes;
              var network;
              var container;
              var options, data;
              var filter = {
                  item : '',
                  property : '',
                  value : []
              };

              

              

              // This method is responsible for drawing the graph, returns the drawn network
              function drawGraph() {
                  var container = document.getElementById('mynetwork');

                  

                  // parsing and collecting nodes and edges from the python
                  nodes = new vis.DataSet([{"color": "#15c9d8", "id": "I Supervised learning", "label": "I Supervised learning", "shape": "star", "size": 25, "title": "Overview of supervised learning techniques and models."}, {"color": "#96f277", "id": "1 Linear regression", "label": "1 Linear regression", "shape": "dot", "size": 10, "title": "Introduction to linear regression including LMS algorithm and normal equations."}, {"color": "#acfe8f", "id": "1.1 LMS algorithm", "label": "1.1 LMS algorithm", "shape": "dot", "size": 10, "title": "Least Mean Squares algorithm for linear regression."}, {"color": "#6feb71", "id": "1.2 The normal equations", "label": "1.2 The normal equations", "shape": "dot", "size": 10, "title": "Derivation and use of the normal equation method in linear regression."}, {"color": "#f2adfe", "id": "1.2.1 Matrix derivatives", "label": "1.2.1 Matrix derivatives", "shape": "dot", "size": 10, "title": "Matrix calculus involved in deriving the normal equations."}, {"color": "#78c328", "id": "1.2.2 Least squares revisited", "label": "1.2.2 Least squares revisited", "shape": "dot", "size": 10, "title": "Revisiting least squares method with matrix algebra."}, {"color": "#21c584", "id": "1.3 Probabilistic interpretation", "label": "1.3 Probabilistic interpretation", "shape": "dot", "size": 10, "title": "Probabilistic view of linear regression models."}, {"color": "#f014c1", "id": "1.4 Locally weighted linear regression (optional reading)", "label": "1.4 Locally weighted linear regression (optional reading)", "shape": "dot", "size": 10, "title": "Advanced topic: locally weighted linear regression for non-stationary data."}, {"color": "#df5b43", "id": "2 Classification and logistic regression", "label": "2 Classification and logistic regression", "shape": "dot", "size": 10, "title": "Introduction to classification problems and logistic regression."}, {"color": "#5305cc", "id": "2.1 Logistic regression", "label": "2.1 Logistic regression", "shape": "dot", "size": 10, "title": "Logistic function for binary classification."}, {"color": "#eabed8", "id": "2.2 Digression: the perceptron learning algorithm", "label": "2.2 Digression: the perceptron learning algorithm", "shape": "dot", "size": 10, "title": "Perceptron algorithm as a precursor to modern neural networks."}, {"color": "#4d3586", "id": "2.3 Multi-class classification", "label": "2.3 Multi-class classification", "shape": "dot", "size": 10, "title": "Techniques for handling more than two classes in classification problems."}, {"color": "#9b594a", "id": "2.4 Another algorithm for maximizing \u03bb(\u03b8)", "label": "2.4 Another algorithm for maximizing \u03bb(\u03b8)", "shape": "dot", "size": 10, "title": "Alternative methods to maximize the likelihood function in logistic regression."}, {"color": "#44f0be", "id": "3 Generalized linear models", "label": "3 Generalized linear models", "shape": "dot", "size": 10, "title": "Introduction to generalized linear models (GLMs)."}, {"color": "#36d426", "id": "3.1 The exponential family", "label": "3.1 The exponential family", "shape": "dot", "size": 10, "title": "Overview of the exponential family in statistics."}, {"color": "#7b8ac9", "id": "3.2 Constructing GLMs", "label": "3.2 Constructing GLMs", "shape": "dot", "size": 10, "title": "Steps to construct generalized linear models from exponential families."}, {"color": "#b6be9a", "id": "3.2.1 Ordinary least squares", "label": "3.2.1 Ordinary least squares", "shape": "dot", "size": 10, "title": "Ordinary least squares as a special case of GLM for Gaussian distribution."}, {"color": "#af80b2", "id": "3.2.2 Logistic regression", "label": "3.2.2 Logistic regression", "shape": "dot", "size": 10, "title": "Logistic regression as an example of GLM for binary classification."}, {"color": "#885450", "id": "4 Generative learning algorithms", "label": "4 Generative learning algorithms", "shape": "dot", "size": 10, "title": "Introduction to generative models in machine learning."}, {"color": "#65cfe4", "id": "4.1 Gaussian discriminant analysis", "label": "4.1 Gaussian discriminant analysis", "shape": "dot", "size": 10, "title": "Gaussian Discriminant Analysis for classification tasks."}, {"color": "#194a1f", "id": "4.1.1 The multivariate normal distribution", "label": "4.1.1 The multivariate normal distribution", "shape": "dot", "size": 10, "title": "Properties and applications of the multivariate normal distribution."}, {"color": "#99c3ce", "id": "4.1.2 The Gaussian discriminant analysis model", "label": "4.1.2 The Gaussian discriminant analysis model", "shape": "dot", "size": 10, "title": "Model formulation for GDA using multivariate normals."}, {"color": "#1737f8", "id": "4.1.3 Discussion: GDA and logistic regression", "label": "4.1.3 Discussion: GDA and logistic regression", "shape": "dot", "size": 10, "title": "Comparison between GDA and logistic regression models."}, {"color": "#a77086", "id": "4.2 Naive bayes (Option Reading)", "label": "4.2 Naive bayes (Option Reading)", "shape": "dot", "size": 10, "title": "Naive Bayes classifier for text classification tasks."}, {"color": "#418c4a", "id": "4.2.1 Laplace smoothing", "label": "4.2.1 Laplace smoothing", "shape": "dot", "size": 10, "title": "Technique to handle zero probabilities in naive Bayes classifiers."}, {"color": "#e8336b", "id": "4.2.2 Event models for text classification", "label": "4.2.2 Event models for text classification", "shape": "dot", "size": 10, "title": "Models based on word events for document categorization."}, {"color": "#de911a", "id": "5 Kernel methods", "label": "5 Kernel methods", "shape": "dot", "size": 10, "title": "Techniques using kernel functions to extend linear models."}, {"color": "#3107cd", "id": "5.1 Feature maps", "label": "5.1 Feature maps", "shape": "dot", "size": 10, "title": "Mapping data into higher-dimensional feature spaces."}, {"color": "#2e8a19", "id": "5.2 LMS (least mean squares) with features", "label": "5.2 LMS (least mean squares) with features", "shape": "dot", "size": 10, "title": "LMS algorithm applied in the context of feature maps."}, {"color": "#e5f222", "id": "5.3 LMS with the kernel trick", "label": "5.3 LMS with the kernel trick", "shape": "dot", "size": 10, "title": "Using kernels to implicitly perform LMS in high-dimensional spaces."}, {"color": "#d2d13f", "id": "5.4 Properties of kernels", "label": "5.4 Properties of kernels", "shape": "dot", "size": 10, "title": "Mathematical properties and requirements for valid kernel functions."}, {"color": "#3c00b0", "id": "6 Support vector machines", "label": "6 Support vector machines", "shape": "dot", "size": 10, "title": "Introduction to support vector machines (SVMs) for classification tasks."}, {"color": "#6586d2", "id": "II Deep learning", "label": "II Deep learning", "shape": "star", "size": 25, "title": "Overview of deep learning techniques and neural networks."}, {"color": "#d4b551", "id": "7 Deep learning", "label": "7 Deep learning", "shape": "dot", "size": 10, "title": "Introduction to deep learning concepts and architectures."}, {"color": "#f4ef47", "id": "7.1 Supervised learning with non-linear models", "label": "7.1 Supervised learning with non-linear models", "shape": "dot", "size": 10, "title": "Supervised learning using non-linear models in deep learning."}, {"color": "#2f6069", "id": "7.2 Neural networks", "label": "7.2 Neural networks", "shape": "dot", "size": 10, "title": "Architecture and training of neural network models."}, {"color": "#082535", "id": "7.3 Modules in Modern Neural Networks", "label": "7.3 Modules in Modern Neural Networks", "shape": "dot", "size": 10, "title": "Discussion on various modules used in modern deep learning architectures."}, {"color": "#b42ed7", "id": "7.4 Backpropagation", "label": "7.4 Backpropagation", "shape": "dot", "size": 10, "title": "Algorithm for computing gradients in neural networks using backpropagation."}, {"color": "#d83af6", "id": "7.4.1 Preliminaries on partial derivatives", "label": "7.4.1 Preliminaries on partial derivatives", "shape": "dot", "size": 10, "title": "Basics of partial derivatives needed for understanding backpropagation."}, {"color": "#60ffac", "id": "7.4.2 General strategy of backpropagation", "label": "7.4.2 General strategy of backpropagation", "shape": "dot", "size": 10, "title": "Overview of the general approach to implementing backpropagation."}, {"color": "#582810", "id": "7.4.3 Backward functions for basic modules", "label": "7.4.3 Backward functions for basic modules", "shape": "dot", "size": 10, "title": "Derivation and implementation of backward functions for simple neural network layers."}, {"color": "#f5bada", "id": "7.4.4 Back-propagation for MLPs", "label": "7.4.4 Back-propagation for MLPs", "shape": "dot", "size": 10, "title": "Detailed explanation of backpropagation in multi-layer perceptrons (MLPs)."}, {"color": "#dae4ff", "id": "Neural Networks", "label": "Neural Networks", "shape": "star", "size": 25, "title": "Non-linear models using matrix multiplications and non-linear operations to solve complex problems."}, {"color": "#26a199", "id": "Modules in Modern Neural Networks", "label": "Modules in Modern Neural Networks", "shape": "dot", "size": 10, "title": "Discussion on the components used in modern neural networks."}, {"color": "#78fcb5", "id": "Backpropagation", "label": "Backpropagation", "shape": "dot", "size": 10, "title": "Introduction to backpropagation for efficient gradient computation in neural networks."}, {"color": "#77d56e", "id": "Preliminaries on partial derivatives", "label": "Preliminaries on partial derivatives", "shape": "dot", "size": 10, "title": "Introduction to the mathematical concepts needed for backpropagation."}, {"color": "#fa8957", "id": "General strategy of backpropagation", "label": "General strategy of backpropagation", "shape": "dot", "size": 10, "title": "Overview of the algorithmic steps in backpropagation."}, {"color": "#f4957b", "id": "Backward functions for basic modules", "label": "Backward functions for basic modules", "shape": "dot", "size": 10, "title": "Explanation of backward propagation for simple network components."}, {"color": "#f0bf69", "id": "Back-propagation for MLPs", "label": "Back-propagation for MLPs", "shape": "dot", "size": 10, "title": "Specific application of backpropagation to multi-layer perceptrons."}, {"color": "#d62cc1", "id": "Vectorization over training examples", "label": "Vectorization over training examples", "shape": "dot", "size": 10, "title": "Techniques for efficient computation in neural networks using vector operations."}, {"color": "#9cd38c", "id": "Generalization and regularization", "label": "Generalization and regularization", "shape": "star", "size": 25, "title": "Topics related to improving model performance on unseen data."}, {"color": "#74cbb5", "id": "Generalization", "label": "Generalization", "shape": "dot", "size": 10, "title": "Concepts and techniques for enhancing the generalizability of machine learning models."}, {"color": "#b1414d", "id": "Bias-variance tradeoff", "label": "Bias-variance tradeoff", "shape": "dot", "size": 10, "title": "Balancing model complexity to avoid overfitting or underfitting."}, {"color": "#79c054", "id": "A mathematical decomposition (for regression)", "label": "A mathematical decomposition (for regression)", "shape": "dot", "size": 10, "title": "Mathematical analysis of bias and variance in regression models."}, {"color": "#1299e8", "id": "The double descent phenomenon", "label": "The double descent phenomenon", "shape": "dot", "size": 10, "title": "Observation that model performance can improve after initial degradation with increased complexity."}, {"color": "#46c81b", "id": "Sample complexity bounds (optional readings)", "label": "Sample complexity bounds (optional readings)", "shape": "dot", "size": 10, "title": "Theoretical analysis of the number of samples needed for learning tasks."}, {"color": "#90a62e", "id": "Regularization and model selection", "label": "Regularization and model selection", "shape": "star", "size": 25, "title": "Techniques to prevent overfitting by penalizing complexity in models."}, {"color": "#8e2a96", "id": "Regularization", "label": "Regularization", "shape": "dot", "size": 10, "title": "Technique to control model complexity and prevent overfitting"}, {"color": "#9df833", "id": "Implicit regularization effect (optional reading)", "label": "Implicit regularization effect (optional reading)", "shape": "dot", "size": 10, "title": "Natural tendency of optimization algorithms to regularize models."}, {"color": "#ccae77", "id": "Model selection via cross validation", "label": "Model selection via cross validation", "shape": "dot", "size": 10, "title": "Procedure for choosing the best model based on performance metrics."}, {"color": "#f73a2d", "id": "Bayesian statistics and regularization", "label": "Bayesian statistics and regularization", "shape": "dot", "size": 10, "title": "Application of Bayesian principles to regularize models."}, {"color": "#15e95f", "id": "Unsupervised learning", "label": "Unsupervised learning", "shape": "star", "size": 25, "title": "Techniques for learning from data without labeled responses."}, {"color": "#9497b3", "id": "Clustering and the k-means algorithm", "label": "Clustering and the k-means algorithm", "shape": "dot", "size": 10, "title": "Algorithm for partitioning data into clusters based on similarity."}, {"color": "#edc967", "id": "EM algorithms", "label": "EM algorithms", "shape": "dot", "size": 10, "title": "Expectation-Maximization algorithm for parameter estimation in probabilistic models."}, {"color": "#6e4678", "id": "EM for mixture of Gaussians", "label": "EM for mixture of Gaussians", "shape": "dot", "size": 10, "title": "Application of EM to model data as a combination of Gaussian distributions."}, {"color": "#9c5b42", "id": "Jensen\u0027s inequality", "label": "Jensen\u0027s inequality", "shape": "dot", "size": 10, "title": "Mathematical result used in the derivation and application of the EM algorithm."}, {"color": "#8fbaad", "id": "General EM algorithms", "label": "General EM algorithms", "shape": "dot", "size": 10, "title": "Discussion on the general framework and variations of the EM algorithm."}, {"color": "#4b3f2e", "id": "Other interpretation of ELBO", "label": "Other interpretation of ELBO", "shape": "dot", "size": 10, "title": "Alternative understanding of the Evidence Lower Bound in variational inference."}, {"color": "#f60250", "id": "Mixture of Gaussians revisited", "label": "Mixture of Gaussians revisited", "shape": "dot", "size": 10, "title": "Re-examination and extension of mixture models using Gaussian distributions."}, {"color": "#93661a", "id": "Variational inference and variational auto-encoder (optional reading)", "label": "Variational inference and variational auto-encoder (optional reading)", "shape": "dot", "size": 10, "title": "Advanced topic on probabilistic modeling with latent variables."}, {"color": "#0b581a", "id": "Principal components analysis", "label": "Principal components analysis", "shape": "dot", "size": 10, "title": "Dimensionality reduction technique for identifying principal components in data."}, {"color": "#296e75", "id": "Independent components analysis", "label": "Independent components analysis", "shape": "dot", "size": 10, "title": "Technique to separate mixed signals into their independent sources."}, {"color": "#b541d4", "id": "ICA ambiguities", "label": "ICA ambiguities", "shape": "dot", "size": 10, "title": "Discussion on the inherent limitations and challenges in ICA."}, {"color": "#67621e", "id": "Densities and linear transformations", "label": "Densities and linear transformations", "shape": "dot", "size": 10, "title": "Mathematical foundations for understanding densities and transformations in ICA."}, {"color": "#32c8c9", "id": "ICA algorithm", "label": "ICA algorithm", "shape": "dot", "size": 10, "title": "Detailed explanation of the Independent Components Analysis procedure."}, {"color": "#d76ee8", "id": "Self-supervised learning and foundation models", "label": "Self-supervised learning and foundation models", "shape": "star", "size": 25, "title": "Techniques for training models on large datasets without explicit supervision."}, {"color": "#35b33f", "id": "Pretraining and adaptation", "label": "Pretraining and adaptation", "shape": "dot", "size": 10, "title": "Overview of pre-training methods and their role in model adaptation."}, {"color": "#347a19", "id": "Pretraining methods in computer vision", "label": "Pretraining methods in computer vision", "shape": "dot", "size": 10, "title": "Specific approaches to self-supervised learning for visual data."}, {"color": "#2bcc8f", "id": "Pretrained large language models", "label": "Pretrained large language models", "shape": "dot", "size": 10, "title": "Discussion on the development and applications of pre-trained language models."}, {"color": "#989562", "id": "Open up the blackbox of Transformers", "label": "Open up the blackbox of Transformers", "shape": "dot", "size": 10, "title": "Exploration of the architecture and workings of Transformer-based models."}, {"color": "#8d4ba9", "id": "Zero-shot learning and in-context learning", "label": "Zero-shot learning and in-context learning", "shape": "dot", "size": 10, "title": "Capabilities of models to perform tasks without prior training data."}, {"color": "#593522", "id": "Reinforcement Learning and Control", "label": "Reinforcement Learning and Control", "shape": "star", "size": 25, "title": "Techniques for agents to learn optimal actions through interaction with an environment."}, {"color": "#485ebc", "id": "Reinforcement learning", "label": "Reinforcement learning", "shape": "dot", "size": 10, "title": "Introduction to the principles of reinforcement learning."}, {"color": "#69207d", "id": "Markov decision processes", "label": "Markov decision processes", "shape": "dot", "size": 10, "title": "Mathematical framework for modeling sequential decision-making problems."}, {"color": "#e43e25", "id": "Value iteration and policy iteration", "label": "Value iteration and policy iteration", "shape": "dot", "size": 10, "title": "Algorithms for finding optimal policies in MDPs."}, {"color": "#569913", "id": "Learning a model for an MDP", "label": "Learning a model for an MDP", "shape": "dot", "size": 10, "title": "Techniques to estimate the transition dynamics of an environment."}, {"color": "#b146df", "id": "Continuous state MDPs", "label": "Continuous state MDPs", "shape": "dot", "size": 10, "title": "Discussion on handling continuous states in reinforcement learning problems."}, {"color": "#6d473c", "id": "Discretization", "label": "Discretization", "shape": "dot", "size": 10, "title": "Method to convert continuous state spaces into discrete ones for computation."}, {"color": "#00420b", "id": "Value function approximation", "label": "Value function approximation", "shape": "dot", "size": 10, "title": "Techniques for estimating value functions in large or continuous state spaces."}, {"color": "#341447", "id": "Connections between Policy and Value Iteration (Optional)", "label": "Connections between Policy and Value Iteration (Optional)", "shape": "dot", "size": 10, "title": "Theoretical insights into the relationship between policy and value iteration methods."}, {"color": "#6908e3", "id": "LQR, DDP and LQG", "label": "LQR, DDP and LQG", "shape": "star", "size": 25, "title": "Control theory concepts applied to reinforcement learning problems."}, {"color": "#9ccb01", "id": "Finite-horizon MDPs", "label": "Finite-horizon MDPs", "shape": "dot", "size": 10, "title": "Analysis of Markov decision processes with a fixed time horizon."}, {"color": "#e0350f", "id": "Linear Quadratic Regulation (LQR)", "label": "Linear Quadratic Regulation (LQR)", "shape": "dot", "size": 10, "title": "Optimal control problem for linear systems with quadratic cost functions"}, {"color": "#88e0ba", "id": "From non-linear dynamics to LQR", "label": "From non-linear dynamics to LQR", "shape": "dot", "size": 10, "title": "Approaches to apply LQR to nonlinear systems"}, {"color": "#b8ff79", "id": "Linearization of dynamics", "label": "Linearization of dynamics", "shape": "dot", "size": 10, "title": "Approximating nonlinear dynamics with linear models"}, {"color": "#59cc06", "id": "Differential Dynamic Programming (DDP)", "label": "Differential Dynamic Programming (DDP)", "shape": "dot", "size": 10, "title": "Optimization technique for nonlinear systems using differential dynamic programming"}, {"color": "#29526a", "id": "Linear Quadratic Gaussian (LQG)", "label": "Linear Quadratic Gaussian (LQG)", "shape": "dot", "size": 10, "title": "Combination of LQR with stochastic dynamics and noisy measurements"}, {"color": "#0faacc", "id": "Policy Gradient (REINFORCE)", "label": "Policy Gradient (REINFORCE)", "shape": "star", "size": 25, "title": "Method for learning policies in reinforcement learning using gradients"}, {"color": "#b389ae", "id": "Supervised Learning Overview", "label": "Supervised Learning Overview", "shape": "star", "size": 25, "title": "Introduction to supervised learning problems and concepts"}, {"color": "#20b2d9", "id": "Supervised Learning Problem", "label": "Supervised Learning Problem", "shape": "star", "size": 25, "title": "Formal description of the goal in supervised learning to predict y from x using a hypothesis h."}, {"color": "#9086c8", "id": "Regression", "label": "Regression", "shape": "dot", "size": 10, "title": "Type of supervised learning where target variable is continuous and needs to be predicted accurately."}, {"color": "#35e839", "id": "Classification", "label": "Classification", "shape": "dot", "size": 10, "title": "Type of supervised learning where target variable takes on discrete values, predicting one from several categories."}, {"color": "#81a88d", "id": "Linear Regression", "label": "Linear Regression", "shape": "star", "size": 25, "title": "Technique in machine learning for modeling the relationship between a scalar response and one or more explanatory variables using a linear predictor function."}, {"color": "#faa90d", "id": "Housing Example Dataset", "label": "Housing Example Dataset", "shape": "dot", "size": 10, "title": "Example dataset including living area, number of bedrooms, and price to illustrate regression problems."}, {"color": "#690143", "id": "Features Selection", "label": "Features Selection", "shape": "dot", "size": 10, "title": "Process of selecting which features (variables) are relevant for predicting the target variable in a model."}, {"color": "#bd2d58", "id": "Machine_Learning_Basics", "label": "Machine_Learning_Basics", "shape": "star", "size": 25, "title": "Introduction to fundamental concepts in machine learning."}, {"color": "#5d09ef", "id": "Function_Representation", "label": "Function_Representation", "shape": "dot", "size": 10, "title": "How functions and hypotheses are represented in machine learning models."}, {"color": "#0881fa", "id": "Linear_Functions", "label": "Linear_Functions", "shape": "dot", "size": 10, "title": "Approximating y as a linear function of x with parameters theta."}, {"color": "#c90a47", "id": "Parameters_Weights", "label": "Parameters_Weights", "shape": "dot", "size": 10, "title": "Theta values parameterizing the space of linear functions."}, {"color": "#0240f8", "id": "Cost_Function", "label": "Cost_Function", "shape": "dot", "size": 10, "title": "Measures how close h(x) is to y for training examples."}, {"color": "#29d74e", "id": "Ordinary_Least_Squares", "label": "Ordinary_Least_Squares", "shape": "dot", "size": 10, "title": "Least-squares cost function giving rise to OLS regression model."}, {"color": "#299f84", "id": "LMS_Algorithm", "label": "LMS_Algorithm", "shape": "star", "size": 25, "title": "Learning algorithm for minimizing the cost function J(theta)."}, {"color": "#3f27d5", "id": "GradientDescentAlgorithm", "label": "GradientDescentAlgorithm", "shape": "star", "size": 25, "title": "Optimization algorithm that iteratively adjusts parameters to minimize a cost function."}, {"color": "#e87fd7", "id": "LearningRate", "label": "LearningRate", "shape": "dot", "size": 10, "title": "Hyperparameter controlling the step size in gradient descent."}, {"color": "#903f76", "id": "CostFunctionJ", "label": "CostFunctionJ", "shape": "dot", "size": 10, "title": "Function that measures the error between predicted and actual values, to be minimized."}, {"color": "#50364f", "id": "PartialDerivativeCalculation", "label": "PartialDerivativeCalculation", "shape": "dot", "size": 10, "title": "Process of calculating partial derivatives for updating parameters in gradient descent."}, {"color": "#bef5e1", "id": "SingleTrainingExampleCase", "label": "SingleTrainingExampleCase", "shape": "dot", "size": 10, "title": "Specific case where the cost function is calculated for a single training example."}, {"color": "#5f907b", "id": "LMSUpdateRule", "label": "LMSUpdateRule", "shape": "dot", "size": 10, "title": "Update rule derived from least mean squares method, also known as Widrow-Hoff learning rule."}, {"color": "#2d5d51", "id": "LMS_Update_Rule", "label": "LMS_Update_Rule", "shape": "star", "size": 25, "title": "Update rule for adjusting parameters in machine learning models."}, {"color": "#301b7b", "id": "Widrow_Hoff_Learning_Rule", "label": "Widrow_Hoff_Learning_Rule", "shape": "dot", "size": 10, "title": "Alternative name for LMS update rule."}, {"color": "#155a48", "id": "Error_Term", "label": "Error_Term", "shape": "dot", "size": 10, "title": "Difference between actual and predicted values used to adjust parameters."}, {"color": "#c1dc70", "id": "Single_Training_Example", "label": "Single_Training_Example", "shape": "dot", "size": 10, "title": "Derivation of LMS rule for a single training example."}, {"color": "#0b48f5", "id": "Batch_Gradient_Descent", "label": "Batch_Gradient_Descent", "shape": "star", "size": 25, "title": "Method that considers all examples in the dataset at once to update parameters."}, {"color": "#04695b", "id": "Gradient_Descent", "label": "Gradient_Descent", "shape": "dot", "size": 10, "title": "Optimization algorithm that minimizes cost function by moving towards the steepest descent direction."}, {"color": "#07bf88", "id": "LinearRegressionOptimization", "label": "LinearRegressionOptimization", "shape": "star", "size": 25, "title": "Discusses the optimization problem in linear regression."}, {"color": "#f37680", "id": "GradientDescentConvergence", "label": "GradientDescentConvergence", "shape": "dot", "size": 10, "title": "Explains how gradient descent converges to a global minimum for convex functions."}, {"color": "#696cda", "id": "BatchGradientDescentExample", "label": "BatchGradientDescentExample", "shape": "dot", "size": 10, "title": "Provides an example of batch gradient descent with housing price prediction."}, {"color": "#632547", "id": "StochasticGradientDescent", "label": "StochasticGradientDescent", "shape": "star", "size": 25, "title": "Explanation and implementation of stochastic gradient descent for minimizing loss functions."}, {"color": "#d7b3d0", "id": "SGDAlgorithm", "label": "SGDAlgorithm", "shape": "dot", "size": 10, "title": "Presents the algorithm for updating parameters in stochastic gradient descent."}, {"color": "#44bb45", "id": "Machine_Learning_Optimization", "label": "Machine_Learning_Optimization", "shape": "star", "size": 25, "title": "Optimization techniques in machine learning including coordinate ascent and SVMs."}, {"color": "#8a933e", "id": "Stochastic_Gradient_Descent", "label": "Stochastic_Gradient_Descent", "shape": "dot", "size": 10, "title": "Algorithm for updating parameters based on single training example gradients"}, {"color": "#5c3896", "id": "Learning_Rate_Decay", "label": "Learning_Rate_Decay", "shape": "dot", "size": 10, "title": "Technique to ensure convergence by gradually reducing learning rate"}, {"color": "#987661", "id": "Normal_Equations_Method", "label": "Normal_Equations_Method", "shape": "star", "size": 25, "title": "Alternative method for minimizing cost function without iterative algorithms"}, {"color": "#374fe7", "id": "Matrix_Derivatives", "label": "Matrix_Derivatives", "shape": "dot", "size": 10, "title": "Notation and rules for differentiating functions of matrices"}, {"color": "#563a87", "id": "Matrix Derivatives", "label": "Matrix Derivatives", "shape": "star", "size": 25, "title": "Derivation of matrix derivatives for functions mapping matrices to real numbers."}, {"color": "#53258c", "id": "Gradient Calculation", "label": "Gradient Calculation", "shape": "dot", "size": 10, "title": "Calculation of gradients for a specific function involving matrix elements."}, {"color": "#6bf413", "id": "Least Squares Revisited", "label": "Least Squares Revisited", "shape": "star", "size": 25, "title": "Revisiting least squares problems using matrix derivatives and design matrices."}, {"color": "#c6005b", "id": "Design Matrix", "label": "Design Matrix", "shape": "dot", "size": 10, "title": "Matrix containing training examples\u0027 input values in its rows."}, {"color": "#a97728", "id": "Target Vector", "label": "Target Vector", "shape": "dot", "size": 10, "title": "Vector containing target values from the training set."}, {"color": "#78e0b9", "id": "MachineLearningOverview", "label": "MachineLearningOverview", "shape": "star", "size": 25, "title": "General overview of machine learning concepts and architectures."}, {"color": "#f52427", "id": "LinearRegression", "label": "LinearRegression", "shape": "dot", "size": 10, "title": "Algorithm for modeling the relationship between a scalar dependent variable y and one or more explanatory variables (or independent variables) denoted X."}, {"color": "#e20588", "id": "HypothesisFunction", "label": "HypothesisFunction", "shape": "dot", "size": 10, "title": "Formulation of the hypothesis function for classification using a sigmoid function."}, {"color": "#84f9e3", "id": "CostFunction", "label": "CostFunction", "shape": "dot", "size": 10, "title": "A measure of how well a hypothesis fits the data, aiming for minimization."}, {"color": "#a58ae5", "id": "GradientDescent", "label": "GradientDescent", "shape": "dot", "size": 10, "title": "An optimization algorithm to minimize cost functions by iteratively moving towards a minimum."}, {"color": "#083779", "id": "NormalEquations", "label": "NormalEquations", "shape": "dot", "size": 10, "title": "A direct method for finding the parameters that minimize the cost function without iteration."}, {"color": "#270010", "id": "Probabilistic Interpretation of Linear Regression", "label": "Probabilistic Interpretation of Linear Regression", "shape": "star", "size": 25, "title": "Explains the probabilistic assumptions behind least-squares regression."}, {"color": "#2614fe", "id": "Regression Problem", "label": "Regression Problem", "shape": "dot", "size": 10, "title": "Context for why linear regression is used in certain scenarios."}, {"color": "#8b6c07", "id": "Least-Squares Cost Function J", "label": "Least-Squares Cost Function J", "shape": "dot", "size": 10, "title": "Derivation and justification of the least-squares cost function."}, {"color": "#7278d8", "id": "Target Variables and Inputs Relation", "label": "Target Variables and Inputs Relation", "shape": "dot", "size": 10, "title": "Equation describing relationship between target variables and inputs with error term."}, {"color": "#0e4280", "id": "Error Term Distribution", "label": "Error Term Distribution", "shape": "dot", "size": 10, "title": "Assumption that the error terms are normally distributed with mean zero and variance sigma squared."}, {"color": "#c17996", "id": "Conditional Probability of y given x", "label": "Conditional Probability of y given x", "shape": "dot", "size": 10, "title": "Expression for the conditional probability density function of y given x and theta."}, {"color": "#823ad6", "id": "ProbabilisticModeling", "label": "ProbabilisticModeling", "shape": "dot", "size": 10, "title": "Models that use probability distributions to describe data generation processes."}, {"color": "#b9a661", "id": "ConditionalProbabilityDistribution", "label": "ConditionalProbabilityDistribution", "shape": "dot", "size": 10, "title": "Describes the distribution of one variable given another in a probabilistic model."}, {"color": "#109fdd", "id": "LikelihoodFunction", "label": "LikelihoodFunction", "shape": "dot", "size": 10, "title": "Mathematical function that measures how likely the data is given specific parameters."}, {"color": "#1ef02c", "id": "MaximumLikelihoodEstimation", "label": "MaximumLikelihoodEstimation", "shape": "dot", "size": 10, "title": "Method to estimate parameters by maximizing the likelihood of observing given data."}, {"color": "#408a9c", "id": "LogLikelihoodFunction", "label": "LogLikelihoodFunction", "shape": "dot", "size": 10, "title": "The natural logarithm of the likelihood function, often used for computational convenience."}, {"color": "#022ff1", "id": "MachineLearningConcepts", "label": "MachineLearningConcepts", "shape": "star", "size": 25, "title": "Overview of key concepts in machine learning including optimization methods and models."}, {"color": "#41ada0", "id": "LeastSquaresRegression", "label": "LeastSquaresRegression", "shape": "dot", "size": 10, "title": "Method for fitting a linear model to data by minimizing squared error."}, {"color": "#34fbd0", "id": "LocallyWeightedLinearRegression", "label": "LocallyWeightedLinearRegression", "shape": "star", "size": 25, "title": "Variant of linear regression that provides higher weight to training examples close to the query point x."}, {"color": "#171218", "id": "Underfitting", "label": "Underfitting", "shape": "dot", "size": 10, "title": "Explains the scenario where a model is too simple to capture underlying patterns in data."}, {"color": "#05c187", "id": "Overfitting", "label": "Overfitting", "shape": "dot", "size": 10, "title": "Model captures noise and details in training data too well, harming generalization."}, {"color": "#4edd32", "id": "FeatureSelection", "label": "FeatureSelection", "shape": "dot", "size": 10, "title": "Choosing relevant features to improve model performance."}, {"color": "#208f7d", "id": "FittingTheta", "label": "FittingTheta", "shape": "dot", "size": 10, "title": "Process of determining model parameters (theta) by minimizing error functions."}, {"color": "#79d4d2", "id": "WeightsInLWLR", "label": "WeightsInLWLR", "shape": "dot", "size": 10, "title": "Non-negative weights used in locally weighted linear regression to emphasize certain training examples over others based on proximity to the query point x."}, {"color": "#78a669", "id": "BandwidthParameter", "label": "BandwidthParameter", "shape": "dot", "size": 10, "title": "Controls how quickly the weight of a training example falls off with distance from the query point; denoted by tau."}, {"color": "#68518d", "id": "Machine Learning Overview", "label": "Machine Learning Overview", "shape": "star", "size": 25, "title": "Introduction to machine learning concepts and algorithms."}, {"color": "#b5187c", "id": "Locally Weighted Linear Regression", "label": "Locally Weighted Linear Regression", "shape": "dot", "size": 10, "title": "A non-parametric algorithm that uses weighted linear regression for predictions."}, {"color": "#6f7a7e", "id": "Non-Parametric Algorithms", "label": "Non-Parametric Algorithms", "shape": "dot", "size": 10, "title": "Algorithms where the amount of data needed to represent hypotheses grows with training set size."}, {"color": "#bc9142", "id": "Parametric Algorithms", "label": "Parametric Algorithms", "shape": "dot", "size": 10, "title": "Algorithms that have a fixed number of parameters independent of training set size."}, {"color": "#43bb3e", "id": "Classification Problem", "label": "Classification Problem", "shape": "star", "size": 25, "title": "A machine learning task where the output variable is categorical and discrete-valued."}, {"color": "#9e54f9", "id": "Binary Classification", "label": "Binary Classification", "shape": "dot", "size": 10, "title": "A classification problem with two possible outcomes (0 or 1)."}, {"color": "#38cb69", "id": "Logistic Regression", "label": "Logistic Regression", "shape": "star", "size": 25, "title": "Application of generalized Newton\u0027s method in logistic regression for multidimensional settings."}, {"color": "#c129ce", "id": "Spam Classification Example", "label": "Spam Classification Example", "shape": "dot", "size": 10, "title": "Example of using logistic regression to classify emails as spam or not spam."}, {"color": "#e1cd9e", "id": "MachineLearning", "label": "MachineLearning", "shape": "star", "size": 25, "title": "Field of study focusing on algorithms that learn from and make predictions on data."}, {"color": "#3f67ad", "id": "ClassificationProblem", "label": "ClassificationProblem", "shape": "dot", "size": 10, "title": "Explanation of classification problems including binary and multi-class scenarios."}, {"color": "#ed41d9", "id": "LogisticRegression", "label": "LogisticRegression", "shape": "dot", "size": 10, "title": "A discriminative algorithm used to model the probability of a binary outcome given input variables."}, {"color": "#a28cee", "id": "LogisticFunction", "label": "LogisticFunction", "shape": "dot", "size": 10, "title": "Use of logistic function to convert logits into probabilities for binary classification."}, {"color": "#68693d", "id": "DerivativeOfSigmoid", "label": "DerivativeOfSigmoid", "shape": "dot", "size": 10, "title": "Mathematical expression describing the rate of change of the logistic function."}, {"color": "#d38f4b", "id": "MachineLearningModels", "label": "MachineLearningModels", "shape": "star", "size": 25, "title": "Overview of models used in machine learning including regression and classification."}, {"color": "#d4cc86", "id": "ClassificationModelAssumptions", "label": "ClassificationModelAssumptions", "shape": "dot", "size": 10, "title": "Probabilistic assumptions made for a binary classification model."}, {"color": "#16d131", "id": "LogLikelihood", "label": "LogLikelihood", "shape": "dot", "size": 10, "title": "Derivation and use of log-likelihood for easier optimization."}, {"color": "#644d00", "id": "GradientAscent", "label": "GradientAscent", "shape": "dot", "size": 10, "title": "Method to maximize the likelihood using gradient ascent updates."}, {"color": "#ee46c4", "id": "GradientAscentRule", "label": "GradientAscentRule", "shape": "dot", "size": 10, "title": "Derivation of the gradient ascent rule for updating parameters in logistic regression."}, {"color": "#3c480a", "id": "StochasticGradientAscent", "label": "StochasticGradientAscent", "shape": "dot", "size": 10, "title": "Update rule using stochastic gradient ascent method."}, {"color": "#4d6a00", "id": "LogisticLossFunction", "label": "LogisticLossFunction", "shape": "dot", "size": 10, "title": "Definition and properties of the logistic loss function used in logistic regression."}, {"color": "#94e853", "id": "NegativeLogLikelihood", "label": "NegativeLogLikelihood", "shape": "dot", "size": 10, "title": "Expression for negative log-likelihood in terms of logistic loss function."}, {"color": "#827b5d", "id": "Machine_Learning_Concepts", "label": "Machine_Learning_Concepts", "shape": "star", "size": 25, "title": "Overview of key concepts in machine learning including loss functions and optimization."}, {"color": "#bb8838", "id": "Logistic_Regression_Gradient_Descent", "label": "Logistic_Regression_Gradient_Descent", "shape": "dot", "size": 10, "title": "Derivation and application of gradient descent for logistic regression."}, {"color": "#3dc6f7", "id": "Perceptron_Algorithm", "label": "Perceptron_Algorithm", "shape": "dot", "size": 10, "title": "Historical learning algorithm that outputs binary values."}, {"color": "#9f66c9", "id": "Multi_Class_Classification", "label": "Multi_Class_Classification", "shape": "dot", "size": 10, "title": "Classification problems where the response variable can take on multiple discrete values."}, {"color": "#518e71", "id": "Multi-classClassification", "label": "Multi-classClassification", "shape": "dot", "size": 10, "title": "Extension of binary classification to more than two classes."}, {"color": "#a62184", "id": "ResponseVariable", "label": "ResponseVariable", "shape": "dot", "size": 10, "title": "Discrete variable that can take on any one of k values."}, {"color": "#084707", "id": "MultinomialDistribution", "label": "MultinomialDistribution", "shape": "dot", "size": 10, "title": "Probability distribution over k possible outcomes."}, {"color": "#f85be5", "id": "ParameterizedModel", "label": "ParameterizedModel", "shape": "dot", "size": 10, "title": "Models that output probabilities for each class given input x."}, {"color": "#a38d40", "id": "SoftmaxFunction", "label": "SoftmaxFunction", "shape": "dot", "size": 10, "title": "Transformation of logits into a probability distribution over multiple classes."}, {"color": "#ae7d65", "id": "Softmax_Function", "label": "Softmax_Function", "shape": "dot", "size": 10, "title": "Converts logits into probabilities for each possible value."}, {"color": "#9e6ca5", "id": "Logits", "label": "Logits", "shape": "dot", "size": 10, "title": "Inputs to the softmax function, often represented as a vector of real numbers."}, {"color": "#cf3c6b", "id": "Probability_Vector_Output", "label": "Probability_Vector_Output", "shape": "dot", "size": 10, "title": "Output probabilities that sum up to 1."}, {"color": "#6b952a", "id": "Probabilistic_Model", "label": "Probabilistic_Model", "shape": "dot", "size": 10, "title": "Model using softmax output as conditional probabilities."}, {"color": "#f277ab", "id": "Negative_Log_Likelihood", "label": "Negative_Log_Likelihood", "shape": "dot", "size": 10, "title": "Loss function for evaluating model performance based on log-likelihood."}, {"color": "#a2bfbd", "id": "Cross_Entropy_Loss", "label": "Cross_Entropy_Loss", "shape": "dot", "size": 10, "title": "A loss function that measures the dissimilarity between two probability distributions."}, {"color": "#55ae23", "id": "Loss_Functions", "label": "Loss_Functions", "shape": "dot", "size": 10, "title": "Functions used to evaluate the performance of a model during training."}, {"color": "#2f54bd", "id": "Softmax_Cross_Entropy_Loss", "label": "Softmax_Cross_Entropy_Loss", "shape": "dot", "size": 10, "title": "Specific form of cross-entropy loss used in multi-class classification problems."}, {"color": "#7a991a", "id": "Gradient_Computation", "label": "Gradient_Computation", "shape": "dot", "size": 10, "title": "Computation of gradients with respect to intermediate variables and parameters."}, {"color": "#0bde69", "id": "LossFunction", "label": "LossFunction", "shape": "dot", "size": 10, "title": "Mathematical function used to measure the performance of a model and guide its training."}, {"color": "#52fcdc", "id": "CrossEntropyLoss", "label": "CrossEntropyLoss", "shape": "dot", "size": 10, "title": "Explanation of cross-entropy loss function used for classification tasks."}, {"color": "#4c5b77", "id": "GradientCalculation", "label": "GradientCalculation", "shape": "dot", "size": 10, "title": "Process of calculating gradients with respect to parameters using the chain rule."}, {"color": "#30b9ac", "id": "NewtonMethod", "label": "NewtonMethod", "shape": "dot", "size": 10, "title": "Explanation of Newton\u0027s method for finding zeros of a function and its use in optimization problems."}, {"color": "#140a71", "id": "Newton\u0027s Method", "label": "Newton\u0027s Method", "shape": "star", "size": 25, "title": "An iterative optimization algorithm for finding roots of a real-valued function."}, {"color": "#8c6a45", "id": "Finding Roots", "label": "Finding Roots", "shape": "dot", "size": 10, "title": "The process of using Newton\u0027s method to find the value of \u03b8 where f(\u03b8) = 0."}, {"color": "#98e093", "id": "Maximizing Functions", "label": "Maximizing Functions", "shape": "dot", "size": 10, "title": "Using Newton\u0027s method to maximize a function by finding points where its first derivative is zero."}, {"color": "#aeb718", "id": "Hessian Matrix", "label": "Hessian Matrix", "shape": "dot", "size": 10, "title": "A matrix used in the Newton-Raphson method, containing second derivatives of \u039b(\u03b8) with respect to \u03b8_i and \u03b8_j."}, {"color": "#01b231", "id": "Gradient Descent", "label": "Gradient Descent", "shape": "dot", "size": 10, "title": "An optimization algorithm that uses the gradient of a function to find its minimum or maximum values."}, {"color": "#817484", "id": "OptimizationMethods", "label": "OptimizationMethods", "shape": "dot", "size": 10, "title": "Techniques used to optimize the parameters of a model."}, {"color": "#1e7983", "id": "FisherScoring", "label": "FisherScoring", "shape": "dot", "size": 10, "title": "Application of Newton\u0027s method to logistic regression likelihood function maximization."}, {"color": "#f330e6", "id": "GeneralizedLinearModels", "label": "GeneralizedLinearModels", "shape": "star", "size": 25, "title": "Introduction to GLMs for solving problems involving exponential family distributions."}, {"color": "#4edcf5", "id": "ExponentialFamilyDistributions", "label": "ExponentialFamilyDistributions", "shape": "dot", "size": 10, "title": "Overview of distributions that belong to the exponential family, including Poisson and Gamma distributions."}, {"color": "#1cd126", "id": "NaturalParameter", "label": "NaturalParameter", "shape": "dot", "size": 10, "title": "The parameter \u03b7 that defines an exponential family distribution."}, {"color": "#e18270", "id": "SufficientStatistic", "label": "SufficientStatistic", "shape": "dot", "size": 10, "title": "A statistic T(y) that captures all the information about a parameter in a given sample."}, {"color": "#91fef4", "id": "LogPartitionFunction", "label": "LogPartitionFunction", "shape": "dot", "size": 10, "title": "Ensures the distribution sums/integrates to 1 over y."}, {"color": "#7d60da", "id": "BernoulliDistribution", "label": "BernoulliDistribution", "shape": "star", "size": 25, "title": "Formulation of Bernoulli distribution within the exponential family framework."}, {"color": "#d6df97", "id": "NaturalParameterForBernoulli", "label": "NaturalParameterForBernoulli", "shape": "dot", "size": 10, "title": "Defined as log(\u03c6/(1-\u03c6))."}, {"color": "#5e6635", "id": "SufficientStatisticForBernoulli", "label": "SufficientStatisticForBernoulli", "shape": "dot", "size": 10, "title": "T(y) = y, the outcome itself."}, {"color": "#f31df1", "id": "LogPartitionFunctionForBernoulli", "label": "LogPartitionFunctionForBernoulli", "shape": "dot", "size": 10, "title": "a(\u03b7) = log(1+e^\u03b7)."}, {"color": "#fa0d71", "id": "SigmoidFunction", "label": "SigmoidFunction", "shape": "star", "size": 25, "title": "Use of sigmoid function as a default cdf for ICA due to its monotonic increase from 0 to 1."}, {"color": "#a1a65e", "id": "LogisticRegressionAsGLM", "label": "LogisticRegressionAsGLM", "shape": "dot", "size": 10, "title": "Explanation of logistic regression as a Generalized Linear Model (GLM)."}, {"color": "#a0aeba", "id": "GaussianDistribution", "label": "GaussianDistribution", "shape": "dot", "size": 10, "title": "Explanation of Gaussian distribution as a member of the exponential family used in linear regression."}, {"color": "#781c75", "id": "GLMConstruction", "label": "GLMConstruction", "shape": "dot", "size": 10, "title": "General method for constructing GLMs using various distributions."}, {"color": "#e67db3", "id": "PoissonDistributionModeling", "label": "PoissonDistributionModeling", "shape": "dot", "size": 10, "title": "Using Poisson distribution for modeling website visitor counts based on features like promotions and weather."}, {"color": "#3835a8", "id": "GLMAssumptions", "label": "GLMAssumptions", "shape": "dot", "size": 10, "title": "Three key assumptions/design choices for constructing GLM models."}, {"color": "#826b18", "id": "OrdinaryLeastSquares", "label": "OrdinaryLeastSquares", "shape": "dot", "size": 10, "title": "Regression method for continuous target variables modeled as Gaussian distributions."}, {"color": "#de4a60", "id": "Assumption1", "label": "Assumption1", "shape": "dot", "size": 10, "title": "First assumption in GLM formulation, relating mean and natural parameter."}, {"color": "#36c979", "id": "Assumption2", "label": "Assumption2", "shape": "dot", "size": 10, "title": "Second assumption about the expected value of y given x."}, {"color": "#5eec22", "id": "Assumption3", "label": "Assumption3", "shape": "dot", "size": 10, "title": "Third assumption in GLM formulation, relating parameters to input features."}, {"color": "#147d22", "id": "ExponentialFamilyDistribution", "label": "ExponentialFamilyDistribution", "shape": "dot", "size": 10, "title": "A family of probability distributions that includes Gaussian and Bernoulli distributions."}, {"color": "#44fc0c", "id": "Conditional_Distribution_Modeling", "label": "Conditional_Distribution_Modeling", "shape": "dot", "size": 10, "title": "Techniques for modeling the conditional distribution p(y|x;\u03b8)."}, {"color": "#f55f3b", "id": "Bernoulli_Distribution", "label": "Bernoulli_Distribution", "shape": "dot", "size": 10, "title": "Binary-valued random variable distribution used in logistic regression."}, {"color": "#3212ed", "id": "Exponential_Family_Distributions", "label": "Exponential_Family_Distributions", "shape": "dot", "size": 10, "title": "Family of distributions including Bernoulli and Gaussian, characterized by natural parameters."}, {"color": "#6678d7", "id": "Hypothesis_Functions", "label": "Hypothesis_Functions", "shape": "dot", "size": 10, "title": "Functions used to predict the expected value of y given x in machine learning models."}, {"color": "#02db2d", "id": "Logistic_Function", "label": "Logistic_Function", "shape": "dot", "size": 10, "title": "Sigmoid function derived from Bernoulli distribution for predicting probabilities."}, {"color": "#ccd2d4", "id": "Canonical_Response_Function", "label": "Canonical_Response_Function", "shape": "dot", "size": 10, "title": "Function mapping natural parameters to the expected value of a random variable."}, {"color": "#c138a9", "id": "Canonical_Link_Function", "label": "Canonical_Link_Function", "shape": "dot", "size": 10, "title": "Inverse function of canonical response, relating observed data to model parameters."}, {"color": "#0ed15d", "id": "MachineLearningAlgorithms", "label": "MachineLearningAlgorithms", "shape": "star", "size": 25, "title": "Overview of machine learning algorithms as optimization search in model space."}, {"color": "#e4e762", "id": "DiscriminativeLearning", "label": "DiscriminativeLearning", "shape": "dot", "size": 10, "title": "Algorithms that learn p(y|x) directly or map inputs to labels."}, {"color": "#2fd857", "id": "GenerativeLearning", "label": "GenerativeLearning", "shape": "dot", "size": 10, "title": "Algorithms that model p(x|y) and p(y) to derive p(y|x)."}, {"color": "#47fb3e", "id": "ConditionalDistribution", "label": "ConditionalDistribution", "shape": "dot", "size": 10, "title": "p(y|x;\u03b8), the conditional distribution of y given x."}, {"color": "#d41c0a", "id": "PerceptronAlgorithm", "label": "PerceptronAlgorithm", "shape": "dot", "size": 10, "title": "Attempts to find a decision boundary between classes."}, {"color": "#825d0b", "id": "DecisionBoundary", "label": "DecisionBoundary", "shape": "dot", "size": 10, "title": "Line that separates regions where different classes are predicted as most likely."}, {"color": "#51d7f4", "id": "ClassPriors", "label": "ClassPriors", "shape": "dot", "size": 10, "title": "p(y), the prior probability of each class before observing data."}, {"color": "#c38e8c", "id": "ConditionalProbabilityModel", "label": "ConditionalProbabilityModel", "shape": "dot", "size": 10, "title": "Models p(x|y) for each class to understand feature distributions."}, {"color": "#fa651b", "id": "BayesRule", "label": "BayesRule", "shape": "dot", "size": 10, "title": "Uses prior probabilities and conditional probabilities to calculate posterior distribution p(y|x)."}, {"color": "#a9d16d", "id": "Bayesian Classification", "label": "Bayesian Classification", "shape": "star", "size": 25, "title": "Classification using Bayes\u0027 theorem to calculate posterior probabilities."}, {"color": "#221553", "id": "Class Priors", "label": "Class Priors", "shape": "dot", "size": 10, "title": "Prior probability of each class before observing data."}, {"color": "#21b691", "id": "Conditional Probability p(x|y)", "label": "Conditional Probability p(x|y)", "shape": "dot", "size": 10, "title": "Probability distribution of features given the class label."}, {"color": "#4030ee", "id": "Posterior Distribution p(y|x)", "label": "Posterior Distribution p(y|x)", "shape": "dot", "size": 10, "title": "Derived using Bayes\u0027 rule to predict class labels based on observed data."}, {"color": "#7a81a0", "id": "Gaussian Discriminant Analysis (GDA)", "label": "Gaussian Discriminant Analysis (GDA)", "shape": "star", "size": 25, "title": "Generative learning algorithm assuming multivariate normal distribution for p(x|y)."}, {"color": "#70cc9a", "id": "Multivariate Normal Distribution", "label": "Multivariate Normal Distribution", "shape": "dot", "size": 10, "title": "Distribution parameterized by mean vector and covariance matrix."}, {"color": "#5ff6bb", "id": "Mean Vector", "label": "Mean Vector", "shape": "dot", "size": 10, "title": "Vector representing the expected value of a multivariate normal distribution."}, {"color": "#af6321", "id": "Covariance Matrix", "label": "Covariance Matrix", "shape": "dot", "size": 10, "title": "Matrix describing the variance and covariance between variables in a multivariate normal distribution."}, {"color": "#e2aa48", "id": "MachineLearningBasics", "label": "MachineLearningBasics", "shape": "star", "size": 25, "title": "Fundamental concepts in machine learning including probability distributions and random variables."}, {"color": "#d600a9", "id": "MeanOfGaussian", "label": "MeanOfGaussian", "shape": "dot", "size": 10, "title": "The expected value or mean of a Gaussian distribution is given by \u03bc."}, {"color": "#a3c88d", "id": "CovarianceMatrix", "label": "CovarianceMatrix", "shape": "dot", "size": 10, "title": "A matrix that generalizes the notion of variance to multiple dimensions for random vectors."}, {"color": "#d30914", "id": "StandardNormalDistribution", "label": "StandardNormalDistribution", "shape": "dot", "size": 10, "title": "A Gaussian distribution with zero mean and identity covariance matrix, representing a standard case."}, {"color": "#9ea415", "id": "DensityExamples", "label": "DensityExamples", "shape": "dot", "size": 10, "title": "Illustrative examples of how changes in the covariance matrix affect the shape of the Gaussian density function."}, {"color": "#2dc871", "id": "MultivariateNormalDistribution", "label": "MultivariateNormalDistribution", "shape": "dot", "size": 10, "title": "Exploration of multivariate normal distribution properties and examples."}, {"color": "#fb6513", "id": "CovarianceMatrixImpact", "label": "CovarianceMatrixImpact", "shape": "dot", "size": 10, "title": "Effect of covariance matrix on the density contours."}, {"color": "#1d65d8", "id": "MeanVectorMovement", "label": "MeanVectorMovement", "shape": "dot", "size": 10, "title": "Impact of mean vector changes when covariance is fixed."}, {"color": "#1d71b5", "id": "GaussianDiscriminantAnalysisModel", "label": "GaussianDiscriminantAnalysisModel", "shape": "star", "size": 25, "title": "Introduction to the Gaussian Discriminant Analysis model for classification problems."}, {"color": "#f0d5d6", "id": "MultivariateNormalForClasses", "label": "MultivariateNormalForClasses", "shape": "dot", "size": 10, "title": "Using multivariate normal distributions to model class-conditional probabilities."}, {"color": "#12ccfd", "id": "GaussianDiscriminantAnalysis(GDA)", "label": "GaussianDiscriminantAnalysis(GDA)", "shape": "dot", "size": 10, "title": "Model that uses Gaussian distributions to classify data points."}, {"color": "#f89f7e", "id": "ParametersEstimation", "label": "ParametersEstimation", "shape": "dot", "size": 10, "title": "Process of finding values for model parameters to maximize likelihood."}, {"color": "#0ff82c", "id": "GaussianDiscriminantAnalysis", "label": "GaussianDiscriminantAnalysis", "shape": "dot", "size": 10, "title": "A probabilistic model for classification tasks based on the assumption that the data is generated from a Gaussian distribution."}, {"color": "#135d79", "id": "DecisionBoundaries", "label": "DecisionBoundaries", "shape": "dot", "size": 10, "title": "The boundary that separates different classes in classification models."}, {"color": "#b8b4c5", "id": "ModelAssumptions", "label": "ModelAssumptions", "shape": "dot", "size": 10, "title": "The underlying assumptions made by machine learning models about the data distribution."}, {"color": "#89715b", "id": "AsymptoticEfficiency", "label": "AsymptoticEfficiency", "shape": "dot", "size": 10, "title": "Property of GDA that ensures optimal performance in large datasets when model assumptions are correct."}, {"color": "#e12772", "id": "GDA", "label": "GDA", "shape": "dot", "size": 10, "title": "Generative Discriminative Algorithm with strong modeling assumptions."}, {"color": "#b6c56d", "id": "Robustness", "label": "Robustness", "shape": "dot", "size": 10, "title": "Strength of logistic regression in handling non-Gaussian data."}, {"color": "#404a51", "id": "NaiveBayes", "label": "NaiveBayes", "shape": "star", "size": 25, "title": "A generative learning algorithm that works well for many classification problems, including text classification with modifications."}, {"color": "#170ed1", "id": "DiscreteFeatures", "label": "DiscreteFeatures", "shape": "dot", "size": 10, "title": "Handling of discrete-valued feature vectors in Naive Bayes algorithm."}, {"color": "#f18511", "id": "Machine_Learning", "label": "Machine_Learning", "shape": "star", "size": 25, "title": "Field of study focusing on algorithms that learn from and make predictions on data."}, {"color": "#9c9f8b", "id": "Text_Classification", "label": "Text_Classification", "shape": "dot", "size": 10, "title": "Techniques for classifying text into predefined categories."}, {"color": "#b2d4c1", "id": "Spam_Filtering", "label": "Spam_Filtering", "shape": "dot", "size": 10, "title": "Application of machine learning to classify emails as spam or non-spam."}, {"color": "#c8d24e", "id": "Training_Set", "label": "Training_Set", "shape": "dot", "size": 10, "title": "Set of labeled examples used for training a model."}, {"color": "#82c177", "id": "Feature_Vector", "label": "Feature_Vector", "shape": "dot", "size": 10, "title": "Vector representation of an email based on its content words."}, {"color": "#7bb5dd", "id": "Vocabulary", "label": "Vocabulary", "shape": "dot", "size": 10, "title": "Set of unique words used to construct the feature vector."}, {"color": "#a55f50", "id": "Stop_Words", "label": "Stop_Words", "shape": "dot", "size": 10, "title": "Commonly excluded high-frequency words in text processing."}, {"color": "#690fcf", "id": "Machine_Learning_Topic", "label": "Machine_Learning_Topic", "shape": "star", "size": 25, "title": "Overview of machine learning concepts and algorithms."}, {"color": "#e75b42", "id": "Feature_Vector_Selection", "label": "Feature_Vector_Selection", "shape": "dot", "size": 10, "title": "Choosing relevant features from the text data, excluding stop words."}, {"color": "#9ab83b", "id": "Generative_Modeling", "label": "Generative_Modeling", "shape": "dot", "size": 10, "title": "Building models that generate probabilities for feature vectors given a class label."}, {"color": "#95015a", "id": "Naive_Bayes_Assumption", "label": "Naive_Bayes_Assumption", "shape": "dot", "size": 10, "title": "Assumption of conditional independence among features given the class label."}, {"color": "#76357a", "id": "Naive_Bayes_Classifier", "label": "Naive_Bayes_Classifier", "shape": "dot", "size": 10, "title": "Algorithm for classification based on Bayes\u0027 theorem with strong independence assumptions."}, {"color": "#aa5f0c", "id": "NaiveBayesAlgorithm", "label": "NaiveBayesAlgorithm", "shape": "star", "size": 25, "title": "A probabilistic classifier based on applying Bayes\u0027 theorem with strong independence assumptions between the features."}, {"color": "#e6905d", "id": "ConditionalProbability", "label": "ConditionalProbability", "shape": "dot", "size": 10, "title": "The probability of a token given the preceding tokens in a sequence."}, {"color": "#1ba494", "id": "JointLikelihood", "label": "JointLikelihood", "shape": "dot", "size": 10, "title": "Probability of observing the data given model parameters."}, {"color": "#ebaf7e", "id": "BinaryFeatures", "label": "BinaryFeatures", "shape": "dot", "size": 10, "title": "Features are binary-valued in the context of spam email classification."}, {"color": "#7af460", "id": "ParameterEstimation", "label": "ParameterEstimation", "shape": "dot", "size": 10, "title": "Process of estimating parameters from training data in the context of the multinomial model"}, {"color": "#000002", "id": "Machine Learning Algorithms", "label": "Machine Learning Algorithms", "shape": "star", "size": 25, "title": "Collection of algorithms used in machine learning for model training and optimization."}, {"color": "#1e7c05", "id": "Naive Bayes Algorithm", "label": "Naive Bayes Algorithm", "shape": "dot", "size": 10, "title": "Probabilistic classifier based on applying Bayes\u0027 theorem with strong independence assumptions between the features"}, {"color": "#0491b5", "id": "Binary Features", "label": "Binary Features", "shape": "dot", "size": 10, "title": "Features that can take only two values, typically 0 or 1"}, {"color": "#936a05", "id": "Multinomial Distribution", "label": "Multinomial Distribution", "shape": "dot", "size": 10, "title": "Probability distribution of the outcomes of a fixed number of independent trials with different possible outcomes for each trial"}, {"color": "#d94f0b", "id": "Feature Representation", "label": "Feature Representation", "shape": "dot", "size": 10, "title": "Method for representing features in a dataset, such as using intervals for continuous variables"}, {"color": "#27c66c", "id": "Laplace Smoothing", "label": "Laplace Smoothing", "shape": "dot", "size": 10, "title": "Technique to prevent zero probability estimates by adding a small constant to the counts of each feature value"}, {"color": "#8e30d7", "id": "MachineLearningConferences", "label": "MachineLearningConferences", "shape": "star", "size": 25, "title": "Top machine learning conferences including NeurIPS."}, {"color": "#6da1fd", "id": "NeurIPS20xxSubmission", "label": "NeurIPS20xxSubmission", "shape": "dot", "size": 10, "title": "Submitting work to the NeurIPS conference in May 20xx."}, {"color": "#0311ed", "id": "NaiveBayesFilter", "label": "NaiveBayesFilter", "shape": "star", "size": 25, "title": "Description of a Naive Bayes spam filter and its limitations."}, {"color": "#91c972", "id": "NewWordDetection", "label": "NewWordDetection", "shape": "dot", "size": 10, "title": "Issues with detecting new words like \u0027neurips\u0027 in the training set."}, {"color": "#1efc78", "id": "Probability_Estimation", "label": "Probability_Estimation", "shape": "dot", "size": 10, "title": "Techniques for estimating probabilities from data samples."}, {"color": "#e05cab", "id": "Maximum_Likelihood_Estimates", "label": "Maximum_Likelihood_Estimates", "shape": "dot", "size": 10, "title": "Estimating parameters based on observed frequencies in a dataset."}, {"color": "#c92de5", "id": "Laplace_Smoothing", "label": "Laplace_Smoothing", "shape": "dot", "size": 10, "title": "Adjusting estimates to avoid zero probabilities by adding small constants."}, {"color": "#5923b8", "id": "Event_Models_Text_Classification", "label": "Event_Models_Text_Classification", "shape": "dot", "size": 10, "title": "Models used in text classification tasks, incorporating Laplace smoothing."}, {"color": "#ea8693", "id": "EventModelsForTextClassification", "label": "EventModelsForTextClassification", "shape": "star", "size": 25, "title": "Discussion of models specifically for text classification in machine learning."}, {"color": "#248fe9", "id": "BernoulliEventModel", "label": "BernoulliEventModel", "shape": "dot", "size": 10, "title": "A model where each word is included independently according to probabilities based on class priors."}, {"color": "#5b96fc", "id": "MultinomialEventModel", "label": "MultinomialEventModel", "shape": "dot", "size": 10, "title": "A model where each word in a document is generated independently from the same multinomial distribution based on class label"}, {"color": "#ec0b87", "id": "NaiveBayesClassifier", "label": "NaiveBayesClassifier", "shape": "dot", "size": 10, "title": "A probabilistic classifier based on applying Bayes\u0027 theorem with strong (naive) independence assumptions between the features."}, {"color": "#ad3f86", "id": "LaplaceSmoothing", "label": "LaplaceSmoothing", "shape": "dot", "size": 10, "title": "Technique to prevent zero probabilities in categorical data estimation."}, {"color": "#7cdad0", "id": "KernelMethods", "label": "KernelMethods", "shape": "star", "size": 25, "title": "Techniques that allow algorithms to work in high-dimensional feature spaces without explicit computation of the space."}, {"color": "#f98470", "id": "FeatureMaps", "label": "FeatureMaps", "shape": "dot", "size": 10, "title": "Transformation of input data to higher-dimensional space for better model fitting."}, {"color": "#d5c856", "id": "Machine Learning Concepts", "label": "Machine Learning Concepts", "shape": "star", "size": 25, "title": "Overview of key concepts in machine learning including model complexity and regularization."}, {"color": "#5b20e8", "id": "Feature Mapping", "label": "Feature Mapping", "shape": "dot", "size": 10, "title": "Alternative method of testing if a function is a valid kernel through feature mapping."}, {"color": "#c878af", "id": "Linear Function Over Features", "label": "Linear Function Over Features", "shape": "dot", "size": 10, "title": "Rewriting a cubic function as a linear combination of features."}, {"color": "#529364", "id": "Cubic Function Representation", "label": "Cubic Function Representation", "shape": "dot", "size": 10, "title": "Expressing a cubic polynomial using feature map \u03c6(x)."}, {"color": "#b21cf3", "id": "Feature Map Definition", "label": "Feature Map Definition", "shape": "dot", "size": 10, "title": "Definition of the function \u03c6 that maps attributes to features."}, {"color": "#9a90ea", "id": "LMS with Features", "label": "LMS with Features", "shape": "star", "size": 25, "title": "Derivation and application of least mean squares algorithm using feature variables."}, {"color": "#dd877c", "id": "Gradient Descent Update", "label": "Gradient Descent Update", "shape": "dot", "size": 10, "title": "Update rule for fitting the model \u03b8\u2091\u03c6(x) using gradient descent."}, {"color": "#026e1e", "id": "FeatureMapping", "label": "FeatureMapping", "shape": "dot", "size": 10, "title": "Transformation that maps original feature vectors into a new space where linear algorithms can be applied more effectively."}, {"color": "#6575ef", "id": "HighDimensionalFeatures", "label": "HighDimensionalFeatures", "shape": "dot", "size": 10, "title": "Complex feature mappings leading to high-dimensional vectors in machine learning problems."}, {"color": "#ec3073", "id": "KernelTrick", "label": "KernelTrick", "shape": "dot", "size": 10, "title": "Technique for efficiently computing inner products of high-dimensional feature space without explicitly mapping the data."}, {"color": "#982e20", "id": "FeatureMappingPhiX", "label": "FeatureMappingPhiX", "shape": "dot", "size": 10, "title": "Description of the feature mapping function \u03c6(x) in machine learning models."}, {"color": "#cb7ed1", "id": "RuntimeAndMemoryEfficiency", "label": "RuntimeAndMemoryEfficiency", "shape": "dot", "size": 10, "title": "Discussion on improving runtime and memory efficiency using techniques like the kernel trick."}, {"color": "#21247a", "id": "InitializationThetaZero", "label": "InitializationThetaZero", "shape": "dot", "size": 10, "title": "Initial setup where \u03b8 is set to zero for simplification."}, {"color": "#802e89", "id": "IterativeUpdateRule", "label": "IterativeUpdateRule", "shape": "dot", "size": 10, "title": "Explanation of iterative update rules in machine learning models."}, {"color": "#d2c6ac", "id": "LinearCombinationRepresentation", "label": "LinearCombinationRepresentation", "shape": "dot", "size": 10, "title": "Concept of representing \u03b8 as a linear combination of feature mappings \u03c6(x)."}, {"color": "#2a8025", "id": "CoefficientUpdateRule", "label": "CoefficientUpdateRule", "shape": "dot", "size": 10, "title": "Derivation and explanation of the update rule for coefficients \u03b2 in machine learning models."}, {"color": "#17cb8d", "id": "MachineLearningAlgorithm", "label": "MachineLearningAlgorithm", "shape": "star", "size": 25, "title": "Overview of machine learning algorithms and their properties."}, {"color": "#c74ad6", "id": "BatchGradientDescent", "label": "BatchGradientDescent", "shape": "dot", "size": 10, "title": "A method for minimizing loss functions in machine learning models."}, {"color": "#23878d", "id": "BetaUpdateEquation", "label": "BetaUpdateEquation", "shape": "dot", "size": 10, "title": "The equation used to update the beta values iteratively during gradient descent."}, {"color": "#f155c5", "id": "FeatureMapPhi", "label": "FeatureMapPhi", "shape": "dot", "size": 10, "title": "A mapping function that transforms input data into a higher-dimensional space for better separation."}, {"color": "#168827", "id": "InnerProductEfficiency", "label": "InnerProductEfficiency", "shape": "dot", "size": 10, "title": "Techniques to efficiently compute inner products in high-dimensional spaces without explicit feature map computation."}, {"color": "#5e78d3", "id": "FeatureMapsAndKernels", "label": "FeatureMapsAndKernels", "shape": "dot", "size": 10, "title": "Discussion on feature maps and their corresponding kernel functions."}, {"color": "#c66f06", "id": "KernelFunctionDefinition", "label": "KernelFunctionDefinition", "shape": "dot", "size": 10, "title": "Definition of the kernel function in relation to feature maps."}, {"color": "#a78617", "id": "InnerProductCalculation", "label": "InnerProductCalculation", "shape": "dot", "size": 10, "title": "Method for calculating inner products using kernels."}, {"color": "#74c477", "id": "AlgorithmEfficiency", "label": "AlgorithmEfficiency", "shape": "dot", "size": 10, "title": "Explanation of the efficiency of algorithms based on kernel functions."}, {"color": "#2cce30", "id": "PropertiesOfKernels", "label": "PropertiesOfKernels", "shape": "star", "size": 25, "title": "Exploration of properties and characteristics of kernels in machine learning."}, {"color": "#b6767c", "id": "Kernels_in_Machine_Learning", "label": "Kernels_in_Machine_Learning", "shape": "dot", "size": 10, "title": "Discussion on kernel functions in the context of machine learning algorithms."}, {"color": "#1879d8", "id": "Feature_Map_Phi", "label": "Feature_Map_Phi", "shape": "dot", "size": 10, "title": "Explicitly defined feature map phi and its role in inducing kernel function K."}, {"color": "#ca845e", "id": "Kernel_Function_K", "label": "Kernel_Function_K", "shape": "dot", "size": 10, "title": "Definition of the kernel function K(x,z) and its intrinsic properties."}, {"color": "#2fe4bb", "id": "Algorithm_5.11", "label": "Algorithm_5.11", "shape": "dot", "size": 10, "title": "Description of algorithm 5.11 which operates based on kernel functions without explicit feature maps."}, {"color": "#f80060", "id": "Characterization_of_Kernels", "label": "Characterization_of_Kernels", "shape": "dot", "size": 10, "title": "Conditions for a function to be considered as a valid kernel function K(x,z)."}, {"color": "#d38a97", "id": "Example_Kernel_Functions", "label": "Example_Kernel_Functions", "shape": "dot", "size": 10, "title": "Concrete examples of kernel functions, such as polynomial kernels."}, {"color": "#ef694b", "id": "KernelFunctions", "label": "KernelFunctions", "shape": "dot", "size": 10, "title": "Functions that compute the dot product of feature mappings in a high-dimensional space."}, {"color": "#bfc466", "id": "PolynomialKernels", "label": "PolynomialKernels", "shape": "dot", "size": 10, "title": "A type of kernel function used to map input data into higher dimensional spaces."}, {"color": "#fb8ba2", "id": "ComputationalEfficiency", "label": "ComputationalEfficiency", "shape": "dot", "size": 10, "title": "Discussion on the efficiency of computing kernel functions compared to direct computation in high-dimensional spaces."}, {"color": "#22ff1c", "id": "KernelsAsSimilarityMetrics", "label": "KernelsAsSimilarityMetrics", "shape": "dot", "size": 10, "title": "Use of kernels as a measure of similarity between data points."}, {"color": "#3a4342", "id": "GaussianKernel", "label": "GaussianKernel", "shape": "dot", "size": 10, "title": "Specific kernel function that measures the similarity based on distance in input space."}, {"color": "#19e2ba", "id": "Kernel_Functions", "label": "Kernel_Functions", "shape": "star", "size": 25, "title": "Properties and conditions for a function to be a valid kernel."}, {"color": "#fadddb", "id": "Necessary_Conditions", "label": "Necessary_Conditions", "shape": "dot", "size": 10, "title": "Conditions that must be satisfied by a valid kernel function."}, {"color": "#e321dd", "id": "Symmetry_Property", "label": "Symmetry_Property", "shape": "dot", "size": 10, "title": "A valid kernel matrix is symmetric."}, {"color": "#2925dd", "id": "Positive_Semi_Definite", "label": "Positive_Semi_Definite", "shape": "dot", "size": 10, "title": "A valid kernel matrix must be positive semi-definite."}, {"color": "#82ed96", "id": "Kernel_Matrix", "label": "Kernel_Matrix", "shape": "dot", "size": 10, "title": "Matrix representation of the kernel function for a set of points."}, {"color": "#c42a1f", "id": "Sufficient_Conditions", "label": "Sufficient_Conditions", "shape": "dot", "size": 10, "title": "Conditions that are both necessary and sufficient for a valid kernel function."}, {"color": "#227cf5", "id": "Kernel Matrix Properties", "label": "Kernel Matrix Properties", "shape": "star", "size": 25, "title": "Properties of the kernel matrix including symmetry and positive semidefiniteness."}, {"color": "#60975a", "id": "Sufficient Conditions for Kernels", "label": "Sufficient Conditions for Kernels", "shape": "dot", "size": 10, "title": "Conditions under which a function can be considered a valid Mercer kernel."}, {"color": "#008494", "id": "Mercer\u0027s Theorem", "label": "Mercer\u0027s Theorem", "shape": "dot", "size": 10, "title": "Theorem stating necessary and sufficient conditions for a function to be a valid kernel."}, {"color": "#e9ba86", "id": "Kernel Examples", "label": "Kernel Examples", "shape": "star", "size": 25, "title": "Examples of kernels used in machine learning problems such as SVMs."}, {"color": "#7f5c1e", "id": "Digit Recognition Problem", "label": "Digit Recognition Problem", "shape": "dot", "size": 10, "title": "Use of polynomial and Gaussian kernels for recognizing handwritten digits."}, {"color": "#9c1a44", "id": "String Classification Example", "label": "String Classification Example", "shape": "dot", "size": 10, "title": "Example involving classification of strings, such as amino acid sequences."}, {"color": "#54f167", "id": "FeatureExtraction", "label": "FeatureExtraction", "shape": "dot", "size": 10, "title": "Techniques for converting raw data into features suitable for machine learning models."}, {"color": "#602f15", "id": "StringFeatureExtraction", "label": "StringFeatureExtraction", "shape": "dot", "size": 10, "title": "Method for extracting features from strings by counting occurrences of substrings."}, {"color": "#10805d", "id": "SupportVectorMachines", "label": "SupportVectorMachines", "shape": "star", "size": 25, "title": "Supervised learning model for classification and regression analysis."}, {"color": "#ab0a04", "id": "Machine_Learning_Algorithms", "label": "Machine_Learning_Algorithms", "shape": "star", "size": 25, "title": "Overview of algorithms in machine learning including kernel trick and SVMs."}, {"color": "#332022", "id": "Kernel_Trick", "label": "Kernel_Trick", "shape": "dot", "size": 10, "title": "Method to derive algorithms like the kernel perceptron algorithm."}, {"color": "#f1a0fa", "id": "Support_Vector_Machines", "label": "Support_Vector_Machines", "shape": "dot", "size": 10, "title": "Supervised learning algorithm known for its effectiveness in classification tasks."}, {"color": "#35a965", "id": "Margins_Intuition", "label": "Margins_Intuition", "shape": "dot", "size": 10, "title": "Introduction to the concept of margins and confidence in predictions."}, {"color": "#8779a8", "id": "Optimal_Margin_Classifier", "label": "Optimal_Margin_Classifier", "shape": "dot", "size": 10, "title": "Classifier that maximizes the margin between classes, leading into Lagrange duality discussion."}, {"color": "#a6f78f", "id": "Lagrange_Duality", "label": "Lagrange_Duality", "shape": "dot", "size": 10, "title": "Theory for converting constrained optimization problems into dual forms."}, {"color": "#dd10ed", "id": "Kernels_in_SVMs", "label": "Kernels_in_SVMs", "shape": "dot", "size": 10, "title": "Technique allowing efficient application of SVMs in high-dimensional spaces."}, {"color": "#5e0601", "id": "SMO_Algorithm", "label": "SMO_Algorithm", "shape": "dot", "size": 10, "title": "Algorithm that updates two Lagrange multipliers simultaneously to optimize the dual problem."}, {"color": "#e4b04e", "id": "Functional Margins", "label": "Functional Margins", "shape": "dot", "size": 10, "title": "Concept used to formalize confident classifications based on dot product of parameters and input."}, {"color": "#c160e2", "id": "Geometric Margins", "label": "Geometric Margins", "shape": "dot", "size": 10, "title": "Distance-based measure for confidence in classification predictions far from decision boundary."}, {"color": "#19630a", "id": "Decision Boundary", "label": "Decision Boundary", "shape": "dot", "size": 10, "title": "Line or hyperplane separating different classes in a feature space."}, {"color": "#db3e00", "id": "Separating Hyperplane", "label": "Separating Hyperplane", "shape": "dot", "size": 10, "title": "Specific term for decision boundary used in SVMs and linear classification problems."}, {"color": "#40bead", "id": "Confidence in Predictions", "label": "Confidence in Predictions", "shape": "dot", "size": 10, "title": "Measure of certainty about predictions based on distance from the decision boundary."}, {"color": "#254c60", "id": "Support Vector Machines (SVMs)", "label": "Support Vector Machines (SVMs)", "shape": "dot", "size": 10, "title": "Binary classification algorithm that maximizes the margin between classes."}, {"color": "#7acb62", "id": "Notation for SVMs", "label": "Notation for SVMs", "shape": "dot", "size": 10, "title": "Introduction to notation used in discussing SVMs, including parameters w and b."}, {"color": "#a9912d", "id": "Functional Margin", "label": "Functional Margin", "shape": "dot", "size": 10, "title": "Definition of the functional margin for a training example with respect to classifier parameters."}, {"color": "#ad72e5", "id": "Geometric Margin", "label": "Geometric Margin", "shape": "dot", "size": 10, "title": "Conceptual understanding of geometric margins in SVM context, not fully detailed here."}, {"color": "#0a7cb9", "id": "Functional_Margin", "label": "Functional_Margin", "shape": "dot", "size": 10, "title": "Definition and properties of functional margin for a linear classifier."}, {"color": "#2e1baa", "id": "Confidence_and_Prediction", "label": "Confidence_and_Prediction", "shape": "dot", "size": 10, "title": "Relationship between functional margin and prediction accuracy."}, {"color": "#4dc647", "id": "Scaling_Issue", "label": "Scaling_Issue", "shape": "dot", "size": 10, "title": "Problem with scaling the parameters of a linear classifier affecting the functional margin."}, {"color": "#385553", "id": "Function_Margin_of_S", "label": "Function_Margin_of_S", "shape": "dot", "size": 10, "title": "Definition of function margin for a set of training examples."}, {"color": "#b46c6d", "id": "Geometric_Margins", "label": "Geometric_Margins", "shape": "dot", "size": 10, "title": "Introduction to geometric margins in machine learning context."}, {"color": "#9ea324", "id": "VectorW", "label": "VectorW", "shape": "dot", "size": 10, "title": "A vector orthogonal to the decision boundary and pointing towards positive class."}, {"color": "#e8a83e", "id": "DistanceToBoundary", "label": "DistanceToBoundary", "shape": "dot", "size": 10, "title": "The shortest distance from a point to the decision boundary."}, {"color": "#2462a0", "id": "UnitVectorW", "label": "UnitVectorW", "shape": "dot", "size": 10, "title": "A unit vector in the direction of W, used for calculating distances."}, {"color": "#0bff03", "id": "GeometricMargin", "label": "GeometricMargin", "shape": "star", "size": 25, "title": "The perpendicular distance from a data point to the decision boundary multiplied by the class label."}, {"color": "#b9507d", "id": "FunctionalMargin", "label": "FunctionalMargin", "shape": "dot", "size": 10, "title": "A measure of how confidently a model can classify a training example."}, {"color": "#fdee62", "id": "Scaling_Parameters", "label": "Scaling_Parameters", "shape": "dot", "size": 10, "title": "Discussion on the scaling of parameters w and b."}, {"color": "#e02019", "id": "Geometric_Margin", "label": "Geometric_Margin", "shape": "dot", "size": 10, "title": "Definition and importance of geometric margin in training sets."}, {"color": "#04f60c", "id": "Maximize_Geometric_Margin", "label": "Maximize_Geometric_Margin", "shape": "dot", "size": 10, "title": "Formulation of optimization problem to maximize geometric margin."}, {"color": "#60e91f", "id": "Support Vector Machine (SVM)", "label": "Support Vector Machine (SVM)", "shape": "star", "size": 25, "title": "A supervised learning model for classification and regression analysis."}, {"color": "#e2c9b1", "id": "Optimization Problem in SVM", "label": "Optimization Problem in SVM", "shape": "dot", "size": 10, "title": "The core mathematical problem to find the optimal hyperplane."}, {"color": "#4050c4", "id": "Non-Convex Constraint", "label": "Non-Convex Constraint", "shape": "dot", "size": 10, "title": "Constraint that complicates optimization due to its non-convex nature."}, {"color": "#93ce4d", "id": "Scaling Constraint", "label": "Scaling Constraint", "shape": "dot", "size": 10, "title": "A constraint used to simplify the original problem by setting functional margin to 1."}, {"color": "#0f82ad", "id": "Support_Vector_Machines_SVM", "label": "Support_Vector_Machines_SVM", "shape": "dot", "size": 10, "title": "Technique to find optimal margin classifiers through optimization."}, {"color": "#823d22", "id": "Convex_Quadratic_Objective", "label": "Convex_Quadratic_Objective", "shape": "dot", "size": 10, "title": "Objective function in SVMs that is a convex quadratic form."}, {"color": "#3c16f7", "id": "Linear_Constraints", "label": "Linear_Constraints", "shape": "dot", "size": 10, "title": "Constraints in the optimization problem are linear."}, {"color": "#e85989", "id": "Dual_Form_Optimization", "label": "Dual_Form_Optimization", "shape": "dot", "size": 10, "title": "Alternative form of the original problem that can be easier to solve."}, {"color": "#d2685a", "id": "Kernels_High_Dimensional_Spaces", "label": "Kernels_High_Dimensional_Spaces", "shape": "dot", "size": 10, "title": "Use kernels in SVMs for efficient computation in high dimensions."}, {"color": "#f85547", "id": "ConstrainedOptimization", "label": "ConstrainedOptimization", "shape": "star", "size": 25, "title": "Generalization of Lagrange multipliers to include inequality constraints."}, {"color": "#3850d6", "id": "LagrangeMultipliers", "label": "LagrangeMultipliers", "shape": "dot", "size": 10, "title": "Coefficients used in the generalized Lagrangian for optimization problems."}, {"color": "#ca09a4", "id": "PrimalProblem", "label": "PrimalProblem", "shape": "dot", "size": 10, "title": "Minimizing a function subject to inequality and equality constraints."}, {"color": "#e240ee", "id": "GeneralizedLagrangian", "label": "GeneralizedLagrangian", "shape": "dot", "size": 10, "title": "Combination of objective function with Lagrange multipliers for constraints."}, {"color": "#ab7b86", "id": "ThetaP", "label": "ThetaP", "shape": "dot", "size": 10, "title": "Function representing the maximum value of the generalized Lagrangian under primal constraints."}, {"color": "#c6c161", "id": "Primal Problem", "label": "Primal Problem", "shape": "star", "size": 25, "title": "Original optimization problem in machine learning."}, {"color": "#91a45f", "id": "Objective Function Primal", "label": "Objective Function Primal", "shape": "dot", "size": 10, "title": "Function to be minimized or maximized in the primal problem."}, {"color": "#68d98c", "id": "Theta P", "label": "Theta P", "shape": "dot", "size": 10, "title": "Indicator function for the primal constraints satisfaction."}, {"color": "#ebd02d", "id": "Dual Problem", "label": "Dual Problem", "shape": "star", "size": 25, "title": "Optimization problem derived from the primal by exchanging max and min operations."}, {"color": "#1a947d", "id": "Objective Function Dual", "label": "Objective Function Dual", "shape": "dot", "size": 10, "title": "Function to be maximized in the dual problem."}, {"color": "#64a6e5", "id": "Theta D", "label": "Theta D", "shape": "dot", "size": 10, "title": "Indicator function for the dual constraints satisfaction."}, {"color": "#c65f88", "id": "Relationship Between Primal and Dual", "label": "Relationship Between Primal and Dual", "shape": "star", "size": 25, "title": "Theoretical relationship between primal and dual problems in terms of their optimal values."}, {"color": "#c8a027", "id": "Optimization_Problems", "label": "Optimization_Problems", "shape": "dot", "size": 10, "title": "Problems involving finding the best solution in a set of possible solutions."}, {"color": "#75e28e", "id": "Primal_Dual_Pairing", "label": "Primal_Dual_Pairing", "shape": "dot", "size": 10, "title": "Relationship between primal and dual optimization problems."}, {"color": "#355412", "id": "Dual_Problem_Solution", "label": "Dual_Problem_Solution", "shape": "dot", "size": 10, "title": "Conditions under which the solution to the dual problem equals that of the primal."}, {"color": "#c61b63", "id": "Convexity_Conditions", "label": "Convexity_Conditions", "shape": "dot", "size": 10, "title": "Requirements for functions f, g_i and h_i to ensure d* = p*."}, {"color": "#2ecb64", "id": "Feasibility_Constraints", "label": "Feasibility_Constraints", "shape": "dot", "size": 10, "title": "Conditions ensuring the constraints are strictly feasible."}, {"color": "#9f9f4d", "id": "KKT_Conditions", "label": "KKT_Conditions", "shape": "dot", "size": 10, "title": "Set of necessary conditions for a solution to be optimal in constrained optimization problems."}, {"color": "#3c2ec7", "id": "Dual_Complementarity", "label": "Dual_Complementarity", "shape": "dot", "size": 10, "title": "Condition indicating active constraints in the dual form of SVMs."}, {"color": "#0a3e0b", "id": "Primal_Dual_Equivalence", "label": "Primal_Dual_Equivalence", "shape": "dot", "size": 10, "title": "Equivalence between primal and dual optimization problems in SVM context."}, {"color": "#48dd1e", "id": "Support_Vectors", "label": "Support_Vectors", "shape": "dot", "size": 10, "title": "Training examples that influence the optimal solution due to active constraints."}, {"color": "#992dab", "id": "SupportVectorsConcept", "label": "SupportVectorsConcept", "shape": "star", "size": 25, "title": "Explanation of support vectors in machine learning problems."}, {"color": "#f3713a", "id": "KernelTrickIntroduction", "label": "KernelTrickIntroduction", "shape": "star", "size": 25, "title": "Preview of the kernel trick application in algorithms."}, {"color": "#a7cf86", "id": "LagrangianFormulation", "label": "LagrangianFormulation", "shape": "star", "size": 25, "title": "Description and formulation of Lagrangian for optimization problem."}, {"color": "#c86e20", "id": "DualProblemDerivation", "label": "DualProblemDerivation", "shape": "dot", "size": 10, "title": "Steps to derive the dual form from the Lagrangian."}, {"color": "#ccddd4", "id": "Lagrangian Function", "label": "Lagrangian Function", "shape": "dot", "size": 10, "title": "Used to incorporate constraints into the optimization problem."}, {"color": "#3d3bd3", "id": "Dual Optimization Problem", "label": "Dual Optimization Problem", "shape": "dot", "size": 10, "title": "Formulated by transforming the primal problem to a dual form for easier solving."}, {"color": "#2e72fd", "id": "KKT Conditions", "label": "KKT Conditions", "shape": "dot", "size": 10, "title": "Conditions that must be satisfied at an optimal solution in constrained optimization problems."}, {"color": "#0189ce", "id": "Optimal Parameters w and b", "label": "Optimal Parameters w and b", "shape": "dot", "size": 10, "title": "Parameters derived from solving the dual problem to define the decision boundary."}, {"color": "#53e03d", "id": "Machine Learning Models", "label": "Machine Learning Models", "shape": "star", "size": 25, "title": "Overview of various machine learning models and their applications."}, {"color": "#45cf1e", "id": "Support Vector Machines (SVM)", "label": "Support Vector Machines (SVM)", "shape": "dot", "size": 10, "title": "A model that efficiently learns in high-dimensional spaces using kernels."}, {"color": "#3ae6e8", "id": "Optimal Parameters Calculation", "label": "Optimal Parameters Calculation", "shape": "dot", "size": 10, "title": "Calculation of optimal parameters w and b for SVM."}, {"color": "#580dc4", "id": "Dual Form Optimization", "label": "Dual Form Optimization", "shape": "dot", "size": 10, "title": "Derivation using dual form to find optimal alpha values."}, {"color": "#55888f", "id": "Prediction with Inner Products", "label": "Prediction with Inner Products", "shape": "dot", "size": 10, "title": "Using inner products for prediction based on support vectors."}, {"color": "#fde3f8", "id": "Regularization and Non-Separable Data", "label": "Regularization and Non-Separable Data", "shape": "dot", "size": 10, "title": "Handling non-separable data with regularization techniques."}, {"color": "#9226ce", "id": "Support Vector Machines", "label": "Support Vector Machines", "shape": "star", "size": 25, "title": "Algorithm for learning in high-dimensional spaces."}, {"color": "#62ab14", "id": "Regularization and Non-Separable Case", "label": "Regularization and Non-Separable Case", "shape": "dot", "size": 10, "title": "Handling non-linearly separable datasets with regularization."}, {"color": "#981a68", "id": "Linear Separability Assumption", "label": "Linear Separability Assumption", "shape": "dot", "size": 10, "title": "Initial SVM derivation assumes linear separability of data."}, {"color": "#1e773e", "id": "Outlier Sensitivity", "label": "Outlier Sensitivity", "shape": "dot", "size": 10, "title": "SVMs can be sensitive to outliers in the dataset."}, {"color": "#de4edd", "id": "L1 Regularization", "label": "L1 Regularization", "shape": "dot", "size": 10, "title": "Introduces L1 regularization to handle non-linear separability and reduce outlier impact."}, {"color": "#6cc0b5", "id": "Optimization Problem", "label": "Optimization Problem", "shape": "dot", "size": 10, "title": "Formulates the optimization problem with L1 regularization term."}, {"color": "#d30207", "id": "Objective Function", "label": "Objective Function", "shape": "dot", "size": 10, "title": "Minimizes a function that includes both norm of w and sum of slack variables weighted by C."}, {"color": "#67a117", "id": "Slack Variables", "label": "Slack Variables", "shape": "dot", "size": 10, "title": "Introduces slack variables to allow for some data points to be within the margin."}, {"color": "#6c0221", "id": "Parameter C", "label": "Parameter C", "shape": "dot", "size": 10, "title": "Controls trade-off between maximizing margin and minimizing classification errors."}, {"color": "#071d1c", "id": "Lagrangian Formulation", "label": "Lagrangian Formulation", "shape": "dot", "size": 10, "title": "Derives the Lagrangian to solve the optimization problem with constraints."}, {"color": "#dc25c3", "id": "Dual_Problem_Formulation", "label": "Dual_Problem_Formulation", "shape": "dot", "size": 10, "title": "Formulating the optimization problem in SVMs."}, {"color": "#d4b5b4", "id": "Lagrange_Multipliers", "label": "Lagrange_Multipliers", "shape": "dot", "size": 10, "title": "Use of Lagrange multipliers to solve constrained optimization problems."}, {"color": "#bee5ee", "id": "Sequential_Minimal_Optimization_SMO", "label": "Sequential_Minimal_Optimization_SMO", "shape": "dot", "size": 10, "title": "Efficient algorithm for solving SVM\u0027s dual problem."}, {"color": "#ff195b", "id": "Coordinate_Ascend_Algorithm", "label": "Coordinate_Ascend_Algorithm", "shape": "star", "size": 25, "title": "Optimization technique used in various machine learning contexts."}, {"color": "#5d0cbc", "id": "Unconstrained_Optimization_Problem", "label": "Unconstrained_Optimization_Problem", "shape": "dot", "size": 10, "title": "Maximizing a function over multiple parameters without constraints."}, {"color": "#f0a7b3", "id": "Gradient_Ascend_Newtons_Method", "label": "Gradient_Ascend_Newtons_Method", "shape": "dot", "size": 10, "title": "Alternative optimization methods compared to coordinate ascent."}, {"color": "#b8c004", "id": "Coordinate_Ascend_Method", "label": "Coordinate_Ascend_Method", "shape": "dot", "size": 10, "title": "A method for optimizing functions by moving along one axis at a time."}, {"color": "#bf3dcc", "id": "Support_Vector_Machines_SVMs", "label": "Support_Vector_Machines_SVMs", "shape": "dot", "size": 10, "title": "Models that use support vectors to classify data in high-dimensional spaces."}, {"color": "#92b141", "id": "SVM_Dual_Problem", "label": "SVM_Dual_Problem", "shape": "dot", "size": 10, "title": "The dual form of the optimization problem for SVMs."}, {"color": "#f30226", "id": "Heuristic_Selection", "label": "Heuristic_Selection", "shape": "dot", "size": 10, "title": "Process of selecting alpha pairs to optimize efficiency."}, {"color": "#cf2c7e", "id": "Efficient_Update", "label": "Efficient_Update", "shape": "dot", "size": 10, "title": "Derivation and explanation of efficient update mechanism."}, {"color": "#5d2f99", "id": "Constraints_Satisfaction", "label": "Constraints_Satisfaction", "shape": "dot", "size": 10, "title": "Explanation of constraints satisfaction for alpha values."}, {"color": "#7dba95", "id": "Alpha_Parameters", "label": "Alpha_Parameters", "shape": "dot", "size": 10, "title": "Parameters \u03b11 and \u03b12 with constraints."}, {"color": "#149548", "id": "Constraint_Equation", "label": "Constraint_Equation", "shape": "dot", "size": 10, "title": "Equation \u03b11y^(1) + \u03b12y^(2) = \u03c7."}, {"color": "#ad1dfc", "id": "Bounds_L_H", "label": "Bounds_L_H", "shape": "dot", "size": 10, "title": "Lower-bound L and upper-bound H for permissible values of \u03b12."}, {"color": "#aef9bd", "id": "Objective_Function_W", "label": "Objective_Function_W", "shape": "dot", "size": 10, "title": "Objective function W(\u03b1) expressed in terms of \u03b12 and other parameters."}, {"color": "#21bd6a", "id": "Quadratic_Formulation", "label": "Quadratic_Formulation", "shape": "dot", "size": 10, "title": "W as a quadratic function in \u03b12: a\u03b12^2 + b\u03b12 + c."}, {"color": "#b12077", "id": "Maximization_Process", "label": "Maximization_Process", "shape": "dot", "size": 10, "title": "Process of maximizing W by setting derivative to zero and solving."}, {"color": "#87c438", "id": "Sequential Minimal Optimization (SMO) Algorithm", "label": "Sequential Minimal Optimization (SMO) Algorithm", "shape": "dot", "size": 10, "title": "Efficient algorithm for solving the optimization problem in SVM."}, {"color": "#b7f26c", "id": "Alpha Updates", "label": "Alpha Updates", "shape": "dot", "size": 10, "title": "Process of updating alpha values within SMO."}, {"color": "#dc4b22", "id": "Deep Learning Introduction", "label": "Deep Learning Introduction", "shape": "star", "size": 25, "title": "Introduction to deep learning concepts and neural networks."}, {"color": "#62f470", "id": "Supervised Learning with Non-Linear Models", "label": "Supervised Learning with Non-Linear Models", "shape": "dot", "size": 10, "title": "Exploration of non-linear models in supervised learning context."}, {"color": "#15d411", "id": "NonLinearModel", "label": "NonLinearModel", "shape": "dot", "size": 10, "title": "Abstract non-linear model used in machine learning problems."}, {"color": "#85538b", "id": "TrainingExamples", "label": "TrainingExamples", "shape": "dot", "size": 10, "title": "Set of training examples used to define the cost function."}, {"color": "#bf1b4e", "id": "RegressionProblems", "label": "RegressionProblems", "shape": "dot", "size": 10, "title": "Introduction to regression problems where output is a real number."}, {"color": "#e05884", "id": "LeastSquareCostFunction", "label": "LeastSquareCostFunction", "shape": "dot", "size": 10, "title": "Definition of the least square cost function for individual examples."}, {"color": "#385311", "id": "MeanSquaredLoss", "label": "MeanSquaredLoss", "shape": "dot", "size": 10, "title": "Definition and properties of the mean squared loss function over a dataset."}, {"color": "#18cd1e", "id": "BinaryClassification", "label": "BinaryClassification", "shape": "dot", "size": 10, "title": "Introduction to binary classification problems in machine learning."}, {"color": "#b5f480", "id": "LogitFunction", "label": "LogitFunction", "shape": "dot", "size": 10, "title": "Linear combination of input features and weights before applying the logistic function."}, {"color": "#125ac4", "id": "ProbabilityPrediction", "label": "ProbabilityPrediction", "shape": "dot", "size": 10, "title": "Transformation of logit to a probability using the sigmoid function."}, {"color": "#b593ca", "id": "NegativeLikelihoodLoss", "label": "NegativeLikelihoodLoss", "shape": "dot", "size": 10, "title": "Loss function used for logistic regression, based on negative log-likelihood."}, {"color": "#b560a8", "id": "TotalLossFunction", "label": "TotalLossFunction", "shape": "dot", "size": 10, "title": "Average of individual training example losses over the entire dataset."}, {"color": "#a2f350", "id": "MultiClassClassification", "label": "MultiClassClassification", "shape": "star", "size": 25, "title": "Extension of binary classification to multiple classes using softmax function."}, {"color": "#7460c1", "id": "NegativeLogLikelihoodLossMulticlass", "label": "NegativeLogLikelihoodLossMulticlass", "shape": "dot", "size": 10, "title": "Loss function used in multi-class classification based on negative log-likelihood."}, {"color": "#fc58e9", "id": "Loss Function", "label": "Loss Function", "shape": "star", "size": 25, "title": "Function measuring model performance for a single training example or average over all examples."}, {"color": "#054c63", "id": "Negative Log-Likelihood", "label": "Negative Log-Likelihood", "shape": "dot", "size": 10, "title": "Specific form of loss function used in probabilistic models."}, {"color": "#51e1e3", "id": "Cross-Entropy Loss", "label": "Cross-Entropy Loss", "shape": "dot", "size": 10, "title": "Simplified notation for negative log-likelihood loss function."}, {"color": "#83786a", "id": "Average Loss Function", "label": "Average Loss Function", "shape": "dot", "size": 10, "title": "Total loss averaged over all training examples."}, {"color": "#e83242", "id": "Conditional Probabilistic Models", "label": "Conditional Probabilistic Models", "shape": "star", "size": 25, "title": "Models where the distribution of output depends on input features."}, {"color": "#6b53d8", "id": "Exponential Family Distribution", "label": "Exponential Family Distribution", "shape": "dot", "size": 10, "title": "Distribution family for conditional probabilistic models with exponential form."}, {"color": "#290221", "id": "Optimizers", "label": "Optimizers", "shape": "star", "size": 25, "title": "Discussion on the impact of optimizers on model generalization."}, {"color": "#b9a7ac", "id": "Gradient Descent (GD)", "label": "Gradient Descent (GD)", "shape": "dot", "size": 10, "title": "Algorithm that updates parameters in the direction of steepest descent of loss function."}, {"color": "#ae297b", "id": "Stochastic Gradient Descent (SGD)", "label": "Stochastic Gradient Descent (SGD)", "shape": "dot", "size": 10, "title": "Variant of GD using a single training example for each update to speed up convergence and reduce computational cost."}, {"color": "#68cbae", "id": "Mini-batch Stochastic Gradient Descent", "label": "Mini-batch Stochastic Gradient Descent", "shape": "dot", "size": 10, "title": "Variant of SGD where gradients are computed over small batches of data for efficiency."}, {"color": "#9ac183", "id": "Hyperparameters", "label": "Hyperparameters", "shape": "dot", "size": 10, "title": "Parameters like learning rate and number of iterations that control the optimization process."}, {"color": "#230427", "id": "Initialization", "label": "Initialization", "shape": "dot", "size": 10, "title": "Different initializations can lead to different generalization performance."}, {"color": "#0511f9", "id": "Backpropagation Algorithm", "label": "Backpropagation Algorithm", "shape": "dot", "size": 10, "title": "Method for efficiently computing gradients of loss functions in neural networks."}, {"color": "#2ff4da", "id": "NeuralNetworks", "label": "NeuralNetworks", "shape": "dot", "size": 10, "title": "Description and formal representation of neural networks used for prediction."}, {"color": "#e6f8c2", "id": "RegressionProblem", "label": "RegressionProblem", "shape": "dot", "size": 10, "title": "Explanation of regression problems in the context of neural networks."}, {"color": "#5bd54b", "id": "SingleNeuronNN", "label": "SingleNeuronNN", "shape": "dot", "size": 10, "title": "Introduction to neural networks with a single neuron focusing on parametrization functions."}, {"color": "#9f1c86", "id": "HousingPricePrediction", "label": "HousingPricePrediction", "shape": "dot", "size": 10, "title": "Example of using a single neuron network for predicting housing prices based on size."}, {"color": "#6a8455", "id": "ReLUFunction", "label": "ReLUFunction", "shape": "dot", "size": 10, "title": "Introduction to the ReLU function used in neural networks to prevent negative outputs."}, {"color": "#2912bc", "id": "Neural_Networks", "label": "Neural_Networks", "shape": "dot", "size": 10, "title": "System of algorithms designed to recognize patterns and extract features from complex data sets."}, {"color": "#e228ff", "id": "Activation_Functions", "label": "Activation_Functions", "shape": "dot", "size": 10, "title": "Functions that introduce non-linearity in the model, such as ReLU."}, {"color": "#24ed5d", "id": "ReLU", "label": "ReLU", "shape": "dot", "size": 10, "title": "Rectified Linear Unit function used to activate neurons."}, {"color": "#595a73", "id": "Single_Neuron_Model", "label": "Single_Neuron_Model", "shape": "dot", "size": 10, "title": "A model with a single neuron and its mathematical representation."}, {"color": "#c40f9c", "id": "Bias_and_Weights", "label": "Bias_and_Weights", "shape": "dot", "size": 10, "title": "Explanation of bias term and weight vector in the context of neural networks."}, {"color": "#f2245a", "id": "Stacking_Neurons", "label": "Stacking_Neurons", "shape": "dot", "size": 10, "title": "Process of combining multiple neurons to form a more complex network."}, {"color": "#2f89a7", "id": "Complex_Neural_Networks", "label": "Complex_Neural_Networks", "shape": "dot", "size": 10, "title": "Building neural networks with multiple layers and neurons."}, {"color": "#2776ca", "id": "DerivedFeatures", "label": "DerivedFeatures", "shape": "dot", "size": 10, "title": "Description of family size, walkability, and school quality as derived features."}, {"color": "#3a83b4", "id": "FamilySize", "label": "FamilySize", "shape": "dot", "size": 10, "title": "Feature indicating the maximum number of people a house can accommodate."}, {"color": "#8b607a", "id": "WalkableNeighborhood", "label": "WalkableNeighborhood", "shape": "dot", "size": 10, "title": "Measure of how easily one can walk to local amenities such as grocery stores."}, {"color": "#5ecda2", "id": "SchoolQuality", "label": "SchoolQuality", "shape": "dot", "size": 10, "title": "Indicator of the quality of the elementary school in a neighborhood."}, {"color": "#6e4f81", "id": "InputFeatures", "label": "InputFeatures", "shape": "dot", "size": 10, "title": "Set of input features (x1, x2, x3, x4) to the neural network."}, {"color": "#cb8dc7", "id": "HiddenUnits", "label": "HiddenUnits", "shape": "dot", "size": 10, "title": "Intermediate variables a1, a2, a3 representing derived features as hidden units in the neural network."}, {"color": "#90be9e", "id": "ReLUActivation", "label": "ReLUActivation", "shape": "dot", "size": 10, "title": "Rectified Linear Unit (ReLU) activation function used in neural networks."}, {"color": "#a6265d", "id": "NeuralNetworksInspiration", "label": "NeuralNetworksInspiration", "shape": "dot", "size": 10, "title": "Explanation of how artificial neural networks are inspired by biological ones."}, {"color": "#ce4a84", "id": "TwoLayerNN", "label": "TwoLayerNN", "shape": "dot", "size": 10, "title": "A simple model consisting of an input layer, a hidden layer, and an output layer."}, {"color": "#a9fd39", "id": "ParametersTheta", "label": "ParametersTheta", "shape": "dot", "size": 10, "title": "Details on the parameters \u03b8 used in neural networks."}, {"color": "#b6610c", "id": "BiologicalSimilarity", "label": "BiologicalSimilarity", "shape": "dot", "size": 10, "title": "Discussion on similarities and differences between artificial and biological neural networks."}, {"color": "#6ce5f8", "id": "PriorKnowledge", "label": "PriorKnowledge", "shape": "dot", "size": 10, "title": "Explanation of the role of prior knowledge in constructing neural network models."}, {"color": "#30246d", "id": "FullyConnectedNN", "label": "FullyConnectedNN", "shape": "dot", "size": 10, "title": "A type of neural network where each neuron is connected to every neuron in the previous layer."}, {"color": "#fefb54", "id": "IntermediateVariables", "label": "IntermediateVariables", "shape": "dot", "size": 10, "title": "Variables like a1, a2, and a3 that are functions of input variables x1-x4."}, {"color": "#329122", "id": "Parameterization", "label": "Parameterization", "shape": "dot", "size": 10, "title": "Generic parameterization using weights w and biases b for each neuron."}, {"color": "#6a4ebd", "id": "Vectorization", "label": "Vectorization", "shape": "dot", "size": 10, "title": "Use of matrix and vector notations to simplify expressions for neural networks and improve computational efficiency."}, {"color": "#e88576", "id": "VectorizationInNN", "label": "VectorizationInNN", "shape": "star", "size": 25, "title": "Overview of vectorization in neural networks for efficiency."}, {"color": "#e0aa2e", "id": "SpeedPerspective", "label": "SpeedPerspective", "shape": "dot", "size": 10, "title": "Discussion on the importance of speed in implementing neural networks."}, {"color": "#32bea9", "id": "ParallelismGPUs", "label": "ParallelismGPUs", "shape": "dot", "size": 10, "title": "Role of parallel processing in GPUs for deep learning efficiency."}, {"color": "#62908e", "id": "MatrixAlgebra", "label": "MatrixAlgebra", "shape": "dot", "size": 10, "title": "Use of matrix algebra and optimized numerical packages in vectorization."}, {"color": "#40fdcf", "id": "TwoLayerNetwork", "label": "TwoLayerNetwork", "shape": "dot", "size": 10, "title": "Definition and vectorization of a two-layer fully-connected neural network."}, {"color": "#8fa0bd", "id": "WeightMatrices", "label": "WeightMatrices", "shape": "dot", "size": 10, "title": "Matrix representation of weights in a neural network layer."}, {"color": "#f6b98c", "id": "BiasVectors", "label": "BiasVectors", "shape": "dot", "size": 10, "title": "Vector representation of biases in a neural network layer."}, {"color": "#a7a797", "id": "ActivationFunctions", "label": "ActivationFunctions", "shape": "dot", "size": 10, "title": "Non-linear functions applied element-wise to the output of a neuron or layer."}, {"color": "#c5d1b0", "id": "LayerArchitecture", "label": "LayerArchitecture", "shape": "dot", "size": 10, "title": "Structure of layers including input, hidden, and output layers in a neural network."}, {"color": "#177aaa", "id": "Multi-layer Neural Networks", "label": "Multi-layer Neural Networks", "shape": "star", "size": 25, "title": "A neural network with multiple layers of neurons connected fully to each other."}, {"color": "#9c73c7", "id": "Weight Matrices and Biases", "label": "Weight Matrices and Biases", "shape": "dot", "size": 10, "title": "Matrices and vectors that define the connections and biases in a multi-layer network."}, {"color": "#f64243", "id": "ReLU Activation Function", "label": "ReLU Activation Function", "shape": "dot", "size": 10, "title": "A non-linear activation function used to introduce non-linearity into the model."}, {"color": "#bf6eb2", "id": "Total Number of Neurons", "label": "Total Number of Neurons", "shape": "dot", "size": 10, "title": "The sum of neurons across all layers in a multi-layer network."}, {"color": "#3f5137", "id": "Total Parameters Count", "label": "Total Parameters Count", "shape": "dot", "size": 10, "title": "Sum of weights and biases across all layers."}, {"color": "#0cdf0b", "id": "Notational Consistency", "label": "Notational Consistency", "shape": "dot", "size": 10, "title": "Consistent notation for inputs and outputs in multi-layer networks."}, {"color": "#f9265b", "id": "Other Activation Functions", "label": "Other Activation Functions", "shape": "star", "size": 25, "title": "Alternative non-linear functions used instead of ReLU in neural networks."}, {"color": "#51371c", "id": "TanhFunction", "label": "TanhFunction", "shape": "dot", "size": 10, "title": "Hyperbolic tangent function, similar to sigmoid but ranges from -1 to 1."}, {"color": "#b8c8ce", "id": "LeakyReLUFunction", "label": "LeakyReLUFunction", "shape": "dot", "size": 10, "title": "Variant of ReLU with a small slope for negative inputs."}, {"color": "#edd319", "id": "GELUFunction", "label": "GELUFunction", "shape": "dot", "size": 10, "title": "Gaussian Error Linear Unit, smooth non-linear function used in NLP models."}, {"color": "#573cf5", "id": "SoftplusFunction", "label": "SoftplusFunction", "shape": "dot", "size": 10, "title": "Smoothed ReLU variant with a proper second-order derivative."}, {"color": "#e4b5da", "id": "IdentityFunction", "label": "IdentityFunction", "shape": "dot", "size": 10, "title": "Linear function where output is equal to input, not commonly used in neural networks."}, {"color": "#292c38", "id": "Feature_Engineering", "label": "Feature_Engineering", "shape": "dot", "size": 10, "title": "Process of selecting and transforming raw data into features for use in machine learning models."}, {"color": "#4d5f30", "id": "Deep_Learning", "label": "Deep_Learning", "shape": "dot", "size": 10, "title": "Subfield of machine learning that uses neural networks to learn representations from data."}, {"color": "#9c5ae3", "id": "Feature_Maps", "label": "Feature_Maps", "shape": "dot", "size": 10, "title": "Functions used by deep learning models to transform input into a useful representation."}, {"color": "#e0df6c", "id": "Linear_Model", "label": "Linear_Model", "shape": "dot", "size": 10, "title": "Model used to predict outcomes based on features generated by a neural network."}, {"color": "#a46d4d", "id": "Deep Learning Representations", "label": "Deep Learning Representations", "shape": "star", "size": 25, "title": "Discusses how neural networks discover features for prediction."}, {"color": "#b904b4", "id": "House Price Prediction Example", "label": "House Price Prediction Example", "shape": "dot", "size": 10, "title": "Illustrates use of fully-connected network in predicting house prices."}, {"color": "#497496", "id": "Feature Discovery", "label": "Feature Discovery", "shape": "dot", "size": 10, "title": "Explains how neural networks automatically discover useful features."}, {"color": "#7f7b9b", "id": "Black Box Nature", "label": "Black Box Nature", "shape": "dot", "size": 10, "title": "Highlights difficulty in interpreting the discovered features by humans."}, {"color": "#e4aee4", "id": "Modern Neural Network Modules", "label": "Modern Neural Network Modules", "shape": "star", "size": 25, "title": "Introduces various building blocks of modern neural networks."}, {"color": "#64fc79", "id": "Matrix Multiplication Module", "label": "Matrix Multiplication Module", "shape": "dot", "size": 10, "title": "Describes the basic operation with parameters W and b."}, {"color": "#980fef", "id": "Nonlinear Activation Module", "label": "Nonlinear Activation Module", "shape": "dot", "size": 10, "title": "Explains role of nonlinear activation in neural network architecture."}, {"color": "#42c1cb", "id": "MLP Composition", "label": "MLP Composition", "shape": "dot", "size": 10, "title": "Describes MLP as a composition of matrix multiplication and activation modules."}, {"color": "#bdae8b", "id": "MLPArchitecture", "label": "MLPArchitecture", "shape": "dot", "size": 10, "title": "Description of the architecture of a Multi-Layer Perceptron (MLP)."}, {"color": "#9fddc2", "id": "MatrixMultiplicationModule", "label": "MatrixMultiplicationModule", "shape": "dot", "size": 10, "title": "Component of MLP that involves matrix multiplication and parameter sets."}, {"color": "#2bcf83", "id": "NonlinearActivationModule", "label": "NonlinearActivationModule", "shape": "dot", "size": 10, "title": "Component of MLP involving nonlinear activation functions such as ReLU, sigmoid, etc."}, {"color": "#088e0a", "id": "ResNetOverview", "label": "ResNetOverview", "shape": "star", "size": 25, "title": "Introduction to Residual Networks (ResNets) and their architecture."}, {"color": "#af905e", "id": "ResidualBlockDefinition", "label": "ResidualBlockDefinition", "shape": "dot", "size": 10, "title": "Simplified definition of a residual block in ResNet using matrix multiplication and activation functions."}, {"color": "#3636d7", "id": "ResNetComposition", "label": "ResNetComposition", "shape": "dot", "size": 10, "title": "Description of how ResNets are composed from multiple residual blocks followed by matrix multiplication."}, {"color": "#ed2485", "id": "ResNetArchitecture", "label": "ResNetArchitecture", "shape": "dot", "size": 10, "title": "Introduction to Residual Network architecture."}, {"color": "#be46fd", "id": "ConvolutionalLayers", "label": "ConvolutionalLayers", "shape": "dot", "size": 10, "title": "Discussion on convolution layers used in ResNet."}, {"color": "#65debc", "id": "BatchNormalization", "label": "BatchNormalization", "shape": "dot", "size": 10, "title": "Explanation of batch normalization technique."}, {"color": "#3c84cf", "id": "LayerNormalization", "label": "LayerNormalization", "shape": "star", "size": 25, "title": "Technique for normalizing the layer inputs in neural networks."}, {"color": "#1d0fe3", "id": "LN-SModule", "label": "LN-SModule", "shape": "dot", "size": 10, "title": "Definition and formula for the sub-module LN-S of layer normalization."}, {"color": "#26f4ec", "id": "TransformerArchitecture", "label": "TransformerArchitecture", "shape": "star", "size": 25, "title": "Overview of Transformer architecture including ResNet-S and layer normalization."}, {"color": "#ea6db4", "id": "LN-S", "label": "LN-S", "shape": "dot", "size": 10, "title": "Standardized version of layer normalization before affine transformation."}, {"color": "#081246", "id": "AffineTransformation", "label": "AffineTransformation", "shape": "dot", "size": 10, "title": "Transforms the output to have desired mean and standard deviation using learnable parameters."}, {"color": "#048c1e", "id": "LearnableParameters", "label": "LearnableParameters", "shape": "dot", "size": 10, "title": "Scalars beta and gamma that are learned during training."}, {"color": "#2b459e", "id": "ScalingInvariantProperty", "label": "ScalingInvariantProperty", "shape": "star", "size": 25, "title": "Property of layer normalization making the model invariant to parameter scaling."}, {"color": "#242750", "id": "MM_Wb", "label": "MM_Wb", "shape": "dot", "size": 10, "title": "Matrix multiplication with weights W and bias b used in the proof of scaling invariance."}, {"color": "#f0bafd", "id": "Normalization Techniques", "label": "Normalization Techniques", "shape": "star", "size": 25, "title": "Techniques for normalizing data in machine learning."}, {"color": "#facaee", "id": "Layer Normalization (LN)", "label": "Layer Normalization (LN)", "shape": "dot", "size": 10, "title": "Normalizes the inputs of each layer for stable training."}, {"color": "#1190e2", "id": "Equation 7.43", "label": "Equation 7.43", "shape": "dot", "size": 10, "title": "Mathematical representation of LN-S transformation."}, {"color": "#d4471f", "id": "Equation 7.44-7.47", "label": "Equation 7.44-7.47", "shape": "dot", "size": 10, "title": "Series of equations showing properties and transformations in layer normalization."}, {"color": "#fe187e", "id": "Scale-Invariant Property", "label": "Scale-Invariant Property", "shape": "dot", "size": 10, "title": "Property indicating network stability under weight scaling except for the last layer."}, {"color": "#829e97", "id": "Batch Normalization (BN)", "label": "Batch Normalization (BN)", "shape": "dot", "size": 10, "title": "Normalizes intermediate layers, commonly used in computer vision."}, {"color": "#a28a05", "id": "Group Normalization", "label": "Group Normalization", "shape": "dot", "size": 10, "title": "Alternative normalization method for fixed and controllable scaling."}, {"color": "#4387b6", "id": "Convolutional Layers", "label": "Convolutional Layers", "shape": "star", "size": 25, "title": "Layers in neural networks designed to process data with a grid-like topology."}, {"color": "#1f62c2", "id": "1-D Convolution (Conv1D)", "label": "1-D Convolution (Conv1D)", "shape": "dot", "size": 10, "title": "Simplified version of convolution layer for sequential data processing."}, {"color": "#14ce97", "id": "Convolutional_Neural_Networks", "label": "Convolutional_Neural_Networks", "shape": "dot", "size": 10, "title": "Introduction to convolutional neural networks in machine learning."}, {"color": "#7cdf34", "id": "1D_Convolution", "label": "1D_Convolution", "shape": "dot", "size": 10, "title": "Explanation of 1-dimensional convolution layers used in NLP."}, {"color": "#49ac66", "id": "Simplified_1D_Convolution", "label": "Simplified_1D_Convolution", "shape": "dot", "size": 10, "title": "Description of a simplified version of the 1-D convolution layer, Conv1D-S."}, {"color": "#b0d765", "id": "Filter_Vector", "label": "Filter_Vector", "shape": "dot", "size": 10, "title": "Definition and properties of the filter vector used in Conv1D-S."}, {"color": "#195649", "id": "Bias_Scalar", "label": "Bias_Scalar", "shape": "dot", "size": 10, "title": "Explanation of the bias scalar parameter in Conv1D-S."}, {"color": "#071bd7", "id": "Matrix_Multiplication", "label": "Matrix_Multiplication", "shape": "dot", "size": 10, "title": "Representation of Conv1D-S as a matrix multiplication with shared parameters Qz."}, {"color": "#fb0e7c", "id": "Convolutional_Layers", "label": "Convolutional_Layers", "shape": "dot", "size": 10, "title": "Discussion on convolutional layers in neural networks."}, {"color": "#a92d5d", "id": "Parameter_Sharing", "label": "Parameter_Sharing", "shape": "dot", "size": 10, "title": "Explanation of parameter sharing in convolutional layers."}, {"color": "#144766", "id": "Efficiency_of_Convolution", "label": "Efficiency_of_Convolution", "shape": "dot", "size": 10, "title": "Comparison of efficiency between convolution and generic matrix multiplication."}, {"color": "#04f58e", "id": "Conv1D_Channel_Variants", "label": "Conv1D_Channel_Variants", "shape": "dot", "size": 10, "title": "Discussion on the variants of Conv1D layers with multiple channels."}, {"color": "#f01588", "id": "Conv1D-S", "label": "Conv1D-S", "shape": "dot", "size": 10, "title": "One-dimensional convolutional module with distinct parameters for each channel."}, {"color": "#b205e7", "id": "TotalParametersConv1D", "label": "TotalParametersConv1D", "shape": "dot", "size": 10, "title": "Calculation of total number of parameters in Conv1D model."}, {"color": "#dd4072", "id": "LinearMappingComparison", "label": "LinearMappingComparison", "shape": "dot", "size": 10, "title": "Comparison with generic linear mapping parameter count."}, {"color": "#c7f546", "id": "ParameterTensorRepresentation", "label": "ParameterTensorRepresentation", "shape": "dot", "size": 10, "title": "Three-dimensional tensor representation of parameters."}, {"color": "#db2aa1", "id": "Conv2D-S", "label": "Conv2D-S", "shape": "dot", "size": 10, "title": "Two-dimensional convolutional module with distinct parameters for each channel."}, {"color": "#a3e833", "id": "TotalParametersConv2D", "label": "TotalParametersConv2D", "shape": "dot", "size": 10, "title": "Calculation of total number of parameters in Conv2D model."}, {"color": "#847426", "id": "Differentiable Circuit", "label": "Differentiable Circuit", "shape": "dot", "size": 10, "title": "Composition of arithmetic operations and elementary functions that can compute a real-valued function efficiently."}, {"color": "#caa187", "id": "Gradient Computation", "label": "Gradient Computation", "shape": "dot", "size": 10, "title": "Efficient computation of gradients for differentiable circuits in O(N) time."}, {"color": "#7bdba0", "id": "Theorem 7.4.1", "label": "Theorem 7.4.1", "shape": "dot", "size": 10, "title": "Informal statement of the theorem regarding gradient computation efficiency."}, {"color": "#67c85d", "id": "Chain Rule", "label": "Chain Rule", "shape": "dot", "size": 10, "title": "Mathematical rule for computing derivatives of composite functions."}, {"color": "#c32c86", "id": "Auto-differentiation", "label": "Auto-differentiation", "shape": "dot", "size": 10, "title": "Automatic computation of gradients in deep learning frameworks."}, {"color": "#c7065a", "id": "Deep Learning Packages", "label": "Deep Learning Packages", "shape": "dot", "size": 10, "title": "Software libraries and frameworks used for implementing deep learning models."}, {"color": "#78717d", "id": "MLPs (Multilayer Perceptrons)", "label": "MLPs (Multilayer Perceptrons)", "shape": "dot", "size": 10, "title": "Feedforward neural networks with multiple layers of perceptrons."}, {"color": "#1706f0", "id": "Partial_Derivatives", "label": "Partial_Derivatives", "shape": "dot", "size": 10, "title": "Discussion on partial derivatives and their complexities."}, {"color": "#8af506", "id": "Chain_Rule", "label": "Chain_Rule", "shape": "dot", "size": 10, "title": "Explanation of the chain rule in calculus for auto-differentiation."}, {"color": "#785e7a", "id": "Scalar_Variable_J", "label": "Scalar_Variable_J", "shape": "dot", "size": 10, "title": "Description of scalar variable J as a composition of functions f and g on z."}, {"color": "#63fc28", "id": "Vectorized_Notation", "label": "Vectorized_Notation", "shape": "dot", "size": 10, "title": "Explanation of the chain rule in vectorized notation for vectors z and u."}, {"color": "#b07cf8", "id": "Machine_Learning_Backward_Propagation", "label": "Machine_Learning_Backward_Propagation", "shape": "star", "size": 25, "title": "Overview of backward propagation in machine learning"}, {"color": "#91c4fe", "id": "Backward_Function_Linear_Map", "label": "Backward_Function_Linear_Map", "shape": "dot", "size": 10, "title": "Explanation of the backward function as a linear map"}, {"color": "#ce84db", "id": "Jacobian_Matrix_Transpose", "label": "Jacobian_Matrix_Transpose", "shape": "dot", "size": 10, "title": "The matrix in equation (7.54) is the transpose of Jacobian matrix"}, {"color": "#e7835c", "id": "Complexity_of_Jacobian_Matrices", "label": "Complexity_of_Jacobian_Matrices", "shape": "dot", "size": 10, "title": "Avoiding complications with Jacobian matrices for tensors"}, {"color": "#d938be", "id": "Equation_7.53_Usefulness", "label": "Equation_7.53_Usefulness", "shape": "dot", "size": 10, "title": "Explanation of the convenience and effectiveness of equation (7.53)"}, {"color": "#054436", "id": "Derivations_Section_7.4.3", "label": "Derivations_Section_7.4.3", "shape": "dot", "size": 10, "title": "Use of equation (7.53) in derivations"}, {"color": "#3866f5", "id": "Chain_Rule_Interpretation", "label": "Chain_Rule_Interpretation", "shape": "dot", "size": 10, "title": "Interpreting the chain rule for computing partial derivatives"}, {"color": "#c7c937", "id": "Loss Function Composition", "label": "Loss Function Composition", "shape": "dot", "size": 10, "title": "Abstract representation of loss functions as compositions of modules."}, {"color": "#37537e", "id": "Modules", "label": "Modules", "shape": "dot", "size": 10, "title": "Building blocks such as MM, \u03c3, Conv2D, LN used in neural networks."}, {"color": "#e9e996", "id": "BinaryClassificationProblem", "label": "BinaryClassificationProblem", "shape": "dot", "size": 10, "title": "A specific problem involving binary classification using a neural network model."}, {"color": "#d0431c", "id": "MLPModelDefinition", "label": "MLPModelDefinition", "shape": "dot", "size": 10, "title": "Definition of the MLP model used in the example, including parameters and layers."}, {"color": "#2feae8", "id": "LossFunctionFormulation", "label": "LossFunctionFormulation", "shape": "dot", "size": 10, "title": "Description of how the loss function is formulated for a binary classification problem."}, {"color": "#cbafd0", "id": "ModulesInMLP", "label": "ModulesInMLP", "shape": "dot", "size": 10, "title": "Explanation of different modules in an MLP, including linear and activation layers."}, {"color": "#830707", "id": "ParameterizationOfModules", "label": "ParameterizationOfModules", "shape": "dot", "size": 10, "title": "Discussion on how each module is parameterized or may involve fixed operations."}, {"color": "#79c7ab", "id": "ForwardPass", "label": "ForwardPass", "shape": "dot", "size": 10, "title": "Description of the forward pass, where intermediate variables are computed and saved."}, {"color": "#0ed72e", "id": "BackwardPass", "label": "BackwardPass", "shape": "dot", "size": 10, "title": "Explanation of the backward pass, involving computation of derivatives w.r.t. parameters and intermediate variables."}, {"color": "#54c26e", "id": "Machine_Learning_Backpropagation", "label": "Machine_Learning_Backpropagation", "shape": "star", "size": 25, "title": "Overview of backpropagation in machine learning."}, {"color": "#45d3cd", "id": "Chain_Rule_Application", "label": "Chain_Rule_Application", "shape": "dot", "size": 10, "title": "Application of the chain rule to compute gradients efficiently."}, {"color": "#ddec44", "id": "Efficient_Backward_Propagation", "label": "Efficient_Backward_Propagation", "shape": "dot", "size": 10, "title": "Discussion on the efficiency of backward propagation in neural networks."}, {"color": "#0478bf", "id": "NeuralNetworksComposition", "label": "NeuralNetworksComposition", "shape": "dot", "size": 10, "title": "Viewing neural networks as compositions of atomic operations."}, {"color": "#ff0a66", "id": "BackpropagationDiscussion", "label": "BackpropagationDiscussion", "shape": "dot", "size": 10, "title": "Discussion on backpropagation and its role in computing gradients."}, {"color": "#5ad49e", "id": "ModulesInPractice", "label": "ModulesInPractice", "shape": "dot", "size": 10, "title": "Practical modularization of neural networks using basic modules like matrix multiplication."}, {"color": "#ab4e7e", "id": "BackwardFunctionsBasics", "label": "BackwardFunctionsBasics", "shape": "star", "size": 25, "title": "Introduction to computing backward functions for basic modules in machine learning models."}, {"color": "#8f39cb", "id": "LossFunctionBackward", "label": "LossFunctionBackward", "shape": "dot", "size": 10, "title": "Details on the backward function computation for loss functions."}, {"color": "#27a443", "id": "BackwardFunctionWb", "label": "BackwardFunctionWb", "shape": "dot", "size": 10, "title": "Explanation of the backward function for parameters W and b."}, {"color": "#73bbc3", "id": "VectorizedNotation", "label": "VectorizedNotation", "shape": "dot", "size": 10, "title": "Expression in vectorized form for clarity."}, {"color": "#120bfb", "id": "EfficiencyConsiderations", "label": "EfficiencyConsiderations", "shape": "dot", "size": 10, "title": "Discussion on computational efficiency of the backward function."}, {"color": "#1cf7b1", "id": "BackwardFunctionActivations", "label": "BackwardFunctionActivations", "shape": "dot", "size": 10, "title": "Explanation of the backward function for element-wise activations."}, {"color": "#00a8a9", "id": "Machine_Learning_Backward_Functions", "label": "Machine_Learning_Backward_Functions", "shape": "star", "size": 25, "title": "Overview of backward functions in machine learning modules"}, {"color": "#2c0e57", "id": "Efficiency_of_Backward_Pass", "label": "Efficiency_of_Backward_Pass", "shape": "dot", "size": 10, "title": "Discussion on the computational efficiency of the backward pass"}, {"color": "#4fe875", "id": "Vectorized_Notation_Backward_Func", "label": "Vectorized_Notation_Backward_Func", "shape": "dot", "size": 10, "title": "Explanation of vectorized notation for backward functions"}, {"color": "#bb12f5", "id": "Squared_Loss_Backward", "label": "Squared_Loss_Backward", "shape": "dot", "size": 10, "title": "Derivation and explanation of the backward function for squared loss"}, {"color": "#378daa", "id": "Logistic_Loss_Backward", "label": "Logistic_Loss_Backward", "shape": "dot", "size": 10, "title": "Explanation of the backward function for logistic loss"}, {"color": "#7b3708", "id": "Cross_Entropy_Loss_Backward", "label": "Cross_Entropy_Loss_Backward", "shape": "dot", "size": 10, "title": "Explanation of the backward function for cross-entropy loss"}, {"color": "#f003ff", "id": "Loss Functions", "label": "Loss Functions", "shape": "dot", "size": 10, "title": "Different loss functions used in machine learning models."}, {"color": "#dd54fd", "id": "Logistic Loss", "label": "Logistic Loss", "shape": "dot", "size": 10, "title": "The logistic loss function and its gradient calculation."}, {"color": "#780f38", "id": "Backpropagation for MLPs", "label": "Backpropagation for MLPs", "shape": "star", "size": 25, "title": "Explanation of backpropagation in multi-layer perceptrons (MLPs)."}, {"color": "#6399d3", "id": "Forward Pass", "label": "Forward Pass", "shape": "dot", "size": 10, "title": "Sequence of operations during the forward pass through an r-layer MLP."}, {"color": "#6d88b9", "id": "Intermediate Values Storage", "label": "Intermediate Values Storage", "shape": "dot", "size": 10, "title": "Storing intermediate values for gradient computation in the backward pass."}, {"color": "#4296fb", "id": "Vectorization Over Training Examples", "label": "Vectorization Over Training Examples", "shape": "star", "size": 25, "title": "Techniques to vectorize neural network computations over multiple training examples."}, {"color": "#4e09dd", "id": "TrainingSetExamples", "label": "TrainingSetExamples", "shape": "dot", "size": 10, "title": "Explanation of training set examples and their representation."}, {"color": "#08d4f2", "id": "MatrixNotation", "label": "MatrixNotation", "shape": "dot", "size": 10, "title": "Use of matrix notation in representing multiple training examples."}, {"color": "#d0585b", "id": "LayerActivations", "label": "LayerActivations", "shape": "dot", "size": 10, "title": "First-layer activations for each example using matrix operations."}, {"color": "#735227", "id": "VectorizationTechniques", "label": "VectorizationTechniques", "shape": "dot", "size": 10, "title": "Techniques to vectorize operations in machine learning models."}, {"color": "#41c84a", "id": "Broadcasting", "label": "Broadcasting", "shape": "dot", "size": 10, "title": "Explanation of broadcasting technique for adding bias terms."}, {"color": "#ae536e", "id": "LayerGeneralization", "label": "LayerGeneralization", "shape": "dot", "size": 10, "title": "Discussion on generalizing matricization approach to multiple layers."}, {"color": "#370d97", "id": "Machine Learning", "label": "Machine Learning", "shape": "star", "size": 25, "title": "Field of study focusing on algorithms that learn from and make predictions on data."}, {"color": "#cdd231", "id": "Matricization Approach", "label": "Matricization Approach", "shape": "dot", "size": 10, "title": "Technique for organizing data in matrix form to facilitate multi-layer neural network implementation."}, {"color": "#6f4972", "id": "Data Matrix Representation", "label": "Data Matrix Representation", "shape": "dot", "size": 10, "title": "Representation of data points as rows in a matrix, differing from theoretical notation."}, {"color": "#ea686a", "id": "Conversion Between Representations", "label": "Conversion Between Representations", "shape": "dot", "size": 10, "title": "Process to convert between row-major and column-major representations in deep learning implementations."}, {"color": "#2d75e5", "id": "Training Loss Function", "label": "Training Loss Function", "shape": "dot", "size": 10, "title": "Function used during training to minimize error between predicted and actual outcomes."}, {"color": "#48c307", "id": "Training_Loss", "label": "Training_Loss", "shape": "dot", "size": 10, "title": "Evaluation metric for training data, often mean squared error."}, {"color": "#b4123b", "id": "Test_Error", "label": "Test_Error", "shape": "dot", "size": 10, "title": "Most important evaluation metric on unseen test examples."}, {"color": "#c721e6", "id": "Empirical_Distribution", "label": "Empirical_Distribution", "shape": "dot", "size": 10, "title": "Distribution based on the training dataset, denoted by \u02dc\u0394."}, {"color": "#e2072f", "id": "Population_Distribution", "label": "Population_Distribution", "shape": "dot", "size": 10, "title": "True distribution of data from which test examples are drawn, denoted by D."}, {"color": "#e654d9", "id": "Training_Data_Set", "label": "Training_Data_Set", "shape": "dot", "size": 10, "title": "Data used to train the model, seen during training."}, {"color": "#8ad3af", "id": "Test_Data_Set", "label": "Test_Data_Set", "shape": "dot", "size": 10, "title": "Unseen data used for evaluating model performance."}, {"color": "#b3c30e", "id": "Learning_Settings", "label": "Learning_Settings", "shape": "dot", "size": 10, "title": "Settings under which a model learns from data."}, {"color": "#940118", "id": "Training_Distribution", "label": "Training_Distribution", "shape": "dot", "size": 10, "title": "The distribution of training examples in machine learning."}, {"color": "#7df3ab", "id": "Test_Distribution", "label": "Test_Distribution", "shape": "dot", "size": 10, "title": "The distribution of test examples used to evaluate model performance."}, {"color": "#e60031", "id": "Domain_Shift", "label": "Domain_Shift", "shape": "dot", "size": 10, "title": "Situation where training and test distributions differ."}, {"color": "#c7aa47", "id": "Overfitting_Underfitting", "label": "Overfitting_Underfitting", "shape": "dot", "size": 10, "title": "Concepts describing model performance on unseen data."}, {"color": "#2c4781", "id": "Test_Error_Training_Error", "label": "Test_Error_Training_Error", "shape": "dot", "size": 10, "title": "Difference between errors measured on training and test datasets."}, {"color": "#419072", "id": "Generalization_Gap", "label": "Generalization_Gap", "shape": "dot", "size": 10, "title": "The difference between the test error and training error."}, {"color": "#23be0a", "id": "Bias_Variance_Tradoff", "label": "Bias_Variance_Tradoff", "shape": "star", "size": 25, "title": "Analysis of model performance in terms of bias and variance."}, {"color": "#f1ba76", "id": "Model_Parameterizations", "label": "Model_Parameterizations", "shape": "dot", "size": 10, "title": "Impact of parameter choices on test error decomposition."}, {"color": "#c6e487", "id": "Bias-Variance Tradeoff", "label": "Bias-Variance Tradeoff", "shape": "dot", "size": 10, "title": "Formalization and discussion of bias-variance tradeoff in machine learning models."}, {"color": "#9904b5", "id": "Double Descent Phenomenon", "label": "Double Descent Phenomenon", "shape": "dot", "size": 10, "title": "Describes the unexpected decrease in test error after an initial increase with model complexity or data size."}, {"color": "#38d534", "id": "Training and Test Datasets", "label": "Training and Test Datasets", "shape": "dot", "size": 10, "title": "Illustrates the use of training and test datasets to evaluate model performance."}, {"color": "#6d5e77", "id": "Linear Regression Example", "label": "Linear Regression Example", "shape": "dot", "size": 10, "title": "Provides an example using linear regression models on a quadratic function dataset."}, {"color": "#ae7d54", "id": "Model Complexity", "label": "Model Complexity", "shape": "dot", "size": 10, "title": "Discusses the impact of model complexity on underfitting and overfitting."}, {"color": "#c6e9da", "id": "Machine_Learning_Bias_Variance_Tradeoff", "label": "Machine_Learning_Bias_Variance_Tradeoff", "shape": "star", "size": 25, "title": "Exploration of bias and variance in machine learning hypothesis classes."}, {"color": "#f8890c", "id": "Linear_Model_Failure", "label": "Linear_Model_Failure", "shape": "dot", "size": 10, "title": "Failure of linear models to capture data structure despite large training sets."}, {"color": "#c55e89", "id": "Bias_Definition", "label": "Bias_Definition", "shape": "dot", "size": 10, "title": "Definition and implications of model bias in the context of fitting errors."}, {"color": "#e01c22", "id": "5th_Degree_Polynomial_Failure", "label": "5th_Degree_Polynomial_Failure", "shape": "dot", "size": 10, "title": "Failure of 5th-degree polynomials to generalize despite capturing training data well."}, {"color": "#ef9fe7", "id": "Generalization_Error", "label": "Generalization_Error", "shape": "dot", "size": 10, "title": "Probability that a hypothesis will misclassify new data drawn from the same distribution."}, {"color": "#bc853e", "id": "PolynomialFitting", "label": "PolynomialFitting", "shape": "dot", "size": 10, "title": "Discussion on fitting polynomials to data sets."}, {"color": "#d74969", "id": "Variance", "label": "Variance", "shape": "dot", "size": 10, "title": "Description of variance in model fitting procedures."}, {"color": "#b32254", "id": "BiasVsVarianceTradeoff", "label": "BiasVsVarianceTradeoff", "shape": "dot", "size": 10, "title": "Discussion on the trade-off between bias and variance in models."}, {"color": "#fe90a3", "id": "Test Error Decomposition", "label": "Test Error Decomposition", "shape": "dot", "size": 10, "title": "Decomposes test error into bias and variance components."}, {"color": "#e805e4", "id": "Mathematical Decomposition for Regression", "label": "Mathematical Decomposition for Regression", "shape": "star", "size": 25, "title": "Formal mathematical description of bias-variance tradeoff in regression problems."}, {"color": "#d23bf9", "id": "BiasVarianceTradeoff", "label": "BiasVarianceTradeoff", "shape": "dot", "size": 10, "title": "Exploration of the trade-off between model complexity and error types."}, {"color": "#03cfa4", "id": "TrainingDataset", "label": "TrainingDataset", "shape": "dot", "size": 10, "title": "Description of a training dataset for regression problems."}, {"color": "#5d0ae2", "id": "TestExample", "label": "TestExample", "shape": "dot", "size": 10, "title": "Explanation of how to use a test example in the context of model evaluation."}, {"color": "#c0c1ae", "id": "MSE", "label": "MSE", "shape": "dot", "size": 10, "title": "Definition and calculation of Mean Squared Error (MSE) for evaluating models."}, {"color": "#f9226c", "id": "Claim8.1.1", "label": "Claim8.1.1", "shape": "dot", "size": 10, "title": "Mathematical claim used to decompose MSE into bias and variance terms."}, {"color": "#7b2e51", "id": "MSEDecomposition", "label": "MSEDecomposition", "shape": "dot", "size": 10, "title": "Breakdown of Mean Squared Error into bias and variance components."}, {"color": "#049485", "id": "AverageModel", "label": "AverageModel", "shape": "dot", "size": 10, "title": "Definition and properties of the average model in machine learning theory."}, {"color": "#1db7c2", "id": "BiasTerm", "label": "BiasTerm", "shape": "dot", "size": 10, "title": "Explanation of bias as a component of error due to model limitations."}, {"color": "#28852e", "id": "VarianceTerm", "label": "VarianceTerm", "shape": "dot", "size": 10, "title": "Explanation of variance as the variability in predictions across different datasets."}, {"color": "#48d77c", "id": "Machine_Learning_Bias_Variance", "label": "Machine_Learning_Bias_Variance", "shape": "star", "size": 25, "title": "Overview of bias and variance in machine learning models."}, {"color": "#03776f", "id": "Bias_Term", "label": "Bias_Term", "shape": "dot", "size": 10, "title": "Explains the concept of bias term in model approximation."}, {"color": "#e49f00", "id": "Variance_Term", "label": "Variance_Term", "shape": "dot", "size": 10, "title": "Describes how variance captures errors due to dataset randomness."}, {"color": "#fee6b0", "id": "Noise_Prediction_Impact", "label": "Noise_Prediction_Impact", "shape": "dot", "size": 10, "title": "Discusses the impact of noise prediction limitations."}, {"color": "#846e6f", "id": "Bias_Variance_Classification", "label": "Bias_Variance_Classification", "shape": "dot", "size": 10, "title": "Notes on bias-variance decomposition for classification problems."}, {"color": "#e57c5f", "id": "Double_Descent_Phenomenon", "label": "Double_Descent_Phenomenon", "shape": "star", "size": 25, "title": "Introduction to the double descent phenomenon in machine learning models."}, {"color": "#df120c", "id": "Model_Wise_Double_Descent", "label": "Model_Wise_Double_Descent", "shape": "dot", "size": 10, "title": "Explains model-wise double descent observed in various ML models."}, {"color": "#ef99e7", "id": "Model-wise Double Descent", "label": "Model-wise Double Descent", "shape": "dot", "size": 10, "title": "Test error decreases, increases, then decreases again as model parameters exceed training data size."}, {"color": "#ae69d2", "id": "Sample-wise Double Descent", "label": "Sample-wise Double Descent", "shape": "dot", "size": 10, "title": "Similar to model-wise but considers the effect of increasing sample size on test error."}, {"color": "#a50840", "id": "Overparameterized Models", "label": "Overparameterized Models", "shape": "dot", "size": 10, "title": "Models with more parameters than necessary, often used in deep learning."}, {"color": "#b86684", "id": "Historical Context", "label": "Historical Context", "shape": "dot", "size": 10, "title": "Early work by Opper and recent popularization by Belkin et al., Hastie et al."}, {"color": "#0118fd", "id": "Optimal Regularization", "label": "Optimal Regularization", "shape": "dot", "size": 10, "title": "Improves performance by tuning regularization parameters effectively."}, {"color": "#26e59d", "id": "Implicit Regularization", "label": "Implicit Regularization", "shape": "dot", "size": 10, "title": "Effect of optimizers like gradient descent in overparameterized models."}, {"color": "#dd3681", "id": "Regularization Techniques", "label": "Regularization Techniques", "shape": "dot", "size": 10, "title": "Techniques to prevent overfitting such as L2 regularization."}, {"color": "#824dfe", "id": "Parameter Count vs. Model Norm", "label": "Parameter Count vs. Model Norm", "shape": "dot", "size": 10, "title": "Comparison between using number of parameters and norm as measures of model complexity."}, {"color": "#29b269", "id": "Linear Regression Setup", "label": "Linear Regression Setup", "shape": "dot", "size": 10, "title": "Setup for linear regression with specific dataset and input/output configurations."}, {"color": "#54807b", "id": "Sample Complexity Bounds", "label": "Sample Complexity Bounds", "shape": "star", "size": 25, "title": "Theoretical bounds on the number of samples required for learning algorithms to generalize well."}, {"color": "#36cb9b", "id": "Model Selection Methods", "label": "Model Selection Methods", "shape": "dot", "size": 10, "title": "Methods for selecting the appropriate model complexity based on training data performance."}, {"color": "#9ec76a", "id": "Generalization Error", "label": "Generalization Error", "shape": "dot", "size": 10, "title": "Discussion on how well a model performs on unseen data compared to training set accuracy."}, {"color": "#9a978e", "id": "Learning Theory Proofs", "label": "Learning Theory Proofs", "shape": "star", "size": 25, "title": "Conditions and lemmas to prove learning algorithms work well."}, {"color": "#ae34cb", "id": "Union Bound Lemma", "label": "Union Bound Lemma", "shape": "dot", "size": 10, "title": "Probability bound for union of events."}, {"color": "#719e0d", "id": "Hoeffding Inequality", "label": "Hoeffding Inequality", "shape": "dot", "size": 10, "title": "Bound on deviation between sample mean and true probability."}, {"color": "#32848c", "id": "Training Set", "label": "Training Set", "shape": "dot", "size": 10, "title": "Set of input-output pairs used for training the model."}, {"color": "#594a71", "id": "Binary_Classification", "label": "Binary_Classification", "shape": "dot", "size": 10, "title": "Classification where labels are binary (0 or 1)."}, {"color": "#b32b70", "id": "Hypothesis", "label": "Hypothesis", "shape": "dot", "size": 10, "title": "A function that maps input data to predicted labels."}, {"color": "#cecb61", "id": "Training_Error", "label": "Training_Error", "shape": "dot", "size": 10, "title": "Error calculated on the training dataset used to optimize model parameters."}, {"color": "#d14376", "id": "PAC_Assumptions", "label": "PAC_Assumptions", "shape": "dot", "size": 10, "title": "Probably approximately correct assumptions in learning theory."}, {"color": "#b6c0f9", "id": "Linear_Classification", "label": "Linear_Classification", "shape": "dot", "size": 10, "title": "Classification using linear functions to separate data into classes."}, {"color": "#2a69d4", "id": "Empirical_Risk_Minimization", "label": "Empirical_Risk_Minimization", "shape": "dot", "size": 10, "title": "Process of selecting a hypothesis with the smallest training error from a set of hypotheses."}, {"color": "#0e77cf", "id": "Hypothesis_Class", "label": "Hypothesis_Class", "shape": "dot", "size": 10, "title": "Set of all classifiers considered by a learning algorithm, abstracting from specific parameterization."}, {"color": "#2bd852", "id": "Finite_Hypothesis_Class", "label": "Finite_Hypothesis_Class", "shape": "dot", "size": 10, "title": "Case where hypothesis class consists of finite number of hypotheses for easier analysis."}, {"color": "#56f92d", "id": "Hoeffding_Inequality", "label": "Hoeffding_Inequality", "shape": "dot", "size": 10, "title": "Statistical tool used to bound the probability that the observed error deviates from true error by more than a specified amount."}, {"color": "#5ed1aa", "id": "Bernoulli_Random_Variables", "label": "Bernoulli_Random_Variables", "shape": "dot", "size": 10, "title": "Random variables indicating whether a hypothesis misclassifies an example drawn from distribution D."}, {"color": "#145b8c", "id": "Uniform_Convergence", "label": "Uniform_Convergence", "shape": "star", "size": 25, "title": "Property ensuring that training error closely approximates generalization error for all hypotheses in the hypothesis space."}, {"color": "#6b5276", "id": "Training_Error_Generalization_Error_Difference", "label": "Training_Error_Generalization_Error_Difference", "shape": "dot", "size": 10, "title": "Difference between training and generalization errors for a hypothesis set"}, {"color": "#db1197", "id": "Union_Bound_Application", "label": "Union_Bound_Application", "shape": "dot", "size": 10, "title": "Application of the union bound to extend individual hypothesis error bounds to all hypotheses"}, {"color": "#5e34f9", "id": "Probability_Error_Bounds", "label": "Probability_Error_Bounds", "shape": "dot", "size": 10, "title": "Bounds on the probability that training error differs from generalization error by more than gamma"}, {"color": "#c3a97e", "id": "Quantities_of_Interest", "label": "Quantities_of_Interest", "shape": "star", "size": 25, "title": "Three key quantities in machine learning: n (sample size), \u03b3 (error margin), and \u03b4 (probability of error)"}, {"color": "#b87f2c", "id": "Sample_Size_Calculation", "label": "Sample_Size_Calculation", "shape": "dot", "size": 10, "title": "Calculation to determine necessary sample size for a given probability of error bound"}, {"color": "#61a6b2", "id": "Error_Margin_Determination", "label": "Error_Margin_Determination", "shape": "dot", "size": 10, "title": "Determining the margin \u03b3 based on desired confidence level and sample size"}, {"color": "#418515", "id": "Machine_Learning_Theory", "label": "Machine_Learning_Theory", "shape": "star", "size": 25, "title": "Theoretical foundations of machine learning including bounds and complexity."}, {"color": "#d1243c", "id": "Generalization_Error_Bound", "label": "Generalization_Error_Bound", "shape": "dot", "size": 10, "title": "Bound on how much worse \u02c6h can be compared to h* in terms of generalization error."}, {"color": "#ce0d69", "id": "Sample_Complexity", "label": "Sample_Complexity", "shape": "dot", "size": 10, "title": "Number of samples needed to achieve a certain level of performance with high probability."}, {"color": "#da3561", "id": "Hypothesis_Space_Size", "label": "Hypothesis_Space_Size", "shape": "dot", "size": 10, "title": "The number of hypotheses impacts the sample complexity logarithmically."}, {"color": "#1f7e72", "id": "Best_Hypothesis", "label": "Best_Hypothesis", "shape": "dot", "size": 10, "title": "The hypothesis with the lowest generalization error in the hypothesis space."}, {"color": "#0173d6", "id": "Machine_Learning_Theorem", "label": "Machine_Learning_Theorem", "shape": "star", "size": 25, "title": "A theorem relating uniform convergence and generalization error in machine learning."}, {"color": "#a42caa", "id": "Uniform_Convergence_Assumption", "label": "Uniform_Convergence_Assumption", "shape": "dot", "size": 10, "title": "Assumption that the empirical risk is close to true risk with high probability."}, {"color": "#436885", "id": "Bias_Variance_Tradeoff", "label": "Bias_Variance_Tradeoff", "shape": "dot", "size": 10, "title": "Discusses the reconciliation of modern ML practice with classical bias-variance trade-off concepts."}, {"color": "#e74e27", "id": "Hypothesis_Class_Switching", "label": "Hypothesis_Class_Switching", "shape": "dot", "size": 10, "title": "Discussion on switching from \u03a9 to a larger hypothesis class \u03a9\u0027."}, {"color": "#a15985", "id": "Bias_Decrease", "label": "Bias_Decrease", "shape": "dot", "size": 10, "title": "Explanation of how bias decreases when moving to a larger hypothesis class."}, {"color": "#13cb4c", "id": "Variance_Increase", "label": "Variance_Increase", "shape": "dot", "size": 10, "title": "Discussion on the increase in variance with a larger hypothesis class."}, {"color": "#311fa7", "id": "Sample_Complexity_Bound", "label": "Sample_Complexity_Bound", "shape": "dot", "size": 10, "title": "Derivation of sample complexity bound for finite \u03a9 classes."}, {"color": "#94b38f", "id": "Infinite_Hypothesis_Classes", "label": "Infinite_Hypothesis_Classes", "shape": "dot", "size": 10, "title": "Consideration of hypothesis classes parameterized by real numbers and their implications."}, {"color": "#204a39", "id": "Hypothesis_Class_Size", "label": "Hypothesis_Class_Size", "shape": "dot", "size": 10, "title": "Size of the hypothesis class in terms of parameters and bits used."}, {"color": "#59472d", "id": "Parameterization_Impact", "label": "Parameterization_Impact", "shape": "dot", "size": 10, "title": "Effect of model parameterization on sample complexity and learning guarantees."}, {"color": "#91f4dc", "id": "HypothesisClassParameterization", "label": "HypothesisClassParameterization", "shape": "dot", "size": 10, "title": "Exploration of how different parameterizations can represent the same hypothesis class."}, {"color": "#b5f7ce", "id": "LinearClassifierDefinition", "label": "LinearClassifierDefinition", "shape": "dot", "size": 10, "title": "Definition and representation of linear classifiers with varying parameters."}, {"color": "#c649b1", "id": "ShatteringConcept", "label": "ShatteringConcept", "shape": "dot", "size": 10, "title": "Explanation of the concept of shattering in hypothesis classes."}, {"color": "#2bf376", "id": "VCDimensionIntroduction", "label": "VCDimensionIntroduction", "shape": "dot", "size": 10, "title": "Definition and importance of Vapnik-Chervonenkis dimension for a hypothesis class."}, {"color": "#4b42ac", "id": "VC Dimension", "label": "VC Dimension", "shape": "star", "size": 25, "title": "Measure of the capacity of a statistical classification algorithm"}, {"color": "#5377b9", "id": "Shattering", "label": "Shattering", "shape": "dot", "size": 10, "title": "A set is shattered if every possible labeling can be achieved by some hypothesis in H"}, {"color": "#4f0614", "id": "Vapnik\u0027s Theorem", "label": "Vapnik\u0027s Theorem", "shape": "star", "size": 25, "title": "Theorem linking VC dimension to uniform convergence with high probability"}, {"color": "#e73809", "id": "Uniform Convergence", "label": "Uniform Convergence", "shape": "dot", "size": 10, "title": "Asymptotic property ensuring that empirical risk approximates true risk"}, {"color": "#60bafc", "id": "Corollary of Vapnik\u0027s Theorem", "label": "Corollary of Vapnik\u0027s Theorem", "shape": "star", "size": 25, "title": "Number of training examples needed for learning well is linear in VC dimension"}, {"color": "#d6aa6d", "id": "Chapter9", "label": "Chapter9", "shape": "star", "size": 25, "title": "Regularization and model selection in machine learning"}, {"color": "#e9353a", "id": "ModelComplexity", "label": "ModelComplexity", "shape": "dot", "size": 10, "title": "Measured by number of parameters or function of parameters"}, {"color": "#8124d1", "id": "RegularizerFunction", "label": "RegularizerFunction", "shape": "dot", "size": 10, "title": "Additional term added to training loss to control complexity"}, {"color": "#5767d8", "id": "TrainingLoss", "label": "TrainingLoss", "shape": "dot", "size": 10, "title": "Cost function that includes the regularizer term"}, {"color": "#911582", "id": "RegularizationParameter", "label": "RegularizationParameter", "shape": "dot", "size": 10, "title": "Controls the impact of regularization on model complexity"}, {"color": "#65c68a", "id": "Regularized Loss Function", "label": "Regularized Loss Function", "shape": "star", "size": 25, "title": "Combination of loss and regularizer to balance model fit and complexity."}, {"color": "#55d4d3", "id": "Loss J(\u03b8)", "label": "Loss J(\u03b8)", "shape": "dot", "size": 10, "title": "Measures how well the model fits the training data."}, {"color": "#4471b9", "id": "Regularizer R(\u03b8)", "label": "Regularizer R(\u03b8)", "shape": "dot", "size": 10, "title": "Penalizes model complexity to prevent overfitting."}, {"color": "#3cde86", "id": "Regularization Parameter \u03bb", "label": "Regularization Parameter \u03bb", "shape": "dot", "size": 10, "title": "Controls the trade-off between loss and regularizer."}, {"color": "#6226df", "id": "L2 Regularization", "label": "L2 Regularization", "shape": "dot", "size": 10, "title": "Encourages small model weights to reduce complexity."}, {"color": "#b51c2e", "id": "Weight Decay", "label": "Weight Decay", "shape": "dot", "size": 10, "title": "In deep learning, L2 regularization is equivalent to decaying the weights during training."}, {"color": "#2841ba", "id": "Sparsity Inducing Regularization", "label": "Sparsity Inducing Regularization", "shape": "dot", "size": 10, "title": "Promotes models with fewer non-zero parameters based on prior belief of sparsity."}, {"color": "#843671", "id": "Regularization in Machine Learning", "label": "Regularization in Machine Learning", "shape": "star", "size": 25, "title": "Techniques to prevent overfitting by adding constraints on model parameters."}, {"color": "#18312e", "id": "Sparsity Regularization", "label": "Sparsity Regularization", "shape": "dot", "size": 10, "title": "Encourages models to have fewer non-zero parameter values."}, {"color": "#fd6977", "id": "L1 Norm (LASSO)", "label": "L1 Norm (LASSO)", "shape": "dot", "size": 10, "title": "Promotes sparsity by penalizing the absolute value of parameters."}, {"color": "#bf220a", "id": "L2 Norm Regularization", "label": "L2 Norm Regularization", "shape": "dot", "size": 10, "title": "Penalizes the square of parameter values to reduce model complexity."}, {"color": "#47463b", "id": "Gradient Descent Incompatibility", "label": "Gradient Descent Incompatibility", "shape": "dot", "size": 10, "title": "L1 norm is not differentiable, making it incompatible with gradient descent methods."}, {"color": "#af0077", "id": "Kernel Methods Compatibility", "label": "Kernel Methods Compatibility", "shape": "dot", "size": 10, "title": "L2 regularization works well with kernel methods due to compatibility issues with L1."}, {"color": "#6d8f48", "id": "Deep Learning Regularization Techniques", "label": "Deep Learning Regularization Techniques", "shape": "dot", "size": 10, "title": "Includes weight decay, dropout, data augmentation, and more."}, {"color": "#e83e04", "id": "Regularization in Deep Learning", "label": "Regularization in Deep Learning", "shape": "star", "size": 25, "title": "Overview of regularization techniques and concepts in deep learning."}, {"color": "#4a2593", "id": "Explicit Regularization Techniques", "label": "Explicit Regularization Techniques", "shape": "dot", "size": 10, "title": "Techniques such as weight decay, dropout, data augmentation, spectral norm regularization, and Lipschitz regularization."}, {"color": "#62491e", "id": "Implicit Regularization Effect", "label": "Implicit Regularization Effect", "shape": "dot", "size": 10, "title": "The impact of optimizers on model parameters beyond explicit regularization."}, {"color": "#f977d0", "id": "Global Minima Diversity", "label": "Global Minima Diversity", "shape": "dot", "size": 10, "title": "Different optimizers converge to different global minima with varying generalization performance."}, {"color": "#f0b9b1", "id": "Optimizer Impact", "label": "Optimizer Impact", "shape": "dot", "size": 10, "title": "Optimizers can bias towards certain types of global minima, affecting model generalization."}, {"color": "#a43909", "id": "Global Minima", "label": "Global Minima", "shape": "dot", "size": 10, "title": "Different global minima can have varying test performance."}, {"color": "#b87669", "id": "Learning Rate Schedules", "label": "Learning Rate Schedules", "shape": "dot", "size": 10, "title": "Impact of learning rate schedules on model generalization."}, {"color": "#291a11", "id": "Model Selection via Cross Validation", "label": "Model Selection via Cross Validation", "shape": "star", "size": 25, "title": "Process of selecting among different models using cross validation."}, {"color": "#477b9c", "id": "Model Selection", "label": "Model Selection", "shape": "star", "size": 25, "title": "Process of choosing the best model for a given task."}, {"color": "#37e890", "id": "Cross Validation", "label": "Cross Validation", "shape": "dot", "size": 10, "title": "Techniques for selecting and evaluating models in machine learning."}, {"color": "#6647db", "id": "Polynomial Regression Model", "label": "Polynomial Regression Model", "shape": "dot", "size": 10, "title": "Regression model using polynomial functions of the input variables."}, {"color": "#2b6a79", "id": "Bias and Variance Tradeoff", "label": "Bias and Variance Tradeoff", "shape": "dot", "size": 10, "title": "Balancing underfitting (high bias) and overfitting (high variance)."}, {"color": "#6577e4", "id": "Model Set M", "label": "Model Set M", "shape": "dot", "size": 10, "title": "Finite set of models to choose from."}, {"color": "#d5eda5", "id": "SVM", "label": "SVM", "shape": "dot", "size": 10, "title": "Support Vector Machine for classification and regression analysis."}, {"color": "#c4825a", "id": "Neural Network", "label": "Neural Network", "shape": "dot", "size": 10, "title": "Artificial neural network for machine learning tasks."}, {"color": "#a18558", "id": "EmpiricalRiskMinimization", "label": "EmpiricalRiskMinimization", "shape": "dot", "size": 10, "title": "Process of minimizing empirical risk for model selection."}, {"color": "#524394", "id": "CrossValidation", "label": "CrossValidation", "shape": "star", "size": 25, "title": "Technique to estimate the generalization error of models."}, {"color": "#33dbf6", "id": "HoldOutCrossValidation", "label": "HoldOutCrossValidation", "shape": "dot", "size": 10, "title": "Method involving splitting data into training and validation sets for model selection."}, {"color": "#7ece58", "id": "TrainingSetS", "label": "TrainingSetS", "shape": "dot", "size": 10, "title": "Dataset used to train models in empirical risk minimization."}, {"color": "#9f80fd", "id": "HypothesesHi", "label": "HypothesesHi", "shape": "dot", "size": 10, "title": "Set of hypotheses generated from training different models on the dataset."}, {"color": "#f7e4d8", "id": "TrainingError", "label": "TrainingError", "shape": "dot", "size": 10, "title": "Measure of error based on the training set used for model selection."}, {"color": "#beb821", "id": "DegreeOfPolynomial", "label": "DegreeOfPolynomial", "shape": "dot", "size": 10, "title": "Example parameter affecting model complexity and generalization ability."}, {"color": "#f76825", "id": "S_train", "label": "S_train", "shape": "dot", "size": 10, "title": "Subset of the training data used for training models in cross validation."}, {"color": "#f9e7a4", "id": "S_cv", "label": "S_cv", "shape": "dot", "size": 10, "title": "Subset of the training data used to validate model performance."}, {"color": "#a71bf9", "id": "ValidationError", "label": "ValidationError", "shape": "dot", "size": 10, "title": "Measure of error based on validation set, estimating generalization ability of models."}, {"color": "#0ab3f8", "id": "Machine_Learning_Techniques", "label": "Machine_Learning_Techniques", "shape": "star", "size": 25, "title": "Techniques used in machine learning for model evaluation and selection."}, {"color": "#a304bd", "id": "Model_Selection", "label": "Model_Selection", "shape": "dot", "size": 10, "title": "Process of choosing the best model based on validation set performance."}, {"color": "#e1f5d6", "id": "Validation_Set_Size", "label": "Validation_Set_Size", "shape": "dot", "size": 10, "title": "Determining an appropriate size for the validation dataset."}, {"color": "#841d2a", "id": "Cross_Validation", "label": "Cross_Validation", "shape": "dot", "size": 10, "title": "Technique to evaluate models by partitioning data into subsets."}, {"color": "#9b043f", "id": "Hold_Out_Cross_Validation", "label": "Hold_Out_Cross_Validation", "shape": "dot", "size": 10, "title": "Method where a portion of the dataset is held out for validation purposes."}, {"color": "#6a8c56", "id": "k_Fold_Cross_Validation", "label": "k_Fold_Cross_Validation", "shape": "dot", "size": 10, "title": "Technique that splits data into k partitions and iterates over these subsets for training and validation."}, {"color": "#8dbfac", "id": "Machine_Learning_Challenges", "label": "Machine_Learning_Challenges", "shape": "star", "size": 25, "title": "Challenges in machine learning when data is scarce."}, {"color": "#3f3574", "id": "Leave_One_Out_Cross_Validation", "label": "Leave_One_Out_Cross_Validation", "shape": "dot", "size": 10, "title": "Special case of cross validation where one data point is left out for testing."}, {"color": "#935bc4", "id": "Training_and_Testing_Process", "label": "Training_and_Testing_Process", "shape": "dot", "size": 10, "title": "Process of training models on subsets and testing on remaining data."}, {"color": "#cf567e", "id": "Leave-One-Out CV", "label": "Leave-One-Out CV", "shape": "dot", "size": 10, "title": "A method of cross validation where one training example is held out at a time."}, {"color": "#60faa0", "id": "Bayesian Statistics", "label": "Bayesian Statistics", "shape": "star", "size": 25, "title": "An approach to statistics that treats parameters as random variables with prior distributions."}, {"color": "#5e6372", "id": "MLE", "label": "MLE", "shape": "dot", "size": 10, "title": "Maximum likelihood estimation for parameter fitting without considering \u03b8 as a random variable."}, {"color": "#352c52", "id": "Prior Distribution", "label": "Prior Distribution", "shape": "dot", "size": 10, "title": "A probability distribution that represents prior beliefs about the parameters before observing data."}, {"color": "#e6c4e3", "id": "Bayesian Machine Learning", "label": "Bayesian Machine Learning", "shape": "star", "size": 25, "title": "Predictions made using posterior distributions on parameters."}, {"color": "#f009b8", "id": "Posterior Distribution", "label": "Posterior Distribution", "shape": "dot", "size": 10, "title": "Probability distribution over parameters given the data."}, {"color": "#daf98f", "id": "Bayes\u0027 Theorem", "label": "Bayes\u0027 Theorem", "shape": "dot", "size": 10, "title": "Formula for calculating posterior from likelihood and prior."}, {"color": "#ecf1d4", "id": "Likelihood Function", "label": "Likelihood Function", "shape": "dot", "size": 10, "title": "Function to maximize for parameter estimation"}, {"color": "#a22cc4", "id": "Prediction on New Data", "label": "Prediction on New Data", "shape": "dot", "size": 10, "title": "Making predictions using posterior distribution for new inputs."}, {"color": "#04f492", "id": "Expected Value Prediction", "label": "Expected Value Prediction", "shape": "dot", "size": 10, "title": "Predicting the expected value of y given x and S."}, {"color": "#d5a78b", "id": "Fully Bayesian Prediction", "label": "Fully Bayesian Prediction", "shape": "dot", "size": 10, "title": "Averaging predictions over posterior distribution p(\u03b8|S)."}, {"color": "#280094", "id": "Computational Challenges", "label": "Computational Challenges", "shape": "dot", "size": 10, "title": "Difficulty in computing high-dimensional integrals for the posterior."}, {"color": "#de1156", "id": "BayesianInference", "label": "BayesianInference", "shape": "dot", "size": 10, "title": "Techniques for estimating parameters using prior knowledge and data."}, {"color": "#77260a", "id": "PosteriorApproximation", "label": "PosteriorApproximation", "shape": "dot", "size": 10, "title": "Methods to approximate the posterior distribution when exact computation is infeasible."}, {"color": "#6f9e21", "id": "MAPEstimate", "label": "MAPEstimate", "shape": "dot", "size": 10, "title": "Finding a point estimate for parameters that maximizes the posterior probability."}, {"color": "#a98bdd", "id": "MLEvsMAP", "label": "MLEvsMAP", "shape": "dot", "size": 10, "title": "Comparison between maximum likelihood and MAP estimates, highlighting the role of prior distributions."}, {"color": "#dd752f", "id": "PriorDistributions", "label": "PriorDistributions", "shape": "dot", "size": 10, "title": "Selection of appropriate priors for parameter estimation, such as normal distribution with mean 0."}, {"color": "#c3bc83", "id": "UnsupervisedLearning", "label": "UnsupervisedLearning", "shape": "star", "size": 25, "title": "Techniques that learn from data without labeled responses."}, {"color": "#a329df", "id": "Clustering", "label": "Clustering", "shape": "dot", "size": 10, "title": "Grouping a set of objects in such a way that objects in the same group are more similar to each other than to those in other groups."}, {"color": "#ef3c94", "id": "KMeansAlgorithm", "label": "KMeansAlgorithm", "shape": "dot", "size": 10, "title": "A method for partitioning data into clusters based on minimizing the sum of squared distances from points to cluster centers."}, {"color": "#8d0aa9", "id": "k-means_algorithm", "label": "k-means_algorithm", "shape": "star", "size": 25, "title": "Clustering algorithm that partitions data into k clusters."}, {"color": "#0dbf00", "id": "distortion_function", "label": "distortion_function", "shape": "dot", "size": 10, "title": "Measures sum of squared distances between examples and cluster centroids."}, {"color": "#50e584", "id": "centroid_initialization", "label": "centroid_initialization", "shape": "dot", "size": 10, "title": "Randomly selects k training examples as initial centroids."}, {"color": "#cf8349", "id": "inner_loop_steps", "label": "inner_loop_steps", "shape": "dot", "size": 10, "title": "Assigns each example to closest centroid and updates centroid positions."}, {"color": "#bc5349", "id": "coordinate_descent", "label": "coordinate_descent", "shape": "dot", "size": 10, "title": "Optimization technique used in k-means for minimizing distortion function."}, {"color": "#d6ea2c", "id": "k-means Algorithm", "label": "k-means Algorithm", "shape": "dot", "size": 10, "title": "Clustering algorithm that partitions data into k clusters."}, {"color": "#c75922", "id": "Distortion Function J", "label": "Distortion Function J", "shape": "dot", "size": 10, "title": "Function measuring the quality of clustering; aims to minimize this value."}, {"color": "#1da048", "id": "Convergence in k-means", "label": "Convergence in k-means", "shape": "dot", "size": 10, "title": "Process by which cluster centroids and assignments stabilize over iterations."}, {"color": "#ce80e1", "id": "EM Algorithms", "label": "EM Algorithms", "shape": "star", "size": 25, "title": "Techniques for density estimation using iterative expectation-maximization process."}, {"color": "#b872fa", "id": "Mixture of Gaussians", "label": "Mixture of Gaussians", "shape": "dot", "size": 10, "title": "Modeling data with a combination of Gaussian distributions to capture complex patterns."}, {"color": "#5f9199", "id": "Unsupervised Learning", "label": "Unsupervised Learning", "shape": "star", "size": 25, "title": "Learning from data without labels"}, {"color": "#14f31d", "id": "Mixture of Gaussians Model", "label": "Mixture of Gaussians Model", "shape": "dot", "size": 10, "title": "Model using multiple Gaussian distributions for clustering"}, {"color": "#8f2b7d", "id": "Joint Distribution", "label": "Joint Distribution", "shape": "dot", "size": 10, "title": "Distribution combining latent and observed variables"}, {"color": "#cca623", "id": "Latent Variables", "label": "Latent Variables", "shape": "dot", "size": 10, "title": "Hidden random variables influencing data generation"}, {"color": "#5aa510", "id": "Parameter Estimation", "label": "Parameter Estimation", "shape": "dot", "size": 10, "title": "Estimating parameters \u03c6, \u03bc, and \u03a3 from data likelihood"}, {"color": "#06dc47", "id": "Closed Form Solution", "label": "Closed Form Solution", "shape": "dot", "size": 10, "title": "Infeasibility of deriving parameters directly from likelihood"}, {"color": "#aa63c3", "id": "DensityEstimation", "label": "DensityEstimation", "shape": "dot", "size": 10, "title": "Techniques for estimating probability density functions from data."}, {"color": "#6024a0", "id": "GaussianMixtureModel", "label": "GaussianMixtureModel", "shape": "dot", "size": 10, "title": "A probabilistic model that assumes all the data points are generated from a mixture of several Gaussian distributions with unknown parameters."}, {"color": "#e1ab37", "id": "EMAlgorithm", "label": "EMAlgorithm", "shape": "dot", "size": 10, "title": "Iterative method for finding maximum likelihood or maximum a posteriori estimates of parameters in statistical models, where the model depends on unobserved latent variables."}, {"color": "#0b1f82", "id": "EM_Algorithm", "label": "EM_Algorithm", "shape": "star", "size": 25, "title": "Iterative method for finding maximum likelihood or maximum a posteriori estimates in statistical models with latent variables."}, {"color": "#605261", "id": "E_Step", "label": "E_Step", "shape": "dot", "size": 10, "title": "Calculates posterior probabilities of latent variables given data and current parameter estimates."}, {"color": "#d82f7c", "id": "M_Step", "label": "M_Step", "shape": "dot", "size": 10, "title": "Updates parameters to maximize the expected log-likelihood found in E-step."}, {"color": "#73d9d0", "id": "Gaussian_Mixture_Models", "label": "Gaussian_Mixture_Models", "shape": "dot", "size": 10, "title": "Model that uses Gaussian distributions to represent subpopulations within an overall population."}, {"color": "#d22619", "id": "Soft_Assignments", "label": "Soft_Assignments", "shape": "dot", "size": 10, "title": "Assign probabilities to each latent variable rather than hard assignments."}, {"color": "#7b4e48", "id": "K_Means_Clustering", "label": "K_Means_Clustering", "shape": "dot", "size": 10, "title": "Algorithm for partitioning data into clusters, similar in concept but uses hard assignments."}, {"color": "#14ab00", "id": "Convergence_Issues", "label": "Convergence_Issues", "shape": "dot", "size": 10, "title": "Discussion on convergence properties of fitted value iteration compared to traditional value iteration."}, {"color": "#b66847", "id": "EM_Algorithm_Generalization", "label": "EM_Algorithm_Generalization", "shape": "dot", "size": 10, "title": "General view of the EM algorithm for estimation problems with latent variables."}, {"color": "#293f76", "id": "Jensens_Inequality", "label": "Jensens_Inequality", "shape": "star", "size": 25, "title": "A fundamental inequality used in convex analysis and probability theory."}, {"color": "#31e510", "id": "Convex_Functions", "label": "Convex_Functions", "shape": "dot", "size": 10, "title": "Functions where the second derivative is non-negative or positive definite for vector inputs."}, {"color": "#1d792b", "id": "Theorem_Jensens_Inequality", "label": "Theorem_Jensens_Inequality", "shape": "dot", "size": 10, "title": "Statement of Jensen\u0027s inequality and its implications on expectations and random variables."}, {"color": "#2aeae1", "id": "Jensen\u0027s Inequality", "label": "Jensen\u0027s Inequality", "shape": "star", "size": 25, "title": "Inequality relating expected values of convex functions and their arguments."}, {"color": "#9b6952", "id": "Convex Function", "label": "Convex Function", "shape": "dot", "size": 10, "title": "A function where the line segment between any two points on the graph lies above or on the graph."}, {"color": "#8fa3c1", "id": "Concave Function", "label": "Concave Function", "shape": "dot", "size": 10, "title": "Opposite of a convex function, lying below the line segments connecting its points."}, {"color": "#a502fc", "id": "E[f(X)] vs f(E[X])", "label": "E[f(X)] vs f(E[X])", "shape": "dot", "size": 10, "title": "Relationship between expected value of a function and function of an expected value for convex/concave functions."}, {"color": "#0e2867", "id": "EM Algorithm", "label": "EM Algorithm", "shape": "star", "size": 25, "title": "Iterative method to find maximum likelihood estimates in probabilistic models with latent variables."}, {"color": "#1f5303", "id": "OptimizationChallenges", "label": "OptimizationChallenges", "shape": "dot", "size": 10, "title": "Difficulties in optimizing parameters due to non-convex problems."}, {"color": "#be76a2", "id": "EMAlgorithmIntroduction", "label": "EMAlgorithmIntroduction", "shape": "dot", "size": 10, "title": "Efficient method for maximum likelihood estimation using E-step and M-step."}, {"color": "#952eb5", "id": "LatentVariables", "label": "LatentVariables", "shape": "dot", "size": 10, "title": "Use of latent variables to simplify optimization problems."}, {"color": "#139ce9", "id": "EStepMStepProcess", "label": "EStepMStepProcess", "shape": "dot", "size": 10, "title": "Iterative process of constructing a lower-bound and optimizing it."}, {"color": "#65ccbc", "id": "SingleExampleOptimization", "label": "SingleExampleOptimization", "shape": "dot", "size": 10, "title": "Simplifying the problem by considering optimization for a single example first."}, {"color": "#7414e3", "id": "SummationNotEssential", "label": "SummationNotEssential", "shape": "dot", "size": 10, "title": "Explanation that summation is not essential and can be added later."}, {"color": "#a7dbef", "id": "ProbabilityDistributions", "label": "ProbabilityDistributions", "shape": "dot", "size": 10, "title": "Discussion on probability distributions used in ML models."}, {"color": "#f6f93e", "id": "JensensInequality", "label": "JensensInequality", "shape": "dot", "size": 10, "title": "Explanation of Jensen\u0027s inequality and its application in deriving lower bounds."}, {"color": "#55f2ef", "id": "LogLikelihoodBound", "label": "LogLikelihoodBound", "shape": "dot", "size": 10, "title": "Derivation of a lower bound for log likelihood using specific distributions Q."}, {"color": "#ef4995", "id": "EvidenceLowerBoundELBO", "label": "EvidenceLowerBoundELBO", "shape": "dot", "size": 10, "title": "Definition and importance of ELBO in machine learning models."}, {"color": "#d42f37", "id": "Log_Likelihood_Optimization", "label": "Log_Likelihood_Optimization", "shape": "dot", "size": 10, "title": "Process of maximizing the log-likelihood function to estimate parameters in probabilistic models."}, {"color": "#e72e54", "id": "Evidence_Lower_Bound_(ELBO)", "label": "Evidence_Lower_Bound_(ELBO)", "shape": "dot", "size": 10, "title": "Objective function used in variational inference and EM algorithm to approximate posterior distributions."}, {"color": "#620bf5", "id": "Multiple_Examples_Consideration", "label": "Multiple_Examples_Consideration", "shape": "dot", "size": 10, "title": "Extension of ELBO for multiple training examples, summing over individual example bounds."}, {"color": "#857017", "id": "ExpectationMaximizationAlgorithm", "label": "ExpectationMaximizationAlgorithm", "shape": "dot", "size": 10, "title": "Iterative method for finding maximum likelihood or maximum a posteriori estimates of parameters in statistical models."}, {"color": "#6ea269", "id": "EStep", "label": "EStep", "shape": "dot", "size": 10, "title": "Calculates the expected value of the log-likelihood, with respect to the current estimate of the hidden variables, given data and parameters."}, {"color": "#db36e7", "id": "MStep", "label": "MStep", "shape": "dot", "size": 10, "title": "The M-step maximizes the expected log-likelihood found in the E-step with respect to model parameters."}, {"color": "#0580f3", "id": "LogLikelihoodImprovement", "label": "LogLikelihoodImprovement", "shape": "dot", "size": 10, "title": "Explanation of how EM monotonically improves the log-likelihood at each iteration."}, {"color": "#b40690", "id": "ELBO", "label": "ELBO", "shape": "dot", "size": 10, "title": "Evidence Lower Bound (ELBO) is used in the E-step to approximate the posterior distribution."}, {"color": "#bbcc0a", "id": "ELBOExplanation", "label": "ELBOExplanation", "shape": "dot", "size": 10, "title": "Detailed explanation of Evidence Lower Bound (ELBO) definition and its various forms."}, {"color": "#f7bf79", "id": "AlternativeFormulationsOfELBO", "label": "AlternativeFormulationsOfELBO", "shape": "dot", "size": 10, "title": "Different mathematical formulations of ELBO including expectations and KL divergence."}, {"color": "#8b1948", "id": "MarginalDistributionIndependence", "label": "MarginalDistributionIndependence", "shape": "dot", "size": 10, "title": "Discussion on the independence of marginal distribution from parameter theta in ELBO maximization."}, {"color": "#8b1013", "id": "ConditionalLikelihoodSimplification", "label": "ConditionalLikelihoodSimplification", "shape": "dot", "size": 10, "title": "Explanation that maximizing conditional likelihood simplifies optimization problems compared to joint likelihood."}, {"color": "#81f4b3", "id": "EMAlgorithmOverview", "label": "EMAlgorithmOverview", "shape": "dot", "size": 10, "title": "Introduction and overview of the Expectation-Maximization (EM) algorithm."}, {"color": "#b21730", "id": "MixtureOfGaussiansExample", "label": "MixtureOfGaussiansExample", "shape": "dot", "size": 10, "title": "Application of EM to fitting parameters in a mixture of Gaussian distributions example."}, {"color": "#f338d7", "id": "PhiParameterUpdate", "label": "PhiParameterUpdate", "shape": "dot", "size": 10, "title": "Specific derivation of the update rule for parameters \u03c6_j in the M-step."}, {"color": "#f8fcf2", "id": "MuParameterUpdate", "label": "MuParameterUpdate", "shape": "dot", "size": 10, "title": "Update rule for \u03bc_l parameter during M-step iteration."}, {"color": "#0aba02", "id": "SigmaParameterUpdate", "label": "SigmaParameterUpdate", "shape": "dot", "size": 10, "title": "Exercise left to reader: update rule for \u03a3_j parameter during M-step iteration."}, {"color": "#2bbc2f", "id": "MStepUpdateRule", "label": "MStepUpdateRule", "shape": "dot", "size": 10, "title": "Derivation and update rule for the M-step in EM algorithm, focusing on parameter updates."}, {"color": "#7b4c87", "id": "LagrangianConstruction", "label": "LagrangianConstruction", "shape": "dot", "size": 10, "title": "Use of Lagrangian to handle constraints during parameter updates."}, {"color": "#19c88e", "id": "VariationalInference", "label": "VariationalInference", "shape": "star", "size": 25, "title": "Techniques for approximating probability distributions in complex models using variational methods."}, {"color": "#8c6115", "id": "VariationalAutoEncoder", "label": "VariationalAutoEncoder", "shape": "dot", "size": 10, "title": "Family of algorithms extending EM to more complex models parameterized by neural networks."}, {"color": "#622f28", "id": "Variational_Autoencoder", "label": "Variational_Autoencoder", "shape": "dot", "size": 10, "title": "A type of neural network architecture used to learn latent variable models."}, {"color": "#836b35", "id": "EM_Algorithms", "label": "EM_Algorithms", "shape": "dot", "size": 10, "title": "Expectation-Maximization algorithm for parameter estimation in statistical models."}, {"color": "#c69e35", "id": "Variational_Inference", "label": "Variational_Inference", "shape": "dot", "size": 10, "title": "Technique to approximate posterior distributions in Bayesian inference."}, {"color": "#cabc2d", "id": "Reparametrization_Trick", "label": "Reparametrization_Trick", "shape": "dot", "size": 10, "title": "Method for sampling from a distribution using differentiable transformations."}, {"color": "#382ef5", "id": "Posterior_Distribution", "label": "Posterior_Distribution", "shape": "dot", "size": 10, "title": "Probability distribution of an unobserved variable given observed data."}, {"color": "#1dd10e", "id": "Variational Inference", "label": "Variational Inference", "shape": "star", "size": 25, "title": "Approximating true posterior distribution using a family of Q distributions."}, {"color": "#24d56c", "id": "ELBO Lower Bound", "label": "ELBO Lower Bound", "shape": "dot", "size": 10, "title": "Expected lower bound on log-likelihood used in variational inference optimization."}, {"color": "#01bf0e", "id": "Mean Field Assumption", "label": "Mean Field Assumption", "shape": "dot", "size": 10, "title": "Assumption that Q distribution can be decomposed into independent coordinates for discrete latent variables."}, {"color": "#6f753f", "id": "Continuous Latent Variables", "label": "Continuous Latent Variables", "shape": "dot", "size": 10, "title": "Handling continuous latent variables requires additional techniques beyond mean field assumptions."}, {"color": "#a9871f", "id": "Latent_Variables", "label": "Latent_Variables", "shape": "dot", "size": 10, "title": "Continuous latent variables used in probabilistic models."}, {"color": "#22f290", "id": "Gaussian_Distribution_Qi", "label": "Gaussian_Distribution_Qi", "shape": "dot", "size": 10, "title": "Distribution Qi modeled as a Gaussian with mean and variance functions of input x."}, {"color": "#9739f4", "id": "Mean_and_Variance_Functions", "label": "Mean_and_Variance_Functions", "shape": "dot", "size": 10, "title": "Functions q(x;phi) and v(x;psi) that determine the mean and diagonal covariance matrix for Qi."}, {"color": "#e09672", "id": "Encoder_Decoder_Networks", "label": "Encoder_Decoder_Networks", "shape": "dot", "size": 10, "title": "Components q and v (encoder) and g(z;theta) (decoder) in variational autoencoders."}, {"color": "#8b32fb", "id": "ELBO_Optimization", "label": "ELBO_Optimization", "shape": "dot", "size": 10, "title": "Process of optimizing the evidence lower bound for probabilistic models."}, {"color": "#0d6b44", "id": "ELBOOptimization", "label": "ELBOOptimization", "shape": "dot", "size": 10, "title": "Details on optimizing the Evidence Lower Bound (ELBO) in variational inference."}, {"color": "#cfce44", "id": "QFormRequirements", "label": "QFormRequirements", "shape": "dot", "size": 10, "title": "Conditions and requirements for the form of Q_i in ELBO optimization."}, {"color": "#eabf61", "id": "EfficientEvaluationOfELBO", "label": "EfficientEvaluationOfELBO", "shape": "dot", "size": 10, "title": "Methods to efficiently evaluate the value of ELBO given fixed Q and theta."}, {"color": "#8e58f8", "id": "GaussianDistributionQ_i", "label": "GaussianDistributionQ_i", "shape": "dot", "size": 10, "title": "Properties of Gaussian distribution for Q_i in efficient evaluation."}, {"color": "#2dc510", "id": "GradientAscentOptimization", "label": "GradientAscentOptimization", "shape": "dot", "size": 10, "title": "Using gradient ascent to optimize the ELBO parameters phi, psi, and theta."}, {"color": "#c28b2a", "id": "ELBO_Gradient_Computation", "label": "ELBO_Gradient_Computation", "shape": "dot", "size": 10, "title": "Computation of the gradient of the ELBO with respect to parameters."}, {"color": "#ea8e4a", "id": "Gradient_Simple_Case", "label": "Gradient_Simple_Case", "shape": "dot", "size": 10, "title": "Simple case where computing gradients is straightforward."}, {"color": "#4424b4", "id": "Complexity_in_Computing_Gradients", "label": "Complexity_in_Computing_Gradients", "shape": "dot", "size": 10, "title": "Challenges in computing gradients over parameters phi and psi due to dependency on sampling distribution Q_i."}, {"color": "#cef54f", "id": "Reparameterization_Trick", "label": "Reparameterization_Trick", "shape": "dot", "size": 10, "title": "Technique used to simplify gradient computation by re-parameterizing the random variable z^{(i)}."}, {"color": "#ea8dfe", "id": "Mathematical_Formulation_Reparametrization", "label": "Mathematical_Formulation_Reparametrization", "shape": "dot", "size": 10, "title": "Detailed mathematical formulation of the re-parametrization trick for simplifying gradient calculations."}, {"color": "#951557", "id": "Gradient_Estimation", "label": "Gradient_Estimation", "shape": "dot", "size": 10, "title": "Estimating the gradient of a policy\u0027s log probability with respect to parameters."}, {"color": "#a5f7b5", "id": "PCA_Method", "label": "PCA_Method", "shape": "star", "size": 25, "title": "Principal Components Analysis for dimensionality reduction in datasets."}, {"color": "#da323f", "id": "Dataset_Analysis", "label": "Dataset_Analysis", "shape": "dot", "size": 10, "title": "Analysis of dataset attributes using PCA."}, {"color": "#d0fcb9", "id": "RedundancyDetection", "label": "RedundancyDetection", "shape": "star", "size": 25, "title": "Identifying and removing redundant data attributes in machine learning."}, {"color": "#b36935", "id": "CarAttributesExample", "label": "CarAttributesExample", "shape": "dot", "size": 10, "title": "Illustration of redundancy with car speed measurements in mph and kph."}, {"color": "#8940a6", "id": "PilotSurveyExample", "label": "PilotSurveyExample", "shape": "dot", "size": 10, "title": "Illustration of correlation between piloting skill and enjoyment among RC helicopter pilots."}, {"color": "#356d8e", "id": "PCAAlgorithm", "label": "PCAAlgorithm", "shape": "star", "size": 25, "title": "Principal Component Analysis for dimensionality reduction in data sets."}, {"color": "#3480b5", "id": "DataNormalization", "label": "DataNormalization", "shape": "dot", "size": 10, "title": "Preprocessing step to normalize features before PCA application."}, {"color": "#1ce227", "id": "Mean Normalization", "label": "Mean Normalization", "shape": "dot", "size": 10, "title": "Subtracting the mean from each feature to center the data around zero."}, {"color": "#ae3a33", "id": "Variance Scaling", "label": "Variance Scaling", "shape": "dot", "size": 10, "title": "Dividing by standard deviation to ensure unit variance for comparable attributes."}, {"color": "#11f0ed", "id": "Data Rescaling", "label": "Data Rescaling", "shape": "dot", "size": 10, "title": "Adjusting data scale when features are already on the same level."}, {"color": "#5bab0b", "id": "Major Axis of Variation", "label": "Major Axis of Variation", "shape": "star", "size": 25, "title": "Finding the direction that maximizes variance for projected data."}, {"color": "#ee9b62", "id": "Projection Direction", "label": "Projection Direction", "shape": "dot", "size": 10, "title": "Selecting a unit vector to project data onto maximizing retained variance."}, {"color": "#01ad4d", "id": "PrincipalComponentAnalysis", "label": "PrincipalComponentAnalysis", "shape": "star", "size": 25, "title": "Technique for reducing dimensionality while retaining variance."}, {"color": "#344bd3", "id": "ProjectionOntoDirectionU", "label": "ProjectionOntoDirectionU", "shape": "dot", "size": 10, "title": "Process of projecting data points onto a unit vector u to maximize variance."}, {"color": "#628cd4", "id": "VarianceMaximization", "label": "VarianceMaximization", "shape": "dot", "size": 10, "title": "Objective is to find direction u that maximizes the variance of projections."}, {"color": "#3c79a4", "id": "EmpiricalCovarianceMatrix", "label": "EmpiricalCovarianceMatrix", "shape": "dot", "size": 10, "title": "Matrix representing data covariance, used in PCA calculations."}, {"color": "#a6cd63", "id": "LagrangeMultipliersMethod", "label": "LagrangeMultipliersMethod", "shape": "dot", "size": 10, "title": "Mathematical method to solve optimization problems with constraints."}, {"color": "#db616e", "id": "PrincipalEigenvector", "label": "PrincipalEigenvector", "shape": "dot", "size": 10, "title": "Direction u that maximizes variance, found as eigenvector of covariance matrix."}, {"color": "#3faf9b", "id": "kDimensionalSubspace", "label": "kDimensionalSubspace", "shape": "dot", "size": 10, "title": "Generalization to projecting data into k-dimensional subspace using top k eigenvectors."}, {"color": "#0a91aa", "id": "Principal Component Analysis (PCA)", "label": "Principal Component Analysis (PCA)", "shape": "star", "size": 25, "title": "A statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components."}, {"color": "#ce1ded", "id": "Dimensionality Reduction", "label": "Dimensionality Reduction", "shape": "dot", "size": 10, "title": "Process of converting high-dimensional data into a lower-dimensional space."}, {"color": "#8ee901", "id": "Eigenvectors and Eigenvalues", "label": "Eigenvectors and Eigenvalues", "shape": "dot", "size": 10, "title": "Key concepts in PCA for finding principal components."}, {"color": "#08f105", "id": "Orthogonal Basis", "label": "Orthogonal Basis", "shape": "dot", "size": 10, "title": "Set of orthogonal vectors used to represent data in a lower-dimensional space."}, {"color": "#2c1fa1", "id": "Approximation Error Minimization", "label": "Approximation Error Minimization", "shape": "dot", "size": 10, "title": "Method for deriving PCA by minimizing projection errors onto k-dimensional subspace."}, {"color": "#3383b5", "id": "Data Visualization", "label": "Data Visualization", "shape": "dot", "size": 10, "title": "Application of PCA to visualize high-dimensional data in 2D or 3D."}, {"color": "#f977ec", "id": "Compression", "label": "Compression", "shape": "dot", "size": 10, "title": "Use of PCA for compressing high-dimensional data into lower dimensions."}, {"color": "#486d42", "id": "Dimensionality Reduction Techniques", "label": "Dimensionality Reduction Techniques", "shape": "star", "size": 25, "title": "Techniques to reduce the number of random variables under consideration."}, {"color": "#eb3717", "id": "Plotting Similarity", "label": "Plotting Similarity", "shape": "dot", "size": 10, "title": "Visualizing data points in PCA space to identify similar and clustered groups."}, {"color": "#0603f9", "id": "Dimension Reduction Before Supervised Learning", "label": "Dimension Reduction Before Supervised Learning", "shape": "dot", "size": 10, "title": "Preprocessing step that reduces dimensionality before applying supervised learning algorithms."}, {"color": "#2f309b", "id": "Noise Reduction", "label": "Noise Reduction", "shape": "dot", "size": 10, "title": "Using PCA to estimate intrinsic features from noisy data, such as estimating piloting skill from noisy measurements."}, {"color": "#baefa1", "id": "Eigenfaces Method", "label": "Eigenfaces Method", "shape": "dot", "size": 10, "title": "Application of PCA in face recognition by reducing the dimensionality of image vectors."}, {"color": "#1a4563", "id": "Independent Components Analysis (ICA)", "label": "Independent Components Analysis (ICA)", "shape": "star", "size": 25, "title": "A computational method for separating a multivariate signal into independent, non-Gaussian components assuming that the recorded signals are linear mixtures of some unknown latent variables."}, {"color": "#d493c9", "id": "Machine_Learning_Topics", "label": "Machine_Learning_Topics", "shape": "star", "size": 25, "title": "Various topics in machine learning."}, {"color": "#438644", "id": "ICA", "label": "ICA", "shape": "dot", "size": 10, "title": "Independent Component Analysis (ICA) for separating mixed signals."}, {"color": "#e2139f", "id": "Cocktail_Party_Problem", "label": "Cocktail_Party_Problem", "shape": "dot", "size": 10, "title": "Example problem of ICA where multiple speakers\u0027 voices are separated from microphone recordings."}, {"color": "#4836c8", "id": "Mixing_Matrix_A", "label": "Mixing_Matrix_A", "shape": "dot", "size": 10, "title": "Matrix A that mixes independent sources into observed data."}, {"color": "#934eef", "id": "Unmixing_Matrix_W", "label": "Unmixing_Matrix_W", "shape": "dot", "size": 10, "title": "Inverse of mixing matrix used to recover original sources from mixed signals."}, {"color": "#41824f", "id": "ICA_Ambiguities", "label": "ICA_Ambiguities", "shape": "dot", "size": 10, "title": "Discussion on the ambiguities in recovering the unmixing matrix W without prior knowledge."}, {"color": "#5fbc2f", "id": "ICA Ambiguities", "label": "ICA Ambiguities", "shape": "star", "size": 25, "title": "Discusses inherent ambiguities in ICA recovery"}, {"color": "#49d033", "id": "Permutation Matrix", "label": "Permutation Matrix", "shape": "dot", "size": 10, "title": "Matrix that permutes the coordinates of a vector"}, {"color": "#1d3a15", "id": "Scaling Ambiguity", "label": "Scaling Ambiguity", "shape": "dot", "size": 10, "title": "Ambiguity in determining correct scaling factors for sources"}, {"color": "#bddd47", "id": "Volume Adjustment", "label": "Volume Adjustment", "shape": "dot", "size": 10, "title": "Adjusting volume does not affect the identification of sources"}, {"color": "#391246", "id": "Scaling_Impact", "label": "Scaling_Impact", "shape": "dot", "size": 10, "title": "Explains the effect of scaling a speaker\u0027s speech signal."}, {"color": "#9b68e4", "id": "Sign_Ignorance", "label": "Sign_Ignorance", "shape": "dot", "size": 10, "title": "Notes that sign changes in signals are irrelevant."}, {"color": "#4e59c5", "id": "Non_Gaussian_Sources", "label": "Non_Gaussian_Sources", "shape": "dot", "size": 10, "title": "States that non-Gaussian sources resolve ambiguities."}, {"color": "#c5675d", "id": "Gaussian_Data_Issue", "label": "Gaussian_Data_Issue", "shape": "dot", "size": 10, "title": "Describes the problem with Gaussian data in ICA."}, {"color": "#97566e", "id": "Mixing_Matrix_Rotation", "label": "Mixing_Matrix_Rotation", "shape": "dot", "size": 10, "title": "Explains how rotation of mixing matrix affects Gaussian data."}, {"color": "#affa20", "id": "ICAOnGaussianData", "label": "ICAOnGaussianData", "shape": "dot", "size": 10, "title": "Discussion on the limitations of Independent Component Analysis (ICA) when applied to Gaussian data due to rotational symmetry."}, {"color": "#4c9091", "id": "RotationalSymmetry", "label": "RotationalSymmetry", "shape": "dot", "size": 10, "title": "Explanation that multivariate standard normal distribution is rotationally symmetric, making ICA impossible on Gaussian data."}, {"color": "#d6c164", "id": "NonGaussianDataRecovery", "label": "NonGaussianDataRecovery", "shape": "dot", "size": 10, "title": "Discussion on the possibility of recovering independent sources from non-Gaussian data using sufficient data."}, {"color": "#ee3382", "id": "LinearTransformationsEffect", "label": "LinearTransformationsEffect", "shape": "dot", "size": 10, "title": "Explanation of how linear transformations affect densities in random variables."}, {"color": "#3c9de7", "id": "Density Transformation", "label": "Density Transformation", "shape": "star", "size": 25, "title": "Transformation of density functions under linear transformations."}, {"color": "#30eec7", "id": "1D Example", "label": "1D Example", "shape": "dot", "size": 10, "title": "Example in one dimension illustrating the transformation formula."}, {"color": "#55f67e", "id": "General Case", "label": "General Case", "shape": "dot", "size": 10, "title": "Generalization to vector-valued distributions and higher dimensions."}, {"color": "#2d9428", "id": "Volume Calculation", "label": "Volume Calculation", "shape": "dot", "size": 10, "title": "Calculation of volume changes under linear transformations."}, {"color": "#cefc86", "id": "ICA Algorithm", "label": "ICA Algorithm", "shape": "star", "size": 25, "title": "Derivation and interpretation of an ICA algorithm based on maximum likelihood estimation."}, {"color": "#7183e0", "id": "Bell and Sejnowski\u0027s Method", "label": "Bell and Sejnowski\u0027s Method", "shape": "dot", "size": 10, "title": "Description of the ICA method by Bell and Sejnowski."}, {"color": "#46fe2a", "id": "ICAConcepts", "label": "ICAConcepts", "shape": "dot", "size": 10, "title": "Introduction to Independent Component Analysis (ICA) principles and applications."}, {"color": "#9af59c", "id": "JointDistributionModeling", "label": "JointDistributionModeling", "shape": "dot", "size": 10, "title": "Description of modeling joint distribution as a product of marginals for independent sources."}, {"color": "#62c8bf", "id": "CumulativeDistributionFunction", "label": "CumulativeDistributionFunction", "shape": "dot", "size": 10, "title": "Definition and properties of cumulative distribution function (CDF)."}, {"color": "#48c90e", "id": "DataPreprocessing", "label": "DataPreprocessing", "shape": "dot", "size": 10, "title": "Process of preparing data for analysis by normalizing or transforming it."}, {"color": "#797fc6", "id": "LogisticFunctionDerivative", "label": "LogisticFunctionDerivative", "shape": "dot", "size": 10, "title": "The derivative of the logistic function and its properties."}, {"color": "#a49af1", "id": "ModelParameters", "label": "ModelParameters", "shape": "dot", "size": 10, "title": "Description of model parameters such as matrix W."}, {"color": "#d11f2e", "id": "LogLikelihoodCalculation", "label": "LogLikelihoodCalculation", "shape": "dot", "size": 10, "title": "Computation of log likelihood for a given set of data and parameters."}, {"color": "#207f2b", "id": "ConvergenceAndSourceRecovery", "label": "ConvergenceAndSourceRecovery", "shape": "dot", "size": 10, "title": "Process after convergence to recover original sources from data."}, {"color": "#dd6455", "id": "IndependenceAssumption", "label": "IndependenceAssumption", "shape": "dot", "size": 10, "title": "Discussion on the independence assumption of training examples and its implications."}, {"color": "#58b028", "id": "Stochastic Gradient Ascent", "label": "Stochastic Gradient Ascent", "shape": "dot", "size": 10, "title": "Optimization technique used for minimizing loss functions in machine learning models."}, {"color": "#40cf72", "id": "Self-supervised Learning", "label": "Self-supervised Learning", "shape": "star", "size": 25, "title": "Learning paradigm where the model learns from unlabelled data using implicit supervision from the input itself."}, {"color": "#a88a9e", "id": "Foundation Models", "label": "Foundation Models", "shape": "dot", "size": 10, "title": "Models pre-trained on large datasets that can be adapted to various downstream tasks with limited labeled data."}, {"color": "#387e46", "id": "Pretraining and Adaptation", "label": "Pretraining and Adaptation", "shape": "dot", "size": 10, "title": "Two-phase process involving training a model on unlabeled data followed by adapting it for specific tasks."}, {"color": "#5948f5", "id": "Transfer_Learning", "label": "Transfer_Learning", "shape": "dot", "size": 10, "title": "Technique where a pre-trained model is adapted to new tasks."}, {"color": "#e3f306", "id": "Pretraining_Phase", "label": "Pretraining_Phase", "shape": "dot", "size": 10, "title": "Initial training phase on large, unlabeled datasets."}, {"color": "#eb899c", "id": "Adaptation_Phase", "label": "Adaptation_Phase", "shape": "dot", "size": 10, "title": "Customizing the pre-trained model for specific tasks."}, {"color": "#62636a", "id": "Unlabeled_Dataset", "label": "Unlabeled_Dataset", "shape": "dot", "size": 10, "title": "Dataset used in initial training without labels."}, {"color": "#5451c1", "id": "Labeled_Task_Dataset", "label": "Labeled_Task_Dataset", "shape": "dot", "size": 10, "title": "Dataset with labeled data for specific tasks."}, {"color": "#f4fdc0", "id": "Model_Parameter_Theta", "label": "Model_Parameter_Theta", "shape": "dot", "size": 10, "title": "Parameters of the model during pretraining."}, {"color": "#c8084e", "id": "Embedding_Features", "label": "Embedding_Features", "shape": "dot", "size": 10, "title": "Representations learned by the model from data."}, {"color": "#2dadb7", "id": "Self_Supervised_Loss", "label": "Self_Supervised_Loss", "shape": "dot", "size": 10, "title": "Loss function using data point itself as supervision."}, {"color": "#4553b6", "id": "Machine_Learning_Adaptation", "label": "Machine_Learning_Adaptation", "shape": "star", "size": 25, "title": "Overview of machine learning adaptation techniques."}, {"color": "#bfa317", "id": "Downstream_Task_Dataset", "label": "Downstream_Task_Dataset", "shape": "dot", "size": 10, "title": "Labeled dataset for a specific task with varying sizes."}, {"color": "#647af0", "id": "Zero_Shot_Learning", "label": "Zero_Shot_Learning", "shape": "dot", "size": 10, "title": "Scenario where no labeled data is available for the downstream task."}, {"color": "#f0f84a", "id": "Few_Shot_Learning", "label": "Few_Shot_Learning", "shape": "dot", "size": 10, "title": "Situation with a small number of labeled examples, typically between 1 and 50."}, {"color": "#3d8472", "id": "Adaptation_Algorithm", "label": "Adaptation_Algorithm", "shape": "dot", "size": 10, "title": "Process that modifies a pretrained model to fit the downstream task."}, {"color": "#115e7e", "id": "Linear_Probe_Method", "label": "Linear_Probe_Method", "shape": "dot", "size": 10, "title": "Uses a linear head on top of fixed pretrained model representations for prediction."}, {"color": "#10eb5b", "id": "Finetuning_Method", "label": "Finetuning_Method", "shape": "dot", "size": 10, "title": "Modifies both the downstream task parameters and the pretrained model parameters."}, {"color": "#f3d38c", "id": "Language_Problems_Adaptation", "label": "Language_Problems_Adaptation", "shape": "dot", "size": 10, "title": "Specific adaptation methods for language tasks discussed in 14.3.2."}, {"color": "#f05d23", "id": "Machine_Learning_Adaptation_Methods", "label": "Machine_Learning_Adaptation_Methods", "shape": "star", "size": 25, "title": "Methods for adapting pre-trained models to new tasks."}, {"color": "#5041b5", "id": "Finetuning_Pretrained_Models", "label": "Finetuning_Pretrained_Models", "shape": "dot", "size": 10, "title": "Adjusting both weights and pretrained model parameters during downstream task training."}, {"color": "#2db386", "id": "Linear_Head_Initiation", "label": "Linear_Head_Initiation", "shape": "dot", "size": 10, "title": "Random initialization of the linear head weight vector w."}, {"color": "#3af573", "id": "Optimization_Objective", "label": "Optimization_Objective", "shape": "dot", "size": 10, "title": "Objective function to minimize loss for downstream tasks."}, {"color": "#c19cd9", "id": "Pretraining_Methods_Computer_Vision", "label": "Pretraining_Methods_Computer_Vision", "shape": "star", "size": 25, "title": "Techniques used in computer vision for pre-training models."}, {"color": "#045aa6", "id": "Supervised_Pretraining", "label": "Supervised_Pretraining", "shape": "dot", "size": 10, "title": "Training with large labeled datasets to learn representations."}, {"color": "#1d625b", "id": "Contrastive_Learning", "label": "Contrastive_Learning", "shape": "dot", "size": 10, "title": "Self-supervised method using unlabeled data for representation learning."}, {"color": "#522462", "id": "Self-Supervised Learning", "label": "Self-Supervised Learning", "shape": "star", "size": 25, "title": "Method using unlabeled data for pretraining."}, {"color": "#d349c5", "id": "Representation Function", "label": "Representation Function", "shape": "dot", "size": 10, "title": "Function \u03c6\u03b8(\u2218) maps images to representations."}, {"color": "#6ea0b4", "id": "Positive Pair", "label": "Positive Pair", "shape": "dot", "size": 10, "title": "Augmentations of the same image, semantically related."}, {"color": "#ae3afb", "id": "Negative Pair", "label": "Negative Pair", "shape": "dot", "size": 10, "title": "Randomly selected augmentations from different images."}, {"color": "#ba7a25", "id": "Data Augmentation", "label": "Data Augmentation", "shape": "dot", "size": 10, "title": "Technique to generate variations of an image for training."}, {"color": "#3041ed", "id": "Supervised Contrastive Algorithms", "label": "Supervised Contrastive Algorithms", "shape": "dot", "size": 10, "title": "Use labeled data and class similarity for pretraining."}, {"color": "#7ca4ff", "id": "ContrastiveLearning", "label": "ContrastiveLearning", "shape": "dot", "size": 10, "title": "Technique used to learn representations by contrasting positive and negative pairs."}, {"color": "#8804e3", "id": "SIMCLRAlgorithm", "label": "SIMCLRAlgorithm", "shape": "dot", "size": 10, "title": "Specific algorithm based on contrastive learning for unsupervised representation learning."}, {"color": "#53a891", "id": "AugmentationTechniques", "label": "AugmentationTechniques", "shape": "dot", "size": 10, "title": "Methods for creating variations of input data to improve model robustness."}, {"color": "#643674", "id": "PositivePairs", "label": "PositivePairs", "shape": "dot", "size": 10, "title": "Data pairs that should be close in the learned representation space."}, {"color": "#96b5db", "id": "NegativePairs", "label": "NegativePairs", "shape": "dot", "size": 10, "title": "Data pairs that should be far apart in the learned representation space."}, {"color": "#1653d9", "id": "Pretrained_Models", "label": "Pretrained_Models", "shape": "star", "size": 25, "title": "Overview of pretrained models in machine learning, particularly focusing on language processing."}, {"color": "#d94a39", "id": "Natural_Language_Processing", "label": "Natural_Language_Processing", "shape": "dot", "size": 10, "title": "Application of pretraining techniques to natural language processing tasks."}, {"color": "#04fceb", "id": "Language_Models", "label": "Language_Models", "shape": "dot", "size": 10, "title": "Probabilistic models representing the probability distribution over sequences of words in a document."}, {"color": "#a3d0ff", "id": "ConditionalProbabilityModeling", "label": "ConditionalProbabilityModeling", "shape": "dot", "size": 10, "title": "Modeling conditional probabilities in sequence prediction tasks."}, {"color": "#a2e913", "id": "ParameterizedFunction", "label": "ParameterizedFunction", "shape": "dot", "size": 10, "title": "Using parameterized functions to model conditional probabilities."}, {"color": "#cb93bb", "id": "Embeddings", "label": "Embeddings", "shape": "dot", "size": 10, "title": "Introduction of word embeddings for numerical input handling."}, {"color": "#517ccb", "id": "TransformerModel", "label": "TransformerModel", "shape": "dot", "size": 10, "title": "Overview and use of the Transformer model in sequence prediction tasks."}, {"color": "#9a5809", "id": "InputOutputInterface", "label": "InputOutputInterface", "shape": "dot", "size": 10, "title": "Description of how input sequences are transformed into output logits using a Transformer model."}, {"color": "#394eb9", "id": "Transformer_Models", "label": "Transformer_Models", "shape": "star", "size": 25, "title": "Models that map inputs to outputs using a blackbox function."}, {"color": "#c950e0", "id": "Conditional_Probability", "label": "Conditional_Probability", "shape": "dot", "size": 10, "title": "Probability of the next input given previous inputs."}, {"color": "#78d75a", "id": "Training_Transformer", "label": "Training_Transformer", "shape": "dot", "size": 10, "title": "Minimizing negative log-likelihood to train the model parameters."}, {"color": "#75080a", "id": "Autoregressive_Decoding", "label": "Autoregressive_Decoding", "shape": "dot", "size": 10, "title": "Generating text sequentially using the conditional distribution from previous tokens."}, {"color": "#c0ac36", "id": "LanguageModels", "label": "LanguageModels", "shape": "dot", "size": 10, "title": "Models that generate text based on learned patterns from large datasets."}, {"color": "#b77e35", "id": "TemperatureParameter", "label": "TemperatureParameter", "shape": "dot", "size": 10, "title": "A parameter that adjusts the randomness or determinism of generated text."}, {"color": "#3d2895", "id": "TextGeneration", "label": "TextGeneration", "shape": "dot", "size": 10, "title": "The process of generating sequences of tokens based on learned probabilities."}, {"color": "#8a8848", "id": "AdaptiveSampling", "label": "AdaptiveSampling", "shape": "dot", "size": 10, "title": "Adjusting the sampling method to control randomness in text generation."}, {"color": "#90f980", "id": "ModelAdaptation", "label": "ModelAdaptation", "shape": "dot", "size": 10, "title": "Techniques for adapting pretrained models to new tasks without fine-tuning."}, {"color": "#8d3416", "id": "Finetuning", "label": "Finetuning", "shape": "dot", "size": 10, "title": "Adjusting model parameters based on a specific task\u0027s dataset."}, {"color": "#579b53", "id": "ZeroShotLearning", "label": "ZeroShotLearning", "shape": "dot", "size": 10, "title": "Using pretrained models for tasks without additional training data."}, {"color": "#af0eaa", "id": "InContextLearning", "label": "InContextLearning", "shape": "dot", "size": 10, "title": "A method where the model learns from a small number of examples provided during inference."}, {"color": "#9fa0eb", "id": "Machine_Learning_Adaptation_Techniques", "label": "Machine_Learning_Adaptation_Techniques", "shape": "star", "size": 25, "title": "Techniques for adapting machine learning models to new tasks without additional training data."}, {"color": "#a29a68", "id": "Zero-Shot_Adaptation", "label": "Zero-Shot_Adaptation", "shape": "dot", "size": 10, "title": "Method where no input-output pairs from downstream tasks are available."}, {"color": "#4a511e", "id": "In-Context_Learning", "label": "In-Context_Learning", "shape": "dot", "size": 10, "title": "Approach for few-shot settings using a small number of labeled examples to guide model predictions."}, {"color": "#cb8486", "id": "Task_Formulation", "label": "Task_Formulation", "shape": "dot", "size": 10, "title": "Formulating tasks as questions or cloze tests for language problems."}, {"color": "#145c42", "id": "Prompting_Strategy", "label": "Prompting_Strategy", "shape": "dot", "size": 10, "title": "Strategy of using labeled examples to construct prompts for guiding model predictions."}, {"color": "#f6f650", "id": "Model_Optimization", "label": "Model_Optimization", "shape": "dot", "size": 10, "title": "Optimizing parameters in a pretrained model for new tasks."}, {"color": "#4b096a", "id": "Reinforcement Learning", "label": "Reinforcement Learning", "shape": "star", "size": 25, "title": "Type of machine learning where an agent learns to make decisions based on rewards and punishments."}, {"color": "#6a0505", "id": "Sequential Decision Making", "label": "Sequential Decision Making", "shape": "dot", "size": 10, "title": "Decision making in sequences without explicit supervision."}, {"color": "#43a54d", "id": "Markov Decision Processes (MDP)", "label": "Markov Decision Processes (MDP)", "shape": "dot", "size": 10, "title": "Formal framework for modeling decision-making situations in reinforcement learning."}, {"color": "#815aa5", "id": "States", "label": "States", "shape": "dot", "size": 10, "title": "Set of all possible conditions or configurations in an environment."}, {"color": "#d763b7", "id": "Actions", "label": "Actions", "shape": "dot", "size": 10, "title": "Set of all possible actions that can be taken in a given state."}, {"color": "#8e3b37", "id": "State Transition Probabilities", "label": "State Transition Probabilities", "shape": "dot", "size": 10, "title": "Probabilities associated with moving from one state to another based on an action."}, {"color": "#9d0ca7", "id": "Discount Factor", "label": "Discount Factor", "shape": "dot", "size": 10, "title": "Parameter that determines the importance of future rewards relative to immediate ones."}, {"color": "#2b95da", "id": "Reward Function", "label": "Reward Function", "shape": "dot", "size": 10, "title": "Function mapping state-action pairs or states to real numbers representing rewards."}, {"color": "#fe7a7e", "id": "Markov Decision Process (MDP)", "label": "Markov Decision Process (MDP)", "shape": "star", "size": 25, "title": "Framework for modeling decision-making in situations where outcomes are partly random and partly under the control of a decision maker."}, {"color": "#0d204a", "id": "State Transition", "label": "State Transition", "shape": "dot", "size": 10, "title": "Random transition from one state to another based on action taken."}, {"color": "#6761fc", "id": "Action Selection", "label": "Action Selection", "shape": "dot", "size": 10, "title": "Choosing actions in each state according to a policy."}, {"color": "#75793f", "id": "Total Payoff", "label": "Total Payoff", "shape": "dot", "size": 10, "title": "Cumulative reward over time, discounted by \u03b3^t."}, {"color": "#78cd80", "id": "Discount Factor (\u03b3)", "label": "Discount Factor (\u03b3)", "shape": "dot", "size": 10, "title": "Factor that discounts future rewards based on their temporal distance from the present."}, {"color": "#609950", "id": "Policy", "label": "Policy", "shape": "star", "size": 25, "title": "Function mapping states to actions in reinforcement learning."}, {"color": "#5640d3", "id": "Value Function", "label": "Value Function", "shape": "dot", "size": 10, "title": "Expected sum of discounted rewards for starting in state s and following policy \u03c0."}, {"color": "#18f327", "id": "Policy Execution", "label": "Policy Execution", "shape": "star", "size": 25, "title": "Process of selecting actions based on a policy in given states."}, {"color": "#895d81", "id": "Bellman Equations", "label": "Bellman Equations", "shape": "dot", "size": 10, "title": "Set of equations used to solve for value function V^\u03c0(s) in finite-state MDPs."}, {"color": "#8770ae", "id": "Immediate Reward", "label": "Immediate Reward", "shape": "dot", "size": 10, "title": "Reward received immediately upon entering a state s."}, {"color": "#0cc48e", "id": "Future Discounted Rewards", "label": "Future Discounted Rewards", "shape": "dot", "size": 10, "title": "Expected sum of discounted rewards after the first step in an MDP from state s."}, {"color": "#c52c28", "id": "Policy Evaluation", "label": "Policy Evaluation", "shape": "dot", "size": 10, "title": "Process of calculating value function given a fixed policy."}, {"color": "#b46a99", "id": "Optimal Value Function", "label": "Optimal Value Function", "shape": "dot", "size": 10, "title": "Best possible expected sum of discounted rewards using any policy."}, {"color": "#798672", "id": "Bellman\u0027s Equation", "label": "Bellman\u0027s Equation", "shape": "dot", "size": 10, "title": "Equation defining the optimal value function recursively."}, {"color": "#8a4fed", "id": "Optimal Policy", "label": "Optimal Policy", "shape": "dot", "size": 10, "title": "Policy that maximizes the expected sum of discounted rewards for all states."}, {"color": "#4f58b9", "id": "Optimal_Policy", "label": "Optimal_Policy", "shape": "dot", "size": 10, "title": "Policy \u03c0* that maximizes the value function for all states in an MDP."}, {"color": "#80758d", "id": "MDP_Finite_State_Space", "label": "MDP_Finite_State_Space", "shape": "dot", "size": 10, "title": "Markov Decision Process with a finite set of states and actions."}, {"color": "#516779", "id": "Value_Iteration", "label": "Value_Iteration", "shape": "dot", "size": 10, "title": "Algorithm used to find the optimal policy in reinforcement learning through iterative application of Bellman\u0027s equation."}, {"color": "#b2b2cd", "id": "Synchronous_Update", "label": "Synchronous_Update", "shape": "dot", "size": 10, "title": "Update method where all state values are updated simultaneously before overwriting."}, {"color": "#21816c", "id": "Asynchronous_Update", "label": "Asynchronous_Update", "shape": "dot", "size": 10, "title": "Update method where each state value is updated one at a time in sequence."}, {"color": "#78b465", "id": "ValueIterationAlgorithm", "label": "ValueIterationAlgorithm", "shape": "dot", "size": 10, "title": "Iterative algorithm to find optimal value function in an MDP."}, {"color": "#10ce69", "id": "PolicyIterationAlgorithm", "label": "PolicyIterationAlgorithm", "shape": "dot", "size": 10, "title": "Alternative method for finding the optimal policy by iteratively improving policies and their corresponding value functions."}, {"color": "#018f62", "id": "ConvergenceOfAlgorithms", "label": "ConvergenceOfAlgorithms", "shape": "dot", "size": 10, "title": "Conditions under which value iteration and policy iteration converge to optimal solutions."}, {"color": "#7c15c2", "id": "ComparisonValuePolicyIteration", "label": "ComparisonValuePolicyIteration", "shape": "dot", "size": 10, "title": "Discussion on the relative merits of value iteration versus policy iteration in different contexts."}, {"color": "#e0c4da", "id": "BellmanEquations", "label": "BellmanEquations", "shape": "dot", "size": 10, "title": "Set of equations used to solve for optimal policies and values in MDPs."}, {"color": "#df217c", "id": "Policy_Iteration", "label": "Policy_Iteration", "shape": "dot", "size": 10, "title": "Alternative method to find optimal policies through successive policy improvements"}, {"color": "#b56418", "id": "Learning_Model_for_MDP", "label": "Learning_Model_for_MDP", "shape": "star", "size": 25, "title": "Discussion on learning state transition probabilities and rewards in MDPs from data."}, {"color": "#08af90", "id": "Inverted_Pendulum_Problem", "label": "Inverted_Pendulum_Problem", "shape": "dot", "size": 10, "title": "Example problem used to illustrate the process of estimating MDP parameters from experience."}, {"color": "#52924e", "id": "State_Transition_Probabilities", "label": "State_Transition_Probabilities", "shape": "dot", "size": 10, "title": "Estimation method for state transition probabilities using maximum likelihood estimation based on observed data."}, {"color": "#1d4bda", "id": "MDP_Model_Learning", "label": "MDP_Model_Learning", "shape": "star", "size": 25, "title": "Process of learning state transition probabilities and rewards in an MDP"}, {"color": "#db1c8d", "id": "Expected_Immediate_Rewards", "label": "Expected_Immediate_Rewards", "shape": "dot", "size": 10, "title": "Calculation of average rewards observed in a particular state"}, {"color": "#fbfd80", "id": "Optimization_Techniques", "label": "Optimization_Techniques", "shape": "dot", "size": 10, "title": "Techniques for improving the efficiency of learning algorithms in MDPs"}, {"color": "#739730", "id": "Continuous_State_MDPs", "label": "Continuous_State_MDPs", "shape": "dot", "size": 10, "title": "Discussion on Markov Decision Processes with infinite state spaces, such as those involving continuous variables."}, {"color": "#bd8dbf", "id": "Discretization_Method", "label": "Discretization_Method", "shape": "dot", "size": 10, "title": "Technique to convert continuous-state MDPs into discrete-state MDPs for easier computation and algorithm application."}, {"color": "#6cd636", "id": "Discretization in MDPs", "label": "Discretization in MDPs", "shape": "dot", "size": 10, "title": "Process of converting continuous state space into discrete states for easier computation."}, {"color": "#044492", "id": "Value Iteration", "label": "Value Iteration", "shape": "dot", "size": 10, "title": "Algorithm to find optimal value function in a discrete state Markov Decision Process."}, {"color": "#db9823", "id": "Policy Iteration", "label": "Policy Iteration", "shape": "dot", "size": 10, "title": "Algorithm to find optimal policy in a discrete state Markov Decision Process."}, {"color": "#fecc0b", "id": "Supervised Learning", "label": "Supervised Learning", "shape": "star", "size": 25, "title": "Learning from labeled data to predict outcomes for new inputs."}, {"color": "#c6e939", "id": "Piecewise Constant Representation", "label": "Piecewise Constant Representation", "shape": "dot", "size": 10, "title": "Representation that assumes constant value within each discrete state interval."}, {"color": "#ad9a1f", "id": "Curse of Dimensionality", "label": "Curse of Dimensionality", "shape": "dot", "size": 10, "title": "Exponential increase in volume required to represent the state space as dimensionality increases."}, {"color": "#2bc1dc", "id": "StateRepresentation", "label": "StateRepresentation", "shape": "dot", "size": 10, "title": "Methods for representing states in machine learning models."}, {"color": "#c1be3d", "id": "GridCellRepresentation", "label": "GridCellRepresentation", "shape": "dot", "size": 10, "title": "Using grid cells to represent continuous state spaces, requiring fine discretization."}, {"color": "#67811b", "id": "CurseOfDimensionality", "label": "CurseOfDimensionality", "shape": "dot", "size": 10, "title": "Exponential increase in discrete states with dimensionality, limiting scalability of discretization methods."}, {"color": "#391083", "id": "ValueFunctionApproximation", "label": "ValueFunctionApproximation", "shape": "star", "size": 25, "title": "Techniques for approximating the value function in reinforcement learning using supervised learning methods."}, {"color": "#67dbb0", "id": "ModelOrSimulator", "label": "ModelOrSimulator", "shape": "dot", "size": 10, "title": "Use of models or simulators to approximate value functions in continuous-state MDPs."}, {"color": "#fe219e", "id": "Model Creation Methods", "label": "Model Creation Methods", "shape": "star", "size": 25, "title": "Different methods to create a model for state transitions in an MDP."}, {"color": "#9709a0", "id": "Physics Simulation", "label": "Physics Simulation", "shape": "dot", "size": 10, "title": "Using physical laws or software to simulate system behavior."}, {"color": "#8cf113", "id": "Off-the-Shelf Software", "label": "Off-the-Shelf Software", "shape": "dot", "size": 10, "title": "Utilizing existing physics simulation tools like Open Dynamics Engine."}, {"color": "#c8e08a", "id": "Learning from Data", "label": "Learning from Data", "shape": "dot", "size": 10, "title": "Inferring state transition probabilities from collected data in an MDP."}, {"color": "#e4b639", "id": "Data Collection Process", "label": "Data Collection Process", "shape": "dot", "size": 10, "title": "Executing trials and observing state sequences to learn model parameters."}, {"color": "#d2a71a", "id": "LinearModelPrediction", "label": "LinearModelPrediction", "shape": "dot", "size": 10, "title": "Using linear models to predict the next state based on current state and action."}, {"color": "#d0b477", "id": "DeterministicModel", "label": "DeterministicModel", "shape": "dot", "size": 10, "title": "Predicting exact outcomes given inputs in a deterministic manner."}, {"color": "#d17639", "id": "StochasticModel", "label": "StochasticModel", "shape": "dot", "size": 10, "title": "Introducing randomness to model predictions with noise terms."}, {"color": "#c48dc7", "id": "NonLinearFeatureMapping", "label": "NonLinearFeatureMapping", "shape": "dot", "size": 10, "title": "Using non-linear transformations of states and actions for more complex models."}, {"color": "#d66082", "id": "LossFunctions", "label": "LossFunctions", "shape": "dot", "size": 10, "title": "Different functions used to measure the error in predictions, like L2 norm."}, {"color": "#f8e206", "id": "Non-linear Feature Mappings", "label": "Non-linear Feature Mappings", "shape": "dot", "size": 10, "title": "Feature mappings that transform states and actions into non-linear representations."}, {"color": "#14922f", "id": "MDP Simulators", "label": "MDP Simulators", "shape": "dot", "size": 10, "title": "Simulations of Markov Decision Processes using learned models."}, {"color": "#6486e4", "id": "Fitted Value Iteration", "label": "Fitted Value Iteration", "shape": "star", "size": 25, "title": "Algorithm for approximating the value function in continuous state MDPs."}, {"color": "#bd4649", "id": "Continuous State Space", "label": "Continuous State Space", "shape": "dot", "size": 10, "title": "The state space is modeled as a continuous set of real numbers."}, {"color": "#0ded53", "id": "Discrete Action Space", "label": "Discrete Action Space", "shape": "dot", "size": 10, "title": "Action space consists of a small, finite number of discrete actions."}, {"color": "#8c8720", "id": "Value Function Approximation", "label": "Value Function Approximation", "shape": "dot", "size": 10, "title": "Approximating the value function using supervised learning algorithms like linear regression."}, {"color": "#4b3c08", "id": "FittedValueIteration", "label": "FittedValueIteration", "shape": "dot", "size": 10, "title": "Algorithm that uses linear regression to approximate the value function over a finite sample of states."}, {"color": "#2e31ab", "id": "SupervisedLearning", "label": "SupervisedLearning", "shape": "dot", "size": 10, "title": "Process of training models on labeled data to predict outcomes for new inputs."}, {"color": "#0cfc61", "id": "StateSampling", "label": "StateSampling", "shape": "dot", "size": 10, "title": "Random selection of states to approximate the value function over a finite sample."}, {"color": "#b8b7ba", "id": "ActionEvaluation", "label": "ActionEvaluation", "shape": "dot", "size": 10, "title": "Process of evaluating actions in sampled states to estimate future rewards and state values."}, {"color": "#42a0d8", "id": "Supervised_Learning", "label": "Supervised_Learning", "shape": "dot", "size": 10, "title": "Learning with labeled training data to predict outcomes for new inputs."}, {"color": "#96be6e", "id": "Linear_Regression", "label": "Linear_Regression", "shape": "dot", "size": 10, "title": "Statistical method for modeling relationships between a dependent variable and one or more independent variables."}, {"color": "#45de11", "id": "Fitted_Value_Iteration", "label": "Fitted_Value_Iteration", "shape": "dot", "size": 10, "title": "Technique used in reinforcement learning to approximate value functions using regression methods."}, {"color": "#d01379", "id": "Deterministic_Simulator", "label": "Deterministic_Simulator", "shape": "dot", "size": 10, "title": "Simplification when using a deterministic model for the Markov Decision Process (MDP)."}, {"color": "#0380fa", "id": "Policy_Definition", "label": "Policy_Definition", "shape": "dot", "size": 10, "title": "Definition of policy based on approximated value function in reinforcement learning context."}, {"color": "#3e2f73", "id": "Expectation_Computation", "label": "Expectation_Computation", "shape": "dot", "size": 10, "title": "Process of approximating expectations in reinforcement learning algorithms."}, {"color": "#99cf8e", "id": "Gaussian_Noise_Model", "label": "Gaussian_Noise_Model", "shape": "dot", "size": 10, "title": "Model involving deterministic function and Gaussian noise."}, {"color": "#490831", "id": "Approximation_Methods", "label": "Approximation_Methods", "shape": "dot", "size": 10, "title": "Techniques for approximating expectations in value iteration."}, {"color": "#622be5", "id": "Linear_System_Solver", "label": "Linear_System_Solver", "shape": "dot", "size": 10, "title": "Method used to compute policy evaluation in policy iteration."}, {"color": "#2a75d0", "id": "Bellman_Updates", "label": "Bellman_Updates", "shape": "dot", "size": 10, "title": "Iterative process for evaluating policies using Bellman equations."}, {"color": "#ea8bab", "id": "Algorithm 6", "label": "Algorithm 6", "shape": "dot", "size": 10, "title": "Variant of policy iteration that uses value evaluation procedure VE."}, {"color": "#89e0d9", "id": "Procedure VE", "label": "Procedure VE", "shape": "dot", "size": 10, "title": "Evaluation function used in Algorithm 6 to update state values."}, {"color": "#057646", "id": "Option 1 and Option 2", "label": "Option 1 and Option 2", "shape": "dot", "size": 10, "title": "Two initialization options for the value evaluation procedure VE."}, {"color": "#57f1f8", "id": "Chapter_15_Summary", "label": "Chapter_15_Summary", "shape": "star", "size": 25, "title": "Summary of Chapter 15 on MDPs and iterative methods."}, {"color": "#ccb908", "id": "k_steps_update_frequency", "label": "k_steps_update_frequency", "shape": "dot", "size": 10, "title": "Discussion on optimal update frequency for k steps in iterative processes."}, {"color": "#dc8340", "id": "Policy_Iteration_Speedup", "label": "Policy_Iteration_Speedup", "shape": "dot", "size": 10, "title": "Explanation of policy iteration\u0027s efficiency advantage over value iteration."}, {"color": "#81c9f3", "id": "Value_Iteration_Preference", "label": "Value_Iteration_Preference", "shape": "dot", "size": 10, "title": "Conditions under which value iteration is preferred over policy iteration."}, {"color": "#3bf9ea", "id": "Chapter_16_LQR_DDP_LQG", "label": "Chapter_16_LQR_DDP_LQG", "shape": "star", "size": 25, "title": "Introduction to Chapter 16 on Linear Quadratic Regulator (LQR), Differential Dynamic Programming (DDP) and Linear Quadratic Gaussian (LQG)."}, {"color": "#74b915", "id": "Finite_Horizon_MDPs", "label": "Finite_Horizon_MDPs", "shape": "dot", "size": 10, "title": "Introduction to finite-horizon MDPs in a general setting."}, {"color": "#95ba1c", "id": "Optimal_Bellman_Equation", "label": "Optimal_Bellman_Equation", "shape": "dot", "size": 10, "title": "Definition of the optimal Bellman equation for finding the optimal value function and policy."}, {"color": "#4f9333", "id": "General_Setting_Equations", "label": "General_Setting_Equations", "shape": "dot", "size": 10, "title": "Formulation of equations that apply to both discrete and continuous state spaces."}, {"color": "#8e3c45", "id": "ExpectationRewriting", "label": "ExpectationRewriting", "shape": "star", "size": 25, "title": "Rewriting expectations for finite and continuous states."}, {"color": "#2d208e", "id": "RewardsDependency", "label": "RewardsDependency", "shape": "dot", "size": 10, "title": "Rewards depend on both states and actions."}, {"color": "#d0ffcc", "id": "OptimalActionComputation", "label": "OptimalActionComputation", "shape": "dot", "size": 10, "title": "Computing optimal action considering state-action rewards and future value expectations."}, {"color": "#9dd833", "id": "FiniteHorizonMDP", "label": "FiniteHorizonMDP", "shape": "star", "size": 25, "title": "Definition of a finite horizon Markov Decision Process (MDP)."}, {"color": "#9d7f1c", "id": "TimeHorizonTuple", "label": "TimeHorizonTuple", "shape": "dot", "size": 10, "title": "Defining MDP with time horizon T."}, {"color": "#f773f1", "id": "PayoffDefinition", "label": "PayoffDefinition", "shape": "dot", "size": 10, "title": "Summation of rewards over a finite number of steps without discount factor."}, {"color": "#d708b5", "id": "DiscountFactorImpact", "label": "DiscountFactorImpact", "shape": "dot", "size": 10, "title": "Explanation for the absence of discount factor in finite horizon MDPs."}, {"color": "#83f37a", "id": "NonStationaryOptimalPolicy", "label": "NonStationaryOptimalPolicy", "shape": "star", "size": 25, "title": "Optimal policy changes over time in a finite horizon setting."}, {"color": "#b79673", "id": "Non-Stationary Policies", "label": "Non-Stationary Policies", "shape": "star", "size": 25, "title": "Policies that change over time in finite-horizon MDPs."}, {"color": "#32674b", "id": "Finite Horizon MDP Dynamics", "label": "Finite Horizon MDP Dynamics", "shape": "dot", "size": 10, "title": "Dynamics of an MDP with a changing policy over time steps."}, {"color": "#393caa", "id": "Time Dependent Transition Probabilities", "label": "Time Dependent Transition Probabilities", "shape": "dot", "size": 10, "title": "Transition probabilities that vary based on the current time step and state-action pair."}, {"color": "#3c01cd", "id": "Optimal Policy in Finite Horizon", "label": "Optimal Policy in Finite Horizon", "shape": "dot", "size": 10, "title": "The optimal policy varies depending on remaining steps to goal attainment."}, {"color": "#b3e333", "id": "Value Function for Non-Stationary MDPs", "label": "Value Function for Non-Stationary MDPs", "shape": "dot", "size": 10, "title": "Definition of the value function considering time-varying policies and states."}, {"color": "#0c5db6", "id": "Reinforcement_Learning", "label": "Reinforcement_Learning", "shape": "dot", "size": 10, "title": "Field of machine learning concerned with how software agents ought to take actions in an environment."}, {"color": "#ef58a2", "id": "Value_Functions", "label": "Value_Functions", "shape": "dot", "size": 10, "title": "Function that estimates the expected cumulative reward from a given state under a policy."}, {"color": "#8e20da", "id": "Policy_Evaluation", "label": "Policy_Evaluation", "shape": "dot", "size": 10, "title": "Process of estimating the value function for a fixed policy in reinforcement learning."}, {"color": "#253df9", "id": "Bellman_Equation", "label": "Bellman_Equation", "shape": "dot", "size": 10, "title": "Equation used to express the relationship between the value of a state and its successor states."}, {"color": "#cf62d9", "id": "Dynamic_Programming", "label": "Dynamic_Programming", "shape": "dot", "size": 10, "title": "Technique for solving complex problems by breaking them down into simpler subproblems."}, {"color": "#04801f", "id": "Machine_Learning_Overview", "label": "Machine_Learning_Overview", "shape": "star", "size": 25, "title": "General overview of machine learning concepts and techniques."}, {"color": "#48b707", "id": "Bellman_Equations", "label": "Bellman_Equations", "shape": "dot", "size": 10, "title": "Equations used to describe the relationship between value functions at different time steps."}, {"color": "#179972", "id": "Geometric_Convergence", "label": "Geometric_Convergence", "shape": "dot", "size": 10, "title": "Rate of convergence in value iteration, characterized by a geometric progression."}, {"color": "#717145", "id": "Linear_Quadratic_Regulation_(LQR)", "label": "Linear_Quadratic_Regulation_(LQR)", "shape": "star", "size": 25, "title": "A special case of finite-horizon setting where exact solutions are tractable."}, {"color": "#5ee6b4", "id": "Continuous_Model_Assumptions", "label": "Continuous_Model_Assumptions", "shape": "dot", "size": 10, "title": "Assumptions about the state and action spaces in a continuous setting."}, {"color": "#35d6a2", "id": "Linear_Transitions", "label": "Linear_Transitions", "shape": "dot", "size": 10, "title": "Model of system dynamics with linear transitions between states."}, {"color": "#78c016", "id": "Gaussian_Noise", "label": "Gaussian_Noise", "shape": "dot", "size": 10, "title": "Assumption about noise in the transition model, specifically Gaussian with zero mean."}, {"color": "#4abf61", "id": "Quadratic_Rewards", "label": "Quadratic_Rewards", "shape": "dot", "size": 10, "title": "Rewards are modeled as quadratic functions of state and action variables."}, {"color": "#596a14", "id": "LQRModelAssumptions", "label": "LQRModelAssumptions", "shape": "dot", "size": 10, "title": "Assumptions made in the Linear Quadratic Regulator (LQR) model for optimal control problems."}, {"color": "#4616e1", "id": "LQRAlgorithmSteps", "label": "LQRAlgorithmSteps", "shape": "dot", "size": 10, "title": "Two-step process of estimating and applying the LQR algorithm to find an optimal policy."}, {"color": "#e20ef0", "id": "Step1Estimation", "label": "Step1Estimation", "shape": "dot", "size": 10, "title": "First step involves collecting data and using linear regression to estimate model parameters."}, {"color": "#f750e7", "id": "Step2OptimalPolicy", "label": "Step2OptimalPolicy", "shape": "dot", "size": 10, "title": "Second step uses dynamic programming to derive the optimal policy given known or estimated parameters."}, {"color": "#4ff32b", "id": "DynamicProgrammingApplication", "label": "DynamicProgrammingApplication", "shape": "dot", "size": 10, "title": "Application of dynamic programming in solving for the optimal value function in LQR problems."}, {"color": "#6292d1", "id": "Optimal_Value_Function", "label": "Optimal_Value_Function", "shape": "star", "size": 25, "title": "Definition and properties of the optimal value function in a quadratic form."}, {"color": "#923346", "id": "Quadratic_Form", "label": "Quadratic_Form", "shape": "dot", "size": 10, "title": "The optimal value function is expressed as a quadratic function of state."}, {"color": "#9a9c41", "id": "Dynamics_Model", "label": "Dynamics_Model", "shape": "dot", "size": 10, "title": "Model dynamics and their integration into the value function calculation."}, {"color": "#8c1881", "id": "Linear_Policy_Derivation", "label": "Linear_Policy_Derivation", "shape": "dot", "size": 10, "title": "Process to derive the linear form of the optimal policy from quadratic value functions."}, {"color": "#48b88d", "id": "Optimal Policy in LQR", "label": "Optimal Policy in LQR", "shape": "dot", "size": 10, "title": "Discussion on optimal policy formulation in Linear Quadratic Regulator (LQR)."}, {"color": "#031fd0", "id": "Discrete Ricatti Equations", "label": "Discrete Ricatti Equations", "shape": "dot", "size": 10, "title": "Equations used to update \u03a6_t and \u03a8_t iteratively."}, {"color": "#6fb4fe", "id": "Independence of Noise", "label": "Independence of Noise", "shape": "dot", "size": 10, "title": "Explanation that optimal policy does not depend on noise but cost function value does."}, {"color": "#463f8f", "id": "LQR Algorithm Steps", "label": "LQR Algorithm Steps", "shape": "dot", "size": 10, "title": "Steps involved in implementing the Linear Quadratic Regulator algorithm."}, {"color": "#97224d", "id": "Efficiency Improvement", "label": "Efficiency Improvement", "shape": "dot", "size": 10, "title": "Method to optimize algorithm performance by updating only \u03a6_t."}, {"color": "#770178", "id": "Non-linear Dynamics and LQR", "label": "Non-linear Dynamics and LQR", "shape": "star", "size": 25, "title": "Exploration of how non-linear systems can be approximated using Linear Quadratic Regulator methods."}, {"color": "#6fb67e", "id": "Inverted Pendulum Example", "label": "Inverted Pendulum Example", "shape": "dot", "size": 10, "title": "Illustration of applying LQR to the inverted pendulum problem."}, {"color": "#d38f9e", "id": "Inverted_Pendulum_Model", "label": "Inverted_Pendulum_Model", "shape": "dot", "size": 10, "title": "A model used to illustrate control theory principles."}, {"color": "#8031e7", "id": "State_Transitions", "label": "State_Transitions", "shape": "dot", "size": 10, "title": "Describes how states change over time in the inverted pendulum system."}, {"color": "#48226e", "id": "Linearization_of_Dynamics", "label": "Linearization_of_Dynamics", "shape": "star", "size": 25, "title": "Process of approximating nonlinear dynamics with linear equations around each point on the nominal trajectory."}, {"color": "#ea4580", "id": "Taylor_Expansion_Method", "label": "Taylor_Expansion_Method", "shape": "dot", "size": 10, "title": "Uses Taylor series to approximate functions around a point."}, {"color": "#737d22", "id": "LQR_Assumptions", "label": "LQR_Assumptions", "shape": "dot", "size": 10, "title": "Connection between linearized dynamics and Linear Quadratic Regulator assumptions."}, {"color": "#2192b9", "id": "Differential_Dynamic_Programming", "label": "Differential_Dynamic_Programming", "shape": "star", "size": 25, "title": "Optimization technique for nonlinear systems aiming to stay near a target state."}, {"color": "#65169d", "id": "Optimization_Methods", "label": "Optimization_Methods", "shape": "dot", "size": 10, "title": "Techniques used to find optimal solutions in machine learning problems."}, {"color": "#26d267", "id": "Differential_Dynamic_Programming_(DDP)", "label": "Differential_Dynamic_Programming_(DDP)", "shape": "dot", "size": 10, "title": "Method for trajectory optimization that discretizes the path and uses linear approximations around each point."}, {"color": "#2826ab", "id": "Nominal_Trajectory", "label": "Nominal_Trajectory", "shape": "dot", "size": 10, "title": "Initial approximation of the desired trajectory using a simple controller."}, {"color": "#735bfa", "id": "Rewriting_Dynamics", "label": "Rewriting_Dynamics", "shape": "dot", "size": 10, "title": "Expressing the system\u0027s dynamics in a linear form using matrices A and B for state and action respectively."}, {"color": "#5399f9", "id": "Reward_Function_Approximation", "label": "Reward_Function_Approximation", "shape": "dot", "size": 10, "title": "Approximating the reward function around each point on the trajectory with Taylor expansion."}, {"color": "#bd3557", "id": "Optimization_Frameworks", "label": "Optimization_Frameworks", "shape": "dot", "size": 10, "title": "Frameworks used for optimization in machine learning problems."}, {"color": "#16a677", "id": "Linear_Quadratic_Regulator_(LQR)", "label": "Linear_Quadratic_Regulator_(LQR)", "shape": "dot", "size": 10, "title": "A control strategy that minimizes a quadratic cost function over time."}, {"color": "#569ac6", "id": "Hessian_Matrix", "label": "Hessian_Matrix", "shape": "dot", "size": 10, "title": "Matrix of second-order partial derivatives used in optimization problems."}, {"color": "#62aad9", "id": "LQR_Framework_Applications", "label": "LQR_Framework_Applications", "shape": "dot", "size": 10, "title": "Application of LQR to find optimal policies and controllers."}, {"color": "#515764", "id": "Trajectory_Generation", "label": "Trajectory_Generation", "shape": "dot", "size": 10, "title": "Process of generating new trajectories using the derived policy in an iterative manner."}, {"color": "#04270f", "id": "Linear_Quadratic_Gaussian_(LQG)", "label": "Linear_Quadratic_Gaussian_(LQG)", "shape": "star", "size": 25, "title": "Extension of LQR to handle systems with partial state observability."}, {"color": "#1f9a94", "id": "Observation vs State", "label": "Observation vs State", "shape": "dot", "size": 10, "title": "Discussion on the difference between observation and state in real-world problems."}, {"color": "#5e9d8f", "id": "Partially Observable MDPs (POMDP)", "label": "Partially Observable MDPs (POMDP)", "shape": "dot", "size": 10, "title": "Introduction to POMDP as a tool for modeling partially observable environments."}, {"color": "#e3bf32", "id": "Belief State", "label": "Belief State", "shape": "dot", "size": 10, "title": "Maintaining belief state based on observations in POMDP framework."}, {"color": "#ed4c96", "id": "LQR Extension to POMDP", "label": "LQR Extension to POMDP", "shape": "dot", "size": 10, "title": "Extension of Linear Quadratic Regulator (LQR) to Partially Observable MDPs."}, {"color": "#244747", "id": "Kalman Filter", "label": "Kalman Filter", "shape": "dot", "size": 10, "title": "A recursive algorithm that estimates the state of a system over time using a series of measurements."}, {"color": "#6d9c8f", "id": "Step 1", "label": "Step 1", "shape": "dot", "size": 10, "title": "Initial step where the system dynamics are defined without action dependence."}, {"color": "#691628", "id": "Gaussian Distributions", "label": "Gaussian Distributions", "shape": "star", "size": 25, "title": "Statistical distributions used in Kalman filter for modeling uncertainties."}, {"color": "#c1ed19", "id": "Predict Step", "label": "Predict Step", "shape": "dot", "size": 10, "title": "Computes the distribution of the next state given current observations."}, {"color": "#0f0033", "id": "Update Step", "label": "Update Step", "shape": "dot", "size": 10, "title": "Step to update the predicted state with new observations."}, {"color": "#837ce6", "id": "LQR Algorithm", "label": "LQR Algorithm", "shape": "star", "size": 25, "title": "Linear Quadratic Regulator algorithm used for control problems."}, {"color": "#2aafd9", "id": "Computational Efficiency", "label": "Computational Efficiency", "shape": "dot", "size": 10, "title": "Efficient computation of state estimates over time using Kalman filter."}, {"color": "#01952a", "id": "Belief States Update", "label": "Belief States Update", "shape": "dot", "size": 10, "title": "Combination of predict and update steps to refine belief states over time."}, {"color": "#8e4132", "id": "Gaussian Distribution", "label": "Gaussian Distribution", "shape": "dot", "size": 10, "title": "The distribution used in the predict step is Gaussian with mean s_t|t and covariance Sigma_t|t."}, {"color": "#ad860c", "id": "Kalman Gain (K_t)", "label": "Kalman Gain (K_t)", "shape": "dot", "size": 10, "title": "A matrix that determines how much new measurements should be trusted over previous estimates."}, {"color": "#02835a", "id": "Backward Pass (LQR Updates)", "label": "Backward Pass (LQR Updates)", "shape": "dot", "size": 10, "title": "Computes Psi_t, Psi_t and L_t to refine the optimal policy based on previous estimates."}, {"color": "#f07bb9", "id": "Chapter_17_Policy_Gradient_REINFORCE", "label": "Chapter_17_Policy_Gradient_REINFORCE", "shape": "star", "size": 25, "title": "Model-free algorithm that optimizes policy parameters without value functions."}, {"color": "#374970", "id": "Finite_Horizon_Case", "label": "Finite_Horizon_Case", "shape": "dot", "size": 10, "title": "Assumption for trajectory length in REINFORCE method."}, {"color": "#083ff0", "id": "Randomized_Policy", "label": "Randomized_Policy", "shape": "dot", "size": 10, "title": "REINFORCE applies to learning policies that output actions probabilistically."}, {"color": "#88587f", "id": "Transition_Probabilities_Sampling", "label": "Transition_Probabilities_Sampling", "shape": "dot", "size": 10, "title": "Sampling from transition probabilities is sufficient for REINFORCE, no need for analytical form."}, {"color": "#360838", "id": "Reward_Function_Querying", "label": "Reward_Function_Querying", "shape": "dot", "size": 10, "title": "REINFORCE queries reward function at state-action pairs without needing its analytical form."}, {"color": "#ad0596", "id": "Expected_Total_Payoff_Optimization", "label": "Expected_Total_Payoff_Optimization", "shape": "dot", "size": 10, "title": "Optimizing expected total payoff over policy parameters in finite horizon setting."}, {"color": "#f1bc39", "id": "Policy Gradient Methods", "label": "Policy Gradient Methods", "shape": "dot", "size": 10, "title": "Techniques for optimizing policies in reinforcement learning."}, {"color": "#166227", "id": "Gradient Ascent", "label": "Gradient Ascent", "shape": "dot", "size": 10, "title": "Optimization technique to maximize the expected reward function."}, {"color": "#56102d", "id": "Reward Function Estimation", "label": "Reward Function Estimation", "shape": "dot", "size": 10, "title": "Challenges in estimating gradients without knowing the exact form of the reward function."}, {"color": "#1c490d", "id": "Variational Auto-Encoder (VAE)", "label": "Variational Auto-Encoder (VAE)", "shape": "dot", "size": 10, "title": "Technique for learning latent variable models by optimizing variational lower bound."}, {"color": "#cbd6fc", "id": "Re-parametrization Technique", "label": "Re-parametrization Technique", "shape": "dot", "size": 10, "title": "Method to compute gradients through random variables in VAEs."}, {"color": "#bfc69e", "id": "REINFORCE Algorithm", "label": "REINFORCE Algorithm", "shape": "dot", "size": 10, "title": "Algorithm for estimating policy gradients using likelihood ratio methods."}, {"color": "#be6aac", "id": "GradientEstimation", "label": "GradientEstimation", "shape": "star", "size": 25, "title": "Overview of estimating gradients in machine learning."}, {"color": "#d97ed5", "id": "PolicyGradients", "label": "PolicyGradients", "shape": "dot", "size": 10, "title": "Methods for computing policy gradients using samples."}, {"color": "#4ca8b5", "id": "SampleBasedEstimator", "label": "SampleBasedEstimator", "shape": "dot", "size": 10, "title": "Using empirical samples to estimate the gradient of expected values."}, {"color": "#c2b354", "id": "LogProbabilityDerivative", "label": "LogProbabilityDerivative", "shape": "dot", "size": 10, "title": "Computing the derivative of log probability with respect to policy parameters."}, {"color": "#5d9a73", "id": "AnalyticalFormula", "label": "AnalyticalFormula", "shape": "dot", "size": 10, "title": "Deriving an analytical formula for \u03c0_\u03b8(a|s)."}, {"color": "#627019", "id": "AutoDifferentiation", "label": "AutoDifferentiation", "shape": "dot", "size": 10, "title": "Using automatic differentiation to compute gradients."}, {"color": "#6a5b8e", "id": "Policy_Gradient_Theorem", "label": "Policy_Gradient_Theorem", "shape": "star", "size": 25, "title": "Fundamental theorem in reinforcement learning for policy optimization."}, {"color": "#6991a7", "id": "Log_Probability_Ratio", "label": "Log_Probability_Ratio", "shape": "dot", "size": 10, "title": "Ratio of the probabilities of an action given a state according to two different policies, often used in reinforcement learning algorithms."}, {"color": "#056831", "id": "Vanilla_REINFORCE_Algorithm", "label": "Vanilla_REINFORCE_Algorithm", "shape": "dot", "size": 10, "title": "Basic algorithm for learning policies using policy gradients."}, {"color": "#75d5a2", "id": "Empirical_Sample_Trajectories", "label": "Empirical_Sample_Trajectories", "shape": "dot", "size": 10, "title": "Using sample trajectories to estimate gradients in reinforcement learning."}, {"color": "#f629b8", "id": "Trajectory_Probability_Change", "label": "Trajectory_Probability_Change", "shape": "dot", "size": 10, "title": "Change in trajectory probability due to policy parameter changes."}, {"color": "#6eee9d", "id": "PolicyGradientMethods", "label": "PolicyGradientMethods", "shape": "dot", "size": 10, "title": "Techniques that optimize policies in reinforcement learning by using gradients of the performance measure with respect to policy parameters."}, {"color": "#67d283", "id": "TrajectoryProbability", "label": "TrajectoryProbability", "shape": "dot", "size": 10, "title": "The probability of a trajectory given a policy and its importance in gradient calculation."}, {"color": "#683ad0", "id": "RewardFunction", "label": "RewardFunction", "shape": "dot", "size": 10, "title": "A function that assigns a scalar value to each possible outcome, used to evaluate the quality of actions taken by an agent."}, {"color": "#4c0c9c", "id": "ExpectationCalculation", "label": "ExpectationCalculation", "shape": "dot", "size": 10, "title": "Calculating expectations over trajectories under a policy and their implications for gradient estimation."}, {"color": "#8f6d0f", "id": "SimplificationOfFormulas", "label": "SimplificationOfFormulas", "shape": "dot", "size": 10, "title": "Simplifying complex formulas to more manageable expressions based on certain assumptions or properties."}, {"color": "#b5c3e9", "id": "Policy_Gradient_Methods", "label": "Policy_Gradient_Methods", "shape": "dot", "size": 10, "title": "Techniques for optimizing the policy directly by taking gradients of the performance measure with respect to the parameters of the policy."}, {"color": "#0ab9ca", "id": "Law_of_Total_Expectation", "label": "Law_of_Total_Expectation", "shape": "dot", "size": 10, "title": "Theorem that states how to compute expectations under a probability distribution by conditioning on another random variable."}, {"color": "#f0e5e0", "id": "Value_Function_Estimation", "label": "Value_Function_Estimation", "shape": "dot", "size": 10, "title": "Estimating the expected cumulative reward from a state or action under a policy."}, {"color": "#85c718", "id": "Gradient Estimation", "label": "Gradient Estimation", "shape": "dot", "size": 10, "title": "Estimating gradients of policy parameters using trajectories."}, {"color": "#9d9969", "id": "Baseline Function", "label": "Baseline Function", "shape": "dot", "size": 10, "title": "Function used to reduce variance in gradient estimation."}, {"color": "#5ff5ca", "id": "Algorithm 7", "label": "Algorithm 7", "shape": "dot", "size": 10, "title": "Vanilla policy gradient algorithm with baseline function."}, {"color": "#2f78be", "id": "Bibliography References", "label": "Bibliography References", "shape": "star", "size": 25, "title": "References to academic papers related to machine learning concepts."}, {"color": "#5bb409", "id": "Machine_Learning_Papers", "label": "Machine_Learning_Papers", "shape": "star", "size": 25, "title": "Collection of influential papers in machine learning and statistics."}, {"color": "#4a6e0f", "id": "Double_Descent_Weak_Features", "label": "Double_Descent_Weak_Features", "shape": "dot", "size": 10, "title": "Explores double descent phenomenon in machine learning models using weak features."}, {"color": "#b418b3", "id": "Variational_Inference_Review", "label": "Variational_Inference_Review", "shape": "dot", "size": 10, "title": "Provides a comprehensive review of variational inference for statisticians."}, {"color": "#d5f372", "id": "Foundation_Models", "label": "Foundation_Models", "shape": "dot", "size": 10, "title": "Discusses the opportunities and risks associated with foundation models in AI."}, {"color": "#0ab2b3", "id": "Contrastive_Learning_Visual_Representations", "label": "Contrastive_Learning_Visual_Representations", "shape": "dot", "size": 10, "title": "Introduces a simple framework for contrastive learning of visual representations."}, {"color": "#f8235e", "id": "BERT_Model", "label": "BERT_Model", "shape": "dot", "size": 10, "title": "Describes the pre-training of deep bidirectional transformers for language understanding."}, {"color": "#057781", "id": "Implicit_Bias_Noise_Covariance", "label": "Implicit_Bias_Noise_Covariance", "shape": "dot", "size": 10, "title": "Analyzes the implicit bias introduced by noise covariance in machine learning models."}, {"color": "#b5a5cf", "id": "High_Dimensional_Statistics", "label": "High_Dimensional_Statistics", "shape": "dot", "size": 10, "title": "Surveys unexpected phenomena in high-dimensional statistical analysis and modeling."}, {"color": "#7f3973", "id": "Machine Learning Papers", "label": "Machine Learning Papers", "shape": "star", "size": 25, "title": "Collection of papers related to machine learning and statistical learning theory."}, {"color": "#d42c67", "id": "Implicit Bias in Machine Learning", "label": "Implicit Bias in Machine Learning", "shape": "dot", "size": 10, "title": "Studies on the implicit bias introduced by different methods in high-dimensional settings."}, {"color": "#5a0e80", "id": "High-Dimensional Interpolation", "label": "High-Dimensional Interpolation", "shape": "dot", "size": 10, "title": "Research on interpolation properties of machine learning models in high dimensions."}, {"color": "#72544c", "id": "Deep Residual Learning", "label": "Deep Residual Learning", "shape": "dot", "size": 10, "title": "Work on deep residual networks for image recognition tasks."}, {"color": "#2601e9", "id": "Statistical Learning Textbook", "label": "Statistical Learning Textbook", "shape": "dot", "size": 10, "title": "Textbook covering fundamental concepts and techniques in statistical learning."}, {"color": "#a6c656", "id": "Optimization Methods", "label": "Optimization Methods", "shape": "dot", "size": 10, "title": "Methods for stochastic optimization, including Adam and other advanced algorithms."}, {"color": "#c7b768", "id": "Variational Autoencoders", "label": "Variational Autoencoders", "shape": "dot", "size": 10, "title": "Research on variational Bayesian methods applied to auto-encoding models."}, {"color": "#e2e77e", "id": "Model-Based Reinforcement Learning", "label": "Model-Based Reinforcement Learning", "shape": "dot", "size": 10, "title": "Framework for model-based reinforcement learning with theoretical guarantees."}, {"color": "#8a9e52", "id": "Generalization Error Analysis", "label": "Generalization Error Analysis", "shape": "dot", "size": 10, "title": "Analysis of generalization error in random features regression and linear models."}, {"color": "#5d47f0", "id": "Learning Theory Fundamentals", "label": "Learning Theory Fundamentals", "shape": "dot", "size": 10, "title": "Fundamental concepts in learning theory, including statistical mechanics approaches."}, {"color": "#8fdbd5", "id": "double_descent", "label": "double_descent", "shape": "star", "size": 25, "title": "Phenomenon in machine learning where performance initially improves, then worsens, and finally improves again as model complexity increases."}, {"color": "#0fa2f9", "id": "statistical_mechanics_of_learning", "label": "statistical_mechanics_of_learning", "shape": "dot", "size": 10, "title": "Application of statistical mechanics principles to understand learning processes in neural networks."}, {"color": "#676b14", "id": "generalization", "label": "generalization", "shape": "dot", "size": 10, "title": "Concept within statistical mechanics of learning focusing on how well a model performs on unseen data."}, {"color": "#c0a89a", "id": "learning_to_generalize", "label": "learning_to_generalize", "shape": "dot", "size": 10, "title": "Study of methods and theories for improving the generalization ability of machine learning models."}]);
                  edges = new vis.DataSet([{"arrows": "to", "from": "Zero-Shot_Adaptation", "title": "related_to", "to": "Task_Formulation"}, {"arrows": "to", "from": "Regularization Parameter \u03bb", "title": "subtopic", "to": "Regularized Loss Function"}, {"arrows": "to", "from": "LogisticRegression", "title": "has_algorithm", "to": "NewtonMethod"}, {"arrows": "to", "from": "Machine_Learning_Algorithms", "title": "contains", "to": "Value_Iteration"}, {"arrows": "to", "from": "MLPArchitecture", "title": "depends_on", "to": "NonlinearActivationModule"}, {"arrows": "to", "from": "MachineLearningModels", "title": "contains", "to": "OrdinaryLeastSquares"}, {"arrows": "to", "from": "EmpiricalRiskMinimization", "title": "subtopic", "to": "TrainingError"}, {"arrows": "to", "from": "Major Axis of Variation", "title": "has_subtopic", "to": "Projection Direction"}, {"arrows": "to", "from": "ExponentialFamilyDistributions", "title": "has_subtopic", "to": "SufficientStatistic"}, {"arrows": "to", "from": "Logistic_Regression_Gradient_Descent", "title": "subtopic", "to": "Machine_Learning_Concepts"}, {"arrows": "to", "from": "SchoolQuality", "title": "subtopic", "to": "DerivedFeatures"}, {"arrows": "to", "from": "Optimization Problem", "title": "defines", "to": "Objective Function"}, {"arrows": "to", "from": "Learning a model for an MDP", "title": "contains", "to": "Continuous state MDPs"}, {"arrows": "to", "from": "Differentiable Circuit", "title": "depends_on", "to": "Gradient Computation"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "related_to", "to": "Variational Auto-Encoder (VAE)"}, {"arrows": "to", "from": "GradientAscentRule", "title": "depends_on", "to": "ConvergenceAndSourceRecovery"}, {"arrows": "to", "from": "ValueIterationAlgorithm", "title": "depends_on", "to": "ConvergenceOfAlgorithms"}, {"arrows": "to", "from": "Equation_7.53_Usefulness", "title": "related_to", "to": "Derivations_Section_7.4.3"}, {"arrows": "to", "from": "LogLikelihoodImprovement", "title": "related_to", "to": "ExpectationMaximizationAlgorithm"}, {"arrows": "to", "from": "Natural_Language_Processing", "title": "contains", "to": "Language_Models"}, {"arrows": "to", "from": "IntermediateVariables", "title": "follows", "to": "BackwardPass"}, {"arrows": "to", "from": "Primal_Dual_Pairing", "title": "subtopic", "to": "Dual_Problem_Solution"}, {"arrows": "to", "from": "PolicyGradientMethods", "title": "depends_on", "to": "TrajectoryProbability"}, {"arrows": "to", "from": "TwoLayerNN", "title": "contains", "to": "PriorKnowledge"}, {"arrows": "to", "from": "Machine_Learning_Concepts", "title": "subtopic", "to": "Functional_Margin"}, {"arrows": "to", "from": "PoissonDistributionModeling", "title": "depends_on", "to": "MachineLearningOverview"}, {"arrows": "to", "from": "ResNetArchitecture", "title": "subtopic", "to": "BatchNormalization"}, {"arrows": "to", "from": "LQR, DDP and LQG", "title": "has_subtopic", "to": "Finite-horizon MDPs"}, {"arrows": "to", "from": "IndependenceAssumption", "title": "related_to", "to": "MachineLearningOverview"}, {"arrows": "to", "from": "Sparsity Regularization", "title": "related_to", "to": "Gradient Descent Incompatibility"}, {"arrows": "to", "from": "Deep Learning Introduction", "title": "related_to", "to": "Machine Learning Overview"}, {"arrows": "to", "from": "Functional_Margin", "title": "depends_on", "to": "Confidence_and_Prediction"}, {"arrows": "to", "from": "SIMCLRAlgorithm", "title": "depends_on", "to": "LossFunction"}, {"arrows": "to", "from": "Support Vector Machines (SVMs)", "title": "subtopic", "to": "Geometric Margin"}, {"arrows": "to", "from": "Optimization_Frameworks", "title": "subtopic", "to": "Linear_Quadratic_Regulator_(LQR)"}, {"arrows": "to", "from": "Value_Iteration", "title": "depends_on", "to": "Bellman_Equations"}, {"arrows": "to", "from": "GaussianDistribution", "title": "subtopic", "to": "StandardNormalDistribution"}, {"arrows": "to", "from": "Probability_Error_Bounds", "title": "related_to", "to": "Training_Error_Generalization_Error_Difference"}, {"arrows": "to", "from": "KernelsAsSimilarityMetrics", "title": "subtopic", "to": "MachineLearningConcepts"}, {"arrows": "to", "from": "DynamicProgrammingApplication", "title": "related_to", "to": "MachineLearningOverview"}, {"arrows": "to", "from": "RegularizerFunction", "title": "part_of", "to": "TrainingLoss"}, {"arrows": "to", "from": "Model Creation Methods", "title": "has_subtopic", "to": "Learning from Data"}, {"arrows": "to", "from": "Linear_Quadratic_Regulation_(LQR)", "title": "subtopic", "to": "Continuous_Model_Assumptions"}, {"arrows": "to", "from": "NewWordDetection", "title": "subtopic", "to": "NaiveBayesFilter"}, {"arrows": "to", "from": "Necessary_Conditions", "title": "depends_on", "to": "Positive_Semi_Definite"}, {"arrows": "to", "from": "Stochastic Gradient Descent (SGD)", "title": "depends_on", "to": "Hyperparameters"}, {"arrows": "to", "from": "Differential_Dynamic_Programming_(DDP)", "title": "depends_on", "to": "Nominal_Trajectory"}, {"arrows": "to", "from": "Best_Hypothesis", "title": "related_to", "to": "Machine_Learning_Theory"}, {"arrows": "to", "from": "NeurIPS20xxSubmission", "title": "related_to", "to": "MachineLearningConferences"}, {"arrows": "to", "from": "Bernoulli_Random_Variables", "title": "depends_on", "to": "Generalization_Error"}, {"arrows": "to", "from": "ELBO_Gradient_Computation", "title": "subtopic_of", "to": "Complexity_in_Computing_Gradients"}, {"arrows": "to", "from": "Optimizers", "title": "related_to", "to": "Initialization"}, {"arrows": "to", "from": "Discrete Action Space", "title": "subtopic", "to": "Fitted Value Iteration"}, {"arrows": "to", "from": "ModelAdaptation", "title": "subtopic", "to": "ZeroShotLearning"}, {"arrows": "to", "from": "Classification Problem", "title": "example_of", "to": "Spam Classification Example"}, {"arrows": "to", "from": "FeatureMapsAndKernels", "title": "subtopic", "to": "KernelFunctionDefinition"}, {"arrows": "to", "from": "Parameter Estimation", "title": "depends_on", "to": "Likelihood Function"}, {"arrows": "to", "from": "Machine_Learning_Adaptation_Techniques", "title": "subtopic", "to": "Zero-Shot_Adaptation"}, {"arrows": "to", "from": "Feature_Vector", "title": "composed_of", "to": "Vocabulary"}, {"arrows": "to", "from": "MachineLearningBasics", "title": "has_subtopic", "to": "TrainingSetExamples"}, {"arrows": "to", "from": "Generalization_Error_Bound", "title": "depends_on", "to": "Sample_Complexity"}, {"arrows": "to", "from": "Machine Learning", "title": "related_to", "to": "Generalization"}, {"arrows": "to", "from": "EMAlgorithmIntroduction", "title": "subtopic", "to": "LatentVariables"}, {"arrows": "to", "from": "MachineLearningConcepts", "title": "depends_on", "to": "StateRepresentation"}, {"arrows": "to", "from": "Double Descent Phenomenon", "title": "subtopic", "to": "Model-wise Double Descent"}, {"arrows": "to", "from": "Optimization_Techniques", "title": "subtopic", "to": "MDP_Model_Learning"}, {"arrows": "to", "from": "Supervised Learning Problem", "title": "has_subtopic", "to": "Regression"}, {"arrows": "to", "from": "LagrangianFormulation", "title": "subtopic", "to": "DualProblemDerivation"}, {"arrows": "to", "from": "distortion_function", "title": "related_to", "to": "coordinate_descent"}, {"arrows": "to", "from": "Backpropagation", "title": "subtopic", "to": "Modules"}, {"arrows": "to", "from": "EMAlgorithmOverview", "title": "contains", "to": "MixtureOfGaussiansExample"}, {"arrows": "to", "from": "Self_Supervised_Loss", "title": "depends_on", "to": "Pretraining_Phase"}, {"arrows": "to", "from": "Policy Gradient Methods", "title": "subtopic_of", "to": "Gradient Estimation"}, {"arrows": "to", "from": "Reparametrization_Trick", "title": "technique_used_in", "to": "Variational_Autoencoder"}, {"arrows": "to", "from": "Adaptation_Phase", "title": "subtopic", "to": "Transfer_Learning"}, {"arrows": "to", "from": "ActivationFunctions", "title": "subtopic", "to": "SoftplusFunction"}, {"arrows": "to", "from": "Unsupervised learning", "title": "has_subtopic", "to": "Independent components analysis"}, {"arrows": "to", "from": "Policy Evaluation", "title": "depends_on", "to": "Value Function"}, {"arrows": "to", "from": "Bias-Variance Tradeoff", "title": "depends_on", "to": "Training and Test Datasets"}, {"arrows": "to", "from": "Machine_Learning_Optimization", "title": "related_to", "to": "Lagrange_Duality"}, {"arrows": "to", "from": "Sample-wise Double Descent", "title": "depends_on", "to": "Optimal Regularization"}, {"arrows": "to", "from": "7 Deep learning", "title": "has_subtopic", "to": "7.3 Modules in Modern Neural Networks"}, {"arrows": "to", "from": "ActivationFunctions", "title": "subtopic", "to": "ReLUFunction"}, {"arrows": "to", "from": "NaiveBayesClassifier", "title": "depends_on", "to": "MaximumLikelihoodEstimation"}, {"arrows": "to", "from": "NeuralNetworks", "title": "depends_on", "to": "ActivationFunctions"}, {"arrows": "to", "from": "Chapter_15_Summary", "title": "subtopic", "to": "k_steps_update_frequency"}, {"arrows": "to", "from": "EfficientEvaluationOfELBO", "title": "contains", "to": "GaussianDistributionQ_i"}, {"arrows": "to", "from": "EM algorithms", "title": "has_subtopic", "to": "General EM algorithms"}, {"arrows": "to", "from": "Backpropagation", "title": "has_subtopic", "to": "General strategy of backpropagation"}, {"arrows": "to", "from": "1.2 The normal equations", "title": "has_subtopic", "to": "1.2.1 Matrix derivatives"}, {"arrows": "to", "from": "Optimization_Methods", "title": "subtopic", "to": "Differential_Dynamic_Programming_(DDP)"}, {"arrows": "to", "from": "Policy_Iteration", "title": "alternative_to", "to": "Bellman_Updates"}, {"arrows": "to", "from": "MultiClassClassification", "title": "subtopic", "to": "NegativeLogLikelihoodLossMulticlass"}, {"arrows": "to", "from": "Cross Validation", "title": "depends_on", "to": "Bias and Variance Tradeoff"}, {"arrows": "to", "from": "Backpropagation", "title": "has_subtopic", "to": "Back-propagation for MLPs"}, {"arrows": "to", "from": "TransformerModel", "title": "contains", "to": "InputOutputInterface"}, {"arrows": "to", "from": "2 Classification and logistic regression", "title": "has_subtopic", "to": "2.2 Digression: the perceptron learning algorithm"}, {"arrows": "to", "from": "Model Complexity", "title": "has_subtopic", "to": "Parameter Count vs. Model Norm"}, {"arrows": "to", "from": "Variational_Autoencoder", "title": "extends", "to": "EM_Algorithms"}, {"arrows": "to", "from": "Machine Learning Algorithms", "title": "related_to", "to": "Neural Networks"}, {"arrows": "to", "from": "Neural Networks", "title": "has_subtopic", "to": "Backpropagation"}, {"arrows": "to", "from": "NeurIPS20xxSubmission", "title": "depends_on", "to": "NewWordDetection"}, {"arrows": "to", "from": "Loss_Functions", "title": "subtopic", "to": "Training_Loss"}, {"arrows": "to", "from": "MStep", "title": "related_to", "to": "SigmaParameterUpdate"}, {"arrows": "to", "from": "ICA_Ambiguities", "title": "has_subtopic", "to": "Sign_Ignorance"}, {"arrows": "to", "from": "MachineLearningModels", "title": "related_to", "to": "LogisticRegression"}, {"arrows": "to", "from": "3 Generalized linear models", "title": "has_subtopic", "to": "3.2 Constructing GLMs"}, {"arrows": "to", "from": "MSEDecomposition", "title": "subtopic_of", "to": "AverageModel"}, {"arrows": "to", "from": "GaussianDistribution", "title": "subtopic", "to": "MeanOfGaussian"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "subtopic", "to": "Double Descent Phenomenon"}, {"arrows": "to", "from": "ResNetOverview", "title": "subtopic", "to": "ResNetComposition"}, {"arrows": "to", "from": "JensensInequality", "title": "related_to", "to": "LogLikelihoodBound"}, {"arrows": "to", "from": "Chain_Rule_Application", "title": "depends_on", "to": "Gradient_Computation"}, {"arrows": "to", "from": "Transformer_Models", "title": "has_subtopic", "to": "Training_Transformer"}, {"arrows": "to", "from": "ScalingInvariantProperty", "title": "subtopic", "to": "MM_Wb"}, {"arrows": "to", "from": "Simplified_1D_Convolution", "title": "depends_on", "to": "Bias_Scalar"}, {"arrows": "to", "from": "Reinforcement_Learning", "title": "depends_on", "to": "Value_Functions"}, {"arrows": "to", "from": "Implicit_Bias_Noise_Covariance", "title": "belongs_to", "to": "Machine_Learning_Papers"}, {"arrows": "to", "from": "Sufficient Conditions for Kernels", "title": "related_to", "to": "Feature Mapping"}, {"arrows": "to", "from": "7.4 Backpropagation", "title": "has_subtopic", "to": "7.4.3 Backward functions for basic modules"}, {"arrows": "to", "from": "MachineLearningOverview", "title": "depends_on", "to": "FeatureMapsAndKernels"}, {"arrows": "to", "from": "Multi-classClassification", "title": "depends_on", "to": "ResponseVariable"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "related_to", "to": "Backpropagation"}, {"arrows": "to", "from": "Reinforcement learning", "title": "has_subtopic", "to": "Continuous state MDPs"}, {"arrows": "to", "from": "Normalization Techniques", "title": "contains", "to": "Layer Normalization (LN)"}, {"arrows": "to", "from": "Dual_Problem_Solution", "title": "depends_on", "to": "Convexity_Conditions"}, {"arrows": "to", "from": "Step1Estimation", "title": "subtopic", "to": "LQRAlgorithmSteps"}, {"arrows": "to", "from": "Convolutional_Neural_Networks", "title": "contains", "to": "1D_Convolution"}, {"arrows": "to", "from": "Dual_Problem_Solution", "title": "related_to", "to": "KKT_Conditions"}, {"arrows": "to", "from": "Cross_Validation", "title": "specific_type_of", "to": "Hold_Out_Cross_Validation"}, {"arrows": "to", "from": "Discretization in MDPs", "title": "depends_on", "to": "Value Iteration"}, {"arrows": "to", "from": "Machine_Learning_Topics", "title": "contains", "to": "ICA"}, {"arrows": "to", "from": "Principal Component Analysis (PCA)", "title": "subtopic_of", "to": "Eigenfaces Method"}, {"arrows": "to", "from": "Regularization and model selection", "title": "has_subtopic", "to": "Implicit regularization effect (optional reading)"}, {"arrows": "to", "from": "Dual Problem", "title": "related_to", "to": "Relationship Between Primal and Dual"}, {"arrows": "to", "from": "ConditionalProbabilityModeling", "title": "contains", "to": "ParameterizedFunction"}, {"arrows": "to", "from": "Conditional_Distribution_Modeling", "title": "has_subtopic", "to": "Hypothesis_Functions"}, {"arrows": "to", "from": "Linear_Quadratic_Regulator_(LQR)", "title": "subtopic", "to": "LQR_Framework_Applications"}, {"arrows": "to", "from": "ICA Ambiguities", "title": "related_to", "to": "Scaling Ambiguity"}, {"arrows": "to", "from": "Machine_Learning_Concepts", "title": "subtopic", "to": "Test_Data_Set"}, {"arrows": "to", "from": "Differential_Dynamic_Programming_(DDP)", "title": "subtopic", "to": "Linearization_of_Dynamics"}, {"arrows": "to", "from": "Value_Iteration", "title": "related_to", "to": "Geometric_Convergence"}, {"arrows": "to", "from": "LMSUpdateRule", "title": "related_to", "to": "GradientDescentAlgorithm"}, {"arrows": "to", "from": "Normalization Techniques", "title": "has_subtopic", "to": "Mean Normalization"}, {"arrows": "to", "from": "Double_Descent_Phenomenon", "title": "contains", "to": "Model_Wise_Double_Descent"}, {"arrows": "to", "from": "Regularization and Non-Separable Case", "title": "related_to", "to": "Outlier Sensitivity"}, {"arrows": "to", "from": "Support_Vector_Machines_SVM", "title": "contains", "to": "Dual_Problem_Formulation"}, {"arrows": "to", "from": "TwoLayerNN", "title": "depends_on", "to": "SoftplusFunction"}, {"arrows": "to", "from": "Mixture of Gaussians Model", "title": "subtopic", "to": "Parameter Estimation"}, {"arrows": "to", "from": "Dimensionality Reduction Techniques", "title": "related_to", "to": "Independent Components Analysis (ICA)"}, {"arrows": "to", "from": "Fitted_Value_Iteration", "title": "subtopic", "to": "Deterministic_Simulator"}, {"arrows": "to", "from": "GaussianDiscriminantAnalysis(GDA)", "title": "depends_on", "to": "LikelihoodFunction"}, {"arrows": "to", "from": "Machine Learning", "title": "depends_on", "to": "Matricization Approach"}, {"arrows": "to", "from": "LMS_Update_Rule", "title": "subtopic", "to": "Single_Training_Example"}, {"arrows": "to", "from": "RegressionProblems", "title": "depends_on", "to": "TrainingDataset"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "related_to", "to": "Overfitting"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "depends_on", "to": "Gradient Computation"}, {"arrows": "to", "from": "Update Step", "title": "has_component", "to": "Kalman Gain (K_t)"}, {"arrows": "to", "from": "Newton\u0027s Method", "title": "compared_with", "to": "Gradient Descent"}, {"arrows": "to", "from": "Jensens_Inequality", "title": "related_to", "to": "Machine_Learning_Concepts"}, {"arrows": "to", "from": "Logistic Regression", "title": "depends_on", "to": "Hessian Matrix"}, {"arrows": "to", "from": "Convolutional_Layers", "title": "subtopic_of", "to": "Parameter_Sharing"}, {"arrows": "to", "from": "MachineLearningOverview", "title": "subtopic", "to": "KernelTrick"}, {"arrows": "to", "from": "Machine_Learning_Bias_Variance_Tradeoff", "title": "subtopic", "to": "Bias_Definition"}, {"arrows": "to", "from": "Activation_Functions", "title": "subtopic", "to": "ReLU"}, {"arrows": "to", "from": "Machine Learning Algorithms", "title": "related_to", "to": "Laplace Smoothing"}, {"arrows": "to", "from": "Reward_Function_Querying", "title": "subtopic", "to": "Chapter_17_Policy_Gradient_REINFORCE"}, {"arrows": "to", "from": "MachineLearningConcepts", "title": "related_to", "to": "GeneralizedLinearModels"}, {"arrows": "to", "from": "Machine_Learning", "title": "related_to", "to": "Neural_Networks"}, {"arrows": "to", "from": "PolicyGradientMethods", "title": "leads_to", "to": "SimplificationOfFormulas"}, {"arrows": "to", "from": "MachineLearningOverview", "title": "related_to", "to": "EMAlgorithmIntroduction"}, {"arrows": "to", "from": "Pretraining_Methods_Computer_Vision", "title": "subtopic_of", "to": "Supervised_Pretraining"}, {"arrows": "to", "from": "Pretraining_Methods_Computer_Vision", "title": "subtopic_of", "to": "Contrastive_Learning"}, {"arrows": "to", "from": "ELBOOptimization", "title": "subtopic", "to": "GradientAscentOptimization"}, {"arrows": "to", "from": "Backpropagation", "title": "depends_on", "to": "Chain Rule"}, {"arrows": "to", "from": "FittedValueIteration", "title": "subtopic", "to": "ActionEvaluation"}, {"arrows": "to", "from": "Test_Error_Training_Error", "title": "related_to", "to": "Generalization_Gap"}, {"arrows": "to", "from": "KKT Conditions", "title": "related_to", "to": "Support Vector Machine (SVM)"}, {"arrows": "to", "from": "Zero-Shot_Adaptation", "title": "depends_on", "to": "Language_Models"}, {"arrows": "to", "from": "1 Linear regression", "title": "has_subtopic", "to": "1.4 Locally weighted linear regression (optional reading)"}, {"arrows": "to", "from": "ActivationFunctions", "title": "subtopic", "to": "GELUFunction"}, {"arrows": "to", "from": "Generalization and regularization", "title": "has_subtopic", "to": "Generalization"}, {"arrows": "to", "from": "PolicyGradientMethods", "title": "contains", "to": "ExpectationCalculation"}, {"arrows": "to", "from": "Machine_Learning", "title": "depends_on", "to": "Empirical_Risk_Minimization"}, {"arrows": "to", "from": "Kalman Filter", "title": "includes", "to": "Forward Pass"}, {"arrows": "to", "from": "Linear Regression", "title": "depends_on", "to": "Features Selection"}, {"arrows": "to", "from": "Design Matrix", "title": "subtopic", "to": "Least Squares Revisited"}, {"arrows": "to", "from": "OptimalActionComputation", "title": "subtopic", "to": "RewardsDependency"}, {"arrows": "to", "from": "Nonlinear Activation Module", "title": "depends_on", "to": "MLP Composition"}, {"arrows": "to", "from": "Kalman Filter", "title": "includes", "to": "Backward Pass (LQR Updates)"}, {"arrows": "to", "from": "BackwardFunctionsBasics", "title": "has_subtopic", "to": "LossFunctionBackward"}, {"arrows": "to", "from": "Reinforcement learning", "title": "has_subtopic", "to": "Connections between Policy and Value Iteration (Optional)"}, {"arrows": "to", "from": "Optimal Policy in LQR", "title": "depends_on", "to": "Discrete Ricatti Equations"}, {"arrows": "to", "from": "Primal Problem", "title": "depends_on", "to": "Objective Function Primal"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "depends_on", "to": "Backpropagation Algorithm"}, {"arrows": "to", "from": "Efficient_Update", "title": "depends_on", "to": "Constraints_Satisfaction"}, {"arrows": "to", "from": "LogisticRegression", "title": "compared_to", "to": "LinearRegression"}, {"arrows": "to", "from": "MachineLearningOverview", "title": "contains", "to": "KernelFunctions"}, {"arrows": "to", "from": "ParallelismGPUs", "title": "related_to", "to": "VectorizationInNN"}, {"arrows": "to", "from": "Machine_Learning_Backward_Functions", "title": "subtopic", "to": "Cross_Entropy_Loss_Backward"}, {"arrows": "to", "from": "Linear_Classification", "title": "subtopic", "to": "Empirical_Risk_Minimization"}, {"arrows": "to", "from": "NeuralNetworksInspiration", "title": "contains", "to": "BiologicalSimilarity"}, {"arrows": "to", "from": "HoldOutCrossValidation", "title": "subtopic", "to": "ValidationError"}, {"arrows": "to", "from": "SingleNeuronNN", "title": "depends_on", "to": "ReLUFunction"}, {"arrows": "to", "from": "Support_Vector_Machines_SVM", "title": "has_subtopic", "to": "Linear_Constraints"}, {"arrows": "to", "from": "PrimalProblem", "title": "subtopic", "to": "GeneralizedLagrangian"}, {"arrows": "to", "from": "BinaryFeatures", "title": "subtopic", "to": "NaiveBayesAlgorithm"}, {"arrows": "to", "from": "Reinforcement_Learning", "title": "depends_on", "to": "Dynamic_Programming"}, {"arrows": "to", "from": "NeuralNetworks", "title": "depends_on", "to": "WeightMatrices"}, {"arrows": "to", "from": "ELBOOptimization", "title": "subtopic", "to": "QFormRequirements"}, {"arrows": "to", "from": "Functional Margins", "title": "subtopic", "to": "Machine Learning Concepts"}, {"arrows": "to", "from": "Learning_Settings", "title": "depends_on", "to": "Training_Distribution"}, {"arrows": "to", "from": "LinearRegression", "title": "subtopic", "to": "LocallyWeightedLinearRegression"}, {"arrows": "to", "from": "Binary_Classification", "title": "subtopic", "to": "Generalization_Error"}, {"arrows": "to", "from": "Transfer_Learning", "title": "subtopic", "to": "Machine_Learning"}, {"arrows": "to", "from": "Machine_Learning_Backward_Propagation", "title": "subtopic", "to": "Chain_Rule_Interpretation"}, {"arrows": "to", "from": "Kernels_in_Machine_Learning", "title": "subtopic", "to": "Algorithm_5.11"}, {"arrows": "to", "from": "MachineLearningOverview", "title": "has_subtopic", "to": "BackpropagationDiscussion"}, {"arrows": "to", "from": "VariationalAutoEncoder", "title": "subtopic", "to": "VariationalInference"}, {"arrows": "to", "from": "ELBO_Gradient_Computation", "title": "subtopic_of", "to": "Gradient_Simple_Case"}, {"arrows": "to", "from": "Gradient_Estimation", "title": "depends_on", "to": "Reparameterization_Trick"}, {"arrows": "to", "from": "StateRepresentation", "title": "has_subtopic", "to": "CurseOfDimensionality"}, {"arrows": "to", "from": "Softmax_Function", "title": "subtopic", "to": "Probability_Vector_Output"}, {"arrows": "to", "from": "Support Vector Machine (SVM)", "title": "depends_on", "to": "Optimization Problem in SVM"}, {"arrows": "to", "from": "EmpiricalRiskMinimization", "title": "subtopic", "to": "HypothesesHi"}, {"arrows": "to", "from": "Dual Problem", "title": "related_to", "to": "Theta D"}, {"arrows": "to", "from": "Machine_Learning_Theory", "title": "subtopic", "to": "Sample_Complexity"}, {"arrows": "to", "from": "Foundation Models", "title": "subtopic", "to": "Machine Learning Overview"}, {"arrows": "to", "from": "Regularization", "title": "depends_on", "to": "ModelComplexity"}, {"arrows": "to", "from": "Policy Gradient Methods", "title": "contains", "to": "Reward Function Estimation"}, {"arrows": "to", "from": "Expectation_Computation", "title": "subtopic", "to": "Gaussian_Noise_Model"}, {"arrows": "to", "from": "Sample_Size_Calculation", "title": "subtopic", "to": "Quantities_of_Interest"}, {"arrows": "to", "from": "Machine_Learning_Adaptation", "title": "related_to", "to": "Language_Problems_Adaptation"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "related_to", "to": "Discretization in MDPs"}, {"arrows": "to", "from": "k-means Algorithm", "title": "related_to", "to": "Convergence in k-means"}, {"arrows": "to", "from": "MachineLearningModels", "title": "related_to", "to": "GaussianDiscriminantAnalysis"}, {"arrows": "to", "from": "BERT_Model", "title": "belongs_to", "to": "Machine_Learning_Papers"}, {"arrows": "to", "from": "Class Priors", "title": "depends_on", "to": "Bayesian Classification"}, {"arrows": "to", "from": "Machine_Learning_Concepts", "title": "contains", "to": "Partial_Derivatives"}, {"arrows": "to", "from": "double_descent", "title": "related_to", "to": "learning_to_generalize"}, {"arrows": "to", "from": "Continuous State Space", "title": "subtopic", "to": "Fitted Value Iteration"}, {"arrows": "to", "from": "MachineLearningOverview", "title": "subtopic", "to": "Backpropagation"}, {"arrows": "to", "from": "Variational_Autoencoder", "title": "uses", "to": "Neural_Networks"}, {"arrows": "to", "from": "MachineLearningConcepts", "title": "contains", "to": "LikelihoodFunction"}, {"arrows": "to", "from": "Machine_Learning_Concepts", "title": "related_to", "to": "Population_Distribution"}, {"arrows": "to", "from": "Kalman Filter", "title": "includes", "to": "Belief States Update"}, {"arrows": "to", "from": "Bellman\u0027s Equation", "title": "defines", "to": "Optimal Value Function"}, {"arrows": "to", "from": "MachineLearningConcepts", "title": "has_subtopic", "to": "MSEDecomposition"}, {"arrows": "to", "from": "Inverted_Pendulum_Model", "title": "subtopic", "to": "State_Transitions"}, {"arrows": "to", "from": "Exponential_Family_Distributions", "title": "has_subtopic", "to": "Canonical_Response_Function"}, {"arrows": "to", "from": "MachineLearningConcepts", "title": "related_to", "to": "ActivationFunctions"}, {"arrows": "to", "from": "GradientDescentAlgorithm", "title": "subtopic", "to": "StochasticGradientDescent"}, {"arrows": "to", "from": "Evidence_Lower_Bound_(ELBO)", "title": "depends_on", "to": "Log_Likelihood_Optimization"}, {"arrows": "to", "from": "MachineLearningOverview", "title": "contains", "to": "PolicyGradientMethods"}, {"arrows": "to", "from": "MDP_Finite_State_Space", "title": "subtopic", "to": "Machine_Learning"}, {"arrows": "to", "from": "LikelihoodFunction", "title": "leads_to", "to": "MaximumLikelihoodEstimation"}, {"arrows": "to", "from": "LossFunction", "title": "has_subtopic", "to": "CrossEntropyLoss"}, {"arrows": "to", "from": "Model_Selection", "title": "depends_on", "to": "Validation_Set_Size"}, {"arrows": "to", "from": "EM algorithms", "title": "has_subtopic", "to": "Mixture of Gaussians revisited"}, {"arrows": "to", "from": "Simplified_1D_Convolution", "title": "related_to", "to": "Matrix_Multiplication"}, {"arrows": "to", "from": "MachineLearningOverview", "title": "contains", "to": "LayerNormalization"}, {"arrows": "to", "from": "FeatureMapsAndKernels", "title": "subtopic", "to": "AlgorithmEfficiency"}, {"arrows": "to", "from": "Dual_Problem_Formulation", "title": "related_to", "to": "KKT_Conditions"}, {"arrows": "to", "from": "GeneralizedLinearModels", "title": "has_assumption", "to": "Assumption2"}, {"arrows": "to", "from": "Support_Vector_Machines_SVM", "title": "depends_on", "to": "Sequential_Minimal_Optimization_SMO"}, {"arrows": "to", "from": "MachineLearningOverview", "title": "has_subtopic", "to": "NeuralNetworksComposition"}, {"arrows": "to", "from": "Mathematical Decomposition for Regression", "title": "depends_on", "to": "Bias-Variance Tradeoff"}, {"arrows": "to", "from": "Support Vector Machines (SVM)", "title": "depends_on", "to": "Optimal Parameters Calculation"}, {"arrows": "to", "from": "Generalization Error Analysis", "title": "subtopic", "to": "Machine Learning Papers"}, {"arrows": "to", "from": "Future Discounted Rewards", "title": "related_to", "to": "Bellman Equations"}, {"arrows": "to", "from": "Bayesian Machine Learning", "title": "depends_on", "to": "Training Set"}, {"arrows": "to", "from": "Neural Networks", "title": "has_subtopic", "to": "Modules in Modern Neural Networks"}, {"arrows": "to", "from": "PerceptronAlgorithm", "title": "depends_on", "to": "DecisionBoundary"}, {"arrows": "to", "from": "ValueFunctionApproximation", "title": "has_subtopic", "to": "ModelOrSimulator"}, {"arrows": "to", "from": "PolynomialKernels", "title": "contains", "to": "ComputationalEfficiency"}, {"arrows": "to", "from": "ELBOOptimization", "title": "subtopic", "to": "EfficientEvaluationOfELBO"}, {"arrows": "to", "from": "SingleTrainingExampleCase", "title": "subtopic", "to": "PartialDerivativeCalculation"}, {"arrows": "to", "from": "ModelAdaptation", "title": "subtopic", "to": "InContextLearning"}, {"arrows": "to", "from": "Machine_Learning_Backpropagation", "title": "subtopic", "to": "Chain_Rule_Application"}, {"arrows": "to", "from": "MachineLearningOverview", "title": "subtopic", "to": "Conv1D-S"}, {"arrows": "to", "from": "EM_Algorithm", "title": "related_to", "to": "K_Means_Clustering"}, {"arrows": "to", "from": "MachineLearningOverview", "title": "depends_on", "to": "UnsupervisedLearning"}, {"arrows": "to", "from": "Machine_Learning_Concepts", "title": "has_subtopic", "to": "Conditional_Distribution_Modeling"}, {"arrows": "to", "from": "Gradient Computation", "title": "subtopic", "to": "Chain Rule"}, {"arrows": "to", "from": "Bias-variance tradeoff", "title": "has_subtopic", "to": "A mathematical decomposition (for regression)"}, {"arrows": "to", "from": "Machine_Learning_Theorem", "title": "subtopic", "to": "Uniform_Convergence_Assumption"}, {"arrows": "to", "from": "Neural Networks", "title": "has_subtopic", "to": "Vectorization over training examples"}, {"arrows": "to", "from": "Compression", "title": "subtopic", "to": "Principal Component Analysis (PCA)"}, {"arrows": "to", "from": "General EM algorithms", "title": "has_subtopic", "to": "Other interpretation of ELBO"}, {"arrows": "to", "from": "Linear_Transitions", "title": "related_to", "to": "Gaussian_Noise"}, {"arrows": "to", "from": "BernoulliDistribution", "title": "has_subtopic", "to": "SufficientStatisticForBernoulli"}, {"arrows": "to", "from": "FamilySize", "title": "subtopic", "to": "DerivedFeatures"}, {"arrows": "to", "from": "Discretization in MDPs", "title": "has_property", "to": "Piecewise Constant Representation"}, {"arrows": "to", "from": "Dual_Complementarity", "title": "leads_to", "to": "Support_Vectors"}, {"arrows": "to", "from": "Optimization Problem", "title": "controls", "to": "Parameter C"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "contains", "to": "Stochastic Gradient Ascent"}, {"arrows": "to", "from": "Optimization_Problems", "title": "depends_on", "to": "KKT_Conditions"}, {"arrows": "to", "from": "Pretrained large language models", "title": "has_subtopic", "to": "Open up the blackbox of Transformers"}, {"arrows": "to", "from": "Support_Vector_Machines_SVM", "title": "has_subtopic", "to": "Convex_Quadratic_Objective"}, {"arrows": "to", "from": "LinearRegression", "title": "related_to", "to": "GradientDescent"}, {"arrows": "to", "from": "Normalization Techniques", "title": "contains", "to": "Scale-Invariant Property"}, {"arrows": "to", "from": "Regularizer R(\u03b8)", "title": "related_to", "to": "Regularized Loss Function"}, {"arrows": "to", "from": "Reinforcement learning", "title": "has_subtopic", "to": "Value iteration and policy iteration"}, {"arrows": "to", "from": "Learning_Settings", "title": "related_to", "to": "Domain_Shift"}, {"arrows": "to", "from": "Double Descent Phenomenon", "title": "contains", "to": "Sample-wise Double Descent"}, {"arrows": "to", "from": "EvidenceLowerBoundELBO", "title": "related_to", "to": "EMAlgorithm"}, {"arrows": "to", "from": "7 Deep learning", "title": "has_subtopic", "to": "7.1 Supervised learning with non-linear models"}, {"arrows": "to", "from": "Discount Factor (\u03b3)", "title": "related_to", "to": "Total Payoff"}, {"arrows": "to", "from": "MachineLearningOverview", "title": "depends_on", "to": "NeuralNetworks"}, {"arrows": "to", "from": "Linear_Policy_Derivation", "title": "depends_on", "to": "Optimal_Value_Function"}, {"arrows": "to", "from": "MatrixMultiplicationModule", "title": "depends_on", "to": "NeuralNetworksComposition"}, {"arrows": "to", "from": "ActivationFunctions", "title": "subtopic", "to": "SigmoidFunction"}, {"arrows": "to", "from": "Unsupervised learning", "title": "has_subtopic", "to": "Clustering and the k-means algorithm"}, {"arrows": "to", "from": "Geometric Margins", "title": "subtopic", "to": "Machine Learning Concepts"}, {"arrows": "to", "from": "Foundation Models", "title": "subtopic", "to": "Pretraining and Adaptation"}, {"arrows": "to", "from": "Lagrange_Duality", "title": "has_subtopic", "to": "Dual_Form_Optimization"}, {"arrows": "to", "from": "Regularization", "title": "related_to", "to": "BiasVarianceTradeoff"}, {"arrows": "to", "from": "Negative_Log_Likelihood", "title": "subtopic", "to": "Cross_Entropy_Loss"}, {"arrows": "to", "from": "Regularization in Deep Learning", "title": "contains", "to": "Implicit Regularization Effect"}, {"arrows": "to", "from": "InitializationThetaZero", "title": "subtopic", "to": "IterativeUpdateRule"}, {"arrows": "to", "from": "Variational Auto-Encoder (VAE)", "title": "contains", "to": "Re-parametrization Technique"}, {"arrows": "to", "from": "Generative_Modeling", "title": "has_subtopic", "to": "Naive_Bayes_Classifier"}, {"arrows": "to", "from": "LMS_Update_Rule", "title": "depends_on", "to": "Error_Term"}, {"arrows": "to", "from": "TrainingSetExamples", "title": "depends_on", "to": "MatrixNotation"}, {"arrows": "to", "from": "Volume Calculation", "title": "subtopic", "to": "General Case"}, {"arrows": "to", "from": "Kernel Matrix Properties", "title": "subtopic", "to": "Sufficient Conditions for Kernels"}, {"arrows": "to", "from": "LogisticRegression", "title": "subtopic", "to": "NegativeLikelihoodLoss"}, {"arrows": "to", "from": "Machine Learning Algorithms", "title": "depends_on", "to": "Backpropagation Algorithm"}, {"arrows": "to", "from": "Backpropagation for MLPs", "title": "depends_on", "to": "Forward Pass"}, {"arrows": "to", "from": "Neural_Networks", "title": "subtopic", "to": "Stacking_Neurons"}, {"arrows": "to", "from": "MStep", "title": "contains", "to": "PhiParameterUpdate"}, {"arrows": "to", "from": "Scaling Ambiguity", "title": "subtopic", "to": "Volume Adjustment"}, {"arrows": "to", "from": "MachineLearningAlgorithms", "title": "related_to", "to": "CrossValidation"}, {"arrows": "to", "from": "Bias-Variance Tradeoff", "title": "related_to", "to": "Double Descent Phenomenon"}, {"arrows": "to", "from": "MLPArchitecture", "title": "depends_on", "to": "MatrixMultiplicationModule"}, {"arrows": "to", "from": "BackwardFunctionsBasics", "title": "has_subtopic", "to": "ActivationFunctions"}, {"arrows": "to", "from": "LogLikelihood", "title": "subtopic", "to": "GradientAscent"}, {"arrows": "to", "from": "Necessary_Conditions", "title": "depends_on", "to": "Symmetry_Property"}, {"arrows": "to", "from": "Latent_Variables", "title": "depends_on", "to": "Gaussian_Distribution_Qi"}, {"arrows": "to", "from": "MachineLearningConcepts", "title": "has_subtopic", "to": "BackwardFunctionWb"}, {"arrows": "to", "from": "Naive Bayes Algorithm", "title": "subtopic", "to": "Discretization"}, {"arrows": "to", "from": "Mean Vector", "title": "related_to", "to": "Multivariate Normal Distribution"}, {"arrows": "to", "from": "Conditional_Distribution_Modeling", "title": "has_subtopic", "to": "Bernoulli_Distribution"}, {"arrows": "to", "from": "Alpha_Parameters", "title": "related_to", "to": "Bounds_L_H"}, {"arrows": "to", "from": "Optimization Problem in SVM", "title": "related_to", "to": "Functional Margin"}, {"arrows": "to", "from": "LogLikelihoodCalculation", "title": "subtopic", "to": "GradientAscentRule"}, {"arrows": "to", "from": "PilotSurveyExample", "title": "subtopic", "to": "RedundancyDetection"}, {"arrows": "to", "from": "Value_Function_Estimation", "title": "subtopic", "to": "Policy_Gradient_Methods"}, {"arrows": "to", "from": "Logistic_Regression_Gradient_Descent", "title": "related_to", "to": "Perceptron_Algorithm"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "has_subtopic", "to": "Model Complexity"}, {"arrows": "to", "from": "Posterior Distribution", "title": "depends_on", "to": "Bayes\u0027 Theorem"}, {"arrows": "to", "from": "L1 Regularization", "title": "depends_on", "to": "Optimization Problem"}, {"arrows": "to", "from": "Machine_Learning_Algorithms", "title": "has_subtopic", "to": "SMO_Algorithm"}, {"arrows": "to", "from": "Embedding_Features", "title": "subtopic", "to": "Pretraining_Phase"}, {"arrows": "to", "from": "ContrastiveLearning", "title": "example_of", "to": "SIMCLRAlgorithm"}, {"arrows": "to", "from": "ICA Ambiguities", "title": "depends_on", "to": "Permutation Matrix"}, {"arrows": "to", "from": "NonLinearModel", "title": "depends_on", "to": "TrainingExamples"}, {"arrows": "to", "from": "Machine_Learning_Basics", "title": "subtopic", "to": "Linear_Classification"}, {"arrows": "to", "from": "VC Dimension", "title": "defines", "to": "Shattering"}, {"arrows": "to", "from": "Machine_Learning_Concepts", "title": "subtopic", "to": "Naive_Bayes_Classifier"}, {"arrows": "to", "from": "Foundation_Models", "title": "belongs_to", "to": "Machine_Learning_Papers"}, {"arrows": "to", "from": "Self-Supervised Learning", "title": "subtopic", "to": "Supervised Contrastive Algorithms"}, {"arrows": "to", "from": "MultiClassClassification", "title": "depends_on", "to": "SoftmaxFunction"}, {"arrows": "to", "from": "GaussianDistribution", "title": "subtopic", "to": "DensityExamples"}, {"arrows": "to", "from": "Continuous Latent Variables", "title": "subtopic", "to": "Variational Inference"}, {"arrows": "to", "from": "I Supervised learning", "title": "has_subtopic", "to": "2 Classification and logistic regression"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "contains", "to": "Support Vector Machines (SVMs)"}, {"arrows": "to", "from": "EStep", "title": "subtopic", "to": "ExpectationMaximizationAlgorithm"}, {"arrows": "to", "from": "Machine_Learning_Topic", "title": "has_subtopic", "to": "Text_Classification"}, {"arrows": "to", "from": "High_Dimensional_Statistics", "title": "belongs_to", "to": "Machine_Learning_Papers"}, {"arrows": "to", "from": "Optimal_Margin_Classifier", "title": "depends_on", "to": "Lagrange_Duality"}, {"arrows": "to", "from": "Few_Shot_Learning", "title": "belongs_to", "to": "Machine_Learning_Papers"}, {"arrows": "to", "from": "HiddenUnits", "title": "related_to", "to": "NeuralNetworks"}, {"arrows": "to", "from": "Probability_Estimation", "title": "depends_on", "to": "Maximum_Likelihood_Estimates"}, {"arrows": "to", "from": "PolicyGradientMethods", "title": "related_to", "to": "RewardFunction"}, {"arrows": "to", "from": "OptimizationMethods", "title": "contains", "to": "NewtonMethod"}, {"arrows": "to", "from": "TotalParametersConv1D", "title": "related_to", "to": "LinearMappingComparison"}, {"arrows": "to", "from": "Regularization in Machine Learning", "title": "contains", "to": "Kernel Methods Compatibility"}, {"arrows": "to", "from": "Markov Decision Processes (MDP)", "title": "component_of", "to": "States"}, {"arrows": "to", "from": "Statistical Learning Textbook", "title": "subtopic", "to": "Machine Learning Papers"}, {"arrows": "to", "from": "Least-Squares Cost Function J", "title": "subtopic", "to": "Error Term Distribution"}, {"arrows": "to", "from": "Machine_Learning", "title": "has_subtopic", "to": "Text_Classification"}, {"arrows": "to", "from": "MachineLearningConcepts", "title": "contains", "to": "ELBOExplanation"}, {"arrows": "to", "from": "FeatureMappingPhiX", "title": "depends_on", "to": "RuntimeAndMemoryEfficiency"}, {"arrows": "to", "from": "LMS_Update_Rule", "title": "related_to", "to": "Widrow_Hoff_Learning_Rule"}, {"arrows": "to", "from": "Multivariate Normal Distribution", "title": "depends_on", "to": "Gaussian Discriminant Analysis (GDA)"}, {"arrows": "to", "from": "ELBOExplanation", "title": "subtopic", "to": "AlternativeFormulationsOfELBO"}, {"arrows": "to", "from": "Representation Function", "title": "subtopic", "to": "Self-Supervised Learning"}, {"arrows": "to", "from": "Policy_Gradient_Theorem", "title": "has_subtopic", "to": "Vanilla_REINFORCE_Algorithm"}, {"arrows": "to", "from": "MachineLearningConcepts", "title": "contains", "to": "LeastSquaresRegression"}, {"arrows": "to", "from": "ICA_Ambiguities", "title": "has_subtopic", "to": "Gaussian_Data_Issue"}, {"arrows": "to", "from": "Learning Theory Proofs", "title": "subtopic", "to": "Binary Classification"}, {"arrows": "to", "from": "MachineLearningConcepts", "title": "contains", "to": "EMAlgorithmOverview"}, {"arrows": "to", "from": "Gradient Estimation", "title": "describes", "to": "Algorithm 7"}, {"arrows": "to", "from": "Non-linear Feature Mappings", "title": "subtopic", "to": "Machine Learning Models"}, {"arrows": "to", "from": "LanguageModels", "title": "depends_on", "to": "ConditionalProbability"}, {"arrows": "to", "from": "Feature Mapping", "title": "subtopic", "to": "Linear Function Over Features"}, {"arrows": "to", "from": "Text_Classification", "title": "example_of", "to": "Spam_Filtering"}, {"arrows": "to", "from": "Optimization_Frameworks", "title": "depends_on", "to": "Hessian_Matrix"}, {"arrows": "to", "from": "Regularization and Non-Separable Case", "title": "leads_to", "to": "Lagrangian Formulation"}, {"arrows": "to", "from": "2 Classification and logistic regression", "title": "has_subtopic", "to": "2.4 Another algorithm for maximizing \u03bb(\u03b8)"}, {"arrows": "to", "from": "DiscriminativeLearning", "title": "subtopic", "to": "PerceptronAlgorithm"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "related_to", "to": "Deep Learning Packages"}, {"arrows": "to", "from": "CarAttributesExample", "title": "subtopic", "to": "RedundancyDetection"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "has_subtopic", "to": "Regularization Techniques"}, {"arrows": "to", "from": "Machine_Learning_Bias_Variance_Tradeoff", "title": "subtopic", "to": "Infinite_Hypothesis_Classes"}, {"arrows": "to", "from": "Notational Consistency", "title": "subtopic", "to": "Multi-layer Neural Networks"}, {"arrows": "to", "from": "Value Function Approximation", "title": "subtopic", "to": "Fitted Value Iteration"}, {"arrows": "to", "from": "Downstream_Task_Dataset", "title": "subtopic", "to": "Few_Shot_Learning"}, {"arrows": "to", "from": "KernelFunctions", "title": "related_to", "to": "FeatureMapping"}, {"arrows": "to", "from": "PosteriorApproximation", "title": "subtopic", "to": "MAPEstimate"}, {"arrows": "to", "from": "Normalization Techniques", "title": "has_subtopic", "to": "Data Rescaling"}, {"arrows": "to", "from": "Learning_Model_for_MDP", "title": "contains", "to": "State_Transition_Probabilities"}, {"arrows": "to", "from": "Continuous state MDPs", "title": "subtopic", "to": "Discretization"}, {"arrows": "to", "from": "MachineLearningConcepts", "title": "subtopic", "to": "VCDimensionIntroduction"}, {"arrows": "to", "from": "LogProbabilityDerivative", "title": "depends_on", "to": "AnalyticalFormula"}, {"arrows": "to", "from": "BatchGradientDescent", "title": "related_to", "to": "FeatureMapPhi"}, {"arrows": "to", "from": "Generalization", "title": "has_subtopic", "to": "Bias-variance tradeoff"}, {"arrows": "to", "from": "Machine Learning", "title": "has_subtopic", "to": "Reinforcement Learning"}, {"arrows": "to", "from": "SupervisedLearning", "title": "related_to", "to": "LinearRegression"}, {"arrows": "to", "from": "MachineLearningConcepts", "title": "contains", "to": "MaximumLikelihoodEstimation"}, {"arrows": "to", "from": "Normal_Equations_Method", "title": "has_subtopic", "to": "Matrix_Derivatives"}, {"arrows": "to", "from": "Conditional Probability p(x|y)", "title": "depends_on", "to": "Bayesian Classification"}, {"arrows": "to", "from": "Spam_Filtering", "title": "uses", "to": "Feature_Vector"}, {"arrows": "to", "from": "MachineLearningOverview", "title": "has_subtopic", "to": "ValueFunctionApproximation"}, {"arrows": "to", "from": "Optimization_Problems", "title": "subtopic", "to": "Primal_Dual_Pairing"}, {"arrows": "to", "from": "UnsupervisedLearning", "title": "subtopic", "to": "Clustering"}, {"arrows": "to", "from": "I Supervised learning", "title": "has_subtopic", "to": "4 Generative learning algorithms"}, {"arrows": "to", "from": "Machine_Learning_Algorithms", "title": "contains", "to": "Policy_Iteration"}, {"arrows": "to", "from": "Objective Function", "title": "includes", "to": "Slack Variables"}, {"arrows": "to", "from": "EM_Algorithm", "title": "subtopic", "to": "M_Step"}, {"arrows": "to", "from": "Backpropagation for MLPs", "title": "depends_on", "to": "Gradient Calculation"}, {"arrows": "to", "from": "LogisticRegression", "title": "subtopic", "to": "Robustness"}, {"arrows": "to", "from": "MatrixAlgebra", "title": "subtopic", "to": "VectorizationInNN"}, {"arrows": "to", "from": "Loss_Functions", "title": "subtopic", "to": "Test_Error"}, {"arrows": "to", "from": "PropertiesOfKernels", "title": "related_to", "to": "FeatureMapsAndKernels"}, {"arrows": "to", "from": "LogisticRegression", "title": "contains", "to": "GradientAscentRule"}, {"arrows": "to", "from": "GaussianKernel", "title": "subtopic", "to": "KernelsAsSimilarityMetrics"}, {"arrows": "to", "from": "Gaussian_Data_Issue", "title": "has_subtopic", "to": "Mixing_Matrix_Rotation"}, {"arrows": "to", "from": "Functional Margins", "title": "related_to", "to": "Geometric Margins"}, {"arrows": "to", "from": "MachineLearningModels", "title": "has_subtopic", "to": "MultinomialEventModel"}, {"arrows": "to", "from": "ConstrainedOptimization", "title": "subtopic", "to": "PrimalProblem"}, {"arrows": "to", "from": "MachineLearningConcepts", "title": "contains", "to": "MultivariateNormalDistribution"}, {"arrows": "to", "from": "LogisticRegression", "title": "contains", "to": "LogisticLossFunction"}, {"arrows": "to", "from": "Sequential Minimal Optimization (SMO) Algorithm", "title": "subtopic", "to": "Support Vector Machines (SVM)"}, {"arrows": "to", "from": "Empirical_Risk_Minimization", "title": "subtopic", "to": "Machine_Learning_Concepts"}, {"arrows": "to", "from": "LQR Extension to POMDP", "title": "subtopic", "to": "Machine Learning Concepts"}, {"arrows": "to", "from": "Reinforcement Learning and Control", "title": "has_subtopic", "to": "Reinforcement learning"}, {"arrows": "to", "from": "MachineLearningOverview", "title": "related_to", "to": "SupervisedLearning"}, {"arrows": "to", "from": "EM_Algorithm", "title": "related_to", "to": "Soft_Assignments"}, {"arrows": "to", "from": "k-means Algorithm", "title": "depends_on", "to": "Distortion Function J"}, {"arrows": "to", "from": "Deep Residual Learning", "title": "subtopic", "to": "Machine Learning Papers"}, {"arrows": "to", "from": "Model-wise Double Descent", "title": "related_to", "to": "Optimal Regularization"}, {"arrows": "to", "from": "Finite_Horizon_MDPs", "title": "subtopic", "to": "Optimal_Bellman_Equation"}, {"arrows": "to", "from": "Optimization_Problems", "title": "related_to", "to": "Primal_Dual_Equivalence"}, {"arrows": "to", "from": "Sparsity Regularization", "title": "depends_on", "to": "L1 Norm (LASSO)"}, {"arrows": "to", "from": "MachineLearningModels", "title": "contains", "to": "TransformerModel"}, {"arrows": "to", "from": "Logistic Regression", "title": "related_to", "to": "Classification Problem"}, {"arrows": "to", "from": "Machine_Learning", "title": "contains", "to": "Supervised_Learning"}, {"arrows": "to", "from": "MachineLearningOverview", "title": "depends_on", "to": "BayesianInference"}, {"arrows": "to", "from": "Bayesian Machine Learning", "title": "subtopic", "to": "Posterior Distribution"}, {"arrows": "to", "from": "Hypothesis_Class_Switching", "title": "subtopic", "to": "Bias_Decrease"}, {"arrows": "to", "from": "Machine_Learning_Techniques", "title": "has_subtopic", "to": "Model_Selection"}, {"arrows": "to", "from": "Optimizers", "title": "subtopic", "to": "Gradient Descent (GD)"}, {"arrows": "to", "from": "LossFunction", "title": "has_subtopic", "to": "GradientCalculation"}, {"arrows": "to", "from": "HousingPricePrediction", "title": "depends_on", "to": "DerivedFeatures"}, {"arrows": "to", "from": "EM_Algorithm", "title": "subtopic", "to": "Log_Likelihood_Optimization"}, {"arrows": "to", "from": "ModulesInMLP", "title": "depends_on", "to": "ParameterizationOfModules"}, {"arrows": "to", "from": "Cross Validation", "title": "has_subtopic", "to": "Leave-One-Out CV"}, {"arrows": "to", "from": "Data Visualization", "title": "subtopic", "to": "Principal Component Analysis (PCA)"}, {"arrows": "to", "from": "7 Deep learning", "title": "has_subtopic", "to": "7.2 Neural networks"}, {"arrows": "to", "from": "PartialDerivativeCalculation", "title": "subtopic", "to": "GradientDescentAlgorithm"}, {"arrows": "to", "from": "MachineLearningConcepts", "title": "has_subtopic", "to": "FeatureExtraction"}, {"arrows": "to", "from": "Support Vector Machines (SVMs)", "title": "subtopic", "to": "Notation for SVMs"}, {"arrows": "to", "from": "ModelAdaptation", "title": "subtopic", "to": "Finetuning"}, {"arrows": "to", "from": "7.4 Backpropagation", "title": "has_subtopic", "to": "7.4.1 Preliminaries on partial derivatives"}, {"arrows": "to", "from": "4.1 Gaussian discriminant analysis", "title": "has_subtopic", "to": "4.1.1 The multivariate normal distribution"}, {"arrows": "to", "from": "Primal Problem", "title": "related_to", "to": "Relationship Between Primal and Dual"}, {"arrows": "to", "from": "Theorem 7.4.1", "title": "related_to", "to": "Backpropagation"}, {"arrows": "to", "from": "Machine_Learning_Challenges", "title": "has_subtopic", "to": "k_Fold_Cross_Validation"}, {"arrows": "to", "from": "Gradient Calculation", "title": "subtopic", "to": "Matrix Derivatives"}, {"arrows": "to", "from": "ExponentialFamilyDistributions", "title": "has_component", "to": "NaturalParameter"}, {"arrows": "to", "from": "Text_Classification", "title": "subtopic", "to": "Feature_Vector_Selection"}, {"arrows": "to", "from": "State_Transition_Probabilities", "title": "depends_on", "to": "MDP_Model_Learning"}, {"arrows": "to", "from": "Support Vector Machines (SVM)", "title": "depends_on", "to": "Machine Learning Overview"}, {"arrows": "to", "from": "Convex_Functions", "title": "subtopic", "to": "Jensens_Inequality"}, {"arrows": "to", "from": "Generalization_Error", "title": "subtopic", "to": "Machine_Learning_Concepts"}, {"arrows": "to", "from": "Locally Weighted Linear Regression", "title": "related_to", "to": "Value Function Approximation"}, {"arrows": "to", "from": "EM Algorithm", "title": "related_to", "to": "Convex Function"}, {"arrows": "to", "from": "ParameterEstimation", "title": "subtopic", "to": "NaiveBayesAlgorithm"}, {"arrows": "to", "from": "Softmax_Function", "title": "subtopic", "to": "Logits"}, {"arrows": "to", "from": "Backpropagation Algorithm", "title": "related_to", "to": "Intermediate Values Storage"}, {"arrows": "to", "from": "LinearRegression", "title": "has_subtopic", "to": "HypothesisFunction"}, {"arrows": "to", "from": "MachineLearningOverview", "title": "contains", "to": "BinaryClassificationProblem"}, {"arrows": "to", "from": "CrossValidation", "title": "subtopic", "to": "HoldOutCrossValidation"}, {"arrows": "to", "from": "Quadratic_Form", "title": "depends_on", "to": "Dynamics_Model"}, {"arrows": "to", "from": "Machine_Learning_Bias_Variance_Tradeoff", "title": "subtopic", "to": "Linear_Model_Failure"}, {"arrows": "to", "from": "Algorithm 6", "title": "depends_on", "to": "Procedure VE"}, {"arrows": "to", "from": "Step 1", "title": "depends_on", "to": "Gaussian Distributions"}, {"arrows": "to", "from": "Finetuning_Pretrained_Models", "title": "related_to", "to": "Optimization_Objective"}, {"arrows": "to", "from": "Machine_Learning_Concepts", "title": "related_to", "to": "Negative_Log_Likelihood"}, {"arrows": "to", "from": "Transition_Probabilities_Sampling", "title": "subtopic", "to": "Chapter_17_Policy_Gradient_REINFORCE"}, {"arrows": "to", "from": "Variational Autoencoders", "title": "subtopic", "to": "Machine Learning Papers"}, {"arrows": "to", "from": "GeneralizedLinearModels", "title": "has_assumption", "to": "Assumption3"}, {"arrows": "to", "from": "Regularization in Deep Learning", "title": "contains", "to": "Explicit Regularization Techniques"}, {"arrows": "to", "from": "EM Algorithm", "title": "related_to", "to": "Variational Inference"}, {"arrows": "to", "from": "1.2 The normal equations", "title": "has_subtopic", "to": "1.2.2 Least squares revisited"}, {"arrows": "to", "from": "Machine_Learning_Backward_Functions", "title": "subtopic", "to": "Squared_Loss_Backward"}, {"arrows": "to", "from": "PayoffDefinition", "title": "subtopic", "to": "FiniteHorizonMDP"}, {"arrows": "to", "from": "BackwardFunctionWb", "title": "has_subtopic", "to": "VectorizedNotation"}, {"arrows": "to", "from": "ICA_Ambiguities", "title": "has_subtopic", "to": "Scaling_Impact"}, {"arrows": "to", "from": "ICA", "title": "component_of", "to": "Mixing_Matrix_A"}, {"arrows": "to", "from": "SingleExampleOptimization", "title": "depends_on", "to": "SummationNotEssential"}, {"arrows": "to", "from": "MachineLearningConcepts", "title": "contains", "to": "LinearTransformationsEffect"}, {"arrows": "to", "from": "DataPreprocessing", "title": "depends_on", "to": "LogisticFunctionDerivative"}, {"arrows": "to", "from": "4.2 Naive bayes (Option Reading)", "title": "has_subtopic", "to": "4.2.1 Laplace smoothing"}, {"arrows": "to", "from": "Double Descent Phenomenon", "title": "related_to", "to": "Implicit Regularization"}, {"arrows": "to", "from": "NewtonMethod", "title": "special_case_of", "to": "FisherScoring"}, {"arrows": "to", "from": "Multiple_Examples_Consideration", "title": "subtopic", "to": "Evidence_Lower_Bound_(ELBO)"}, {"arrows": "to", "from": "ProbabilisticModeling", "title": "subtopic", "to": "LikelihoodFunction"}, {"arrows": "to", "from": "MStepUpdateRule", "title": "subtopic", "to": "ExpectationMaximizationAlgorithm"}, {"arrows": "to", "from": "Expectation_Computation", "title": "subtopic", "to": "Deterministic_Simulator"}, {"arrows": "to", "from": "statistical_mechanics_of_learning", "title": "subtopic", "to": "generalization"}, {"arrows": "to", "from": "Naive Bayes Algorithm", "title": "depends_on", "to": "Binary Features"}, {"arrows": "to", "from": "HypothesisClassParameterization", "title": "depends_on", "to": "LinearClassifierDefinition"}, {"arrows": "to", "from": "Model_Selection", "title": "includes", "to": "Cross_Validation"}, {"arrows": "to", "from": "Eigenvectors and Eigenvalues", "title": "subtopic", "to": "Principal Component Analysis (PCA)"}, {"arrows": "to", "from": "Machine Learning Algorithms", "title": "related_to", "to": "Value Iteration"}, {"arrows": "to", "from": "Kernel_Functions", "title": "subtopic", "to": "Necessary_Conditions"}, {"arrows": "to", "from": "Objective_Function_W", "title": "subtopic", "to": "Maximization_Process"}, {"arrows": "to", "from": "IntermediateVariables", "title": "contains", "to": "ForwardPass"}, {"arrows": "to", "from": "Machine_Learning_Optimization", "title": "has_subtopic", "to": "Stochastic_Gradient_Descent"}, {"arrows": "to", "from": "Reinforcement Learning", "title": "defines_formalism", "to": "Markov Decision Processes (MDP)"}, {"arrows": "to", "from": "Hypothesis_Space_Size", "title": "subtopic", "to": "Sample_Complexity"}, {"arrows": "to", "from": "Reparameterization_Trick", "title": "subtopic_of", "to": "Mathematical_Formulation_Reparametrization"}, {"arrows": "to", "from": "Asynchronous_Update", "title": "subtopic", "to": "Value_Iteration"}, {"arrows": "to", "from": "Finite Horizon MDP Dynamics", "title": "subtopic", "to": "Time Dependent Transition Probabilities"}, {"arrows": "to", "from": "GaussianDiscriminantAnalysis", "title": "subtopic", "to": "ModelAssumptions"}, {"arrows": "to", "from": "MachineLearningModels", "title": "related_to", "to": "GaussianDistribution"}, {"arrows": "to", "from": "Maximize_Geometric_Margin", "title": "subtopic", "to": "Optimal_Margin_Classifier"}, {"arrows": "to", "from": "KernelMethods", "title": "has_subtopic", "to": "KernelTrick"}, {"arrows": "to", "from": "Cross_Validation", "title": "alternative_method", "to": "k_Fold_Cross_Validation"}, {"arrows": "to", "from": "GaussianMixtureModel", "title": "subtopic", "to": "EMAlgorithm"}, {"arrows": "to", "from": "LogisticRegression", "title": "related_to", "to": "ProbabilityPrediction"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "related_to", "to": "Overparameterized Models"}, {"arrows": "to", "from": "I Supervised learning", "title": "has_subtopic", "to": "3 Generalized linear models"}, {"arrows": "to", "from": "Labeled_Task_Dataset", "title": "depends_on", "to": "Adaptation_Phase"}, {"arrows": "to", "from": "Finding Roots", "title": "related_to", "to": "Maximizing Functions"}, {"arrows": "to", "from": "Markov Decision Processes (MDP)", "title": "component_of", "to": "Actions"}, {"arrows": "to", "from": "Machine_Learning_Bias_Variance_Tradeoff", "title": "subtopic", "to": "Sample_Complexity_Bound"}, {"arrows": "to", "from": "Procedure VE", "title": "related_to", "to": "Option 1 and Option 2"}, {"arrows": "to", "from": "Kernels_in_Machine_Learning", "title": "related_to", "to": "Kernel_Function_K"}, {"arrows": "to", "from": "MachineLearningOverview", "title": "related_to", "to": "FeatureMapping"}, {"arrows": "to", "from": "Finite_Horizon_Case", "title": "subtopic", "to": "Chapter_17_Policy_Gradient_REINFORCE"}, {"arrows": "to", "from": "NaturalParameterForBernoulli", "title": "depends_on", "to": "SigmoidFunction"}, {"arrows": "to", "from": "Variational_Autoencoder", "title": "subtopic", "to": "Encoder_Decoder_Networks"}, {"arrows": "to", "from": "NeuralNetworksInspiration", "title": "contains", "to": "ParametersTheta"}, {"arrows": "to", "from": "Loss Function", "title": "subtopic", "to": "Negative Log-Likelihood"}, {"arrows": "to", "from": "Locally Weighted Linear Regression", "title": "subtopic", "to": "Machine Learning Models"}, {"arrows": "to", "from": "Dimensionality Reduction Techniques", "title": "contains", "to": "Principal Component Analysis (PCA)"}, {"arrows": "to", "from": "SIMCLRAlgorithm", "title": "uses", "to": "AugmentationTechniques"}, {"arrows": "to", "from": "BatchGradientDescent", "title": "depends_on", "to": "BetaUpdateEquation"}, {"arrows": "to", "from": "Gaussian Discriminant Analysis (GDA)", "title": "subtopic", "to": "Bayesian Classification"}, {"arrows": "to", "from": "GaussianDiscriminantAnalysis", "title": "subtopic", "to": "AsymptoticEfficiency"}, {"arrows": "to", "from": "Self-Supervised Learning", "title": "depends_on", "to": "Data Augmentation"}, {"arrows": "to", "from": "BinaryClassification", "title": "depends_on", "to": "LogisticFunction"}, {"arrows": "to", "from": "GaussianDiscriminantAnalysis(GDA)", "title": "has_subtopic", "to": "DecisionBoundary"}, {"arrows": "to", "from": "Machine_Learning_Bias_Variance", "title": "contains", "to": "Bias_Term"}, {"arrows": "to", "from": "Sample Complexity Bounds", "title": "subtopic", "to": "Generalization Error"}, {"arrows": "to", "from": "TwoLayerNN", "title": "depends_on", "to": "GELUFunction"}, {"arrows": "to", "from": "Discretization in MDPs", "title": "depends_on", "to": "Policy Iteration"}, {"arrows": "to", "from": "GeometricMargin", "title": "depends_on", "to": "FunctionalMargin"}, {"arrows": "to", "from": "Regularization and Non-Separable Case", "title": "has_subtopic", "to": "L1 Regularization"}, {"arrows": "to", "from": "LayerNormalization", "title": "related_to", "to": "AffineTransformation"}, {"arrows": "to", "from": "Independent components analysis", "title": "has_subtopic", "to": "ICA ambiguities"}, {"arrows": "to", "from": "SMO_Algorithm", "title": "has_subtopic", "to": "Heuristic_Selection"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "depends_on", "to": "Neural Networks"}, {"arrows": "to", "from": "Jensen\u0027s Inequality", "title": "subtopic", "to": "E[f(X)] vs f(E[X])"}, {"arrows": "to", "from": "Regularization and model selection", "title": "has_subtopic", "to": "Model selection via cross validation"}, {"arrows": "to", "from": "Machine_Learning_Bias_Variance", "title": "contains", "to": "Noise_Prediction_Impact"}, {"arrows": "to", "from": "TwoLayerNN", "title": "depends_on", "to": "TanhFunction"}, {"arrows": "to", "from": "Support_Vector_Machines", "title": "subtopic", "to": "Optimal_Margin_Classifier"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "related_to", "to": "EM Algorithms"}, {"arrows": "to", "from": "MachineLearningOverview", "title": "contains", "to": "ELBOOptimization"}, {"arrows": "to", "from": "Finite Horizon MDP Dynamics", "title": "subtopic", "to": "Value Function for Non-Stationary MDPs"}, {"arrows": "to", "from": "From non-linear dynamics to LQR", "title": "subtopic", "to": "Differential Dynamic Programming (DDP)"}, {"arrows": "to", "from": "4.1 Gaussian discriminant analysis", "title": "has_subtopic", "to": "4.1.2 The Gaussian discriminant analysis model"}, {"arrows": "to", "from": "Conditional Probabilistic Models", "title": "depends_on", "to": "Exponential Family Distribution"}, {"arrows": "to", "from": "Binary_Classification", "title": "subtopic", "to": "Training_Error"}, {"arrows": "to", "from": "Model-Based Reinforcement Learning", "title": "subtopic", "to": "Machine Learning Papers"}, {"arrows": "to", "from": "Generalization", "title": "has_subtopic", "to": "Sample complexity bounds (optional readings)"}, {"arrows": "to", "from": "TextGeneration", "title": "depends_on", "to": "ConditionalProbability"}, {"arrows": "to", "from": "EM algorithms", "title": "has_subtopic", "to": "EM for mixture of Gaussians"}, {"arrows": "to", "from": "GeneralizedLinearModels", "title": "contains", "to": "ExponentialFamilyDistributions"}, {"arrows": "to", "from": "State Transition", "title": "depends_on", "to": "Markov Decision Process (MDP)"}, {"arrows": "to", "from": "Backward_Function_Linear_Map", "title": "depends_on", "to": "Jacobian_Matrix_Transpose"}, {"arrows": "to", "from": "Naive Bayes Algorithm", "title": "depends_on", "to": "Feature Representation"}, {"arrows": "to", "from": "EM_Algorithm", "title": "subtopic", "to": "Convergence_Issues"}, {"arrows": "to", "from": "Machine_Learning_Algorithms", "title": "contains", "to": "Kernel_Trick"}, {"arrows": "to", "from": "Machine_Learning_Optimization", "title": "related_to", "to": "Normal_Equations_Method"}, {"arrows": "to", "from": "MachineLearningAlgorithms", "title": "contains", "to": "DiscriminativeLearning"}, {"arrows": "to", "from": "Regularization", "title": "addresses", "to": "Overfitting"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "depends_on", "to": "Linear Regression Setup"}, {"arrows": "to", "from": "EM algorithms", "title": "has_subtopic", "to": "Jensen\u0027s inequality"}, {"arrows": "to", "from": "MachineLearningConcepts", "title": "contains", "to": "GaussianDiscriminantAnalysisModel"}, {"arrows": "to", "from": "Machine_Learning", "title": "related_to", "to": "Policy_Definition"}, {"arrows": "to", "from": "RegressionProblems", "title": "subtopic", "to": "MeanSquaredLoss"}, {"arrows": "to", "from": "KernelTrick", "title": "related_to", "to": "MachineLearningConcepts"}, {"arrows": "to", "from": "ICA_Ambiguities", "title": "has_subtopic", "to": "Non_Gaussian_Sources"}, {"arrows": "to", "from": "Self-supervised learning and foundation models", "title": "has_subtopic", "to": "Pretrained large language models"}, {"arrows": "to", "from": "4 Generative learning algorithms", "title": "has_subtopic", "to": "4.2 Naive bayes (Option Reading)"}, {"arrows": "to", "from": "Machine_Learning_Concepts", "title": "related_to", "to": "Differential_Dynamic_Programming"}, {"arrows": "to", "from": "Machine_Learning_Optimization", "title": "has_subtopic", "to": "Batch_Gradient_Descent"}, {"arrows": "to", "from": "ReLU Activation Function", "title": "subtopic", "to": "Multi-layer Neural Networks"}, {"arrows": "to", "from": "Convolutional_Layers", "title": "subtopic_of", "to": "Efficiency_of_Convolution"}, {"arrows": "to", "from": "WeightsInLWLR", "title": "related_to", "to": "BandwidthParameter"}, {"arrows": "to", "from": "Machine_Learning_Backward_Functions", "title": "related_to", "to": "Efficiency_of_Backward_Pass"}, {"arrows": "to", "from": "MDP Simulators", "title": "subtopic", "to": "Machine Learning Models"}, {"arrows": "to", "from": "Representation Function", "title": "related_to", "to": "Negative Pair"}, {"arrows": "to", "from": "Machine_Learning_Optimization", "title": "subtopic", "to": "Coordinate_Ascend_Method"}, {"arrows": "to", "from": "ReLUActivation", "title": "subtopic", "to": "NeuralNetworks"}, {"arrows": "to", "from": "AdaptiveSampling", "title": "related_to", "to": "TemperatureParameter"}, {"arrows": "to", "from": "GeneralizedLinearModels", "title": "has_assumption", "to": "Assumption1"}, {"arrows": "to", "from": "Principal Component Analysis (PCA)", "title": "subtopic_of", "to": "Noise Reduction"}, {"arrows": "to", "from": "MachineLearningOverview", "title": "contains", "to": "ResNetArchitecture"}, {"arrows": "to", "from": "Exponential_Family_Distributions", "title": "has_subtopic", "to": "Canonical_Link_Function"}, {"arrows": "to", "from": "Uniform_Convergence", "title": "related_to", "to": "Generalization_Error_Bound"}, {"arrows": "to", "from": "Unsupervised Learning", "title": "depends_on", "to": "Mixture of Gaussians Model"}, {"arrows": "to", "from": "Unsupervised learning", "title": "has_subtopic", "to": "Principal components analysis"}, {"arrows": "to", "from": "Markov Decision Processes (MDP)", "title": "component_of", "to": "Discount Factor"}, {"arrows": "to", "from": "Machine_Learning_Basics", "title": "depends_on", "to": "LMS_Algorithm"}, {"arrows": "to", "from": "Machine_Learning_Adaptation_Methods", "title": "has_subtopic", "to": "Finetuning_Pretrained_Models"}, {"arrows": "to", "from": "Backpropagation", "title": "has_subtopic", "to": "Backward functions for basic modules"}, {"arrows": "to", "from": "LogisticRegression", "title": "uses", "to": "HypothesisFunction"}, {"arrows": "to", "from": "NeuralNetworks", "title": "subtopic", "to": "SingleNeuronNN"}, {"arrows": "to", "from": "Machine_Learning_Backpropagation", "title": "related_to", "to": "Efficient_Backward_Propagation"}, {"arrows": "to", "from": "Variational_Inference_Review", "title": "belongs_to", "to": "Machine_Learning_Papers"}, {"arrows": "to", "from": "FittedValueIteration", "title": "depends_on", "to": "StateSampling"}, {"arrows": "to", "from": "MachineLearningConcepts", "title": "subtopic", "to": "BiasVsVarianceTradeoff"}, {"arrows": "to", "from": "1 Linear regression", "title": "has_subtopic", "to": "1.3 Probabilistic interpretation"}, {"arrows": "to", "from": "5 Kernel methods", "title": "has_subtopic", "to": "5.3 LMS with the kernel trick"}, {"arrows": "to", "from": "E_Step", "title": "depends_on", "to": "Gaussian_Mixture_Models"}, {"arrows": "to", "from": "Machine Learning Overview", "title": "contains", "to": "Non-Parametric Algorithms"}, {"arrows": "to", "from": "5 Kernel methods", "title": "has_subtopic", "to": "5.1 Feature maps"}, {"arrows": "to", "from": "Model Creation Methods", "title": "has_subtopic", "to": "Physics Simulation"}, {"arrows": "to", "from": "MachineLearningModels", "title": "subtopic", "to": "DeterministicModel"}, {"arrows": "to", "from": "Optimal_Margin_Classifier", "title": "related_to", "to": "Machine_Learning_Concepts"}, {"arrows": "to", "from": "KKT_Conditions", "title": "subtopic", "to": "Dual_Complementarity"}, {"arrows": "to", "from": "MAPEstimate", "title": "related_to", "to": "MLEvsMAP"}, {"arrows": "to", "from": "ProbabilityDistributions", "title": "depends_on", "to": "JensensInequality"}, {"arrows": "to", "from": "MachineLearningConcepts", "title": "related_to", "to": "ShatteringConcept"}, {"arrows": "to", "from": "Support_Vector_Machines_SVM", "title": "leads_to", "to": "Optimal_Margin_Classifier"}, {"arrows": "to", "from": "Necessary_Conditions", "title": "related_to", "to": "Kernel_Matrix"}, {"arrows": "to", "from": "SingleNeuronNN", "title": "related_to", "to": "HousingPricePrediction"}, {"arrows": "to", "from": "Machine_Learning_Backward_Functions", "title": "subtopic", "to": "Vectorized_Notation_Backward_Func"}, {"arrows": "to", "from": "Log_Probability_Ratio", "title": "depends_on", "to": "Gradient_Estimation"}, {"arrows": "to", "from": "MachineLearningModels", "title": "subtopic", "to": "BinaryClassification"}, {"arrows": "to", "from": "Value_Iteration", "title": "related_to", "to": "MDP_Model_Learning"}, {"arrows": "to", "from": "Machine_Learning_Concepts", "title": "contains", "to": "Support_Vector_Machines_SVM"}, {"arrows": "to", "from": "GenerativeLearning", "title": "contains", "to": "ConditionalProbabilityModel"}, {"arrows": "to", "from": "Machine_Learning", "title": "depends_on", "to": "Feature_Engineering"}, {"arrows": "to", "from": "MachineLearningOverview", "title": "contains", "to": "KernelMethods"}, {"arrows": "to", "from": "MachineLearningOverview", "title": "contains", "to": "ICAConcepts"}, {"arrows": "to", "from": "MachineLearningModels", "title": "subtopic", "to": "LossFunctions"}, {"arrows": "to", "from": "Machine_Learning_Adaptation_Techniques", "title": "subtopic", "to": "In-Context_Learning"}, {"arrows": "to", "from": "LinearRegression", "title": "subtopic", "to": "FeatureSelection"}, {"arrows": "to", "from": "MachineLearningModels", "title": "contains", "to": "GeneralizedLinearModels"}, {"arrows": "to", "from": "Predict Step", "title": "related_to", "to": "Update Step"}, {"arrows": "to", "from": "FullyConnectedNN", "title": "depends_on", "to": "IntermediateVariables"}, {"arrows": "to", "from": "FeatureMapsAndKernels", "title": "subtopic", "to": "InnerProductCalculation"}, {"arrows": "to", "from": "Computational Efficiency", "title": "depends_on", "to": "Kalman Filter"}, {"arrows": "to", "from": "Machine Learning Algorithms", "title": "subtopic", "to": "Initialization"}, {"arrows": "to", "from": "Training_Transformer", "title": "related_to", "to": "Cross_Entropy_Loss"}, {"arrows": "to", "from": "Probabilistic Interpretation of Linear Regression", "title": "related_to", "to": "Least-Squares Cost Function J"}, {"arrows": "to", "from": "BernoulliDistribution", "title": "has_subtopic", "to": "NaturalParameterForBernoulli"}, {"arrows": "to", "from": "Adaptation_Algorithm", "title": "has_subtopic", "to": "Linear_Probe_Method"}, {"arrows": "to", "from": "Hoeffding_Inequality", "title": "subtopic", "to": "Machine_Learning_Concepts"}, {"arrows": "to", "from": "LogisticRegression", "title": "depends_on", "to": "LogitFunction"}, {"arrows": "to", "from": "MachineLearningOverview", "title": "related_to", "to": "LocallyWeightedLinearRegression"}, {"arrows": "to", "from": "ELBO Lower Bound", "title": "depends_on", "to": "Variational Inference"}, {"arrows": "to", "from": "ICAConcepts", "title": "subtopic", "to": "MaximumLikelihoodEstimation"}, {"arrows": "to", "from": "Support_Vector_Machines_SVMs", "title": "depends_on", "to": "SVM_Dual_Problem"}, {"arrows": "to", "from": "DiscriminativeLearning", "title": "subtopic", "to": "LogisticRegression"}, {"arrows": "to", "from": "Sample_Complexity", "title": "depends_on", "to": "Hypothesis_Class_Size"}, {"arrows": "to", "from": "I Supervised learning", "title": "has_subtopic", "to": "6 Support vector machines"}, {"arrows": "to", "from": "Training_Error_Generalization_Error_Difference", "title": "depends_on", "to": "Uniform_Convergence"}, {"arrows": "to", "from": "BernoulliEventModel", "title": "related_to", "to": "NaiveBayes"}, {"arrows": "to", "from": "Reinforcement learning", "title": "has_subtopic", "to": "Markov decision processes"}, {"arrows": "to", "from": "LinearRegression", "title": "depends_on", "to": "FittingTheta"}, {"arrows": "to", "from": "Non-linear Dynamics and LQR", "title": "example", "to": "Inverted Pendulum Example"}, {"arrows": "to", "from": "Machine_Learning_Concepts", "title": "subtopic", "to": "Geometric_Margins"}, {"arrows": "to", "from": "Randomized_Policy", "title": "subtopic", "to": "Chapter_17_Policy_Gradient_REINFORCE"}, {"arrows": "to", "from": "Machine Learning Overview", "title": "contains", "to": "Parametric Algorithms"}, {"arrows": "to", "from": "TwoLayerNN", "title": "depends_on", "to": "SigmoidFunction"}, {"arrows": "to", "from": "Bayes\u0027 Theorem", "title": "related_to", "to": "Prior Distribution"}, {"arrows": "to", "from": "PolicyGradients", "title": "has_subtopic", "to": "SampleBasedEstimator"}, {"arrows": "to", "from": "7.4 Backpropagation", "title": "has_subtopic", "to": "7.4.2 General strategy of backpropagation"}, {"arrows": "to", "from": "Optimal Value Function", "title": "subtopic", "to": "Value Function"}, {"arrows": "to", "from": "Optimal_Policy", "title": "subtopic", "to": "Machine_Learning"}, {"arrows": "to", "from": "SVM", "title": "subtopic", "to": "Model Set M"}, {"arrows": "to", "from": "Deep_Learning", "title": "depends_on", "to": "Neural_Networks"}, {"arrows": "to", "from": "Machine_Learning", "title": "subtopic", "to": "Deep_Learning"}, {"arrows": "to", "from": "Classification Problem", "title": "subtopic", "to": "Binary Classification"}, {"arrows": "to", "from": "Dimensionality Reduction", "title": "depends_on", "to": "Principal Component Analysis (PCA)"}, {"arrows": "to", "from": "Machine_Learning", "title": "related_to", "to": "Optimization_Techniques"}, {"arrows": "to", "from": "TwoLayerNN", "title": "related_to", "to": "IdentityFunction"}, {"arrows": "to", "from": "3.2 Constructing GLMs", "title": "has_subtopic", "to": "3.2.2 Logistic regression"}, {"arrows": "to", "from": "k-means_algorithm", "title": "has_subtopic", "to": "inner_loop_steps"}, {"arrows": "to", "from": "ELBO", "title": "depends_on", "to": "EStep"}, {"arrows": "to", "from": "LQR Algorithm Steps", "title": "subtopic", "to": "Efficiency Improvement"}, {"arrows": "to", "from": "Optimal Parameters Calculation", "title": "subtopic", "to": "Prediction with Inner Products"}, {"arrows": "to", "from": "GenerativeLearning", "title": "contains", "to": "ClassPriors"}, {"arrows": "to", "from": "BackwardFunctionWb", "title": "has_subtopic", "to": "EfficiencyConsiderations"}, {"arrows": "to", "from": "Machine_Learning_Theorem", "title": "subtopic", "to": "Bias_Variance_Tradeoff"}, {"arrows": "to", "from": "LogisticFunction", "title": "has_derivative", "to": "DerivativeOfSigmoid"}, {"arrows": "to", "from": "Machine_Learning_Concepts", "title": "depends_on", "to": "Softmax_Function"}, {"arrows": "to", "from": "Matricization Approach", "title": "subtopic", "to": "Data Matrix Representation"}, {"arrows": "to", "from": "Generalization", "title": "depends_on", "to": "Training Loss Function"}, {"arrows": "to", "from": "Fitted_Value_Iteration", "title": "related_to", "to": "Convergence_Issues"}, {"arrows": "to", "from": "EmpiricalRiskMinimization", "title": "subtopic", "to": "TrainingSetS"}, {"arrows": "to", "from": "SupportVectorsConcept", "title": "related_to", "to": "KernelTrickIntroduction"}, {"arrows": "to", "from": "Expected_Total_Payoff_Optimization", "title": "subtopic", "to": "Chapter_17_Policy_Gradient_REINFORCE"}, {"arrows": "to", "from": "Machine_Learning_Adaptation_Techniques", "title": "related_to", "to": "Model_Optimization"}, {"arrows": "to", "from": "Contrastive_Learning_Visual_Representations", "title": "belongs_to", "to": "Machine_Learning_Papers"}, {"arrows": "to", "from": "Physics Simulation", "title": "related_to", "to": "Off-the-Shelf Software"}, {"arrows": "to", "from": "IterativeUpdateRule", "title": "subtopic", "to": "LinearCombinationRepresentation"}, {"arrows": "to", "from": "Cost_Function", "title": "related_to", "to": "Ordinary_Least_Squares"}, {"arrows": "to", "from": "Characterization_of_Kernels", "title": "example", "to": "Example_Kernel_Functions"}, {"arrows": "to", "from": "Kernel Examples", "title": "subtopic", "to": "String Classification Example"}, {"arrows": "to", "from": "ModelParameters", "title": "related_to", "to": "LogLikelihoodCalculation"}, {"arrows": "to", "from": "Sample_Complexity", "title": "depends_on", "to": "Parameterization_Impact"}, {"arrows": "to", "from": "Learning a model for an MDP", "title": "contains", "to": "Connections between Policy and Value Iteration (Optional)"}, {"arrows": "to", "from": "LinearRegression", "title": "has_subtopic", "to": "CostFunction"}, {"arrows": "to", "from": "Value_Iteration", "title": "subtopic", "to": "MDP_Finite_State_Space"}, {"arrows": "to", "from": "Policy_Iteration", "title": "uses", "to": "Linear_System_Solver"}, {"arrows": "to", "from": "MachineLearningBasics", "title": "contains", "to": "GaussianDistribution"}, {"arrows": "to", "from": "Variational_Inference", "title": "component_of", "to": "Variational_Autoencoder"}, {"arrows": "to", "from": "HoldOutCrossValidation", "title": "subtopic", "to": "S_train"}, {"arrows": "to", "from": "Step2OptimalPolicy", "title": "subtopic", "to": "LQRAlgorithmSteps"}, {"arrows": "to", "from": "RuntimeAndMemoryEfficiency", "title": "subtopic", "to": "InitializationThetaZero"}, {"arrows": "to", "from": "Negative Log-Likelihood", "title": "related_to", "to": "Cross-Entropy Loss"}, {"arrows": "to", "from": "MachineLearningModels", "title": "contains", "to": "ConditionalProbabilityModeling"}, {"arrows": "to", "from": "Machine_Learning_Theory", "title": "related_to", "to": "Empirical_Risk_Minimization"}, {"arrows": "to", "from": "Support_Vector_Machines_SVMs", "title": "related_to", "to": "SMO_Algorithm"}, {"arrows": "to", "from": "LogisticLossFunction", "title": "depends_on", "to": "NegativeLogLikelihood"}, {"arrows": "to", "from": "Feature_Maps", "title": "subtopic", "to": "Linear_Model"}, {"arrows": "to", "from": "ComparisonValuePolicyIteration", "title": "subtopic", "to": "MachineLearningConcepts"}, {"arrows": "to", "from": "Linear_Model_Failure", "title": "depends_on", "to": "Bias_Definition"}, {"arrows": "to", "from": "Observation vs State", "title": "depends_on", "to": "Partially Observable MDPs (POMDP)"}, {"arrows": "to", "from": "EM_Algorithm", "title": "subtopic", "to": "E_Step"}, {"arrows": "to", "from": "Gradient Estimation", "title": "introduces", "to": "Baseline Function"}, {"arrows": "to", "from": "Log_Probability_Ratio", "title": "subtopic", "to": "Policy_Gradient_Methods"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "depends_on", "to": "Loss Function"}, {"arrows": "to", "from": "Total Parameters Count", "title": "subtopic", "to": "Multi-layer Neural Networks"}, {"arrows": "to", "from": "Dual_Problem_Solution", "title": "depends_on", "to": "Feasibility_Constraints"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "contains", "to": "Policy Gradient Methods"}, {"arrows": "to", "from": "GaussianDiscriminantAnalysisModel", "title": "subtopic", "to": "BernoulliDistribution"}, {"arrows": "to", "from": "MachineLearningOverview", "title": "subtopic", "to": "Conv2D-S"}, {"arrows": "to", "from": "Function_Representation", "title": "subtopic", "to": "Cost_Function"}, {"arrows": "to", "from": "Machine_Learning_Concepts", "title": "has_subtopic", "to": "ELBO_Gradient_Computation"}, {"arrows": "to", "from": "Loss Functions", "title": "subtopic", "to": "Logistic Loss"}, {"arrows": "to", "from": "1D Example", "title": "depends_on", "to": "Density Transformation"}, {"arrows": "to", "from": "Uniform_Convergence_Assumption", "title": "depends_on", "to": "Generalization_Error_Bound"}, {"arrows": "to", "from": "Machine_Learning_Concepts", "title": "depends_on", "to": "Value_Iteration"}, {"arrows": "to", "from": "VectorizationTechniques", "title": "subtopic", "to": "Broadcasting"}, {"arrows": "to", "from": "Theorem_Jensens_Inequality", "title": "subtopic", "to": "Jensens_Inequality"}, {"arrows": "to", "from": "Jensen\u0027s Inequality", "title": "subtopic", "to": "Convex Function"}, {"arrows": "to", "from": "Binary_Classification", "title": "depends_on", "to": "Training_Set"}, {"arrows": "to", "from": "ICAOnGaussianData", "title": "depends_on", "to": "RotationalSymmetry"}, {"arrows": "to", "from": "Support_Vector_Machines_SVM", "title": "contains", "to": "SMO_Algorithm"}, {"arrows": "to", "from": "MachineLearningOverview", "title": "contains", "to": "TwoLayerNN"}, {"arrows": "to", "from": "MachineLearningModels", "title": "depends_on", "to": "DecisionBoundaries"}, {"arrows": "to", "from": "Feature Discovery", "title": "subtopic", "to": "Black Box Nature"}, {"arrows": "to", "from": "BinaryClassificationProblem", "title": "related_to", "to": "LossFunctionFormulation"}, {"arrows": "to", "from": "Mixture of Gaussians Model", "title": "subtopic", "to": "Joint Distribution"}, {"arrows": "to", "from": "Coordinate_Ascend_Algorithm", "title": "subtopic", "to": "Unconstrained_Optimization_Problem"}, {"arrows": "to", "from": "Overfitting_Underfitting", "title": "depends_on", "to": "Test_Error_Training_Error"}, {"arrows": "to", "from": "LayerNormalization", "title": "related_to", "to": "TransformerArchitecture"}, {"arrows": "to", "from": "Loss Functions", "title": "subtopic", "to": "Cross-Entropy Loss"}, {"arrows": "to", "from": "Expected_Immediate_Rewards", "title": "depends_on", "to": "MDP_Model_Learning"}, {"arrows": "to", "from": "Cross_Entropy_Loss", "title": "is_a_specific_type_of", "to": "Softmax_Cross_Entropy_Loss"}, {"arrows": "to", "from": "Machine_Learning_Concepts", "title": "depends_on", "to": "ELBO_Optimization"}, {"arrows": "to", "from": "Pretraining_Phase", "title": "subtopic", "to": "Transfer_Learning"}, {"arrows": "to", "from": "BinaryClassificationProblem", "title": "depends_on", "to": "MLPModelDefinition"}, {"arrows": "to", "from": "OrdinaryLeastSquares", "title": "uses_distribution", "to": "ExponentialFamilyDistribution"}, {"arrows": "to", "from": "ParameterizedModel", "title": "uses", "to": "SoftmaxFunction"}, {"arrows": "to", "from": "Learning_Model_for_MDP", "title": "example_of", "to": "Inverted_Pendulum_Problem"}, {"arrows": "to", "from": "ConditionalProbability", "title": "depends_on", "to": "JointLikelihood"}, {"arrows": "to", "from": "Exponential_Family_Distributions", "title": "related_to", "to": "Logistic_Function"}, {"arrows": "to", "from": "Gradient Computation", "title": "subtopic", "to": "Theorem 7.4.1"}, {"arrows": "to", "from": "Dual Problem", "title": "depends_on", "to": "Objective Function Dual"}, {"arrows": "to", "from": "Bias-Variance Tradeoff", "title": "depends_on", "to": "Underfitting"}, {"arrows": "to", "from": "Union_Bound_Application", "title": "subtopic", "to": "Uniform_Convergence"}, {"arrows": "to", "from": "4.1 Gaussian discriminant analysis", "title": "has_subtopic", "to": "4.1.3 Discussion: GDA and logistic regression"}, {"arrows": "to", "from": "PrimalProblem", "title": "subtopic", "to": "ThetaP"}, {"arrows": "to", "from": "7.4 Backpropagation", "title": "has_subtopic", "to": "7.4.4 Back-propagation for MLPs"}, {"arrows": "to", "from": "Policy", "title": "depends_on", "to": "Value Function"}, {"arrows": "to", "from": "Value_Functions", "title": "related_to", "to": "Bellman_Equation"}, {"arrows": "to", "from": "Least-Squares Cost Function J", "title": "subtopic", "to": "Target Variables and Inputs Relation"}, {"arrows": "to", "from": "Neural Network", "title": "subtopic", "to": "Model Set M"}, {"arrows": "to", "from": "k_Fold_Cross_Validation", "title": "special_case_of", "to": "Leave_One_Out_Cross_Validation"}, {"arrows": "to", "from": "LQR, DDP and LQG", "title": "subtopic", "to": "Linear Quadratic Gaussian (LQG)"}, {"arrows": "to", "from": "Convolutional Layers", "title": "contains", "to": "1-D Convolution (Conv1D)"}, {"arrows": "to", "from": "MultinomialEventModel", "title": "has_subtopic", "to": "ParameterEstimation"}, {"arrows": "to", "from": "Non-Stationary Policies", "title": "subtopic", "to": "Optimal Policy in Finite Horizon"}, {"arrows": "to", "from": "Target Vector", "title": "subtopic", "to": "Least Squares Revisited"}, {"arrows": "to", "from": "Finetuning_Pretrained_Models", "title": "depends_on", "to": "Linear_Head_Initiation"}, {"arrows": "to", "from": "MStep", "title": "contains", "to": "MuParameterUpdate"}, {"arrows": "to", "from": "Backpropagation", "title": "related_to", "to": "Auto-differentiation"}, {"arrows": "to", "from": "Learning Theory Proofs", "title": "depends_on", "to": "Hoeffding Inequality"}, {"arrows": "to", "from": "SpeedPerspective", "title": "depends_on", "to": "VectorizationInNN"}, {"arrows": "to", "from": "Principal Component Analysis (PCA)", "title": "subtopic_of", "to": "Plotting Similarity"}, {"arrows": "to", "from": "Policy_Iteration", "title": "related_to", "to": "MDP_Model_Learning"}, {"arrows": "to", "from": "Machine Learning Models", "title": "contains", "to": "Support Vector Machines (SVM)"}, {"arrows": "to", "from": "CostFunctionJ", "title": "related_to", "to": "GradientDescentAlgorithm"}, {"arrows": "to", "from": "7 Deep learning", "title": "has_subtopic", "to": "7.4 Backpropagation"}, {"arrows": "to", "from": "double_descent", "title": "depends_on", "to": "statistical_mechanics_of_learning"}, {"arrows": "to", "from": "Principal Component Analysis (PCA)", "title": "subtopic_of", "to": "Dimension Reduction Before Supervised Learning"}, {"arrows": "to", "from": "Support Vector Machines (SVMs)", "title": "subtopic", "to": "Functional Margin"}, {"arrows": "to", "from": "ConstrainedOptimization", "title": "depends_on", "to": "LagrangeMultipliers"}, {"arrows": "to", "from": "Training_Error", "title": "depends_on", "to": "Generalization_Error_Bound"}, {"arrows": "to", "from": "ClassificationProblem", "title": "subtopic_of", "to": "LogisticRegression"}, {"arrows": "to", "from": "Bias_Variance_Tradeoff", "title": "belongs_to", "to": "Machine_Learning_Papers"}, {"arrows": "to", "from": "Bias-Variance Tradeoff", "title": "explains", "to": "Model Complexity"}, {"arrows": "to", "from": "DensityEstimation", "title": "depends_on", "to": "GaussianMixtureModel"}, {"arrows": "to", "from": "Machine Learning Overview", "title": "contains", "to": "Locally Weighted Linear Regression"}, {"arrows": "to", "from": "Optimization Methods", "title": "subtopic", "to": "Machine Learning Papers"}, {"arrows": "to", "from": "Machine_Learning_Bias_Variance_Tradeoff", "title": "subtopic", "to": "5th_Degree_Polynomial_Failure"}, {"arrows": "to", "from": "TemperatureParameter", "title": "subtopic", "to": "ConditionalProbability"}, {"arrows": "to", "from": "GaussianDiscriminantAnalysisModel", "title": "subtopic", "to": "MultivariateNormalForClasses"}, {"arrows": "to", "from": "ICA", "title": "component_of", "to": "Unmixing_Matrix_W"}, {"arrows": "to", "from": "MachineLearningOverview", "title": "related_to", "to": "GradientDescentAlgorithm"}, {"arrows": "to", "from": "Posterior Distribution", "title": "subtopic", "to": "Prediction on New Data"}, {"arrows": "to", "from": "Machine_Learning_Concepts", "title": "contains", "to": "Chain_Rule"}, {"arrows": "to", "from": "Pretrained_Models", "title": "contains", "to": "Natural_Language_Processing"}, {"arrows": "to", "from": "3 Generalized linear models", "title": "has_subtopic", "to": "3.1 The exponential family"}, {"arrows": "to", "from": "ContrastiveLearning", "title": "has_subtopic", "to": "PositivePairs"}, {"arrows": "to", "from": "Finite_Horizon_MDPs", "title": "subtopic", "to": "General_Setting_Equations"}, {"arrows": "to", "from": "EventModelsForTextClassification", "title": "contains", "to": "BernoulliEventModel"}, {"arrows": "to", "from": "Prediction on New Data", "title": "depends_on", "to": "Expected Value Prediction"}, {"arrows": "to", "from": "MachineLearningOverview", "title": "contains", "to": "NeuralNetworksInspiration"}, {"arrows": "to", "from": "Learning from Data", "title": "depends_on", "to": "Data Collection Process"}, {"arrows": "to", "from": "Functional_Margin", "title": "related_to", "to": "Scaling_Issue"}, {"arrows": "to", "from": "Optimizers", "title": "depends_on", "to": "Implicit Regularization"}, {"arrows": "to", "from": "MultivariateNormalDistribution", "title": "subtopic", "to": "MeanVectorMovement"}, {"arrows": "to", "from": "Machine_Learning_Concepts", "title": "contains", "to": "Gradient_Estimation"}, {"arrows": "to", "from": "GenerativeLearning", "title": "depends_on", "to": "BayesRule"}, {"arrows": "to", "from": "DistanceToBoundary", "title": "subtopic", "to": "DecisionBoundary"}, {"arrows": "to", "from": "KernelMethods", "title": "subtopic", "to": "FeatureMaps"}, {"arrows": "to", "from": "Newton\u0027s Method", "title": "related_to", "to": "Maximizing Functions"}, {"arrows": "to", "from": "Normalization Techniques", "title": "contains", "to": "Batch Normalization (BN)"}, {"arrows": "to", "from": "BiasVarianceTradeoff", "title": "subtopic", "to": "MSE"}, {"arrows": "to", "from": "MachineLearningModels", "title": "subtopic", "to": "NonLinearFeatureMapping"}, {"arrows": "to", "from": "Objective_Function_W", "title": "subtopic", "to": "Quadratic_Formulation"}, {"arrows": "to", "from": "Chapter_15_Summary", "title": "subtopic", "to": "Value_Iteration_Preference"}, {"arrows": "to", "from": "Action Selection", "title": "depends_on", "to": "Markov Decision Process (MDP)"}, {"arrows": "to", "from": "Approximation Error Minimization", "title": "subtopic", "to": "Principal Component Analysis (PCA)"}, {"arrows": "to", "from": "Sequential Decision Making", "title": "subtopic", "to": "Reinforcement Learning"}, {"arrows": "to", "from": "Neural Networks", "title": "subtopic", "to": "MLPs (Multilayer Perceptrons)"}, {"arrows": "to", "from": "HoldOutCrossValidation", "title": "subtopic", "to": "S_cv"}, {"arrows": "to", "from": "Machine_Learning_Concepts", "title": "related_to", "to": "Variational_Autoencoder"}, {"arrows": "to", "from": "Layer Normalization (LN)", "title": "described_by", "to": "Equation 7.43"}, {"arrows": "to", "from": "StateRepresentation", "title": "has_subtopic", "to": "GridCellRepresentation"}, {"arrows": "to", "from": "GradientAscentRule", "title": "contains", "to": "StochasticGradientAscent"}, {"arrows": "to", "from": "Dynamic_Programming", "title": "subtopic", "to": "Value_Iteration"}, {"arrows": "to", "from": "MachineLearning", "title": "has_subtopic", "to": "Multi-classClassification"}, {"arrows": "to", "from": "GaussianDistribution", "title": "subtopic", "to": "CovarianceMatrix"}, {"arrows": "to", "from": "PolynomialFitting", "title": "depends_on", "to": "Overfitting"}, {"arrows": "to", "from": "RewardsDependency", "title": "depends_on", "to": "ExpectationRewriting"}, {"arrows": "to", "from": "KernelTrickIntroduction", "title": "depends_on", "to": "LagrangianFormulation"}, {"arrows": "to", "from": "Linear_Functions", "title": "subtopic", "to": "Parameters_Weights"}, {"arrows": "to", "from": "MachineLearningConcepts", "title": "contains", "to": "ICAOnGaussianData"}, {"arrows": "to", "from": "ICA", "title": "subtopic", "to": "ICA_Ambiguities"}, {"arrows": "to", "from": "Normalization Techniques", "title": "contains", "to": "Group Normalization"}, {"arrows": "to", "from": "Vapnik\u0027s Theorem", "title": "implies", "to": "Uniform Convergence"}, {"arrows": "to", "from": "Mini-batch Stochastic Gradient Descent", "title": "variant_of", "to": "Stochastic Gradient Descent (SGD)"}, {"arrows": "to", "from": "Bell and Sejnowski\u0027s Method", "title": "subtopic", "to": "ICA Algorithm"}, {"arrows": "to", "from": "Chain_Rule", "title": "subtopic", "to": "Scalar_Variable_J"}, {"arrows": "to", "from": "Text_Classification", "title": "subtopic", "to": "Generative_Modeling"}, {"arrows": "to", "from": "Differential_Dynamic_Programming_(DDP)", "title": "related_to", "to": "Reward_Function_Approximation"}, {"arrows": "to", "from": "MachineLearningAlgorithms", "title": "depends_on", "to": "EmpiricalRiskMinimization"}, {"arrows": "to", "from": "Linear Regression", "title": "uses_example", "to": "Housing Example Dataset"}, {"arrows": "to", "from": "Unsupervised learning", "title": "has_subtopic", "to": "EM algorithms"}, {"arrows": "to", "from": "MSEDecomposition", "title": "subtopic_of", "to": "VarianceTerm"}, {"arrows": "to", "from": "Transformer_Models", "title": "has_subtopic", "to": "Conditional_Probability"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "related_to", "to": "Curse of Dimensionality"}, {"arrows": "to", "from": "Efficiency_of_Backward_Pass", "title": "depends_on", "to": "Vectorized_Notation_Backward_Func"}, {"arrows": "to", "from": "Machine_Learning_Concepts", "title": "subtopic", "to": "Training_Data_Set"}, {"arrows": "to", "from": "Law_of_Total_Expectation", "title": "depends_on", "to": "Log_Probability_Ratio"}, {"arrows": "to", "from": "JointLikelihood", "title": "related_to", "to": "MaximumLikelihoodEstimation"}, {"arrows": "to", "from": "TwoLayerNN", "title": "depends_on", "to": "ReLUFunction"}, {"arrows": "to", "from": "Weight Matrices and Biases", "title": "subtopic", "to": "Multi-layer Neural Networks"}, {"arrows": "to", "from": "Gaussian_Distribution_Qi", "title": "subtopic", "to": "Mean_and_Variance_Functions"}, {"arrows": "to", "from": "Optimizers", "title": "depends_on", "to": "Global Minima"}, {"arrows": "to", "from": "LayerActivations", "title": "related_to", "to": "VectorizationTechniques"}, {"arrows": "to", "from": "Machine_Learning_Concepts", "title": "related_to", "to": "Empirical_Distribution"}, {"arrows": "to", "from": "ProjectionOntoDirectionU", "title": "depends_on", "to": "VarianceMaximization"}, {"arrows": "to", "from": "Regularization in Machine Learning", "title": "contains", "to": "Sparsity Regularization"}, {"arrows": "to", "from": "LayerNormalization", "title": "depends_on", "to": "LN-S"}, {"arrows": "to", "from": "Optimal Parameters w and b", "title": "subtopic", "to": "Dual Optimization Problem"}, {"arrows": "to", "from": "Adaptation_Algorithm", "title": "has_subtopic", "to": "Finetuning_Method"}, {"arrows": "to", "from": "BiasVarianceTradeoff", "title": "depends_on", "to": "Claim8.1.1"}, {"arrows": "to", "from": "Optimizers", "title": "related_to", "to": "Pretraining_Phase"}, {"arrows": "to", "from": "Machine Learning Models", "title": "related_to", "to": "Regularization and Non-Separable Data"}, {"arrows": "to", "from": "NeuralNetworks", "title": "depends_on", "to": "InputFeatures"}, {"arrows": "to", "from": "MachineLearningBasics", "title": "has_subtopic", "to": "MatrixNotation"}, {"arrows": "to", "from": "LinearRegression", "title": "subtopic", "to": "NormalEquations"}, {"arrows": "to", "from": "MultivariateNormalDistribution", "title": "subtopic", "to": "CovarianceMatrixImpact"}, {"arrows": "to", "from": "Unsupervised learning", "title": "related_to", "to": "Self-supervised learning and foundation models"}, {"arrows": "to", "from": "Kalman Filter", "title": "related_to", "to": "Belief State"}, {"arrows": "to", "from": "Continuous_State_MDPs", "title": "subtopic", "to": "Discretization_Method"}, {"arrows": "to", "from": "PolicyGradients", "title": "has_subtopic", "to": "LogProbabilityDerivative"}, {"arrows": "to", "from": "Independent components analysis", "title": "has_subtopic", "to": "ICA algorithm"}, {"arrows": "to", "from": "Bellman Equations", "title": "subtopic", "to": "Value Function"}, {"arrows": "to", "from": "Total Payoff", "title": "subtopic", "to": "Markov Decision Process (MDP)"}, {"arrows": "to", "from": "Binary Classification", "title": "contains", "to": "Training Set"}, {"arrows": "to", "from": "Non-Stationary Policies", "title": "depends_on", "to": "Finite Horizon MDP Dynamics"}, {"arrows": "to", "from": "Markov Decision Processes (MDP)", "title": "component_of", "to": "State Transition Probabilities"}, {"arrows": "to", "from": "GradientEstimation", "title": "has_subtopic", "to": "PolicyGradients"}, {"arrows": "to", "from": "Bias-Variance Tradeoff", "title": "subtopic", "to": "Test Error Decomposition"}, {"arrows": "to", "from": "SMO_Algorithm", "title": "has_subtopic", "to": "KKT_Conditions"}, {"arrows": "to", "from": "Regularization and model selection", "title": "has_subtopic", "to": "Bayesian statistics and regularization"}, {"arrows": "to", "from": "RegularizerFunction", "title": "depends_on", "to": "RegularizationParameter"}, {"arrows": "to", "from": "MachineLearningModels", "title": "subtopic", "to": "ExponentialFamilyDistributions"}, {"arrows": "to", "from": "Batch_Gradient_Descent", "title": "related_to", "to": "Gradient_Descent"}, {"arrows": "to", "from": "Bayesian Machine Learning", "title": "subtopic", "to": "Fully Bayesian Prediction"}, {"arrows": "to", "from": "Learning Theory Fundamentals", "title": "subtopic", "to": "Machine Learning Papers"}, {"arrows": "to", "from": "Total Number of Neurons", "title": "subtopic", "to": "Multi-layer Neural Networks"}, {"arrows": "to", "from": "Sufficient Conditions for Kernels", "title": "depends_on", "to": "Mercer\u0027s Theorem"}, {"arrows": "to", "from": "MachineLearningOverview", "title": "depends_on", "to": "OptimizationChallenges"}, {"arrows": "to", "from": "LinearRegression", "title": "subtopic", "to": "Overfitting"}, {"arrows": "to", "from": "Lagrangian Function", "title": "depends_on", "to": "Dual Optimization Problem"}, {"arrows": "to", "from": "LogisticRegression", "title": "uses_distribution", "to": "ExponentialFamilyDistribution"}, {"arrows": "to", "from": "Bias-Variance Tradeoff", "title": "illustrates", "to": "Linear Regression Example"}, {"arrows": "to", "from": "Posterior Distribution p(y|x)", "title": "related_to", "to": "Bayesian Classification"}, {"arrows": "to", "from": "Backpropagation", "title": "depends_on", "to": "Gradient Computation"}, {"arrows": "to", "from": "MachineLearningOverview", "title": "has_subtopic", "to": "ModulesInPractice"}, {"arrows": "to", "from": "FeatureMapping", "title": "depends_on", "to": "HighDimensionalFeatures"}, {"arrows": "to", "from": "MachineLearningModels", "title": "related_to", "to": "Vectorization"}, {"arrows": "to", "from": "Machine_Learning", "title": "related_to", "to": "Hypothesis_Class"}, {"arrows": "to", "from": "Markov Decision Processes (MDP)", "title": "component_of", "to": "Reward Function"}, {"arrows": "to", "from": "Independent components analysis", "title": "has_subtopic", "to": "Densities and linear transformations"}, {"arrows": "to", "from": "ActivationFunctions", "title": "subtopic", "to": "LeakyReLUFunction"}, {"arrows": "to", "from": "Probabilistic Interpretation of Linear Regression", "title": "depends_on", "to": "Regression Problem"}, {"arrows": "to", "from": "5 Kernel methods", "title": "has_subtopic", "to": "5.4 Properties of kernels"}, {"arrows": "to", "from": "Regularization and Non-Separable Case", "title": "depends_on", "to": "Linear Separability Assumption"}, {"arrows": "to", "from": "OptimizationChallenges", "title": "subtopic", "to": "LikelihoodFunction"}, {"arrows": "to", "from": "MStep", "title": "subtopic", "to": "ExpectationMaximizationAlgorithm"}, {"arrows": "to", "from": "Downstream_Task_Dataset", "title": "subtopic", "to": "Zero_Shot_Learning"}, {"arrows": "to", "from": "NonStationaryOptimalPolicy", "title": "follows_from", "to": "FiniteHorizonMDP"}, {"arrows": "to", "from": "Kernel Examples", "title": "subtopic", "to": "Digit Recognition Problem"}, {"arrows": "to", "from": "KernelFunctions", "title": "subtopic", "to": "PolynomialKernels"}, {"arrows": "to", "from": "Generative_Modeling", "title": "has_subtopic", "to": "Naive_Bayes_Assumption"}, {"arrows": "to", "from": "PCA_Method", "title": "contains", "to": "Dataset_Analysis"}, {"arrows": "to", "from": "LinearCombinationRepresentation", "title": "subtopic", "to": "CoefficientUpdateRule"}, {"arrows": "to", "from": "Single_Neuron_Model", "title": "depends_on", "to": "Bias_and_Weights"}, {"arrows": "to", "from": "TotalParametersConv1D", "title": "related_to", "to": "ParameterTensorRepresentation"}, {"arrows": "to", "from": "Support Vector Machines", "title": "has_subtopic", "to": "Regularization and Non-Separable Case"}, {"arrows": "to", "from": "Supervised Learning Problem", "title": "has_subtopic", "to": "Classification"}, {"arrows": "to", "from": "LogisticRegressionAsGLM", "title": "depends_on", "to": "BernoulliDistribution"}, {"arrows": "to", "from": "Deep_Learning", "title": "related_to", "to": "Feature_Maps"}, {"arrows": "to", "from": "Multi_Class_Classification", "title": "subtopic", "to": "Machine_Learning_Concepts"}, {"arrows": "to", "from": "GaussianDiscriminantAnalysis(GDA)", "title": "has_subtopic", "to": "ParametersEstimation"}, {"arrows": "to", "from": "Value_Functions", "title": "subtopic", "to": "Policy_Evaluation"}, {"arrows": "to", "from": "Support_Vector_Machines", "title": "subtopic", "to": "SMO_Algorithm"}, {"arrows": "to", "from": "Predict Step", "title": "depends_on", "to": "Gaussian Distribution"}, {"arrows": "to", "from": "Orthogonal Basis", "title": "subtopic", "to": "Principal Component Analysis (PCA)"}, {"arrows": "to", "from": "Self-supervised Learning", "title": "subtopic", "to": "Foundation Models"}, {"arrows": "to", "from": "NaiveBayes", "title": "contains", "to": "DiscreteFeatures"}, {"arrows": "to", "from": "ParameterEstimation", "title": "subtopic", "to": "NaiveBayesFilter"}, {"arrows": "to", "from": "Bias_Variance_Tradoff", "title": "depends_on", "to": "Model_Parameterizations"}, {"arrows": "to", "from": "Error_Margin_Determination", "title": "subtopic", "to": "Quantities_of_Interest"}, {"arrows": "to", "from": "Regularization in Machine Learning", "title": "contains", "to": "L2 Norm Regularization"}, {"arrows": "to", "from": "Implicit Regularization Effect", "title": "contains", "to": "Optimizer Impact"}, {"arrows": "to", "from": "Overfitting", "title": "depends_on", "to": "Variance"}, {"arrows": "to", "from": "BayesianInference", "title": "subtopic", "to": "PosteriorApproximation"}, {"arrows": "to", "from": "FeatureExtraction", "title": "has_subtopic", "to": "StringFeatureExtraction"}, {"arrows": "to", "from": "House Price Prediction Example", "title": "related_to", "to": "Feature Discovery"}, {"arrows": "to", "from": "Confidence in Predictions", "title": "subtopic", "to": "Machine Learning Concepts"}, {"arrows": "to", "from": "MachineLearningOverview", "title": "contains", "to": "NaiveBayesClassifier"}, {"arrows": "to", "from": "AlternativeFormulationsOfELBO", "title": "subtopic", "to": "MarginalDistributionIndependence"}, {"arrows": "to", "from": "I Supervised learning", "title": "has_subtopic", "to": "5 Kernel methods"}, {"arrows": "to", "from": "Machine_Learning_Concepts", "title": "next_topic", "to": "Linear_Quadratic_Gaussian_(LQG)"}, {"arrows": "to", "from": "GradientCalculation", "title": "leads_to", "to": "StochasticGradientDescent"}, {"arrows": "to", "from": "Generalization", "title": "has_subtopic", "to": "The double descent phenomenon"}, {"arrows": "to", "from": "I Supervised learning", "title": "has_subtopic", "to": "1 Linear regression"}, {"arrows": "to", "from": "Backpropagation", "title": "has_subtopic", "to": "Preliminaries on partial derivatives"}, {"arrows": "to", "from": "EventModelsForTextClassification", "title": "contains", "to": "MultinomialEventModel"}, {"arrows": "to", "from": "Gaussian_Mixture_Models", "title": "has_analytical_solution_for", "to": "Posterior_Distribution"}, {"arrows": "to", "from": "LinearRegressionOptimization", "title": "depends_on", "to": "GradientDescentConvergence"}, {"arrows": "to", "from": "TwoLayerNetwork", "title": "subtopic", "to": "VectorizationInNN"}, {"arrows": "to", "from": "Decision Boundary", "title": "subtopic", "to": "Machine Learning Concepts"}, {"arrows": "to", "from": "MachineLearningConcepts", "title": "has_subtopic", "to": "KernelMethods"}, {"arrows": "to", "from": "LinearModelPrediction", "title": "related_to", "to": "StochasticModel"}, {"arrows": "to", "from": "Complexity_in_Computing_Gradients", "title": "has_subtopic", "to": "Reparameterization_Trick"}, {"arrows": "to", "from": "RegressionProblems", "title": "related_to", "to": "TestExample"}, {"arrows": "to", "from": "Chain_Rule", "title": "subtopic", "to": "Vectorized_Notation"}, {"arrows": "to", "from": "Chapter_16_LQR_DDP_LQG", "title": "subtopic", "to": "Finite_Horizon_MDPs"}, {"arrows": "to", "from": "1D_Convolution", "title": "subtopic", "to": "Simplified_1D_Convolution"}, {"arrows": "to", "from": "MachineLearningConcepts", "title": "contains", "to": "NonGaussianDataRecovery"}, {"arrows": "to", "from": "Scaling_Parameters", "title": "depends_on", "to": "Machine_Learning_Concepts"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "contains", "to": "Optimal Policy in LQR"}, {"arrows": "to", "from": "Parameter Estimation", "title": "related_to", "to": "Closed Form Solution"}, {"arrows": "to", "from": "3.2 Constructing GLMs", "title": "has_subtopic", "to": "3.2.1 Ordinary least squares"}, {"arrows": "to", "from": "Machine_Learning_Optimization", "title": "subtopic", "to": "Support_Vector_Machines_SVMs"}, {"arrows": "to", "from": "Machine_Learning_Algorithms", "title": "related_to", "to": "Support_Vector_Machines_SVM"}, {"arrows": "to", "from": "Continuous_Model_Assumptions", "title": "depends_on", "to": "Linear_Transitions"}, {"arrows": "to", "from": "Neural_Networks", "title": "subtopic", "to": "Single_Neuron_Model"}, {"arrows": "to", "from": "Geometric_Margin", "title": "subtopic", "to": "Machine_Learning_Concepts"}, {"arrows": "to", "from": "RegressionProblems", "title": "subtopic", "to": "LeastSquareCostFunction"}, {"arrows": "to", "from": "Linearization_of_Dynamics", "title": "subtopic", "to": "Taylor_Expansion_Method"}, {"arrows": "to", "from": "General Case", "title": "subtopic", "to": "Density Transformation"}, {"arrows": "to", "from": "Sparsity Inducing Regularization", "title": "subtopic", "to": "Regularizer R(\u03b8)"}, {"arrows": "to", "from": "Joint Distribution", "title": "related_to", "to": "Latent Variables"}, {"arrows": "to", "from": "ResNetOverview", "title": "subtopic", "to": "ResidualBlockDefinition"}, {"arrows": "to", "from": "2 Classification and logistic regression", "title": "has_subtopic", "to": "2.1 Logistic regression"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "related_to", "to": "Vectorization Over Training Examples"}, {"arrows": "to", "from": "5th_Degree_Polynomial_Failure", "title": "related_to", "to": "Generalization_Error"}, {"arrows": "to", "from": "Kernels_in_Machine_Learning", "title": "depends_on", "to": "Feature_Map_Phi"}, {"arrows": "to", "from": "Conditional_Probability", "title": "depends_on", "to": "Softmax_Function"}, {"arrows": "to", "from": "Loss Function", "title": "subtopic", "to": "Average Loss Function"}, {"arrows": "to", "from": "LQR Algorithm", "title": "subtopic", "to": "Kalman Filter"}, {"arrows": "to", "from": "TwoLayerNN", "title": "depends_on", "to": "LeakyReLUFunction"}, {"arrows": "to", "from": "Implicit Regularization Effect", "title": "contains", "to": "Global Minima Diversity"}, {"arrows": "to", "from": "EMAlgorithmIntroduction", "title": "subtopic", "to": "EStepMStepProcess"}, {"arrows": "to", "from": "MaximumLikelihoodEstimation", "title": "depends_on", "to": "CumulativeDistributionFunction"}, {"arrows": "to", "from": "Supervised Learning", "title": "subtopic", "to": "Linear Regression"}, {"arrows": "to", "from": "Separating Hyperplane", "title": "subtopic", "to": "Decision Boundary"}, {"arrows": "to", "from": "From non-linear dynamics to LQR", "title": "subtopic", "to": "Linearization of dynamics"}, {"arrows": "to", "from": "MLPModelDefinition", "title": "contains", "to": "ModulesInMLP"}, {"arrows": "to", "from": "Newton\u0027s Method", "title": "depends_on", "to": "Finding Roots"}, {"arrows": "to", "from": "MaximumLikelihoodEstimation", "title": "related_to", "to": "LaplaceSmoothing"}, {"arrows": "to", "from": "Continuous state MDPs", "title": "subtopic", "to": "Value function approximation"}, {"arrows": "to", "from": "UnitVectorW", "title": "related_to", "to": "VectorW"}, {"arrows": "to", "from": "4.2 Naive bayes (Option Reading)", "title": "has_subtopic", "to": "4.2.2 Event models for text classification"}, {"arrows": "to", "from": "LogisticRegression", "title": "subtopic", "to": "TotalLossFunction"}, {"arrows": "to", "from": "Value Function", "title": "depends_on", "to": "Policy Execution"}, {"arrows": "to", "from": "Belief State", "title": "subtopic", "to": "Partially Observable MDPs (POMDP)"}, {"arrows": "to", "from": "LayerNormalization", "title": "has", "to": "LearnableParameters"}, {"arrows": "to", "from": "k_Fold_Cross_Validation", "title": "includes", "to": "Training_and_Testing_Process"}, {"arrows": "to", "from": "Unlabeled_Dataset", "title": "related_to", "to": "Pretraining_Phase"}, {"arrows": "to", "from": "4 Generative learning algorithms", "title": "has_subtopic", "to": "4.1 Gaussian discriminant analysis"}, {"arrows": "to", "from": "PhiParameterUpdate", "title": "subtopic", "to": "MStepUpdateRule"}, {"arrows": "to", "from": "Double_Descent_Weak_Features", "title": "belongs_to", "to": "Machine_Learning_Papers"}, {"arrows": "to", "from": "MaximumLikelihoodEstimation", "title": "related_to", "to": "SigmoidFunction"}, {"arrows": "to", "from": "LikelihoodFunction", "title": "subtopic", "to": "LogLikelihood"}, {"arrows": "to", "from": "Binary_Classification", "title": "related_to", "to": "Hypothesis"}, {"arrows": "to", "from": "Backward_Function_Linear_Map", "title": "subtopic", "to": "Equation_7.53_Usefulness"}, {"arrows": "to", "from": "ExpectationMaximizationAlgorithm", "title": "depends_on", "to": "MachineLearningOverview"}, {"arrows": "to", "from": "Optimization Problem in SVM", "title": "subtopic", "to": "Scaling Constraint"}, {"arrows": "to", "from": "LQR, DDP and LQG", "title": "subtopic", "to": "From non-linear dynamics to LQR"}, {"arrows": "to", "from": "Bayesian Statistics", "title": "contrasts_with", "to": "MLE"}, {"arrows": "to", "from": "Value_Iteration", "title": "depends_on", "to": "Expectation_Computation"}, {"arrows": "to", "from": "LQRModelAssumptions", "title": "depends_on", "to": "MachineLearningOverview"}, {"arrows": "to", "from": "Empirical_Risk_Minimization", "title": "depends_on", "to": "Training_Error"}, {"arrows": "to", "from": "Hypothesis_Class_Switching", "title": "subtopic", "to": "Variance_Increase"}, {"arrows": "to", "from": "FullyConnectedNN", "title": "uses", "to": "ReLUActivation"}, {"arrows": "to", "from": "PrincipalComponentAnalysis", "title": "related_to", "to": "kDimensionalSubspace"}, {"arrows": "to", "from": "MachineLearning", "title": "contains", "to": "ExpectationMaximizationAlgorithm"}, {"arrows": "to", "from": "Machine_Learning_Optimization", "title": "contains", "to": "Support_Vector_Machines_SVM"}, {"arrows": "to", "from": "Double Descent Phenomenon", "title": "depends_on", "to": "Historical Context"}, {"arrows": "to", "from": "GLMAssumptions", "title": "subtopic", "to": "GeneralizedLinearModels"}, {"arrows": "to", "from": "DiscriminativeLearning", "title": "subtopic", "to": "ConditionalDistribution"}, {"arrows": "to", "from": "Kernel_Functions", "title": "subtopic", "to": "Sufficient_Conditions"}, {"arrows": "to", "from": "Logistic Regression", "title": "subtopic", "to": "Model Set M"}, {"arrows": "to", "from": "PrincipalComponentAnalysis", "title": "related_to", "to": "EmpiricalCovarianceMatrix"}, {"arrows": "to", "from": "MachineLearningModels", "title": "related_to", "to": "BernoulliEventModel"}, {"arrows": "to", "from": "FeatureMapping", "title": "depends_on", "to": "MachineLearningConcepts"}, {"arrows": "to", "from": "Alpha Updates", "title": "subtopic", "to": "Sequential Minimal Optimization (SMO) Algorithm"}, {"arrows": "to", "from": "Bayesian Statistics", "title": "includes", "to": "Prior Distribution"}, {"arrows": "to", "from": "VarianceMaximization", "title": "subtopic", "to": "PrincipalEigenvector"}, {"arrows": "to", "from": "ActivationFunctions", "title": "has_subtopic", "to": "BackwardFunctionActivations"}, {"arrows": "to", "from": "Self-supervised learning and foundation models", "title": "has_subtopic", "to": "Pretraining methods in computer vision"}, {"arrows": "to", "from": "PolynomialFitting", "title": "related_to", "to": "Variance"}, {"arrows": "to", "from": "Mean Field Assumption", "title": "subtopic", "to": "Variational Inference"}, {"arrows": "to", "from": "Machine_Learning_Concepts", "title": "related_to", "to": "Inverted_Pendulum_Model"}, {"arrows": "to", "from": "Model_Parameter_Theta", "title": "related_to", "to": "Pretraining_Phase"}, {"arrows": "to", "from": "Feature Mapping", "title": "subtopic", "to": "Feature Map Definition"}, {"arrows": "to", "from": "Machine Learning", "title": "related_to", "to": "Deep Learning Packages"}, {"arrows": "to", "from": "Loss J(\u03b8)", "title": "depends_on", "to": "Regularized Loss Function"}, {"arrows": "to", "from": "Machine_Learning_Adaptation_Methods", "title": "has_subtopic", "to": "Pretraining_Methods_Computer_Vision"}, {"arrows": "to", "from": "MachineLearningConcepts", "title": "related_to", "to": "SupportVectorMachines"}, {"arrows": "to", "from": "Learning Theory Proofs", "title": "depends_on", "to": "Union Bound Lemma"}, {"arrows": "to", "from": "Machine Learning Algorithms", "title": "related_to", "to": "Policy Iteration"}, {"arrows": "to", "from": "Machine_Learning_Concepts", "title": "has_subtopic", "to": "Convolutional_Layers"}, {"arrows": "to", "from": "NeuralNetworks", "title": "depends_on", "to": "BiasVectors"}, {"arrows": "to", "from": "MachineLearningModels", "title": "subtopic", "to": "RegressionProblems"}, {"arrows": "to", "from": "Jensen\u0027s Inequality", "title": "subtopic", "to": "Concave Function"}, {"arrows": "to", "from": "BernoulliDistribution", "title": "has_subtopic", "to": "LogPartitionFunctionForBernoulli"}, {"arrows": "to", "from": "LQRAlgorithmSteps", "title": "subtopic", "to": "MachineLearningOverview"}, {"arrows": "to", "from": "LikelihoodFunction", "title": "subtopic", "to": "SingleExampleOptimization"}, {"arrows": "to", "from": "Machine_Learning_Bias_Variance", "title": "contains", "to": "Bias_Variance_Classification"}, {"arrows": "to", "from": "MachineLearningConcepts", "title": "has_subtopic", "to": "LanguageModels"}, {"arrows": "to", "from": "LeastSquaresRegression", "title": "related_to", "to": "MaximumLikelihoodEstimation"}, {"arrows": "to", "from": "Fully Bayesian Prediction", "title": "related_to", "to": "Computational Challenges"}, {"arrows": "to", "from": "MachineLearning", "title": "has_subtopic", "to": "ContrastiveLearning"}, {"arrows": "to", "from": "MachineLearningModels", "title": "subtopic", "to": "FullyConnectedNN"}, {"arrows": "to", "from": "Implicit Bias in Machine Learning", "title": "related_to", "to": "High-Dimensional Interpolation"}, {"arrows": "to", "from": "Alpha_Parameters", "title": "depends_on", "to": "Constraint_Equation"}, {"arrows": "to", "from": "Quantities_of_Interest", "title": "related_to", "to": "Uniform_Convergence"}, {"arrows": "to", "from": "Policy Gradient Methods", "title": "subtopic", "to": "Gradient Ascent"}, {"arrows": "to", "from": "Linearization_of_Dynamics", "title": "related_to", "to": "LQR_Assumptions"}, {"arrows": "to", "from": "Bayes\u0027 Theorem", "title": "related_to", "to": "Likelihood Function"}, {"arrows": "to", "from": "1 Linear regression", "title": "has_subtopic", "to": "1.1 LMS algorithm"}, {"arrows": "to", "from": "Convolutional_Layers", "title": "subtopic_of", "to": "Conv1D_Channel_Variants"}, {"arrows": "to", "from": "VectorW", "title": "depends_on", "to": "DecisionBoundary"}, {"arrows": "to", "from": "Machine Learning Algorithms", "title": "subtopic", "to": "Algorithm 6"}, {"arrows": "to", "from": "Optimal_Policy", "title": "related_to", "to": "Quadratic_Form"}, {"arrows": "to", "from": "Linearization_of_Dynamics", "title": "subtopic", "to": "Rewriting_Dynamics"}, {"arrows": "to", "from": "Bias-Variance Tradeoff", "title": "depends_on", "to": "Overfitting"}, {"arrows": "to", "from": "Chapter9", "title": "contains", "to": "Regularization"}, {"arrows": "to", "from": "ParameterizedFunction", "title": "contains", "to": "Embeddings"}, {"arrows": "to", "from": "Representation Function", "title": "related_to", "to": "Positive Pair"}, {"arrows": "to", "from": "Self-supervised learning and foundation models", "title": "has_subtopic", "to": "Pretraining and adaptation"}, {"arrows": "to", "from": "Layer Normalization (LN)", "title": "described_by", "to": "Equation 7.44-7.47"}, {"arrows": "to", "from": "ActivationFunctions", "title": "subtopic", "to": "IdentityFunction"}, {"arrows": "to", "from": "LagrangianConstruction", "title": "depends_on", "to": "PhiParameterUpdate"}, {"arrows": "to", "from": "Perceptron_Algorithm", "title": "subtopic", "to": "Machine_Learning_Concepts"}, {"arrows": "to", "from": "ActivationFunctions", "title": "subtopic", "to": "TanhFunction"}, {"arrows": "to", "from": "Stacking_Neurons", "title": "related_to", "to": "Complex_Neural_Networks"}, {"arrows": "to", "from": "LayerNormalization", "title": "subtopic", "to": "LN-SModule"}, {"arrows": "to", "from": "5 Kernel methods", "title": "has_subtopic", "to": "5.2 LMS (least mean squares) with features"}, {"arrows": "to", "from": "Stochastic_Gradient_Descent", "title": "has_subtopic", "to": "Learning_Rate_Decay"}, {"arrows": "to", "from": "MachineLearningModels", "title": "has_subtopic", "to": "GaussianDiscriminantAnalysis(GDA)"}, {"arrows": "to", "from": "Optimization Problem in SVM", "title": "depends_on", "to": "Non-Convex Constraint"}, {"arrows": "to", "from": "ProbabilisticModeling", "title": "depends_on", "to": "ConditionalProbabilityDistribution"}, {"arrows": "to", "from": "LocallyWeightedLinearRegression", "title": "has_subcomponent", "to": "WeightsInLWLR"}, {"arrows": "to", "from": "Machine Learning Concepts", "title": "related_to", "to": "Underfitting"}, {"arrows": "to", "from": "Pretrained large language models", "title": "has_subtopic", "to": "Zero-shot learning and in-context learning"}, {"arrows": "to", "from": "PolicyIterationAlgorithm", "title": "related_to", "to": "BellmanEquations"}, {"arrows": "to", "from": "MachineLearningAlgorithms", "title": "contains", "to": "GenerativeLearning"}, {"arrows": "to", "from": "LinearRegressionOptimization", "title": "subtopic", "to": "BatchGradientDescentExample"}, {"arrows": "to", "from": "Machine_Learning_Bias_Variance", "title": "contains", "to": "Variance_Term"}, {"arrows": "to", "from": "Simplified_1D_Convolution", "title": "depends_on", "to": "Filter_Vector"}, {"arrows": "to", "from": "Synchronous_Update", "title": "subtopic", "to": "Value_Iteration"}, {"arrows": "to", "from": "LinearModelPrediction", "title": "depends_on", "to": "DeterministicModel"}, {"arrows": "to", "from": "Machine_Learning_Backward_Functions", "title": "subtopic", "to": "Logistic_Loss_Backward"}, {"arrows": "to", "from": "Linear Function Over Features", "title": "depends_on", "to": "Cubic Function Representation"}, {"arrows": "to", "from": "Functional_Margin", "title": "subtopic", "to": "Function_Margin_of_S"}, {"arrows": "to", "from": "Machine_Learning_Algorithms", "title": "contains", "to": "Support_Vector_Machines"}, {"arrows": "to", "from": "MultiClassClassification", "title": "related_to", "to": "Logits"}, {"arrows": "to", "from": "Value_Iteration", "title": "depends_on", "to": "Approximation_Methods"}, {"arrows": "to", "from": "Policy Gradient Methods", "title": "relates_to", "to": "Value Function"}, {"arrows": "to", "from": "Matricization Approach", "title": "subtopic", "to": "Conversion Between Representations"}, {"arrows": "to", "from": "Naive Bayes Algorithm", "title": "related_to", "to": "Multinomial Distribution"}, {"arrows": "to", "from": "LayerArchitecture", "title": "subtopic", "to": "TwoLayerNN"}, {"arrows": "to", "from": "Matrix Multiplication Module", "title": "depends_on", "to": "MLP Composition"}, {"arrows": "to", "from": "Coordinate_Ascend_Algorithm", "title": "related_to", "to": "Gradient_Ascend_Newtons_Method"}, {"arrows": "to", "from": "Machine_Learning_Adaptation", "title": "has_subtopic", "to": "Adaptation_Algorithm"}, {"arrows": "to", "from": "LogProbabilityDerivative", "title": "related_to", "to": "AutoDifferentiation"}, {"arrows": "to", "from": "MachineLearningConcepts", "title": "contains", "to": "LocallyWeightedLinearRegression"}, {"arrows": "to", "from": "NeuralNetworks", "title": "subtopic", "to": "RegressionProblem"}, {"arrows": "to", "from": "Model Set M", "title": "subtopic", "to": "Model Selection"}, {"arrows": "to", "from": "ICA", "title": "example_of", "to": "Cocktail_Party_Problem"}, {"arrows": "to", "from": "Multi-classClassification", "title": "related_to", "to": "MultinomialDistribution"}, {"arrows": "to", "from": "Machine_Learning_Concepts", "title": "subtopic", "to": "Overfitting_Underfitting"}, {"arrows": "to", "from": "Regularization in Machine Learning", "title": "contains", "to": "Deep Learning Regularization Techniques"}, {"arrows": "to", "from": "MaximumLikelihoodEstimation", "title": "related_to", "to": "LogLikelihoodFunction"}, {"arrows": "to", "from": "Probability_Estimation", "title": "related_to", "to": "Laplace_Smoothing"}, {"arrows": "to", "from": "L2 Regularization", "title": "subtopic", "to": "Regularizer R(\u03b8)"}, {"arrows": "to", "from": "Lagrange_Duality", "title": "has_subtopic", "to": "Kernels_High_Dimensional_Spaces"}, {"arrows": "to", "from": "VarianceMaximization", "title": "subtopic", "to": "LagrangeMultipliersMethod"}, {"arrows": "to", "from": "In-Context_Learning", "title": "subtopic", "to": "Prompting_Strategy"}, {"arrows": "to", "from": "Weight Decay", "title": "related_to", "to": "L2 Regularization"}, {"arrows": "to", "from": "Reinforcement learning", "title": "has_subtopic", "to": "Learning a model for an MDP"}, {"arrows": "to", "from": "Clustering", "title": "subtopic", "to": "KMeansAlgorithm"}, {"arrows": "to", "from": "Machine_Learning_Concepts", "title": "contains", "to": "Loss_Functions"}, {"arrows": "to", "from": "Optimization_Frameworks", "title": "related_to", "to": "Trajectory_Generation"}, {"arrows": "to", "from": "FeatureMapPhi", "title": "subtopic", "to": "InnerProductEfficiency"}, {"arrows": "to", "from": "EM_Algorithm_Generalization", "title": "subtopic", "to": "Machine_Learning_Concepts"}, {"arrows": "to", "from": "Optimal Parameters Calculation", "title": "subtopic", "to": "Dual Form Optimization"}, {"arrows": "to", "from": "DataNormalization", "title": "depends_on", "to": "PCAAlgorithm"}, {"arrows": "to", "from": "WalkableNeighborhood", "title": "subtopic", "to": "DerivedFeatures"}, {"arrows": "to", "from": "MachineLearning", "title": "has_subtopic", "to": "ClassificationProblem"}, {"arrows": "to", "from": "DiscountFactorImpact", "title": "related_to", "to": "FiniteHorizonMDP"}, {"arrows": "to", "from": "BackwardFunctionsBasics", "title": "has_subtopic", "to": "MatrixMultiplicationModule"}, {"arrows": "to", "from": "Machine_Learning_Concepts", "title": "contains", "to": "Convolutional_Neural_Networks"}, {"arrows": "to", "from": "Polynomial Regression Model", "title": "related_to", "to": "Model Selection"}, {"arrows": "to", "from": "Function_Representation", "title": "subtopic", "to": "Linear_Functions"}, {"arrows": "to", "from": "Regularization and model selection", "title": "has_subtopic", "to": "Regularization"}, {"arrows": "to", "from": "Feature_Vector", "title": "related_to", "to": "Stop_Words"}, {"arrows": "to", "from": "MachineLearningOverview", "title": "depends_on", "to": "LinearRegression"}, {"arrows": "to", "from": "GeneralizedLinearModels", "title": "subtopic", "to": "MachineLearningOverview"}, {"arrows": "to", "from": "Conditional_Distribution_Modeling", "title": "has_subtopic", "to": "Exponential_Family_Distributions"}, {"arrows": "to", "from": "VariationalInference", "title": "related_to", "to": "MachineLearningOverview"}, {"arrows": "to", "from": "Dual_Problem_Formulation", "title": "depends_on", "to": "Lagrange_Multipliers"}, {"arrows": "to", "from": "k-means_algorithm", "title": "depends_on", "to": "centroid_initialization"}, {"arrows": "to", "from": "LogisticRegression", "title": "subtopic", "to": "ModelAssumptions"}, {"arrows": "to", "from": "MachineLearningModels", "title": "depends_on", "to": "ClassificationModelAssumptions"}, {"arrows": "to", "from": "Policy_Gradient_Theorem", "title": "has_subtopic", "to": "Log_Probability_Ratio"}, {"arrows": "to", "from": "Chapter_15_Summary", "title": "subtopic", "to": "Policy_Iteration_Speedup"}, {"arrows": "to", "from": "ExponentialFamilyDistributions", "title": "has_subtopic", "to": "LogPartitionFunction"}, {"arrows": "to", "from": "Sample Complexity Bounds", "title": "subtopic", "to": "Model Selection Methods"}, {"arrows": "to", "from": "Binary_Classification", "title": "related_to", "to": "PAC_Assumptions"}, {"arrows": "to", "from": "Gradient_Estimation", "title": "has_subtopic", "to": "Empirical_Sample_Trajectories"}, {"arrows": "to", "from": "Jacobian_Matrix_Transpose", "title": "related_to", "to": "Complexity_of_Jacobian_Matrices"}, {"arrows": "to", "from": "Optimizers", "title": "related_to", "to": "Stochastic Gradient Descent (SGD)"}, {"arrows": "to", "from": "Continuous_Model_Assumptions", "title": "subtopic", "to": "Quadratic_Rewards"}, {"arrows": "to", "from": "Support_Vector_Machines", "title": "subtopic", "to": "Kernels_in_SVMs"}, {"arrows": "to", "from": "Loss_Functions", "title": "has_subtopic", "to": "Cross_Entropy_Loss"}, {"arrows": "to", "from": "LinearRegression", "title": "subtopic", "to": "Underfitting"}, {"arrows": "to", "from": "TimeHorizonTuple", "title": "subtopic", "to": "FiniteHorizonMDP"}, {"arrows": "to", "from": "NeuralNetworks", "title": "subtopic", "to": "ClassificationProblem"}, {"arrows": "to", "from": "Machine_Learning_Theorem", "title": "subtopic", "to": "Generalization_Error_Bound"}, {"arrows": "to", "from": "MarginalDistributionIndependence", "title": "subtopic", "to": "ConditionalLikelihoodSimplification"}, {"arrows": "to", "from": "II Deep learning", "title": "has_subtopic", "to": "7 Deep learning"}, {"arrows": "to", "from": "Regularization", "title": "contains", "to": "RegularizerFunction"}, {"arrows": "to", "from": "LQR, DDP and LQG", "title": "subtopic", "to": "Linear Quadratic Regulation (LQR)"}, {"arrows": "to", "from": "Neural_Networks", "title": "depends_on", "to": "Activation_Functions"}, {"arrows": "to", "from": "Transformer_Models", "title": "has_subtopic", "to": "Autoregressive_Decoding"}, {"arrows": "to", "from": "Optimal Policy in LQR", "title": "related_to", "to": "Independence of Noise"}, {"arrows": "to", "from": "LossFunctionFormulation", "title": "leads_to", "to": "IntermediateVariables"}, {"arrows": "to", "from": "Corollary of Vapnik\u0027s Theorem", "title": "derives_from", "to": "Vapnik\u0027s Theorem"}, {"arrows": "to", "from": "ContrastiveLearning", "title": "has_subtopic", "to": "NegativePairs"}, {"arrows": "to", "from": "Supervised_Learning", "title": "depends_on", "to": "Linear_Regression"}, {"arrows": "to", "from": "MachineLearningOverview", "title": "contains", "to": "LogisticRegression"}, {"arrows": "to", "from": "EmpiricalRiskMinimization", "title": "related_to", "to": "DegreeOfPolynomial"}, {"arrows": "to", "from": "EM Algorithms", "title": "subtopic", "to": "Mixture of Gaussians"}, {"arrows": "to", "from": "Machine_Learning_Concepts", "title": "related_to", "to": "Continuous_State_MDPs"}, {"arrows": "to", "from": "Optimizers", "title": "related_to", "to": "Learning Rate Schedules"}, {"arrows": "to", "from": "MachineLearningBasics", "title": "has_subtopic", "to": "LayerGeneralization"}, {"arrows": "to", "from": "Loss_Functions", "title": "has_subtopic", "to": "Gradient_Computation"}, {"arrows": "to", "from": "ResNetArchitecture", "title": "subtopic", "to": "ConvolutionalLayers"}, {"arrows": "to", "from": "SMO_Algorithm", "title": "has_subtopic", "to": "Efficient_Update"}, {"arrows": "to", "from": "2 Classification and logistic regression", "title": "has_subtopic", "to": "2.3 Multi-class classification"}, {"arrows": "to", "from": "MachineLearningModels", "title": "contains", "to": "GDA"}, {"arrows": "to", "from": "Non-linear Feature Mappings", "title": "depends_on", "to": "MDP Simulators"}, {"arrows": "to", "from": "MachineLearningModels", "title": "subtopic", "to": "LinearModelPrediction"}, {"arrows": "to", "from": "ClassificationModelAssumptions", "title": "subtopic", "to": "LikelihoodFunction"}, {"arrows": "to", "from": "MachineLearningModels", "title": "subtopic", "to": "GLMConstruction"}, {"arrows": "to", "from": "Support_Vector_Machines", "title": "subtopic", "to": "Margins_Intuition"}, {"arrows": "to", "from": "Other Activation Functions", "title": "related_to", "to": "Multi-layer Neural Networks"}, {"arrows": "to", "from": "Policy Gradient Methods", "title": "subtopic", "to": "REINFORCE Algorithm"}, {"arrows": "to", "from": "Error Term Distribution", "title": "subtopic", "to": "Conditional Probability of y given x"}, {"arrows": "to", "from": "PosteriorApproximation", "title": "subtopic", "to": "PriorDistributions"}, {"arrows": "to", "from": "ConvergenceOfAlgorithms", "title": "depends_on", "to": "PolicyIterationAlgorithm"}, {"arrows": "to", "from": "Matrix Derivatives", "title": "depends_on", "to": "Least Squares Revisited"}, {"arrows": "to", "from": "1 Linear regression", "title": "has_subtopic", "to": "1.2 The normal equations"}, {"arrows": "to", "from": "Primal Problem", "title": "related_to", "to": "Theta P"}, {"arrows": "to", "from": "Sample Complexity Bounds", "title": "subtopic", "to": "Bias-Variance Tradeoff"}, {"arrows": "to", "from": "Spam_Filtering", "title": "depends_on", "to": "Training_Set"}, {"arrows": "to", "from": "Machine_Learning_Concepts", "title": "subtopic", "to": "Bias_Variance_Tradoff"}, {"arrows": "to", "from": "Conv1D-S", "title": "depends_on", "to": "TotalParametersConv1D"}, {"arrows": "to", "from": "Naive_Bayes_Classifier", "title": "subtopic", "to": "Event_Models_Text_Classification"}, {"arrows": "to", "from": "Machine_Learning_Concepts", "title": "depends_on", "to": "Probabilistic_Model"}, {"arrows": "to", "from": "Covariance Matrix", "title": "related_to", "to": "Multivariate Normal Distribution"}, {"arrows": "to", "from": "Backpropagation", "title": "subtopic", "to": "Loss Function Composition"}, {"arrows": "to", "from": "Supervised Learning with Non-Linear Models", "title": "subtopic", "to": "Deep Learning Introduction"}, {"arrows": "to", "from": "ICAConcepts", "title": "subtopic", "to": "JointDistributionModeling"}, {"arrows": "to", "from": "Immediate Reward", "title": "related_to", "to": "Bellman Equations"}, {"arrows": "to", "from": "Learning_Settings", "title": "depends_on", "to": "Test_Distribution"}, {"arrows": "to", "from": "Log_Probability_Ratio", "title": "related_to", "to": "Trajectory_Probability_Change"}, {"arrows": "to", "from": "MultinomialDistribution", "title": "has_subtopic", "to": "ParameterizedModel"}, {"arrows": "to", "from": "Conv2D-S", "title": "depends_on", "to": "TotalParametersConv2D"}, {"arrows": "to", "from": "MachineLearningModels", "title": "subtopic", "to": "StochasticModel"}, {"arrows": "to", "from": "ValueFunctionApproximation", "title": "subtopic", "to": "FittedValueIteration"}, {"arrows": "to", "from": "Newton\u0027s Method", "title": "subtopic", "to": "Logistic Regression"}, {"arrows": "to", "from": "Optimization Problem in SVM", "title": "related_to", "to": "Geometric Margin"}, {"arrows": "to", "from": "Normalization Techniques", "title": "has_subtopic", "to": "Variance Scaling"}, {"arrows": "to", "from": "LMS with Features", "title": "subtopic", "to": "Gradient Descent Update"}, {"arrows": "to", "from": "LearningRate", "title": "depends_on", "to": "GradientDescentAlgorithm"}, {"arrows": "to", "from": "ValueFunctionApproximation", "title": "related_to", "to": "FeatureMapping"}, {"arrows": "to", "from": "Empirical_Risk_Minimization", "title": "subtopic", "to": "Finite_Hypothesis_Class"}, {"arrows": "to", "from": "Backpropagation Algorithm", "title": "subtopic", "to": "Gradient Calculation"}, {"arrows": "to", "from": "StochasticGradientDescent", "title": "subtopic", "to": "SGDAlgorithm"}, {"arrows": "to", "from": "MSEDecomposition", "title": "subtopic_of", "to": "BiasTerm"}, {"arrows": "to", "from": "FullyConnectedNN", "title": "depends_on", "to": "Parameterization"}, {"arrows": "to", "from": "JensensInequality", "title": "depends_on", "to": "EvidenceLowerBoundELBO"}, {"arrows": "to", "from": "Optimal Policy", "title": "related_to", "to": "Optimal Value Function"}]);

                  nodeColors = {};
                  allNodes = nodes.get({ returnType: "Object" });
                  for (nodeId in allNodes) {
                    nodeColors[nodeId] = allNodes[nodeId].color;
                  }
                  allEdges = edges.get({ returnType: "Object" });
                  // adding nodes and edges to the graph
                  data = {nodes: nodes, edges: edges};

                  var options = {
    "configure": {
        "enabled": false
    },
    "edges": {
        "color": {
            "inherit": true
        },
        "smooth": {
            "enabled": true,
            "type": "dynamic"
        }
    },
    "interaction": {
        "dragNodes": true,
        "hideEdgesOnDrag": false,
        "hideNodesOnDrag": false
    },
    "physics": {
        "enabled": true,
        "repulsion": {
            "centralGravity": 0.1,
            "damping": 0.09,
            "nodeDistance": 200,
            "springConstant": 0.05,
            "springLength": 150
        },
        "solver": "repulsion",
        "stabilization": {
            "enabled": true,
            "fit": true,
            "iterations": 1000,
            "onlyDynamicEdges": false,
            "updateInterval": 50
        }
    }
};

                  


                  

                  network = new vis.Network(container, data, options);

                  

                  

                  


                  
                      network.on("stabilizationProgress", function(params) {
                          document.getElementById('loadingBar').removeAttribute("style");
                          var maxWidth = 496;
                          var minWidth = 20;
                          var widthFactor = params.iterations/params.total;
                          var width = Math.max(minWidth,maxWidth * widthFactor);
                          document.getElementById('bar').style.width = width + 'px';
                          document.getElementById('text').innerHTML = Math.round(widthFactor*100) + '%';
                      });
                      network.once("stabilizationIterationsDone", function() {
                          document.getElementById('text').innerHTML = '100%';
                          document.getElementById('bar').style.width = '496px';
                          document.getElementById('loadingBar').style.opacity = 0;
                          // really clean the dom element
                          setTimeout(function () {document.getElementById('loadingBar').style.display = 'none';}, 500);
                      });
                  

                  return network;

              }
              drawGraph();
        </script>
    </body>
</html>