{
  "nodes": [
    {
      "id": "I Supervised learning",
      "type": "major",
      "parent": null,
      "description": "Overview of supervised learning techniques and models."
    },
    {
      "id": "1 Linear regression",
      "type": "subnode",
      "parent": "I Supervised learning",
      "description": "Introduction to linear regression including LMS algorithm and normal equations."
    },
    {
      "id": "1.1 LMS algorithm",
      "type": "subnode",
      "parent": "1 Linear regression",
      "description": "Least Mean Squares algorithm for linear regression."
    },
    {
      "id": "1.2 The normal equations",
      "type": "subnode",
      "parent": "1 Linear regression",
      "description": "Derivation and use of the normal equation method in linear regression."
    },
    {
      "id": "1.2.1 Matrix derivatives",
      "type": "subnode",
      "parent": "1.2 The normal equations",
      "description": "Matrix calculus involved in deriving the normal equations."
    },
    {
      "id": "1.2.2 Least squares revisited",
      "type": "subnode",
      "parent": "1.2 The normal equations",
      "description": "Revisiting least squares method with matrix algebra."
    },
    {
      "id": "1.3 Probabilistic interpretation",
      "type": "subnode",
      "parent": "1 Linear regression",
      "description": "Probabilistic view of linear regression models."
    },
    {
      "id": "1.4 Locally weighted linear regression (optional reading)",
      "type": "subnode",
      "parent": "1 Linear regression",
      "description": "Advanced topic: locally weighted linear regression for non-stationary data."
    },
    {
      "id": "2 Classification and logistic regression",
      "type": "subnode",
      "parent": "I Supervised learning",
      "description": "Introduction to classification problems and logistic regression."
    },
    {
      "id": "2.1 Logistic regression",
      "type": "subnode",
      "parent": "2 Classification and logistic regression",
      "description": "Logistic function for binary classification."
    },
    {
      "id": "2.2 Digression: the perceptron learning algorithm",
      "type": "subnode",
      "parent": "2 Classification and logistic regression",
      "description": "Perceptron algorithm as a precursor to modern neural networks."
    },
    {
      "id": "2.3 Multi-class classification",
      "type": "subnode",
      "parent": "2 Classification and logistic regression",
      "description": "Techniques for handling more than two classes in classification problems."
    },
    {
      "id": "2.4 Another algorithm for maximizing λ(θ)",
      "type": "subnode",
      "parent": "2 Classification and logistic regression",
      "description": "Alternative methods to maximize the likelihood function in logistic regression."
    },
    {
      "id": "3 Generalized linear models",
      "type": "subnode",
      "parent": "I Supervised learning",
      "description": "Introduction to generalized linear models (GLMs)."
    },
    {
      "id": "3.1 The exponential family",
      "type": "subnode",
      "parent": "3 Generalized linear models",
      "description": "Overview of the exponential family in statistics."
    },
    {
      "id": "3.2 Constructing GLMs",
      "type": "subnode",
      "parent": "3 Generalized linear models",
      "description": "Steps to construct generalized linear models from exponential families."
    },
    {
      "id": "3.2.1 Ordinary least squares",
      "type": "subnode",
      "parent": "3.2 Constructing GLMs",
      "description": "Ordinary least squares as a special case of GLM for Gaussian distribution."
    },
    {
      "id": "3.2.2 Logistic regression",
      "type": "subnode",
      "parent": "3.2 Constructing GLMs",
      "description": "Logistic regression as an example of GLM for binary classification."
    },
    {
      "id": "4 Generative learning algorithms",
      "type": "subnode",
      "parent": "I Supervised learning",
      "description": "Introduction to generative models in machine learning."
    },
    {
      "id": "4.1 Gaussian discriminant analysis",
      "type": "subnode",
      "parent": "4 Generative learning algorithms",
      "description": "Gaussian Discriminant Analysis for classification tasks."
    },
    {
      "id": "4.1.1 The multivariate normal distribution",
      "type": "subnode",
      "parent": "4.1 Gaussian discriminant analysis",
      "description": "Properties and applications of the multivariate normal distribution."
    },
    {
      "id": "4.1.2 The Gaussian discriminant analysis model",
      "type": "subnode",
      "parent": "4.1 Gaussian discriminant analysis",
      "description": "Model formulation for GDA using multivariate normals."
    },
    {
      "id": "4.1.3 Discussion: GDA and logistic regression",
      "type": "subnode",
      "parent": "4.1 Gaussian discriminant analysis",
      "description": "Comparison between GDA and logistic regression models."
    },
    {
      "id": "4.2 Naive bayes (Option Reading)",
      "type": "subnode",
      "parent": "4 Generative learning algorithms",
      "description": "Naive Bayes classifier for text classification tasks."
    },
    {
      "id": "4.2.1 Laplace smoothing",
      "type": "subnode",
      "parent": "4.2 Naive bayes (Option Reading)",
      "description": "Technique to handle zero probabilities in naive Bayes classifiers."
    },
    {
      "id": "4.2.2 Event models for text classification",
      "type": "subnode",
      "parent": "4.2 Naive bayes (Option Reading)",
      "description": "Models based on word events for document categorization."
    },
    {
      "id": "5 Kernel methods",
      "type": "subnode",
      "parent": "I Supervised learning",
      "description": "Techniques using kernel functions to extend linear models."
    },
    {
      "id": "5.1 Feature maps",
      "type": "subnode",
      "parent": "5 Kernel methods",
      "description": "Mapping data into higher-dimensional feature spaces."
    },
    {
      "id": "5.2 LMS (least mean squares) with features",
      "type": "subnode",
      "parent": "5 Kernel methods",
      "description": "LMS algorithm applied in the context of feature maps."
    },
    {
      "id": "5.3 LMS with the kernel trick",
      "type": "subnode",
      "parent": "5 Kernel methods",
      "description": "Using kernels to implicitly perform LMS in high-dimensional spaces."
    },
    {
      "id": "5.4 Properties of kernels",
      "type": "subnode",
      "parent": "5 Kernel methods",
      "description": "Mathematical properties and requirements for valid kernel functions."
    },
    {
      "id": "6 Support vector machines",
      "type": "subnode",
      "parent": "I Supervised learning",
      "description": "Introduction to support vector machines (SVMs) for classification tasks."
    },
    {
      "id": "II Deep learning",
      "type": "major",
      "parent": null,
      "description": "Overview of deep learning techniques and neural networks."
    },
    {
      "id": "7 Deep learning",
      "type": "subnode",
      "parent": "II Deep learning",
      "description": "Introduction to deep learning concepts and architectures."
    },
    {
      "id": "7.1 Supervised learning with non-linear models",
      "type": "subnode",
      "parent": "7 Deep learning",
      "description": "Supervised learning using non-linear models in deep learning."
    },
    {
      "id": "7.2 Neural networks",
      "type": "subnode",
      "parent": "7 Deep learning",
      "description": "Architecture and training of neural network models."
    },
    {
      "id": "7.3 Modules in Modern Neural Networks",
      "type": "subnode",
      "parent": "7 Deep learning",
      "description": "Discussion on various modules used in modern deep learning architectures."
    },
    {
      "id": "7.4 Backpropagation",
      "type": "subnode",
      "parent": "7 Deep learning",
      "description": "Algorithm for computing gradients in neural networks using backpropagation."
    },
    {
      "id": "7.4.1 Preliminaries on partial derivatives",
      "type": "subnode",
      "parent": "7.4 Backpropagation",
      "description": "Basics of partial derivatives needed for understanding backpropagation."
    },
    {
      "id": "7.4.2 General strategy of backpropagation",
      "type": "subnode",
      "parent": "7.4 Backpropagation",
      "description": "Overview of the general approach to implementing backpropagation."
    },
    {
      "id": "7.4.3 Backward functions for basic modules",
      "type": "subnode",
      "parent": "7.4 Backpropagation",
      "description": "Derivation and implementation of backward functions for simple neural network layers."
    },
    {
      "id": "7.4.4 Back-propagation for MLPs",
      "type": "subnode",
      "parent": "7.4 Backpropagation",
      "description": "Detailed explanation of backpropagation in multi-layer perceptrons (MLPs)."
    },
    {
      "id": "Neural Networks",
      "type": "major",
      "parent": "Machine Learning Concepts",
      "description": "Non-linear models using matrix multiplications and non-linear operations to solve complex problems."
    },
    {
      "id": "Modules in Modern Neural Networks",
      "type": "subnode",
      "parent": "Neural Networks",
      "description": "Discussion on the components used in modern neural networks."
    },
    {
      "id": "Backpropagation",
      "type": "subnode",
      "parent": "Neural Networks",
      "description": "Introduction to backpropagation for efficient gradient computation in neural networks."
    },
    {
      "id": "Preliminaries on partial derivatives",
      "type": "subnode",
      "parent": "Backpropagation",
      "description": "Introduction to the mathematical concepts needed for backpropagation."
    },
    {
      "id": "General strategy of backpropagation",
      "type": "subnode",
      "parent": "Backpropagation",
      "description": "Overview of the algorithmic steps in backpropagation."
    },
    {
      "id": "Backward functions for basic modules",
      "type": "subnode",
      "parent": "Backpropagation",
      "description": "Explanation of backward propagation for simple network components."
    },
    {
      "id": "Back-propagation for MLPs",
      "type": "subnode",
      "parent": "Backpropagation",
      "description": "Specific application of backpropagation to multi-layer perceptrons."
    },
    {
      "id": "Vectorization over training examples",
      "type": "subnode",
      "parent": "Neural Networks",
      "description": "Techniques for efficient computation in neural networks using vector operations."
    },
    {
      "id": "Generalization and regularization",
      "type": "major",
      "parent": null,
      "description": "Topics related to improving model performance on unseen data."
    },
    {
      "id": "Generalization",
      "type": "subnode",
      "parent": "Generalization and regularization",
      "description": "Concepts and techniques for enhancing the generalizability of machine learning models."
    },
    {
      "id": "Bias-variance tradeoff",
      "type": "subnode",
      "parent": "Generalization",
      "description": "Balancing model complexity to avoid overfitting or underfitting."
    },
    {
      "id": "A mathematical decomposition (for regression)",
      "type": "subnode",
      "parent": "Bias-variance tradeoff",
      "description": "Mathematical analysis of bias and variance in regression models."
    },
    {
      "id": "The double descent phenomenon",
      "type": "subnode",
      "parent": "Generalization",
      "description": "Observation that model performance can improve after initial degradation with increased complexity."
    },
    {
      "id": "Sample complexity bounds (optional readings)",
      "type": "subnode",
      "parent": "Generalization",
      "description": "Theoretical analysis of the number of samples needed for learning tasks."
    },
    {
      "id": "Regularization and model selection",
      "type": "major",
      "parent": null,
      "description": "Techniques to prevent overfitting by penalizing complexity in models."
    },
    {
      "id": "Regularization",
      "type": "subnode",
      "parent": "Regularization and model selection",
      "description": "Technique to control model complexity and prevent overfitting"
    },
    {
      "id": "Implicit regularization effect (optional reading)",
      "type": "subnode",
      "parent": "Regularization and model selection",
      "description": "Natural tendency of optimization algorithms to regularize models."
    },
    {
      "id": "Model selection via cross validation",
      "type": "subnode",
      "parent": "Regularization and model selection",
      "description": "Procedure for choosing the best model based on performance metrics."
    },
    {
      "id": "Bayesian statistics and regularization",
      "type": "subnode",
      "parent": "Regularization and model selection",
      "description": "Application of Bayesian principles to regularize models."
    },
    {
      "id": "Unsupervised learning",
      "type": "major",
      "parent": null,
      "description": "Techniques for learning from data without labeled responses."
    },
    {
      "id": "Clustering and the k-means algorithm",
      "type": "subnode",
      "parent": "Unsupervised learning",
      "description": "Algorithm for partitioning data into clusters based on similarity."
    },
    {
      "id": "EM algorithms",
      "type": "subnode",
      "parent": "Unsupervised learning",
      "description": "Expectation-Maximization algorithm for parameter estimation in probabilistic models."
    },
    {
      "id": "EM for mixture of Gaussians",
      "type": "subnode",
      "parent": "EM algorithms",
      "description": "Application of EM to model data as a combination of Gaussian distributions."
    },
    {
      "id": "Jensen's inequality",
      "type": "subnode",
      "parent": "EM algorithms",
      "description": "Mathematical result used in the derivation and application of the EM algorithm."
    },
    {
      "id": "General EM algorithms",
      "type": "subnode",
      "parent": "EM algorithms",
      "description": "Discussion on the general framework and variations of the EM algorithm."
    },
    {
      "id": "Other interpretation of ELBO",
      "type": "subnode",
      "parent": "General EM algorithms",
      "description": "Alternative understanding of the Evidence Lower Bound in variational inference."
    },
    {
      "id": "Mixture of Gaussians revisited",
      "type": "subnode",
      "parent": "EM algorithms",
      "description": "Re-examination and extension of mixture models using Gaussian distributions."
    },
    {
      "id": "Variational inference and variational auto-encoder (optional reading)",
      "type": "subnode",
      "parent": "EM algorithms",
      "description": "Advanced topic on probabilistic modeling with latent variables."
    },
    {
      "id": "Principal components analysis",
      "type": "subnode",
      "parent": "Unsupervised learning",
      "description": "Dimensionality reduction technique for identifying principal components in data."
    },
    {
      "id": "Independent components analysis",
      "type": "subnode",
      "parent": "Unsupervised learning",
      "description": "Technique to separate mixed signals into their independent sources."
    },
    {
      "id": "ICA ambiguities",
      "type": "subnode",
      "parent": "Independent components analysis",
      "description": "Discussion on the inherent limitations and challenges in ICA."
    },
    {
      "id": "Densities and linear transformations",
      "type": "subnode",
      "parent": "Independent components analysis",
      "description": "Mathematical foundations for understanding densities and transformations in ICA."
    },
    {
      "id": "ICA algorithm",
      "type": "subnode",
      "parent": "Independent components analysis",
      "description": "Detailed explanation of the Independent Components Analysis procedure."
    },
    {
      "id": "Self-supervised learning and foundation models",
      "type": "major",
      "parent": null,
      "description": "Techniques for training models on large datasets without explicit supervision."
    },
    {
      "id": "Pretraining and adaptation",
      "type": "subnode",
      "parent": "Self-supervised learning and foundation models",
      "description": "Overview of pre-training methods and their role in model adaptation."
    },
    {
      "id": "Pretraining methods in computer vision",
      "type": "subnode",
      "parent": "Self-supervised learning and foundation models",
      "description": "Specific approaches to self-supervised learning for visual data."
    },
    {
      "id": "Pretrained large language models",
      "type": "subnode",
      "parent": "Self-supervised learning and foundation models",
      "description": "Discussion on the development and applications of pre-trained language models."
    },
    {
      "id": "Open up the blackbox of Transformers",
      "type": "subnode",
      "parent": "Pretrained large language models",
      "description": "Exploration of the architecture and workings of Transformer-based models."
    },
    {
      "id": "Zero-shot learning and in-context learning",
      "type": "subnode",
      "parent": "Pretrained large language models",
      "description": "Capabilities of models to perform tasks without prior training data."
    },
    {
      "id": "Reinforcement Learning and Control",
      "type": "major",
      "parent": null,
      "description": "Techniques for agents to learn optimal actions through interaction with an environment."
    },
    {
      "id": "Reinforcement learning",
      "type": "subnode",
      "parent": "Reinforcement Learning and Control",
      "description": "Introduction to the principles of reinforcement learning."
    },
    {
      "id": "Markov decision processes",
      "type": "subnode",
      "parent": "Reinforcement learning",
      "description": "Mathematical framework for modeling sequential decision-making problems."
    },
    {
      "id": "Value iteration and policy iteration",
      "type": "subnode",
      "parent": "Reinforcement learning",
      "description": "Algorithms for finding optimal policies in MDPs."
    },
    {
      "id": "Learning a model for an MDP",
      "type": "subnode",
      "parent": "Reinforcement learning",
      "description": "Techniques to estimate the transition dynamics of an environment."
    },
    {
      "id": "Continuous state MDPs",
      "type": "subnode",
      "parent": "Reinforcement learning",
      "description": "Discussion on handling continuous states in reinforcement learning problems."
    },
    {
      "id": "Discretization",
      "type": "subnode",
      "parent": "Continuous state MDPs",
      "description": "Method to convert continuous state spaces into discrete ones for computation."
    },
    {
      "id": "Value function approximation",
      "type": "subnode",
      "parent": "Continuous state MDPs",
      "description": "Techniques for estimating value functions in large or continuous state spaces."
    },
    {
      "id": "Connections between Policy and Value Iteration (Optional)",
      "type": "subnode",
      "parent": "Reinforcement learning",
      "description": "Theoretical insights into the relationship between policy and value iteration methods."
    },
    {
      "id": "LQR, DDP and LQG",
      "type": "major",
      "parent": null,
      "description": "Control theory concepts applied to reinforcement learning problems."
    },
    {
      "id": "Finite-horizon MDPs",
      "type": "subnode",
      "parent": "LQR, DDP and LQG",
      "description": "Analysis of Markov decision processes with a fixed time horizon."
    },
    {
      "id": "Linear Quadratic Regulation (LQR)",
      "type": "subnode",
      "parent": "LQR, DDP and LQG",
      "description": "Optimal control problem for linear systems with quadratic cost functions"
    },
    {
      "id": "From non-linear dynamics to LQR",
      "type": "subnode",
      "parent": "LQR, DDP and LQG",
      "description": "Approaches to apply LQR to nonlinear systems"
    },
    {
      "id": "Linearization of dynamics",
      "type": "subnode",
      "parent": "From non-linear dynamics to LQR",
      "description": "Approximating nonlinear dynamics with linear models"
    },
    {
      "id": "Differential Dynamic Programming (DDP)",
      "type": "subnode",
      "parent": "From non-linear dynamics to LQR",
      "description": "Optimization technique for nonlinear systems using differential dynamic programming"
    },
    {
      "id": "Linear Quadratic Gaussian (LQG)",
      "type": "subnode",
      "parent": "LQR, DDP and LQG",
      "description": "Combination of LQR with stochastic dynamics and noisy measurements"
    },
    {
      "id": "Policy Gradient (REINFORCE)",
      "type": "major",
      "parent": null,
      "description": "Method for learning policies in reinforcement learning using gradients"
    },
    {
      "id": "Supervised Learning Overview",
      "type": "major",
      "parent": null,
      "description": "Introduction to supervised learning problems and concepts"
    },
    {
      "id": "Supervised Learning Problem",
      "type": "major",
      "parent": null,
      "description": "Formal description of the goal in supervised learning to predict y from x using a hypothesis h."
    },
    {
      "id": "Regression",
      "type": "subnode",
      "parent": "Supervised Learning Problem",
      "description": "Type of supervised learning where target variable is continuous and needs to be predicted accurately."
    },
    {
      "id": "Classification",
      "type": "subnode",
      "parent": "Supervised Learning Problem",
      "description": "Type of supervised learning where target variable takes on discrete values, predicting one from several categories."
    },
    {
      "id": "Linear Regression",
      "type": "major",
      "parent": "Supervised Learning",
      "description": "Technique in machine learning for modeling the relationship between a scalar response and one or more explanatory variables using a linear predictor function."
    },
    {
      "id": "Housing Example Dataset",
      "type": "subnode",
      "parent": "Linear Regression",
      "description": "Example dataset including living area, number of bedrooms, and price to illustrate regression problems."
    },
    {
      "id": "Features Selection",
      "type": "subnode",
      "parent": "Linear Regression",
      "description": "Process of selecting which features (variables) are relevant for predicting the target variable in a model."
    },
    {
      "id": "Machine_Learning_Basics",
      "type": "major",
      "parent": null,
      "description": "Introduction to fundamental concepts in machine learning."
    },
    {
      "id": "Function_Representation",
      "type": "subnode",
      "parent": "Machine_Learning_Basics",
      "description": "How functions and hypotheses are represented in machine learning models."
    },
    {
      "id": "Linear_Functions",
      "type": "subnode",
      "parent": "Function_Representation",
      "description": "Approximating y as a linear function of x with parameters theta."
    },
    {
      "id": "Parameters_Weights",
      "type": "subnode",
      "parent": "Linear_Functions",
      "description": "Theta values parameterizing the space of linear functions."
    },
    {
      "id": "Cost_Function",
      "type": "subnode",
      "parent": "Function_Representation",
      "description": "Measures how close h(x) is to y for training examples."
    },
    {
      "id": "Ordinary_Least_Squares",
      "type": "subnode",
      "parent": "Cost_Function",
      "description": "Least-squares cost function giving rise to OLS regression model."
    },
    {
      "id": "LMS_Algorithm",
      "type": "major",
      "parent": null,
      "description": "Learning algorithm for minimizing the cost function J(theta)."
    },
    {
      "id": "GradientDescentAlgorithm",
      "type": "major",
      "parent": "MachineLearningOverview",
      "description": "Optimization algorithm that iteratively adjusts parameters to minimize a cost function."
    },
    {
      "id": "LearningRate",
      "type": "subnode",
      "parent": "GradientDescentAlgorithm",
      "description": "Hyperparameter controlling the step size in gradient descent."
    },
    {
      "id": "CostFunctionJ",
      "type": "subnode",
      "parent": "GradientDescentAlgorithm",
      "description": "Function that measures the error between predicted and actual values, to be minimized."
    },
    {
      "id": "PartialDerivativeCalculation",
      "type": "subnode",
      "parent": "GradientDescentAlgorithm",
      "description": "Process of calculating partial derivatives for updating parameters in gradient descent."
    },
    {
      "id": "SingleTrainingExampleCase",
      "type": "subnode",
      "parent": "PartialDerivativeCalculation",
      "description": "Specific case where the cost function is calculated for a single training example."
    },
    {
      "id": "LMSUpdateRule",
      "type": "subnode",
      "parent": "GradientDescentAlgorithm",
      "description": "Update rule derived from least mean squares method, also known as Widrow-Hoff learning rule."
    },
    {
      "id": "LMS_Update_Rule",
      "type": "major",
      "parent": null,
      "description": "Update rule for adjusting parameters in machine learning models."
    },
    {
      "id": "Widrow_Hoff_Learning_Rule",
      "type": "subnode",
      "parent": "LMS_Update_Rule",
      "description": "Alternative name for LMS update rule."
    },
    {
      "id": "Error_Term",
      "type": "subnode",
      "parent": "LMS_Update_Rule",
      "description": "Difference between actual and predicted values used to adjust parameters."
    },
    {
      "id": "Single_Training_Example",
      "type": "subnode",
      "parent": "LMS_Update_Rule",
      "description": "Derivation of LMS rule for a single training example."
    },
    {
      "id": "Batch_Gradient_Descent",
      "type": "major",
      "parent": "Machine_Learning_Optimization",
      "description": "Method that considers all examples in the dataset at once to update parameters."
    },
    {
      "id": "Gradient_Descent",
      "type": "subnode",
      "parent": "Batch_Gradient_Descent",
      "description": "Optimization algorithm that minimizes cost function by moving towards the steepest descent direction."
    },
    {
      "id": "LinearRegressionOptimization",
      "type": "major",
      "parent": null,
      "description": "Discusses the optimization problem in linear regression."
    },
    {
      "id": "GradientDescentConvergence",
      "type": "subnode",
      "parent": "LinearRegressionOptimization",
      "description": "Explains how gradient descent converges to a global minimum for convex functions."
    },
    {
      "id": "BatchGradientDescentExample",
      "type": "subnode",
      "parent": "LinearRegressionOptimization",
      "description": "Provides an example of batch gradient descent with housing price prediction."
    },
    {
      "id": "StochasticGradientDescent",
      "type": "major",
      "parent": "GradientCalculation",
      "description": "Explanation and implementation of stochastic gradient descent for minimizing loss functions."
    },
    {
      "id": "SGDAlgorithm",
      "type": "subnode",
      "parent": "StochasticGradientDescent",
      "description": "Presents the algorithm for updating parameters in stochastic gradient descent."
    },
    {
      "id": "Machine_Learning_Optimization",
      "type": "major",
      "parent": null,
      "description": "Optimization techniques in machine learning including coordinate ascent and SVMs."
    },
    {
      "id": "Stochastic_Gradient_Descent",
      "type": "subnode",
      "parent": "Machine_Learning_Optimization",
      "description": "Algorithm for updating parameters based on single training example gradients"
    },
    {
      "id": "Learning_Rate_Decay",
      "type": "subnode",
      "parent": "Stochastic_Gradient_Descent",
      "description": "Technique to ensure convergence by gradually reducing learning rate"
    },
    {
      "id": "Normal_Equations_Method",
      "type": "major",
      "parent": null,
      "description": "Alternative method for minimizing cost function without iterative algorithms"
    },
    {
      "id": "Matrix_Derivatives",
      "type": "subnode",
      "parent": "Normal_Equations_Method",
      "description": "Notation and rules for differentiating functions of matrices"
    },
    {
      "id": "Matrix Derivatives",
      "type": "major",
      "parent": null,
      "description": "Derivation of matrix derivatives for functions mapping matrices to real numbers."
    },
    {
      "id": "Gradient Calculation",
      "type": "subnode",
      "parent": "Matrix Derivatives",
      "description": "Calculation of gradients for a specific function involving matrix elements."
    },
    {
      "id": "Least Squares Revisited",
      "type": "major",
      "parent": null,
      "description": "Revisiting least squares problems using matrix derivatives and design matrices."
    },
    {
      "id": "Design Matrix",
      "type": "subnode",
      "parent": "Least Squares Revisited",
      "description": "Matrix containing training examples' input values in its rows."
    },
    {
      "id": "Target Vector",
      "type": "subnode",
      "parent": "Least Squares Revisited",
      "description": "Vector containing target values from the training set."
    },
    {
      "id": "MachineLearningOverview",
      "type": "major",
      "parent": null,
      "description": "General overview of machine learning concepts and architectures."
    },
    {
      "id": "LinearRegression",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Algorithm for modeling the relationship between a scalar dependent variable y and one or more explanatory variables (or independent variables) denoted X."
    },
    {
      "id": "HypothesisFunction",
      "type": "subnode",
      "parent": "LinearRegression",
      "description": "Formulation of the hypothesis function for classification using a sigmoid function."
    },
    {
      "id": "CostFunction",
      "type": "subnode",
      "parent": "LinearRegression",
      "description": "A measure of how well a hypothesis fits the data, aiming for minimization."
    },
    {
      "id": "GradientDescent",
      "type": "subnode",
      "parent": "LinearRegression",
      "description": "An optimization algorithm to minimize cost functions by iteratively moving towards a minimum."
    },
    {
      "id": "NormalEquations",
      "type": "subnode",
      "parent": "LinearRegression",
      "description": "A direct method for finding the parameters that minimize the cost function without iteration."
    },
    {
      "id": "Probabilistic Interpretation of Linear Regression",
      "type": "major",
      "parent": null,
      "description": "Explains the probabilistic assumptions behind least-squares regression."
    },
    {
      "id": "Regression Problem",
      "type": "subnode",
      "parent": "Probabilistic Interpretation of Linear Regression",
      "description": "Context for why linear regression is used in certain scenarios."
    },
    {
      "id": "Least-Squares Cost Function J",
      "type": "subnode",
      "parent": "Probabilistic Interpretation of Linear Regression",
      "description": "Derivation and justification of the least-squares cost function."
    },
    {
      "id": "Target Variables and Inputs Relation",
      "type": "subnode",
      "parent": "Least-Squares Cost Function J",
      "description": "Equation describing relationship between target variables and inputs with error term."
    },
    {
      "id": "Error Term Distribution",
      "type": "subnode",
      "parent": "Least-Squares Cost Function J",
      "description": "Assumption that the error terms are normally distributed with mean zero and variance sigma squared."
    },
    {
      "id": "Conditional Probability of y given x",
      "type": "subnode",
      "parent": "Error Term Distribution",
      "description": "Expression for the conditional probability density function of y given x and theta."
    },
    {
      "id": "ProbabilisticModeling",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Models that use probability distributions to describe data generation processes."
    },
    {
      "id": "ConditionalProbabilityDistribution",
      "type": "subnode",
      "parent": "ProbabilisticModeling",
      "description": "Describes the distribution of one variable given another in a probabilistic model."
    },
    {
      "id": "LikelihoodFunction",
      "type": "subnode",
      "parent": "ProbabilisticModeling",
      "description": "Mathematical function that measures how likely the data is given specific parameters."
    },
    {
      "id": "MaximumLikelihoodEstimation",
      "type": "subnode",
      "parent": "ProbabilisticModeling",
      "description": "Method to estimate parameters by maximizing the likelihood of observing given data."
    },
    {
      "id": "LogLikelihoodFunction",
      "type": "subnode",
      "parent": "MaximumLikelihoodEstimation",
      "description": "The natural logarithm of the likelihood function, often used for computational convenience."
    },
    {
      "id": "MachineLearningConcepts",
      "type": "major",
      "parent": null,
      "description": "Overview of key concepts in machine learning including optimization methods and models."
    },
    {
      "id": "LeastSquaresRegression",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Method for fitting a linear model to data by minimizing squared error."
    },
    {
      "id": "LocallyWeightedLinearRegression",
      "type": "major",
      "parent": "MachineLearningOverview",
      "description": "Variant of linear regression that provides higher weight to training examples close to the query point x."
    },
    {
      "id": "Underfitting",
      "type": "subnode",
      "parent": "LinearRegression",
      "description": "Explains the scenario where a model is too simple to capture underlying patterns in data."
    },
    {
      "id": "Overfitting",
      "type": "subnode",
      "parent": "LinearRegression",
      "description": "Model captures noise and details in training data too well, harming generalization."
    },
    {
      "id": "FeatureSelection",
      "type": "subnode",
      "parent": "LinearRegression",
      "description": "Choosing relevant features to improve model performance."
    },
    {
      "id": "FittingTheta",
      "type": "subnode",
      "parent": "LinearRegression",
      "description": "Process of determining model parameters (theta) by minimizing error functions."
    },
    {
      "id": "WeightsInLWLR",
      "type": "subnode",
      "parent": "LocallyWeightedLinearRegression",
      "description": "Non-negative weights used in locally weighted linear regression to emphasize certain training examples over others based on proximity to the query point x."
    },
    {
      "id": "BandwidthParameter",
      "type": "subnode",
      "parent": "WeightsInLWLR",
      "description": "Controls how quickly the weight of a training example falls off with distance from the query point; denoted by tau."
    },
    {
      "id": "Machine Learning Overview",
      "type": "major",
      "parent": null,
      "description": "Introduction to machine learning concepts and algorithms."
    },
    {
      "id": "Locally Weighted Linear Regression",
      "type": "subnode",
      "parent": "Machine Learning Overview",
      "description": "A non-parametric algorithm that uses weighted linear regression for predictions."
    },
    {
      "id": "Non-Parametric Algorithms",
      "type": "subnode",
      "parent": "Machine Learning Overview",
      "description": "Algorithms where the amount of data needed to represent hypotheses grows with training set size."
    },
    {
      "id": "Parametric Algorithms",
      "type": "subnode",
      "parent": "Machine Learning Overview",
      "description": "Algorithms that have a fixed number of parameters independent of training set size."
    },
    {
      "id": "Classification Problem",
      "type": "major",
      "parent": null,
      "description": "A machine learning task where the output variable is categorical and discrete-valued."
    },
    {
      "id": "Binary Classification",
      "type": "subnode",
      "parent": "Classification Problem",
      "description": "A classification problem with two possible outcomes (0 or 1)."
    },
    {
      "id": "Logistic Regression",
      "type": "major",
      "parent": "Newton's Method",
      "description": "Application of generalized Newton's method in logistic regression for multidimensional settings."
    },
    {
      "id": "Spam Classification Example",
      "type": "subnode",
      "parent": "Classification Problem",
      "description": "Example of using logistic regression to classify emails as spam or not spam."
    },
    {
      "id": "MachineLearning",
      "type": "major",
      "parent": null,
      "description": "Field of study focusing on algorithms that learn from and make predictions on data."
    },
    {
      "id": "ClassificationProblem",
      "type": "subnode",
      "parent": "MachineLearning",
      "description": "Explanation of classification problems including binary and multi-class scenarios."
    },
    {
      "id": "LogisticRegression",
      "type": "subnode",
      "parent": "ClassificationProblem",
      "description": "A discriminative algorithm used to model the probability of a binary outcome given input variables."
    },
    {
      "id": "LogisticFunction",
      "type": "subnode",
      "parent": "LogisticRegression",
      "description": "Use of logistic function to convert logits into probabilities for binary classification."
    },
    {
      "id": "DerivativeOfSigmoid",
      "type": "subnode",
      "parent": "LogisticFunction",
      "description": "Mathematical expression describing the rate of change of the logistic function."
    },
    {
      "id": "MachineLearningModels",
      "type": "major",
      "parent": null,
      "description": "Overview of models used in machine learning including regression and classification."
    },
    {
      "id": "ClassificationModelAssumptions",
      "type": "subnode",
      "parent": "MachineLearningModels",
      "description": "Probabilistic assumptions made for a binary classification model."
    },
    {
      "id": "LogLikelihood",
      "type": "subnode",
      "parent": "LikelihoodFunction",
      "description": "Derivation and use of log-likelihood for easier optimization."
    },
    {
      "id": "GradientAscent",
      "type": "subnode",
      "parent": "LogLikelihood",
      "description": "Method to maximize the likelihood using gradient ascent updates."
    },
    {
      "id": "GradientAscentRule",
      "type": "subnode",
      "parent": "LogisticRegression",
      "description": "Derivation of the gradient ascent rule for updating parameters in logistic regression."
    },
    {
      "id": "StochasticGradientAscent",
      "type": "subnode",
      "parent": "GradientAscentRule",
      "description": "Update rule using stochastic gradient ascent method."
    },
    {
      "id": "LogisticLossFunction",
      "type": "subnode",
      "parent": "LogisticRegression",
      "description": "Definition and properties of the logistic loss function used in logistic regression."
    },
    {
      "id": "NegativeLogLikelihood",
      "type": "subnode",
      "parent": "LogisticLossFunction",
      "description": "Expression for negative log-likelihood in terms of logistic loss function."
    },
    {
      "id": "Machine_Learning_Concepts",
      "type": "major",
      "parent": null,
      "description": "Overview of key concepts in machine learning including loss functions and optimization."
    },
    {
      "id": "Logistic_Regression_Gradient_Descent",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Derivation and application of gradient descent for logistic regression."
    },
    {
      "id": "Perceptron_Algorithm",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Historical learning algorithm that outputs binary values."
    },
    {
      "id": "Multi_Class_Classification",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Classification problems where the response variable can take on multiple discrete values."
    },
    {
      "id": "Multi-classClassification",
      "type": "subnode",
      "parent": "MachineLearning",
      "description": "Extension of binary classification to more than two classes."
    },
    {
      "id": "ResponseVariable",
      "type": "subnode",
      "parent": "Multi-classClassification",
      "description": "Discrete variable that can take on any one of k values."
    },
    {
      "id": "MultinomialDistribution",
      "type": "subnode",
      "parent": "Multi-classClassification",
      "description": "Probability distribution over k possible outcomes."
    },
    {
      "id": "ParameterizedModel",
      "type": "subnode",
      "parent": "Multi-classClassification",
      "description": "Models that output probabilities for each class given input x."
    },
    {
      "id": "SoftmaxFunction",
      "type": "subnode",
      "parent": "MultinomialDistribution",
      "description": "Transformation of logits into a probability distribution over multiple classes."
    },
    {
      "id": "Softmax_Function",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Converts logits into probabilities for each possible value."
    },
    {
      "id": "Logits",
      "type": "subnode",
      "parent": "Softmax_Function",
      "description": "Inputs to the softmax function, often represented as a vector of real numbers."
    },
    {
      "id": "Probability_Vector_Output",
      "type": "subnode",
      "parent": "Softmax_Function",
      "description": "Output probabilities that sum up to 1."
    },
    {
      "id": "Probabilistic_Model",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Model using softmax output as conditional probabilities."
    },
    {
      "id": "Negative_Log_Likelihood",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Loss function for evaluating model performance based on log-likelihood."
    },
    {
      "id": "Cross_Entropy_Loss",
      "type": "subnode",
      "parent": "Negative_Log_Likelihood",
      "description": "A loss function that measures the dissimilarity between two probability distributions."
    },
    {
      "id": "Loss_Functions",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Functions used to evaluate the performance of a model during training."
    },
    {
      "id": "Softmax_Cross_Entropy_Loss",
      "type": "subnode",
      "parent": "Cross_Entropy_Loss",
      "description": "Specific form of cross-entropy loss used in multi-class classification problems."
    },
    {
      "id": "Gradient_Computation",
      "type": "subnode",
      "parent": "Loss_Functions",
      "description": "Computation of gradients with respect to intermediate variables and parameters."
    },
    {
      "id": "LossFunction",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Mathematical function used to measure the performance of a model and guide its training."
    },
    {
      "id": "CrossEntropyLoss",
      "type": "subnode",
      "parent": "LossFunction",
      "description": "Explanation of cross-entropy loss function used for classification tasks."
    },
    {
      "id": "GradientCalculation",
      "type": "subnode",
      "parent": "LossFunction",
      "description": "Process of calculating gradients with respect to parameters using the chain rule."
    },
    {
      "id": "NewtonMethod",
      "type": "subnode",
      "parent": "LogisticRegression",
      "description": "Explanation of Newton's method for finding zeros of a function and its use in optimization problems."
    },
    {
      "id": "Newton's Method",
      "type": "major",
      "parent": null,
      "description": "An iterative optimization algorithm for finding roots of a real-valued function."
    },
    {
      "id": "Finding Roots",
      "type": "subnode",
      "parent": "Newton's Method",
      "description": "The process of using Newton's method to find the value of θ where f(θ) = 0."
    },
    {
      "id": "Maximizing Functions",
      "type": "subnode",
      "parent": "Newton's Method",
      "description": "Using Newton's method to maximize a function by finding points where its first derivative is zero."
    },
    {
      "id": "Hessian Matrix",
      "type": "subnode",
      "parent": "Logistic Regression",
      "description": "A matrix used in the Newton-Raphson method, containing second derivatives of Λ(θ) with respect to θ_i and θ_j."
    },
    {
      "id": "Gradient Descent",
      "type": "subnode",
      "parent": null,
      "description": "An optimization algorithm that uses the gradient of a function to find its minimum or maximum values."
    },
    {
      "id": "OptimizationMethods",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Techniques used to optimize the parameters of a model."
    },
    {
      "id": "FisherScoring",
      "type": "subnode",
      "parent": "NewtonMethod",
      "description": "Application of Newton's method to logistic regression likelihood function maximization."
    },
    {
      "id": "GeneralizedLinearModels",
      "type": "major",
      "parent": "MachineLearningOverview",
      "description": "Introduction to GLMs for solving problems involving exponential family distributions."
    },
    {
      "id": "ExponentialFamilyDistributions",
      "type": "subnode",
      "parent": "GeneralizedLinearModels",
      "description": "Overview of distributions that belong to the exponential family, including Poisson and Gamma distributions."
    },
    {
      "id": "NaturalParameter",
      "type": "subnode",
      "parent": "ExponentialFamilyDistributions",
      "description": "The parameter η that defines an exponential family distribution."
    },
    {
      "id": "SufficientStatistic",
      "type": "subnode",
      "parent": "ExponentialFamilyDistributions",
      "description": "A statistic T(y) that captures all the information about a parameter in a given sample."
    },
    {
      "id": "LogPartitionFunction",
      "type": "subnode",
      "parent": "ExponentialFamilyDistributions",
      "description": "Ensures the distribution sums/integrates to 1 over y."
    },
    {
      "id": "BernoulliDistribution",
      "type": "major",
      "parent": "LogisticRegressionAsGLM",
      "description": "Formulation of Bernoulli distribution within the exponential family framework."
    },
    {
      "id": "NaturalParameterForBernoulli",
      "type": "subnode",
      "parent": "BernoulliDistribution",
      "description": "Defined as log(φ/(1-φ))."
    },
    {
      "id": "SufficientStatisticForBernoulli",
      "type": "subnode",
      "parent": "BernoulliDistribution",
      "description": "T(y) = y, the outcome itself."
    },
    {
      "id": "LogPartitionFunctionForBernoulli",
      "type": "subnode",
      "parent": "BernoulliDistribution",
      "description": "a(η) = log(1+e^η)."
    },
    {
      "id": "SigmoidFunction",
      "type": "major",
      "parent": "ActivationFunctions",
      "description": "Use of sigmoid function as a default cdf for ICA due to its monotonic increase from 0 to 1."
    },
    {
      "id": "LogisticRegressionAsGLM",
      "type": "subnode",
      "parent": "MachineLearningModels",
      "description": "Explanation of logistic regression as a Generalized Linear Model (GLM)."
    },
    {
      "id": "GaussianDistribution",
      "type": "subnode",
      "parent": "MachineLearningModels",
      "description": "Explanation of Gaussian distribution as a member of the exponential family used in linear regression."
    },
    {
      "id": "GLMConstruction",
      "type": "subnode",
      "parent": "MachineLearningModels",
      "description": "General method for constructing GLMs using various distributions."
    },
    {
      "id": "PoissonDistributionModeling",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Using Poisson distribution for modeling website visitor counts based on features like promotions and weather."
    },
    {
      "id": "GLMAssumptions",
      "type": "subnode",
      "parent": "GeneralizedLinearModels",
      "description": "Three key assumptions/design choices for constructing GLM models."
    },
    {
      "id": "OrdinaryLeastSquares",
      "type": "subnode",
      "parent": "MachineLearningModels",
      "description": "Regression method for continuous target variables modeled as Gaussian distributions."
    },
    {
      "id": "Assumption1",
      "type": "subnode",
      "parent": "GeneralizedLinearModels",
      "description": "First assumption in GLM formulation, relating mean and natural parameter."
    },
    {
      "id": "Assumption2",
      "type": "subnode",
      "parent": "GeneralizedLinearModels",
      "description": "Second assumption about the expected value of y given x."
    },
    {
      "id": "Assumption3",
      "type": "subnode",
      "parent": "GeneralizedLinearModels",
      "description": "Third assumption in GLM formulation, relating parameters to input features."
    },
    {
      "id": "ExponentialFamilyDistribution",
      "type": "subnode",
      "parent": "GeneralizedLinearModels",
      "description": "A family of probability distributions that includes Gaussian and Bernoulli distributions."
    },
    {
      "id": "Conditional_Distribution_Modeling",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Techniques for modeling the conditional distribution p(y|x;θ)."
    },
    {
      "id": "Bernoulli_Distribution",
      "type": "subnode",
      "parent": "Conditional_Distribution_Modeling",
      "description": "Binary-valued random variable distribution used in logistic regression."
    },
    {
      "id": "Exponential_Family_Distributions",
      "type": "subnode",
      "parent": "Conditional_Distribution_Modeling",
      "description": "Family of distributions including Bernoulli and Gaussian, characterized by natural parameters."
    },
    {
      "id": "Hypothesis_Functions",
      "type": "subnode",
      "parent": "Conditional_Distribution_Modeling",
      "description": "Functions used to predict the expected value of y given x in machine learning models."
    },
    {
      "id": "Logistic_Function",
      "type": "subnode",
      "parent": "Exponential_Family_Distributions",
      "description": "Sigmoid function derived from Bernoulli distribution for predicting probabilities."
    },
    {
      "id": "Canonical_Response_Function",
      "type": "subnode",
      "parent": "Exponential_Family_Distributions",
      "description": "Function mapping natural parameters to the expected value of a random variable."
    },
    {
      "id": "Canonical_Link_Function",
      "type": "subnode",
      "parent": "Exponential_Family_Distributions",
      "description": "Inverse function of canonical response, relating observed data to model parameters."
    },
    {
      "id": "MachineLearningAlgorithms",
      "type": "major",
      "parent": null,
      "description": "Overview of machine learning algorithms as optimization search in model space."
    },
    {
      "id": "DiscriminativeLearning",
      "type": "subnode",
      "parent": "MachineLearningAlgorithms",
      "description": "Algorithms that learn p(y|x) directly or map inputs to labels."
    },
    {
      "id": "GenerativeLearning",
      "type": "subnode",
      "parent": "MachineLearningAlgorithms",
      "description": "Algorithms that model p(x|y) and p(y) to derive p(y|x)."
    },
    {
      "id": "ConditionalDistribution",
      "type": "subnode",
      "parent": "DiscriminativeLearning",
      "description": "p(y|x;θ), the conditional distribution of y given x."
    },
    {
      "id": "PerceptronAlgorithm",
      "type": "subnode",
      "parent": "DiscriminativeLearning",
      "description": "Attempts to find a decision boundary between classes."
    },
    {
      "id": "DecisionBoundary",
      "type": "subnode",
      "parent": "PerceptronAlgorithm",
      "description": "Line that separates regions where different classes are predicted as most likely."
    },
    {
      "id": "ClassPriors",
      "type": "subnode",
      "parent": "GenerativeLearning",
      "description": "p(y), the prior probability of each class before observing data."
    },
    {
      "id": "ConditionalProbabilityModel",
      "type": "subnode",
      "parent": "GenerativeLearning",
      "description": "Models p(x|y) for each class to understand feature distributions."
    },
    {
      "id": "BayesRule",
      "type": "subnode",
      "parent": "GenerativeLearning",
      "description": "Uses prior probabilities and conditional probabilities to calculate posterior distribution p(y|x)."
    },
    {
      "id": "Bayesian Classification",
      "type": "major",
      "parent": null,
      "description": "Classification using Bayes' theorem to calculate posterior probabilities."
    },
    {
      "id": "Class Priors",
      "type": "subnode",
      "parent": "Bayesian Classification",
      "description": "Prior probability of each class before observing data."
    },
    {
      "id": "Conditional Probability p(x|y)",
      "type": "subnode",
      "parent": "Bayesian Classification",
      "description": "Probability distribution of features given the class label."
    },
    {
      "id": "Posterior Distribution p(y|x)",
      "type": "subnode",
      "parent": "Bayesian Classification",
      "description": "Derived using Bayes' rule to predict class labels based on observed data."
    },
    {
      "id": "Gaussian Discriminant Analysis (GDA)",
      "type": "major",
      "parent": null,
      "description": "Generative learning algorithm assuming multivariate normal distribution for p(x|y)."
    },
    {
      "id": "Multivariate Normal Distribution",
      "type": "subnode",
      "parent": "Gaussian Discriminant Analysis (GDA)",
      "description": "Distribution parameterized by mean vector and covariance matrix."
    },
    {
      "id": "Mean Vector",
      "type": "subnode",
      "parent": "Multivariate Normal Distribution",
      "description": "Vector representing the expected value of a multivariate normal distribution."
    },
    {
      "id": "Covariance Matrix",
      "type": "subnode",
      "parent": "Multivariate Normal Distribution",
      "description": "Matrix describing the variance and covariance between variables in a multivariate normal distribution."
    },
    {
      "id": "MachineLearningBasics",
      "type": "major",
      "parent": null,
      "description": "Fundamental concepts in machine learning including probability distributions and random variables."
    },
    {
      "id": "MeanOfGaussian",
      "type": "subnode",
      "parent": "GaussianDistribution",
      "description": "The expected value or mean of a Gaussian distribution is given by μ."
    },
    {
      "id": "CovarianceMatrix",
      "type": "subnode",
      "parent": "GaussianDistribution",
      "description": "A matrix that generalizes the notion of variance to multiple dimensions for random vectors."
    },
    {
      "id": "StandardNormalDistribution",
      "type": "subnode",
      "parent": "GaussianDistribution",
      "description": "A Gaussian distribution with zero mean and identity covariance matrix, representing a standard case."
    },
    {
      "id": "DensityExamples",
      "type": "subnode",
      "parent": "GaussianDistribution",
      "description": "Illustrative examples of how changes in the covariance matrix affect the shape of the Gaussian density function."
    },
    {
      "id": "MultivariateNormalDistribution",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Exploration of multivariate normal distribution properties and examples."
    },
    {
      "id": "CovarianceMatrixImpact",
      "type": "subnode",
      "parent": "MultivariateNormalDistribution",
      "description": "Effect of covariance matrix on the density contours."
    },
    {
      "id": "MeanVectorMovement",
      "type": "subnode",
      "parent": "MultivariateNormalDistribution",
      "description": "Impact of mean vector changes when covariance is fixed."
    },
    {
      "id": "GaussianDiscriminantAnalysisModel",
      "type": "major",
      "parent": null,
      "description": "Introduction to the Gaussian Discriminant Analysis model for classification problems."
    },
    {
      "id": "MultivariateNormalForClasses",
      "type": "subnode",
      "parent": "GaussianDiscriminantAnalysisModel",
      "description": "Using multivariate normal distributions to model class-conditional probabilities."
    },
    {
      "id": "GaussianDiscriminantAnalysis(GDA)",
      "type": "subnode",
      "parent": "MachineLearningModels",
      "description": "Model that uses Gaussian distributions to classify data points."
    },
    {
      "id": "ParametersEstimation",
      "type": "subnode",
      "parent": "GaussianDiscriminantAnalysis(GDA)",
      "description": "Process of finding values for model parameters to maximize likelihood."
    },
    {
      "id": "GaussianDiscriminantAnalysis",
      "type": "subnode",
      "parent": "MachineLearningModels",
      "description": "A probabilistic model for classification tasks based on the assumption that the data is generated from a Gaussian distribution."
    },
    {
      "id": "DecisionBoundaries",
      "type": "subnode",
      "parent": "MachineLearningModels",
      "description": "The boundary that separates different classes in classification models."
    },
    {
      "id": "ModelAssumptions",
      "type": "subnode",
      "parent": "MachineLearningModels",
      "description": "The underlying assumptions made by machine learning models about the data distribution."
    },
    {
      "id": "AsymptoticEfficiency",
      "type": "subnode",
      "parent": "GaussianDiscriminantAnalysis",
      "description": "Property of GDA that ensures optimal performance in large datasets when model assumptions are correct."
    },
    {
      "id": "GDA",
      "type": "subnode",
      "parent": "MachineLearningModels",
      "description": "Generative Discriminative Algorithm with strong modeling assumptions."
    },
    {
      "id": "Robustness",
      "type": "subnode",
      "parent": "LogisticRegression",
      "description": "Strength of logistic regression in handling non-Gaussian data."
    },
    {
      "id": "NaiveBayes",
      "type": "major",
      "parent": "BernoulliEventModel",
      "description": "A generative learning algorithm that works well for many classification problems, including text classification with modifications."
    },
    {
      "id": "DiscreteFeatures",
      "type": "subnode",
      "parent": "NaiveBayes",
      "description": "Handling of discrete-valued feature vectors in Naive Bayes algorithm."
    },
    {
      "id": "Machine_Learning",
      "type": "major",
      "parent": null,
      "description": "Field of study focusing on algorithms that learn from and make predictions on data."
    },
    {
      "id": "Text_Classification",
      "type": "subnode",
      "parent": "Machine_Learning",
      "description": "Techniques for classifying text into predefined categories."
    },
    {
      "id": "Spam_Filtering",
      "type": "subnode",
      "parent": "Text_Classification",
      "description": "Application of machine learning to classify emails as spam or non-spam."
    },
    {
      "id": "Training_Set",
      "type": "subnode",
      "parent": "Spam_Filtering",
      "description": "Set of labeled examples used for training a model."
    },
    {
      "id": "Feature_Vector",
      "type": "subnode",
      "parent": "Spam_Filtering",
      "description": "Vector representation of an email based on its content words."
    },
    {
      "id": "Vocabulary",
      "type": "subnode",
      "parent": "Feature_Vector",
      "description": "Set of unique words used to construct the feature vector."
    },
    {
      "id": "Stop_Words",
      "type": "subnode",
      "parent": "Feature_Vector",
      "description": "Commonly excluded high-frequency words in text processing."
    },
    {
      "id": "Machine_Learning_Topic",
      "type": "major",
      "parent": null,
      "description": "Overview of machine learning concepts and algorithms."
    },
    {
      "id": "Feature_Vector_Selection",
      "type": "subnode",
      "parent": "Text_Classification",
      "description": "Choosing relevant features from the text data, excluding stop words."
    },
    {
      "id": "Generative_Modeling",
      "type": "subnode",
      "parent": "Text_Classification",
      "description": "Building models that generate probabilities for feature vectors given a class label."
    },
    {
      "id": "Naive_Bayes_Assumption",
      "type": "subnode",
      "parent": "Generative_Modeling",
      "description": "Assumption of conditional independence among features given the class label."
    },
    {
      "id": "Naive_Bayes_Classifier",
      "type": "subnode",
      "parent": "Generative_Modeling",
      "description": "Algorithm for classification based on Bayes' theorem with strong independence assumptions."
    },
    {
      "id": "NaiveBayesAlgorithm",
      "type": "major",
      "parent": null,
      "description": "A probabilistic classifier based on applying Bayes' theorem with strong independence assumptions between the features."
    },
    {
      "id": "ConditionalProbability",
      "type": "subnode",
      "parent": "NaiveBayesAlgorithm",
      "description": "The probability of a token given the preceding tokens in a sequence."
    },
    {
      "id": "JointLikelihood",
      "type": "subnode",
      "parent": "NaiveBayesAlgorithm",
      "description": "Probability of observing the data given model parameters."
    },
    {
      "id": "BinaryFeatures",
      "type": "subnode",
      "parent": "NaiveBayesAlgorithm",
      "description": "Features are binary-valued in the context of spam email classification."
    },
    {
      "id": "ParameterEstimation",
      "type": "subnode",
      "parent": "NaiveBayesAlgorithm",
      "description": "Process of estimating parameters from training data in the context of the multinomial model"
    },
    {
      "id": "Machine Learning Algorithms",
      "type": "major",
      "parent": null,
      "description": "Collection of algorithms used in machine learning for model training and optimization."
    },
    {
      "id": "Naive Bayes Algorithm",
      "type": "subnode",
      "parent": "Machine Learning Algorithms",
      "description": "Probabilistic classifier based on applying Bayes' theorem with strong independence assumptions between the features"
    },
    {
      "id": "Binary Features",
      "type": "subnode",
      "parent": "Naive Bayes Algorithm",
      "description": "Features that can take only two values, typically 0 or 1"
    },
    {
      "id": "Multinomial Distribution",
      "type": "subnode",
      "parent": "Naive Bayes Algorithm",
      "description": "Probability distribution of the outcomes of a fixed number of independent trials with different possible outcomes for each trial"
    },
    {
      "id": "Feature Representation",
      "type": "subnode",
      "parent": "Naive Bayes Algorithm",
      "description": "Method for representing features in a dataset, such as using intervals for continuous variables"
    },
    {
      "id": "Laplace Smoothing",
      "type": "subnode",
      "parent": "Machine Learning Algorithms",
      "description": "Technique to prevent zero probability estimates by adding a small constant to the counts of each feature value"
    },
    {
      "id": "MachineLearningConferences",
      "type": "major",
      "parent": null,
      "description": "Top machine learning conferences including NeurIPS."
    },
    {
      "id": "NeurIPS20xxSubmission",
      "type": "subnode",
      "parent": "MachineLearningConferences",
      "description": "Submitting work to the NeurIPS conference in May 20xx."
    },
    {
      "id": "NaiveBayesFilter",
      "type": "major",
      "parent": null,
      "description": "Description of a Naive Bayes spam filter and its limitations."
    },
    {
      "id": "NewWordDetection",
      "type": "subnode",
      "parent": "NaiveBayesFilter",
      "description": "Issues with detecting new words like 'neurips' in the training set."
    },
    {
      "id": "Probability_Estimation",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Techniques for estimating probabilities from data samples."
    },
    {
      "id": "Maximum_Likelihood_Estimates",
      "type": "subnode",
      "parent": "Probability_Estimation",
      "description": "Estimating parameters based on observed frequencies in a dataset."
    },
    {
      "id": "Laplace_Smoothing",
      "type": "subnode",
      "parent": "Probability_Estimation",
      "description": "Adjusting estimates to avoid zero probabilities by adding small constants."
    },
    {
      "id": "Event_Models_Text_Classification",
      "type": "subnode",
      "parent": "Naive_Bayes_Classifier",
      "description": "Models used in text classification tasks, incorporating Laplace smoothing."
    },
    {
      "id": "EventModelsForTextClassification",
      "type": "major",
      "parent": null,
      "description": "Discussion of models specifically for text classification in machine learning."
    },
    {
      "id": "BernoulliEventModel",
      "type": "subnode",
      "parent": "EventModelsForTextClassification",
      "description": "A model where each word is included independently according to probabilities based on class priors."
    },
    {
      "id": "MultinomialEventModel",
      "type": "subnode",
      "parent": "EventModelsForTextClassification",
      "description": "A model where each word in a document is generated independently from the same multinomial distribution based on class label"
    },
    {
      "id": "NaiveBayesClassifier",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "A probabilistic classifier based on applying Bayes' theorem with strong (naive) independence assumptions between the features."
    },
    {
      "id": "LaplaceSmoothing",
      "type": "subnode",
      "parent": "MaximumLikelihoodEstimation",
      "description": "Technique to prevent zero probabilities in categorical data estimation."
    },
    {
      "id": "KernelMethods",
      "type": "major",
      "parent": "MachineLearningConcepts",
      "description": "Techniques that allow algorithms to work in high-dimensional feature spaces without explicit computation of the space."
    },
    {
      "id": "FeatureMaps",
      "type": "subnode",
      "parent": "KernelMethods",
      "description": "Transformation of input data to higher-dimensional space for better model fitting."
    },
    {
      "id": "Machine Learning Concepts",
      "type": "major",
      "parent": null,
      "description": "Overview of key concepts in machine learning including model complexity and regularization."
    },
    {
      "id": "Feature Mapping",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Alternative method of testing if a function is a valid kernel through feature mapping."
    },
    {
      "id": "Linear Function Over Features",
      "type": "subnode",
      "parent": "Feature Mapping",
      "description": "Rewriting a cubic function as a linear combination of features."
    },
    {
      "id": "Cubic Function Representation",
      "type": "subnode",
      "parent": "Linear Function Over Features",
      "description": "Expressing a cubic polynomial using feature map φ(x)."
    },
    {
      "id": "Feature Map Definition",
      "type": "subnode",
      "parent": "Feature Mapping",
      "description": "Definition of the function φ that maps attributes to features."
    },
    {
      "id": "LMS with Features",
      "type": "major",
      "parent": null,
      "description": "Derivation and application of least mean squares algorithm using feature variables."
    },
    {
      "id": "Gradient Descent Update",
      "type": "subnode",
      "parent": "LMS with Features",
      "description": "Update rule for fitting the model θₑφ(x) using gradient descent."
    },
    {
      "id": "FeatureMapping",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Transformation that maps original feature vectors into a new space where linear algorithms can be applied more effectively."
    },
    {
      "id": "HighDimensionalFeatures",
      "type": "subnode",
      "parent": "FeatureMapping",
      "description": "Complex feature mappings leading to high-dimensional vectors in machine learning problems."
    },
    {
      "id": "KernelTrick",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Technique for efficiently computing inner products of high-dimensional feature space without explicitly mapping the data."
    },
    {
      "id": "FeatureMappingPhiX",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Description of the feature mapping function φ(x) in machine learning models."
    },
    {
      "id": "RuntimeAndMemoryEfficiency",
      "type": "subnode",
      "parent": "FeatureMappingPhiX",
      "description": "Discussion on improving runtime and memory efficiency using techniques like the kernel trick."
    },
    {
      "id": "InitializationThetaZero",
      "type": "subnode",
      "parent": "RuntimeAndMemoryEfficiency",
      "description": "Initial setup where θ is set to zero for simplification."
    },
    {
      "id": "IterativeUpdateRule",
      "type": "subnode",
      "parent": "InitializationThetaZero",
      "description": "Explanation of iterative update rules in machine learning models."
    },
    {
      "id": "LinearCombinationRepresentation",
      "type": "subnode",
      "parent": "IterativeUpdateRule",
      "description": "Concept of representing θ as a linear combination of feature mappings φ(x)."
    },
    {
      "id": "CoefficientUpdateRule",
      "type": "subnode",
      "parent": "LinearCombinationRepresentation",
      "description": "Derivation and explanation of the update rule for coefficients β in machine learning models."
    },
    {
      "id": "MachineLearningAlgorithm",
      "type": "major",
      "parent": null,
      "description": "Overview of machine learning algorithms and their properties."
    },
    {
      "id": "BatchGradientDescent",
      "type": "subnode",
      "parent": "MachineLearningAlgorithm",
      "description": "A method for minimizing loss functions in machine learning models."
    },
    {
      "id": "BetaUpdateEquation",
      "type": "subnode",
      "parent": "BatchGradientDescent",
      "description": "The equation used to update the beta values iteratively during gradient descent."
    },
    {
      "id": "FeatureMapPhi",
      "type": "subnode",
      "parent": "BatchGradientDescent",
      "description": "A mapping function that transforms input data into a higher-dimensional space for better separation."
    },
    {
      "id": "InnerProductEfficiency",
      "type": "subnode",
      "parent": "FeatureMapPhi",
      "description": "Techniques to efficiently compute inner products in high-dimensional spaces without explicit feature map computation."
    },
    {
      "id": "FeatureMapsAndKernels",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Discussion on feature maps and their corresponding kernel functions."
    },
    {
      "id": "KernelFunctionDefinition",
      "type": "subnode",
      "parent": "FeatureMapsAndKernels",
      "description": "Definition of the kernel function in relation to feature maps."
    },
    {
      "id": "InnerProductCalculation",
      "type": "subnode",
      "parent": "FeatureMapsAndKernels",
      "description": "Method for calculating inner products using kernels."
    },
    {
      "id": "AlgorithmEfficiency",
      "type": "subnode",
      "parent": "FeatureMapsAndKernels",
      "description": "Explanation of the efficiency of algorithms based on kernel functions."
    },
    {
      "id": "PropertiesOfKernels",
      "type": "major",
      "parent": null,
      "description": "Exploration of properties and characteristics of kernels in machine learning."
    },
    {
      "id": "Kernels_in_Machine_Learning",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Discussion on kernel functions in the context of machine learning algorithms."
    },
    {
      "id": "Feature_Map_Phi",
      "type": "subnode",
      "parent": "Kernels_in_Machine Learning",
      "description": "Explicitly defined feature map phi and its role in inducing kernel function K."
    },
    {
      "id": "Kernel_Function_K",
      "type": "subnode",
      "parent": "Kernels_in_Machine_Learning",
      "description": "Definition of the kernel function K(x,z) and its intrinsic properties."
    },
    {
      "id": "Algorithm_5.11",
      "type": "subnode",
      "parent": "Kernels_in_Machine_Learning",
      "description": "Description of algorithm 5.11 which operates based on kernel functions without explicit feature maps."
    },
    {
      "id": "Characterization_of_Kernels",
      "type": "subnode",
      "parent": "Kernels_in_Machine_Learning",
      "description": "Conditions for a function to be considered as a valid kernel function K(x,z)."
    },
    {
      "id": "Example_Kernel_Functions",
      "type": "subnode",
      "parent": "Characterization_of_Kernels",
      "description": "Concrete examples of kernel functions, such as polynomial kernels."
    },
    {
      "id": "KernelFunctions",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Functions that compute the dot product of feature mappings in a high-dimensional space."
    },
    {
      "id": "PolynomialKernels",
      "type": "subnode",
      "parent": "KernelFunctions",
      "description": "A type of kernel function used to map input data into higher dimensional spaces."
    },
    {
      "id": "ComputationalEfficiency",
      "type": "subnode",
      "parent": "PolynomialKernels",
      "description": "Discussion on the efficiency of computing kernel functions compared to direct computation in high-dimensional spaces."
    },
    {
      "id": "KernelsAsSimilarityMetrics",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Use of kernels as a measure of similarity between data points."
    },
    {
      "id": "GaussianKernel",
      "type": "subnode",
      "parent": "KernelsAsSimilarityMetrics",
      "description": "Specific kernel function that measures the similarity based on distance in input space."
    },
    {
      "id": "Kernel_Functions",
      "type": "major",
      "parent": null,
      "description": "Properties and conditions for a function to be a valid kernel."
    },
    {
      "id": "Necessary_Conditions",
      "type": "subnode",
      "parent": "Kernel_Functions",
      "description": "Conditions that must be satisfied by a valid kernel function."
    },
    {
      "id": "Symmetry_Property",
      "type": "subnode",
      "parent": "Necessary_Conditions",
      "description": "A valid kernel matrix is symmetric."
    },
    {
      "id": "Positive_Semi_Definite",
      "type": "subnode",
      "parent": "Necessary_Conditions",
      "description": "A valid kernel matrix must be positive semi-definite."
    },
    {
      "id": "Kernel_Matrix",
      "type": "subnode",
      "parent": "Necessary_Conditions",
      "description": "Matrix representation of the kernel function for a set of points."
    },
    {
      "id": "Sufficient_Conditions",
      "type": "subnode",
      "parent": "Kernel_Functions",
      "description": "Conditions that are both necessary and sufficient for a valid kernel function."
    },
    {
      "id": "Kernel Matrix Properties",
      "type": "major",
      "parent": null,
      "description": "Properties of the kernel matrix including symmetry and positive semidefiniteness."
    },
    {
      "id": "Sufficient Conditions for Kernels",
      "type": "subnode",
      "parent": "Kernel Matrix Properties",
      "description": "Conditions under which a function can be considered a valid Mercer kernel."
    },
    {
      "id": "Mercer's Theorem",
      "type": "subnode",
      "parent": "Sufficient Conditions for Kernels",
      "description": "Theorem stating necessary and sufficient conditions for a function to be a valid kernel."
    },
    {
      "id": "Kernel Examples",
      "type": "major",
      "parent": null,
      "description": "Examples of kernels used in machine learning problems such as SVMs."
    },
    {
      "id": "Digit Recognition Problem",
      "type": "subnode",
      "parent": "Kernel Examples",
      "description": "Use of polynomial and Gaussian kernels for recognizing handwritten digits."
    },
    {
      "id": "String Classification Example",
      "type": "subnode",
      "parent": "Kernel Examples",
      "description": "Example involving classification of strings, such as amino acid sequences."
    },
    {
      "id": "FeatureExtraction",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Techniques for converting raw data into features suitable for machine learning models."
    },
    {
      "id": "StringFeatureExtraction",
      "type": "subnode",
      "parent": "FeatureExtraction",
      "description": "Method for extracting features from strings by counting occurrences of substrings."
    },
    {
      "id": "SupportVectorMachines",
      "type": "major",
      "parent": null,
      "description": "Supervised learning model for classification and regression analysis."
    },
    {
      "id": "Machine_Learning_Algorithms",
      "type": "major",
      "parent": null,
      "description": "Overview of algorithms in machine learning including kernel trick and SVMs."
    },
    {
      "id": "Kernel_Trick",
      "type": "subnode",
      "parent": "Machine_Learning_Algorithms",
      "description": "Method to derive algorithms like the kernel perceptron algorithm."
    },
    {
      "id": "Support_Vector_Machines",
      "type": "subnode",
      "parent": "Machine_Learning_Algorithms",
      "description": "Supervised learning algorithm known for its effectiveness in classification tasks."
    },
    {
      "id": "Margins_Intuition",
      "type": "subnode",
      "parent": "Support_Vector_Machines",
      "description": "Introduction to the concept of margins and confidence in predictions."
    },
    {
      "id": "Optimal_Margin_Classifier",
      "type": "subnode",
      "parent": "Support_Vector_Machines",
      "description": "Classifier that maximizes the margin between classes, leading into Lagrange duality discussion."
    },
    {
      "id": "Lagrange_Duality",
      "type": "subnode",
      "parent": "Optimal_Margin_Classifier",
      "description": "Theory for converting constrained optimization problems into dual forms."
    },
    {
      "id": "Kernels_in_SVMs",
      "type": "subnode",
      "parent": "Support_Vector_Machines",
      "description": "Technique allowing efficient application of SVMs in high-dimensional spaces."
    },
    {
      "id": "SMO_Algorithm",
      "type": "subnode",
      "parent": "Support_Vector_Machines",
      "description": "Algorithm that updates two Lagrange multipliers simultaneously to optimize the dual problem."
    },
    {
      "id": "Functional Margins",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Concept used to formalize confident classifications based on dot product of parameters and input."
    },
    {
      "id": "Geometric Margins",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Distance-based measure for confidence in classification predictions far from decision boundary."
    },
    {
      "id": "Decision Boundary",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Line or hyperplane separating different classes in a feature space."
    },
    {
      "id": "Separating Hyperplane",
      "type": "subnode",
      "parent": "Decision Boundary",
      "description": "Specific term for decision boundary used in SVMs and linear classification problems."
    },
    {
      "id": "Confidence in Predictions",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Measure of certainty about predictions based on distance from the decision boundary."
    },
    {
      "id": "Support Vector Machines (SVMs)",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Binary classification algorithm that maximizes the margin between classes."
    },
    {
      "id": "Notation for SVMs",
      "type": "subnode",
      "parent": "Support Vector Machines (SVMs)",
      "description": "Introduction to notation used in discussing SVMs, including parameters w and b."
    },
    {
      "id": "Functional Margin",
      "type": "subnode",
      "parent": "Support Vector Machines (SVMs)",
      "description": "Definition of the functional margin for a training example with respect to classifier parameters."
    },
    {
      "id": "Geometric Margin",
      "type": "subnode",
      "parent": "Support Vector Machines (SVMs)",
      "description": "Conceptual understanding of geometric margins in SVM context, not fully detailed here."
    },
    {
      "id": "Functional_Margin",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Definition and properties of functional margin for a linear classifier."
    },
    {
      "id": "Confidence_and_Prediction",
      "type": "subnode",
      "parent": "Functional_Margin",
      "description": "Relationship between functional margin and prediction accuracy."
    },
    {
      "id": "Scaling_Issue",
      "type": "subnode",
      "parent": "Functional_Margin",
      "description": "Problem with scaling the parameters of a linear classifier affecting the functional margin."
    },
    {
      "id": "Function_Margin_of_S",
      "type": "subnode",
      "parent": "Functional_Margin",
      "description": "Definition of function margin for a set of training examples."
    },
    {
      "id": "Geometric_Margins",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Introduction to geometric margins in machine learning context."
    },
    {
      "id": "VectorW",
      "type": "subnode",
      "parent": "DecisionBoundary",
      "description": "A vector orthogonal to the decision boundary and pointing towards positive class."
    },
    {
      "id": "DistanceToBoundary",
      "type": "subnode",
      "parent": "DecisionBoundary",
      "description": "The shortest distance from a point to the decision boundary."
    },
    {
      "id": "UnitVectorW",
      "type": "subnode",
      "parent": "VectorW",
      "description": "A unit vector in the direction of W, used for calculating distances."
    },
    {
      "id": "GeometricMargin",
      "type": "major",
      "parent": null,
      "description": "The perpendicular distance from a data point to the decision boundary multiplied by the class label."
    },
    {
      "id": "FunctionalMargin",
      "type": "subnode",
      "parent": "GeometricMargin",
      "description": "A measure of how confidently a model can classify a training example."
    },
    {
      "id": "Scaling_Parameters",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Discussion on the scaling of parameters w and b."
    },
    {
      "id": "Geometric_Margin",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Definition and importance of geometric margin in training sets."
    },
    {
      "id": "Maximize_Geometric_Margin",
      "type": "subnode",
      "parent": "Optimal_Margin_Classifier",
      "description": "Formulation of optimization problem to maximize geometric margin."
    },
    {
      "id": "Support Vector Machine (SVM)",
      "type": "major",
      "parent": null,
      "description": "A supervised learning model for classification and regression analysis."
    },
    {
      "id": "Optimization Problem in SVM",
      "type": "subnode",
      "parent": "Support Vector Machine (SVM)",
      "description": "The core mathematical problem to find the optimal hyperplane."
    },
    {
      "id": "Non-Convex Constraint",
      "type": "subnode",
      "parent": "Optimization Problem in SVM",
      "description": "Constraint that complicates optimization due to its non-convex nature."
    },
    {
      "id": "Scaling Constraint",
      "type": "subnode",
      "parent": "Optimization Problem in SVM",
      "description": "A constraint used to simplify the original problem by setting functional margin to 1."
    },
    {
      "id": "Support_Vector_Machines_SVM",
      "type": "subnode",
      "parent": "Machine_Learning_Optimization",
      "description": "Technique to find optimal margin classifiers through optimization."
    },
    {
      "id": "Convex_Quadratic_Objective",
      "type": "subnode",
      "parent": "Support_Vector_Machines_SVM",
      "description": "Objective function in SVMs that is a convex quadratic form."
    },
    {
      "id": "Linear_Constraints",
      "type": "subnode",
      "parent": "Support_Vector Machines_SVM",
      "description": "Constraints in the optimization problem are linear."
    },
    {
      "id": "Dual_Form_Optimization",
      "type": "subnode",
      "parent": "Lagrange_Duality",
      "description": "Alternative form of the original problem that can be easier to solve."
    },
    {
      "id": "Kernels_High_Dimensional_Spaces",
      "type": "subnode",
      "parent": "Lagrange_Duality",
      "description": "Use kernels in SVMs for efficient computation in high dimensions."
    },
    {
      "id": "ConstrainedOptimization",
      "type": "major",
      "parent": null,
      "description": "Generalization of Lagrange multipliers to include inequality constraints."
    },
    {
      "id": "LagrangeMultipliers",
      "type": "subnode",
      "parent": "ConstrainedOptimization",
      "description": "Coefficients used in the generalized Lagrangian for optimization problems."
    },
    {
      "id": "PrimalProblem",
      "type": "subnode",
      "parent": "ConstrainedOptimization",
      "description": "Minimizing a function subject to inequality and equality constraints."
    },
    {
      "id": "GeneralizedLagrangian",
      "type": "subnode",
      "parent": "PrimalProblem",
      "description": "Combination of objective function with Lagrange multipliers for constraints."
    },
    {
      "id": "ThetaP",
      "type": "subnode",
      "parent": "PrimalProblem",
      "description": "Function representing the maximum value of the generalized Lagrangian under primal constraints."
    },
    {
      "id": "Primal Problem",
      "type": "major",
      "parent": null,
      "description": "Original optimization problem in machine learning."
    },
    {
      "id": "Objective Function Primal",
      "type": "subnode",
      "parent": "Primal Problem",
      "description": "Function to be minimized or maximized in the primal problem."
    },
    {
      "id": "Theta P",
      "type": "subnode",
      "parent": "Primal Problem",
      "description": "Indicator function for the primal constraints satisfaction."
    },
    {
      "id": "Dual Problem",
      "type": "major",
      "parent": null,
      "description": "Optimization problem derived from the primal by exchanging max and min operations."
    },
    {
      "id": "Objective Function Dual",
      "type": "subnode",
      "parent": "Dual Problem",
      "description": "Function to be maximized in the dual problem."
    },
    {
      "id": "Theta D",
      "type": "subnode",
      "parent": "Dual Problem",
      "description": "Indicator function for the dual constraints satisfaction."
    },
    {
      "id": "Relationship Between Primal and Dual",
      "type": "major",
      "parent": null,
      "description": "Theoretical relationship between primal and dual problems in terms of their optimal values."
    },
    {
      "id": "Optimization_Problems",
      "type": "subnode",
      "parent": "Machine_Learning",
      "description": "Problems involving finding the best solution in a set of possible solutions."
    },
    {
      "id": "Primal_Dual_Pairing",
      "type": "subnode",
      "parent": "Optimization_Problems",
      "description": "Relationship between primal and dual optimization problems."
    },
    {
      "id": "Dual_Problem_Solution",
      "type": "subnode",
      "parent": "Primal_Dual_Pairing",
      "description": "Conditions under which the solution to the dual problem equals that of the primal."
    },
    {
      "id": "Convexity_Conditions",
      "type": "subnode",
      "parent": "Dual_Problem_Solution",
      "description": "Requirements for functions f, g_i and h_i to ensure d* = p*."
    },
    {
      "id": "Feasibility_Constraints",
      "type": "subnode",
      "parent": "Dual_Problem_Solution",
      "description": "Conditions ensuring the constraints are strictly feasible."
    },
    {
      "id": "KKT_Conditions",
      "type": "subnode",
      "parent": "Dual_Problem_Solution",
      "description": "Set of necessary conditions for a solution to be optimal in constrained optimization problems."
    },
    {
      "id": "Dual_Complementarity",
      "type": "subnode",
      "parent": "KKT_Conditions",
      "description": "Condition indicating active constraints in the dual form of SVMs."
    },
    {
      "id": "Primal_Dual_Equivalence",
      "type": "subnode",
      "parent": "Optimization_Problems",
      "description": "Equivalence between primal and dual optimization problems in SVM context."
    },
    {
      "id": "Support_Vectors",
      "type": "subnode",
      "parent": "Dual_Complementarity",
      "description": "Training examples that influence the optimal solution due to active constraints."
    },
    {
      "id": "SupportVectorsConcept",
      "type": "major",
      "parent": null,
      "description": "Explanation of support vectors in machine learning problems."
    },
    {
      "id": "KernelTrickIntroduction",
      "type": "major",
      "parent": null,
      "description": "Preview of the kernel trick application in algorithms."
    },
    {
      "id": "LagrangianFormulation",
      "type": "major",
      "parent": null,
      "description": "Description and formulation of Lagrangian for optimization problem."
    },
    {
      "id": "DualProblemDerivation",
      "type": "subnode",
      "parent": "LagrangianFormulation",
      "description": "Steps to derive the dual form from the Lagrangian."
    },
    {
      "id": "Lagrangian Function",
      "type": "subnode",
      "parent": "Support Vector Machine (SVM)",
      "description": "Used to incorporate constraints into the optimization problem."
    },
    {
      "id": "Dual Optimization Problem",
      "type": "subnode",
      "parent": "Support Vector Machine (SVM)",
      "description": "Formulated by transforming the primal problem to a dual form for easier solving."
    },
    {
      "id": "KKT Conditions",
      "type": "subnode",
      "parent": "Support Vector Machine (SVM)",
      "description": "Conditions that must be satisfied at an optimal solution in constrained optimization problems."
    },
    {
      "id": "Optimal Parameters w and b",
      "type": "subnode",
      "parent": "Support Vector Machine (SVM)",
      "description": "Parameters derived from solving the dual problem to define the decision boundary."
    },
    {
      "id": "Machine Learning Models",
      "type": "major",
      "parent": null,
      "description": "Overview of various machine learning models and their applications."
    },
    {
      "id": "Support Vector Machines (SVM)",
      "type": "subnode",
      "parent": "Machine Learning Models",
      "description": "A model that efficiently learns in high-dimensional spaces using kernels."
    },
    {
      "id": "Optimal Parameters Calculation",
      "type": "subnode",
      "parent": "Support Vector Machines (SVM)",
      "description": "Calculation of optimal parameters w and b for SVM."
    },
    {
      "id": "Dual Form Optimization",
      "type": "subnode",
      "parent": "Optimal Parameters Calculation",
      "description": "Derivation using dual form to find optimal alpha values."
    },
    {
      "id": "Prediction with Inner Products",
      "type": "subnode",
      "parent": "Optimal Parameters Calculation",
      "description": "Using inner products for prediction based on support vectors."
    },
    {
      "id": "Regularization and Non-Separable Data",
      "type": "subnode",
      "parent": "Machine Learning Models",
      "description": "Handling non-separable data with regularization techniques."
    },
    {
      "id": "Support Vector Machines",
      "type": "major",
      "parent": null,
      "description": "Algorithm for learning in high-dimensional spaces."
    },
    {
      "id": "Regularization and Non-Separable Case",
      "type": "subnode",
      "parent": "Support Vector Machines",
      "description": "Handling non-linearly separable datasets with regularization."
    },
    {
      "id": "Linear Separability Assumption",
      "type": "subnode",
      "parent": "Regularization and Non-Separable Case",
      "description": "Initial SVM derivation assumes linear separability of data."
    },
    {
      "id": "Outlier Sensitivity",
      "type": "subnode",
      "parent": "Regularization and Non-Separable Case",
      "description": "SVMs can be sensitive to outliers in the dataset."
    },
    {
      "id": "L1 Regularization",
      "type": "subnode",
      "parent": "Regularization and Non-Separable Case",
      "description": "Introduces L1 regularization to handle non-linear separability and reduce outlier impact."
    },
    {
      "id": "Optimization Problem",
      "type": "subnode",
      "parent": "L1 Regularization",
      "description": "Formulates the optimization problem with L1 regularization term."
    },
    {
      "id": "Objective Function",
      "type": "subnode",
      "parent": "Optimization Problem",
      "description": "Minimizes a function that includes both norm of w and sum of slack variables weighted by C."
    },
    {
      "id": "Slack Variables",
      "type": "subnode",
      "parent": "Objective Function",
      "description": "Introduces slack variables to allow for some data points to be within the margin."
    },
    {
      "id": "Parameter C",
      "type": "subnode",
      "parent": "Optimization Problem",
      "description": "Controls trade-off between maximizing margin and minimizing classification errors."
    },
    {
      "id": "Lagrangian Formulation",
      "type": "subnode",
      "parent": "Regularization and Non-Separable Case",
      "description": "Derives the Lagrangian to solve the optimization problem with constraints."
    },
    {
      "id": "Dual_Problem_Formulation",
      "type": "subnode",
      "parent": "Support_Vector_Machines_SVM",
      "description": "Formulating the optimization problem in SVMs."
    },
    {
      "id": "Lagrange_Multipliers",
      "type": "subnode",
      "parent": "Dual_Problem_Formulation",
      "description": "Use of Lagrange multipliers to solve constrained optimization problems."
    },
    {
      "id": "Sequential_Minimal_Optimization_SMO",
      "type": "subnode",
      "parent": "Support_Vector_Machines_SVM",
      "description": "Efficient algorithm for solving SVM's dual problem."
    },
    {
      "id": "Coordinate_Ascend_Algorithm",
      "type": "major",
      "parent": null,
      "description": "Optimization technique used in various machine learning contexts."
    },
    {
      "id": "Unconstrained_Optimization_Problem",
      "type": "subnode",
      "parent": "Coordinate_Ascend_Algorithm",
      "description": "Maximizing a function over multiple parameters without constraints."
    },
    {
      "id": "Gradient_Ascend_Newtons_Method",
      "type": "subnode",
      "parent": "Coordinate_Ascend_Algorithm",
      "description": "Alternative optimization methods compared to coordinate ascent."
    },
    {
      "id": "Coordinate_Ascend_Method",
      "type": "subnode",
      "parent": "Machine_Learning_Optimization",
      "description": "A method for optimizing functions by moving along one axis at a time."
    },
    {
      "id": "Support_Vector_Machines_SVMs",
      "type": "subnode",
      "parent": "Machine_Learning_Optimization",
      "description": "Models that use support vectors to classify data in high-dimensional spaces."
    },
    {
      "id": "SVM_Dual_Problem",
      "type": "subnode",
      "parent": "Support_Vector_Machines_SVMs",
      "description": "The dual form of the optimization problem for SVMs."
    },
    {
      "id": "Heuristic_Selection",
      "type": "subnode",
      "parent": "SMO_Algorithm",
      "description": "Process of selecting alpha pairs to optimize efficiency."
    },
    {
      "id": "Efficient_Update",
      "type": "subnode",
      "parent": "SMO_Algorithm",
      "description": "Derivation and explanation of efficient update mechanism."
    },
    {
      "id": "Constraints_Satisfaction",
      "type": "subnode",
      "parent": "Efficient_Update",
      "description": "Explanation of constraints satisfaction for alpha values."
    },
    {
      "id": "Alpha_Parameters",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Parameters α1 and α2 with constraints."
    },
    {
      "id": "Constraint_Equation",
      "type": "subnode",
      "parent": "Alpha_Parameters",
      "description": "Equation α1y^(1) + α2y^(2) = χ."
    },
    {
      "id": "Bounds_L_H",
      "type": "subnode",
      "parent": "Alpha_Parameters",
      "description": "Lower-bound L and upper-bound H for permissible values of α2."
    },
    {
      "id": "Objective_Function_W",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Objective function W(α) expressed in terms of α2 and other parameters."
    },
    {
      "id": "Quadratic_Formulation",
      "type": "subnode",
      "parent": "Objective_Function_W",
      "description": "W as a quadratic function in α2: aα2^2 + bα2 + c."
    },
    {
      "id": "Maximization_Process",
      "type": "subnode",
      "parent": "Objective_Function_W",
      "description": "Process of maximizing W by setting derivative to zero and solving."
    },
    {
      "id": "Sequential Minimal Optimization (SMO) Algorithm",
      "type": "subnode",
      "parent": "Support Vector Machines (SVM)",
      "description": "Efficient algorithm for solving the optimization problem in SVM."
    },
    {
      "id": "Alpha Updates",
      "type": "subnode",
      "parent": "Sequential Minimal Optimization (SMO) Algorithm",
      "description": "Process of updating alpha values within SMO."
    },
    {
      "id": "Deep Learning Introduction",
      "type": "major",
      "parent": null,
      "description": "Introduction to deep learning concepts and neural networks."
    },
    {
      "id": "Supervised Learning with Non-Linear Models",
      "type": "subnode",
      "parent": "Deep Learning Introduction",
      "description": "Exploration of non-linear models in supervised learning context."
    },
    {
      "id": "NonLinearModel",
      "type": "subnode",
      "parent": "MachineLearningModels",
      "description": "Abstract non-linear model used in machine learning problems."
    },
    {
      "id": "TrainingExamples",
      "type": "subnode",
      "parent": "NonLinearModel",
      "description": "Set of training examples used to define the cost function."
    },
    {
      "id": "RegressionProblems",
      "type": "subnode",
      "parent": "MachineLearningModels",
      "description": "Introduction to regression problems where output is a real number."
    },
    {
      "id": "LeastSquareCostFunction",
      "type": "subnode",
      "parent": "RegressionProblems",
      "description": "Definition of the least square cost function for individual examples."
    },
    {
      "id": "MeanSquaredLoss",
      "type": "subnode",
      "parent": "RegressionProblems",
      "description": "Definition and properties of the mean squared loss function over a dataset."
    },
    {
      "id": "BinaryClassification",
      "type": "subnode",
      "parent": "MachineLearningModels",
      "description": "Introduction to binary classification problems in machine learning."
    },
    {
      "id": "LogitFunction",
      "type": "subnode",
      "parent": "LogisticRegression",
      "description": "Linear combination of input features and weights before applying the logistic function."
    },
    {
      "id": "ProbabilityPrediction",
      "type": "subnode",
      "parent": "LogisticRegression",
      "description": "Transformation of logit to a probability using the sigmoid function."
    },
    {
      "id": "NegativeLikelihoodLoss",
      "type": "subnode",
      "parent": "LogisticRegression",
      "description": "Loss function used for logistic regression, based on negative log-likelihood."
    },
    {
      "id": "TotalLossFunction",
      "type": "subnode",
      "parent": "LogisticRegression",
      "description": "Average of individual training example losses over the entire dataset."
    },
    {
      "id": "MultiClassClassification",
      "type": "major",
      "parent": null,
      "description": "Extension of binary classification to multiple classes using softmax function."
    },
    {
      "id": "NegativeLogLikelihoodLossMulticlass",
      "type": "subnode",
      "parent": "MultiClassClassification",
      "description": "Loss function used in multi-class classification based on negative log-likelihood."
    },
    {
      "id": "Loss Function",
      "type": "major",
      "parent": "Machine Learning Concepts",
      "description": "Function measuring model performance for a single training example or average over all examples."
    },
    {
      "id": "Negative Log-Likelihood",
      "type": "subnode",
      "parent": "Loss Function",
      "description": "Specific form of loss function used in probabilistic models."
    },
    {
      "id": "Cross-Entropy Loss",
      "type": "subnode",
      "parent": "Negative Log-Likelihood",
      "description": "Simplified notation for negative log-likelihood loss function."
    },
    {
      "id": "Average Loss Function",
      "type": "subnode",
      "parent": "Loss Function",
      "description": "Total loss averaged over all training examples."
    },
    {
      "id": "Conditional Probabilistic Models",
      "type": "major",
      "parent": null,
      "description": "Models where the distribution of output depends on input features."
    },
    {
      "id": "Exponential Family Distribution",
      "type": "subnode",
      "parent": "Conditional Probabilistic Models",
      "description": "Distribution family for conditional probabilistic models with exponential form."
    },
    {
      "id": "Optimizers",
      "type": "major",
      "parent": "Pretraining_Phase",
      "description": "Discussion on the impact of optimizers on model generalization."
    },
    {
      "id": "Gradient Descent (GD)",
      "type": "subnode",
      "parent": "Optimizers",
      "description": "Algorithm that updates parameters in the direction of steepest descent of loss function."
    },
    {
      "id": "Stochastic Gradient Descent (SGD)",
      "type": "subnode",
      "parent": "Optimizers",
      "description": "Variant of GD using a single training example for each update to speed up convergence and reduce computational cost."
    },
    {
      "id": "Mini-batch Stochastic Gradient Descent",
      "type": "subnode",
      "parent": "Machine Learning Algorithms",
      "description": "Variant of SGD where gradients are computed over small batches of data for efficiency."
    },
    {
      "id": "Hyperparameters",
      "type": "subnode",
      "parent": "Stochastic Gradient Descent (SGD)",
      "description": "Parameters like learning rate and number of iterations that control the optimization process."
    },
    {
      "id": "Initialization",
      "type": "subnode",
      "parent": "Machine Learning Algorithms",
      "description": "Different initializations can lead to different generalization performance."
    },
    {
      "id": "Backpropagation Algorithm",
      "type": "subnode",
      "parent": "Machine Learning Algorithms",
      "description": "Method for efficiently computing gradients of loss functions in neural networks."
    },
    {
      "id": "NeuralNetworks",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Description and formal representation of neural networks used for prediction."
    },
    {
      "id": "RegressionProblem",
      "type": "subnode",
      "parent": "NeuralNetworks",
      "description": "Explanation of regression problems in the context of neural networks."
    },
    {
      "id": "SingleNeuronNN",
      "type": "subnode",
      "parent": "NeuralNetworks",
      "description": "Introduction to neural networks with a single neuron focusing on parametrization functions."
    },
    {
      "id": "HousingPricePrediction",
      "type": "subnode",
      "parent": "SingleNeuronNN",
      "description": "Example of using a single neuron network for predicting housing prices based on size."
    },
    {
      "id": "ReLUFunction",
      "type": "subnode",
      "parent": "SingleNeuronNN",
      "description": "Introduction to the ReLU function used in neural networks to prevent negative outputs."
    },
    {
      "id": "Neural_Networks",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "System of algorithms designed to recognize patterns and extract features from complex data sets."
    },
    {
      "id": "Activation_Functions",
      "type": "subnode",
      "parent": "Neural_Networks",
      "description": "Functions that introduce non-linearity in the model, such as ReLU."
    },
    {
      "id": "ReLU",
      "type": "subnode",
      "parent": "Activation_Functions",
      "description": "Rectified Linear Unit function used to activate neurons."
    },
    {
      "id": "Single_Neuron_Model",
      "type": "subnode",
      "parent": "Neural_Networks",
      "description": "A model with a single neuron and its mathematical representation."
    },
    {
      "id": "Bias_and_Weights",
      "type": "subnode",
      "parent": "Single_Neuron_Model",
      "description": "Explanation of bias term and weight vector in the context of neural networks."
    },
    {
      "id": "Stacking_Neurons",
      "type": "subnode",
      "parent": "Neural_Networks",
      "description": "Process of combining multiple neurons to form a more complex network."
    },
    {
      "id": "Complex_Neural_Networks",
      "type": "subnode",
      "parent": "Stacking_Neurons",
      "description": "Building neural networks with multiple layers and neurons."
    },
    {
      "id": "DerivedFeatures",
      "type": "subnode",
      "parent": "HousingPricePrediction",
      "description": "Description of family size, walkability, and school quality as derived features."
    },
    {
      "id": "FamilySize",
      "type": "subnode",
      "parent": "DerivedFeatures",
      "description": "Feature indicating the maximum number of people a house can accommodate."
    },
    {
      "id": "WalkableNeighborhood",
      "type": "subnode",
      "parent": "DerivedFeatures",
      "description": "Measure of how easily one can walk to local amenities such as grocery stores."
    },
    {
      "id": "SchoolQuality",
      "type": "subnode",
      "parent": "DerivedFeatures",
      "description": "Indicator of the quality of the elementary school in a neighborhood."
    },
    {
      "id": "InputFeatures",
      "type": "subnode",
      "parent": "NeuralNetworks",
      "description": "Set of input features (x1, x2, x3, x4) to the neural network."
    },
    {
      "id": "HiddenUnits",
      "type": "subnode",
      "parent": "NeuralNetworks",
      "description": "Intermediate variables a1, a2, a3 representing derived features as hidden units in the neural network."
    },
    {
      "id": "ReLUActivation",
      "type": "subnode",
      "parent": "NeuralNetworks",
      "description": "Rectified Linear Unit (ReLU) activation function used in neural networks."
    },
    {
      "id": "NeuralNetworksInspiration",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Explanation of how artificial neural networks are inspired by biological ones."
    },
    {
      "id": "TwoLayerNN",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "A simple model consisting of an input layer, a hidden layer, and an output layer."
    },
    {
      "id": "ParametersTheta",
      "type": "subnode",
      "parent": "NeuralNetworksInspiration",
      "description": "Details on the parameters θ used in neural networks."
    },
    {
      "id": "BiologicalSimilarity",
      "type": "subnode",
      "parent": "NeuralNetworksInspiration",
      "description": "Discussion on similarities and differences between artificial and biological neural networks."
    },
    {
      "id": "PriorKnowledge",
      "type": "subnode",
      "parent": "TwoLayerNN",
      "description": "Explanation of the role of prior knowledge in constructing neural network models."
    },
    {
      "id": "FullyConnectedNN",
      "type": "subnode",
      "parent": "MachineLearningModels",
      "description": "A type of neural network where each neuron is connected to every neuron in the previous layer."
    },
    {
      "id": "IntermediateVariables",
      "type": "subnode",
      "parent": "FullyConnectedNN",
      "description": "Variables like a1, a2, and a3 that are functions of input variables x1-x4."
    },
    {
      "id": "Parameterization",
      "type": "subnode",
      "parent": "FullyConnectedNN",
      "description": "Generic parameterization using weights w and biases b for each neuron."
    },
    {
      "id": "Vectorization",
      "type": "subnode",
      "parent": "MachineLearningModels",
      "description": "Use of matrix and vector notations to simplify expressions for neural networks and improve computational efficiency."
    },
    {
      "id": "VectorizationInNN",
      "type": "major",
      "parent": null,
      "description": "Overview of vectorization in neural networks for efficiency."
    },
    {
      "id": "SpeedPerspective",
      "type": "subnode",
      "parent": "VectorizationInNN",
      "description": "Discussion on the importance of speed in implementing neural networks."
    },
    {
      "id": "ParallelismGPUs",
      "type": "subnode",
      "parent": "VectorizationInNN",
      "description": "Role of parallel processing in GPUs for deep learning efficiency."
    },
    {
      "id": "MatrixAlgebra",
      "type": "subnode",
      "parent": "VectorizationInNN",
      "description": "Use of matrix algebra and optimized numerical packages in vectorization."
    },
    {
      "id": "TwoLayerNetwork",
      "type": "subnode",
      "parent": "VectorizationInNN",
      "description": "Definition and vectorization of a two-layer fully-connected neural network."
    },
    {
      "id": "WeightMatrices",
      "type": "subnode",
      "parent": "NeuralNetworks",
      "description": "Matrix representation of weights in a neural network layer."
    },
    {
      "id": "BiasVectors",
      "type": "subnode",
      "parent": "NeuralNetworks",
      "description": "Vector representation of biases in a neural network layer."
    },
    {
      "id": "ActivationFunctions",
      "type": "subnode",
      "parent": "NeuralNetworks",
      "description": "Non-linear functions applied element-wise to the output of a neuron or layer."
    },
    {
      "id": "LayerArchitecture",
      "type": "subnode",
      "parent": "NeuralNetworks",
      "description": "Structure of layers including input, hidden, and output layers in a neural network."
    },
    {
      "id": "Multi-layer Neural Networks",
      "type": "major",
      "parent": null,
      "description": "A neural network with multiple layers of neurons connected fully to each other."
    },
    {
      "id": "Weight Matrices and Biases",
      "type": "subnode",
      "parent": "Multi-layer Neural Networks",
      "description": "Matrices and vectors that define the connections and biases in a multi-layer network."
    },
    {
      "id": "ReLU Activation Function",
      "type": "subnode",
      "parent": "Multi-layer Neural Networks",
      "description": "A non-linear activation function used to introduce non-linearity into the model."
    },
    {
      "id": "Total Number of Neurons",
      "type": "subnode",
      "parent": "Multi-layer Neural Networks",
      "description": "The sum of neurons across all layers in a multi-layer network."
    },
    {
      "id": "Total Parameters Count",
      "type": "subnode",
      "parent": "Multi-layer Neural Networks",
      "description": "Sum of weights and biases across all layers."
    },
    {
      "id": "Notational Consistency",
      "type": "subnode",
      "parent": "Multi-layer Neural Networks",
      "description": "Consistent notation for inputs and outputs in multi-layer networks."
    },
    {
      "id": "Other Activation Functions",
      "type": "major",
      "parent": null,
      "description": "Alternative non-linear functions used instead of ReLU in neural networks."
    },
    {
      "id": "TanhFunction",
      "type": "subnode",
      "parent": "ActivationFunctions",
      "description": "Hyperbolic tangent function, similar to sigmoid but ranges from -1 to 1."
    },
    {
      "id": "LeakyReLUFunction",
      "type": "subnode",
      "parent": "ActivationFunctions",
      "description": "Variant of ReLU with a small slope for negative inputs."
    },
    {
      "id": "GELUFunction",
      "type": "subnode",
      "parent": "ActivationFunctions",
      "description": "Gaussian Error Linear Unit, smooth non-linear function used in NLP models."
    },
    {
      "id": "SoftplusFunction",
      "type": "subnode",
      "parent": "ActivationFunctions",
      "description": "Smoothed ReLU variant with a proper second-order derivative."
    },
    {
      "id": "IdentityFunction",
      "type": "subnode",
      "parent": "ActivationFunctions",
      "description": "Linear function where output is equal to input, not commonly used in neural networks."
    },
    {
      "id": "Feature_Engineering",
      "type": "subnode",
      "parent": "Machine_Learning",
      "description": "Process of selecting and transforming raw data into features for use in machine learning models."
    },
    {
      "id": "Deep_Learning",
      "type": "subnode",
      "parent": "Machine_Learning",
      "description": "Subfield of machine learning that uses neural networks to learn representations from data."
    },
    {
      "id": "Feature_Maps",
      "type": "subnode",
      "parent": "Deep_Learning",
      "description": "Functions used by deep learning models to transform input into a useful representation."
    },
    {
      "id": "Linear_Model",
      "type": "subnode",
      "parent": "Feature_Maps",
      "description": "Model used to predict outcomes based on features generated by a neural network."
    },
    {
      "id": "Deep Learning Representations",
      "type": "major",
      "parent": null,
      "description": "Discusses how neural networks discover features for prediction."
    },
    {
      "id": "House Price Prediction Example",
      "type": "subnode",
      "parent": "Deep Learning Representations",
      "description": "Illustrates use of fully-connected network in predicting house prices."
    },
    {
      "id": "Feature Discovery",
      "type": "subnode",
      "parent": "Deep Learning Representations",
      "description": "Explains how neural networks automatically discover useful features."
    },
    {
      "id": "Black Box Nature",
      "type": "subnode",
      "parent": "Deep Learning Representations",
      "description": "Highlights difficulty in interpreting the discovered features by humans."
    },
    {
      "id": "Modern Neural Network Modules",
      "type": "major",
      "parent": null,
      "description": "Introduces various building blocks of modern neural networks."
    },
    {
      "id": "Matrix Multiplication Module",
      "type": "subnode",
      "parent": "Modern Neural Network Modules",
      "description": "Describes the basic operation with parameters W and b."
    },
    {
      "id": "Nonlinear Activation Module",
      "type": "subnode",
      "parent": "Modern Neural Network Modules",
      "description": "Explains role of nonlinear activation in neural network architecture."
    },
    {
      "id": "MLP Composition",
      "type": "subnode",
      "parent": "Modern Neural Network Modules",
      "description": "Describes MLP as a composition of matrix multiplication and activation modules."
    },
    {
      "id": "MLPArchitecture",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Description of the architecture of a Multi-Layer Perceptron (MLP)."
    },
    {
      "id": "MatrixMultiplicationModule",
      "type": "subnode",
      "parent": "MLPArchitecture",
      "description": "Component of MLP that involves matrix multiplication and parameter sets."
    },
    {
      "id": "NonlinearActivationModule",
      "type": "subnode",
      "parent": "MLPArchitecture",
      "description": "Component of MLP involving nonlinear activation functions such as ReLU, sigmoid, etc."
    },
    {
      "id": "ResNetOverview",
      "type": "major",
      "parent": null,
      "description": "Introduction to Residual Networks (ResNets) and their architecture."
    },
    {
      "id": "ResidualBlockDefinition",
      "type": "subnode",
      "parent": "ResNetOverview",
      "description": "Simplified definition of a residual block in ResNet using matrix multiplication and activation functions."
    },
    {
      "id": "ResNetComposition",
      "type": "subnode",
      "parent": "ResNetOverview",
      "description": "Description of how ResNets are composed from multiple residual blocks followed by matrix multiplication."
    },
    {
      "id": "ResNetArchitecture",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Introduction to Residual Network architecture."
    },
    {
      "id": "ConvolutionalLayers",
      "type": "subnode",
      "parent": "ResNetArchitecture",
      "description": "Discussion on convolution layers used in ResNet."
    },
    {
      "id": "BatchNormalization",
      "type": "subnode",
      "parent": "ResNetArchitecture",
      "description": "Explanation of batch normalization technique."
    },
    {
      "id": "LayerNormalization",
      "type": "major",
      "parent": null,
      "description": "Technique for normalizing the layer inputs in neural networks."
    },
    {
      "id": "LN-SModule",
      "type": "subnode",
      "parent": "LayerNormalization",
      "description": "Definition and formula for the sub-module LN-S of layer normalization."
    },
    {
      "id": "TransformerArchitecture",
      "type": "major",
      "parent": null,
      "description": "Overview of Transformer architecture including ResNet-S and layer normalization."
    },
    {
      "id": "LN-S",
      "type": "subnode",
      "parent": "LayerNormalization",
      "description": "Standardized version of layer normalization before affine transformation."
    },
    {
      "id": "AffineTransformation",
      "type": "subnode",
      "parent": "LayerNormalization",
      "description": "Transforms the output to have desired mean and standard deviation using learnable parameters."
    },
    {
      "id": "LearnableParameters",
      "type": "subnode",
      "parent": "LayerNormalization",
      "description": "Scalars beta and gamma that are learned during training."
    },
    {
      "id": "ScalingInvariantProperty",
      "type": "major",
      "parent": null,
      "description": "Property of layer normalization making the model invariant to parameter scaling."
    },
    {
      "id": "MM_Wb",
      "type": "subnode",
      "parent": "ScalingInvariantProperty",
      "description": "Matrix multiplication with weights W and bias b used in the proof of scaling invariance."
    },
    {
      "id": "Normalization Techniques",
      "type": "major",
      "parent": null,
      "description": "Techniques for normalizing data in machine learning."
    },
    {
      "id": "Layer Normalization (LN)",
      "type": "subnode",
      "parent": "Normalization Techniques",
      "description": "Normalizes the inputs of each layer for stable training."
    },
    {
      "id": "Equation 7.43",
      "type": "subnode",
      "parent": "Layer Normalization (LN)",
      "description": "Mathematical representation of LN-S transformation."
    },
    {
      "id": "Equation 7.44-7.47",
      "type": "subnode",
      "parent": "Layer Normalization (LN)",
      "description": "Series of equations showing properties and transformations in layer normalization."
    },
    {
      "id": "Scale-Invariant Property",
      "type": "subnode",
      "parent": "Normalization Techniques",
      "description": "Property indicating network stability under weight scaling except for the last layer."
    },
    {
      "id": "Batch Normalization (BN)",
      "type": "subnode",
      "parent": "Normalization Techniques",
      "description": "Normalizes intermediate layers, commonly used in computer vision."
    },
    {
      "id": "Group Normalization",
      "type": "subnode",
      "parent": "Normalization Techniques",
      "description": "Alternative normalization method for fixed and controllable scaling."
    },
    {
      "id": "Convolutional Layers",
      "type": "major",
      "parent": null,
      "description": "Layers in neural networks designed to process data with a grid-like topology."
    },
    {
      "id": "1-D Convolution (Conv1D)",
      "type": "subnode",
      "parent": "Convolutional Layers",
      "description": "Simplified version of convolution layer for sequential data processing."
    },
    {
      "id": "Convolutional_Neural_Networks",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Introduction to convolutional neural networks in machine learning."
    },
    {
      "id": "1D_Convolution",
      "type": "subnode",
      "parent": "Convolutional_Neural_Networks",
      "description": "Explanation of 1-dimensional convolution layers used in NLP."
    },
    {
      "id": "Simplified_1D_Convolution",
      "type": "subnode",
      "parent": "1D_Convolution",
      "description": "Description of a simplified version of the 1-D convolution layer, Conv1D-S."
    },
    {
      "id": "Filter_Vector",
      "type": "subnode",
      "parent": "Simplified_1D_Convolution",
      "description": "Definition and properties of the filter vector used in Conv1D-S."
    },
    {
      "id": "Bias_Scalar",
      "type": "subnode",
      "parent": "Simplified_1D_Convolution",
      "description": "Explanation of the bias scalar parameter in Conv1D-S."
    },
    {
      "id": "Matrix_Multiplication",
      "type": "subnode",
      "parent": "Simplified_1D_Convolution",
      "description": "Representation of Conv1D-S as a matrix multiplication with shared parameters Qz."
    },
    {
      "id": "Convolutional_Layers",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Discussion on convolutional layers in neural networks."
    },
    {
      "id": "Parameter_Sharing",
      "type": "subnode",
      "parent": "Convolutional_Layers",
      "description": "Explanation of parameter sharing in convolutional layers."
    },
    {
      "id": "Efficiency_of_Convolution",
      "type": "subnode",
      "parent": "Convolutional_Layers",
      "description": "Comparison of efficiency between convolution and generic matrix multiplication."
    },
    {
      "id": "Conv1D_Channel_Variants",
      "type": "subnode",
      "parent": "Convolutional_Layers",
      "description": "Discussion on the variants of Conv1D layers with multiple channels."
    },
    {
      "id": "Conv1D-S",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "One-dimensional convolutional module with distinct parameters for each channel."
    },
    {
      "id": "TotalParametersConv1D",
      "type": "subnode",
      "parent": "Conv1D-S",
      "description": "Calculation of total number of parameters in Conv1D model."
    },
    {
      "id": "LinearMappingComparison",
      "type": "subnode",
      "parent": "TotalParametersConv1D",
      "description": "Comparison with generic linear mapping parameter count."
    },
    {
      "id": "ParameterTensorRepresentation",
      "type": "subnode",
      "parent": "TotalParametersConv1D",
      "description": "Three-dimensional tensor representation of parameters."
    },
    {
      "id": "Conv2D-S",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Two-dimensional convolutional module with distinct parameters for each channel."
    },
    {
      "id": "TotalParametersConv2D",
      "type": "subnode",
      "parent": "Conv2D-S",
      "description": "Calculation of total number of parameters in Conv2D model."
    },
    {
      "id": "Differentiable Circuit",
      "type": "subnode",
      "parent": "Backpropagation",
      "description": "Composition of arithmetic operations and elementary functions that can compute a real-valued function efficiently."
    },
    {
      "id": "Gradient Computation",
      "type": "subnode",
      "parent": "Backpropagation",
      "description": "Efficient computation of gradients for differentiable circuits in O(N) time."
    },
    {
      "id": "Theorem 7.4.1",
      "type": "subnode",
      "parent": "Backpropagation",
      "description": "Informal statement of the theorem regarding gradient computation efficiency."
    },
    {
      "id": "Chain Rule",
      "type": "subnode",
      "parent": "Gradient Computation",
      "description": "Mathematical rule for computing derivatives of composite functions."
    },
    {
      "id": "Auto-differentiation",
      "type": "subnode",
      "parent": "Backpropagation",
      "description": "Automatic computation of gradients in deep learning frameworks."
    },
    {
      "id": "Deep Learning Packages",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Software libraries and frameworks used for implementing deep learning models."
    },
    {
      "id": "MLPs (Multilayer Perceptrons)",
      "type": "subnode",
      "parent": "Neural Networks",
      "description": "Feedforward neural networks with multiple layers of perceptrons."
    },
    {
      "id": "Partial_Derivatives",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Discussion on partial derivatives and their complexities."
    },
    {
      "id": "Chain_Rule",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Explanation of the chain rule in calculus for auto-differentiation."
    },
    {
      "id": "Scalar_Variable_J",
      "type": "subnode",
      "parent": "Chain_Rule",
      "description": "Description of scalar variable J as a composition of functions f and g on z."
    },
    {
      "id": "Vectorized_Notation",
      "type": "subnode",
      "parent": "Chain_Rule",
      "description": "Explanation of the chain rule in vectorized notation for vectors z and u."
    },
    {
      "id": "Machine_Learning_Backward_Propagation",
      "type": "major",
      "parent": null,
      "description": "Overview of backward propagation in machine learning"
    },
    {
      "id": "Backward_Function_Linear_Map",
      "type": "subnode",
      "parent": "Machine_Learning_Backward_Propagation",
      "description": "Explanation of the backward function as a linear map"
    },
    {
      "id": "Jacobian_Matrix_Transpose",
      "type": "subnode",
      "parent": "Backward_Function_Linear_Map",
      "description": "The matrix in equation (7.54) is the transpose of Jacobian matrix"
    },
    {
      "id": "Complexity_of_Jacobian_Matrices",
      "type": "subnode",
      "parent": "Jacobian_Matrix_Transpose",
      "description": "Avoiding complications with Jacobian matrices for tensors"
    },
    {
      "id": "Equation_7.53_Usefulness",
      "type": "subnode",
      "parent": "Backward_Function_Linear_Map",
      "description": "Explanation of the convenience and effectiveness of equation (7.53)"
    },
    {
      "id": "Derivations_Section_7.4.3",
      "type": "subnode",
      "parent": "Equation_7.53_Usefulness",
      "description": "Use of equation (7.53) in derivations"
    },
    {
      "id": "Chain_Rule_Interpretation",
      "type": "subnode",
      "parent": "Machine_Learning_Backward_Propagation",
      "description": "Interpreting the chain rule for computing partial derivatives"
    },
    {
      "id": "Loss Function Composition",
      "type": "subnode",
      "parent": "Backpropagation",
      "description": "Abstract representation of loss functions as compositions of modules."
    },
    {
      "id": "Modules",
      "type": "subnode",
      "parent": "Backpropagation",
      "description": "Building blocks such as MM, σ, Conv2D, LN used in neural networks."
    },
    {
      "id": "BinaryClassificationProblem",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "A specific problem involving binary classification using a neural network model."
    },
    {
      "id": "MLPModelDefinition",
      "type": "subnode",
      "parent": "BinaryClassificationProblem",
      "description": "Definition of the MLP model used in the example, including parameters and layers."
    },
    {
      "id": "LossFunctionFormulation",
      "type": "subnode",
      "parent": "BinaryClassificationProblem",
      "description": "Description of how the loss function is formulated for a binary classification problem."
    },
    {
      "id": "ModulesInMLP",
      "type": "subnode",
      "parent": "MLPModelDefinition",
      "description": "Explanation of different modules in an MLP, including linear and activation layers."
    },
    {
      "id": "ParameterizationOfModules",
      "type": "subnode",
      "parent": "ModulesInMLP",
      "description": "Discussion on how each module is parameterized or may involve fixed operations."
    },
    {
      "id": "ForwardPass",
      "type": "subnode",
      "parent": "IntermediateVariables",
      "description": "Description of the forward pass, where intermediate variables are computed and saved."
    },
    {
      "id": "BackwardPass",
      "type": "subnode",
      "parent": "IntermediateVariables",
      "description": "Explanation of the backward pass, involving computation of derivatives w.r.t. parameters and intermediate variables."
    },
    {
      "id": "Machine_Learning_Backpropagation",
      "type": "major",
      "parent": null,
      "description": "Overview of backpropagation in machine learning."
    },
    {
      "id": "Chain_Rule_Application",
      "type": "subnode",
      "parent": "Machine_Learning_Backpropagation",
      "description": "Application of the chain rule to compute gradients efficiently."
    },
    {
      "id": "Efficient_Backward_Propagation",
      "type": "subnode",
      "parent": "Machine_Learning_Backpropagation",
      "description": "Discussion on the efficiency of backward propagation in neural networks."
    },
    {
      "id": "NeuralNetworksComposition",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Viewing neural networks as compositions of atomic operations."
    },
    {
      "id": "BackpropagationDiscussion",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Discussion on backpropagation and its role in computing gradients."
    },
    {
      "id": "ModulesInPractice",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Practical modularization of neural networks using basic modules like matrix multiplication."
    },
    {
      "id": "BackwardFunctionsBasics",
      "type": "major",
      "parent": null,
      "description": "Introduction to computing backward functions for basic modules in machine learning models."
    },
    {
      "id": "LossFunctionBackward",
      "type": "subnode",
      "parent": "BackwardFunctionsBasics",
      "description": "Details on the backward function computation for loss functions."
    },
    {
      "id": "BackwardFunctionWb",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Explanation of the backward function for parameters W and b."
    },
    {
      "id": "VectorizedNotation",
      "type": "subnode",
      "parent": "BackwardFunctionWb",
      "description": "Expression in vectorized form for clarity."
    },
    {
      "id": "EfficiencyConsiderations",
      "type": "subnode",
      "parent": "BackwardFunctionWb",
      "description": "Discussion on computational efficiency of the backward function."
    },
    {
      "id": "BackwardFunctionActivations",
      "type": "subnode",
      "parent": "ActivationFunctions",
      "description": "Explanation of the backward function for element-wise activations."
    },
    {
      "id": "Machine_Learning_Backward_Functions",
      "type": "major",
      "parent": null,
      "description": "Overview of backward functions in machine learning modules"
    },
    {
      "id": "Efficiency_of_Backward_Pass",
      "type": "subnode",
      "parent": "Machine_Learning_Backward_Functions",
      "description": "Discussion on the computational efficiency of the backward pass"
    },
    {
      "id": "Vectorized_Notation_Backward_Func",
      "type": "subnode",
      "parent": "Machine_Learning_Backward_Functions",
      "description": "Explanation of vectorized notation for backward functions"
    },
    {
      "id": "Squared_Loss_Backward",
      "type": "subnode",
      "parent": "Machine_Learning_Backward_Functions",
      "description": "Derivation and explanation of the backward function for squared loss"
    },
    {
      "id": "Logistic_Loss_Backward",
      "type": "subnode",
      "parent": "Machine_Learning_Backward_Functions",
      "description": "Explanation of the backward function for logistic loss"
    },
    {
      "id": "Cross_Entropy_Loss_Backward",
      "type": "subnode",
      "parent": "Machine_Learning_Backward_Functions",
      "description": "Explanation of the backward function for cross-entropy loss"
    },
    {
      "id": "Loss Functions",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Different loss functions used in machine learning models."
    },
    {
      "id": "Logistic Loss",
      "type": "subnode",
      "parent": "Loss Functions",
      "description": "The logistic loss function and its gradient calculation."
    },
    {
      "id": "Backpropagation for MLPs",
      "type": "major",
      "parent": null,
      "description": "Explanation of backpropagation in multi-layer perceptrons (MLPs)."
    },
    {
      "id": "Forward Pass",
      "type": "subnode",
      "parent": "Backpropagation for MLPs",
      "description": "Sequence of operations during the forward pass through an r-layer MLP."
    },
    {
      "id": "Intermediate Values Storage",
      "type": "subnode",
      "parent": "Backpropagation Algorithm",
      "description": "Storing intermediate values for gradient computation in the backward pass."
    },
    {
      "id": "Vectorization Over Training Examples",
      "type": "major",
      "parent": null,
      "description": "Techniques to vectorize neural network computations over multiple training examples."
    },
    {
      "id": "TrainingSetExamples",
      "type": "subnode",
      "parent": "MachineLearningBasics",
      "description": "Explanation of training set examples and their representation."
    },
    {
      "id": "MatrixNotation",
      "type": "subnode",
      "parent": "MachineLearningBasics",
      "description": "Use of matrix notation in representing multiple training examples."
    },
    {
      "id": "LayerActivations",
      "type": "subnode",
      "parent": "TrainingSetExamples",
      "description": "First-layer activations for each example using matrix operations."
    },
    {
      "id": "VectorizationTechniques",
      "type": "subnode",
      "parent": "MatrixNotation",
      "description": "Techniques to vectorize operations in machine learning models."
    },
    {
      "id": "Broadcasting",
      "type": "subnode",
      "parent": "VectorizationTechniques",
      "description": "Explanation of broadcasting technique for adding bias terms."
    },
    {
      "id": "LayerGeneralization",
      "type": "subnode",
      "parent": "MachineLearningBasics",
      "description": "Discussion on generalizing matricization approach to multiple layers."
    },
    {
      "id": "Machine Learning",
      "type": "major",
      "parent": null,
      "description": "Field of study focusing on algorithms that learn from and make predictions on data."
    },
    {
      "id": "Matricization Approach",
      "type": "subnode",
      "parent": "Machine Learning",
      "description": "Technique for organizing data in matrix form to facilitate multi-layer neural network implementation."
    },
    {
      "id": "Data Matrix Representation",
      "type": "subnode",
      "parent": "Matricization Approach",
      "description": "Representation of data points as rows in a matrix, differing from theoretical notation."
    },
    {
      "id": "Conversion Between Representations",
      "type": "subnode",
      "parent": "Matricization Approach",
      "description": "Process to convert between row-major and column-major representations in deep learning implementations."
    },
    {
      "id": "Training Loss Function",
      "type": "subnode",
      "parent": "Generalization",
      "description": "Function used during training to minimize error between predicted and actual outcomes."
    },
    {
      "id": "Training_Loss",
      "type": "subnode",
      "parent": "Loss_Functions",
      "description": "Evaluation metric for training data, often mean squared error."
    },
    {
      "id": "Test_Error",
      "type": "subnode",
      "parent": "Loss_Functions",
      "description": "Most important evaluation metric on unseen test examples."
    },
    {
      "id": "Empirical_Distribution",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Distribution based on the training dataset, denoted by ˜Δ."
    },
    {
      "id": "Population_Distribution",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "True distribution of data from which test examples are drawn, denoted by D."
    },
    {
      "id": "Training_Data_Set",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Data used to train the model, seen during training."
    },
    {
      "id": "Test_Data_Set",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Unseen data used for evaluating model performance."
    },
    {
      "id": "Learning_Settings",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Settings under which a model learns from data."
    },
    {
      "id": "Training_Distribution",
      "type": "subnode",
      "parent": "Learning_Settings",
      "description": "The distribution of training examples in machine learning."
    },
    {
      "id": "Test_Distribution",
      "type": "subnode",
      "parent": "Learning_Settings",
      "description": "The distribution of test examples used to evaluate model performance."
    },
    {
      "id": "Domain_Shift",
      "type": "subnode",
      "parent": "Learning_Settings",
      "description": "Situation where training and test distributions differ."
    },
    {
      "id": "Overfitting_Underfitting",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Concepts describing model performance on unseen data."
    },
    {
      "id": "Test_Error_Training_Error",
      "type": "subnode",
      "parent": "Overfitting_Underfitting",
      "description": "Difference between errors measured on training and test datasets."
    },
    {
      "id": "Generalization_Gap",
      "type": "subnode",
      "parent": "Test_Error_Training_Error",
      "description": "The difference between the test error and training error."
    },
    {
      "id": "Bias_Variance_Tradoff",
      "type": "major",
      "parent": null,
      "description": "Analysis of model performance in terms of bias and variance."
    },
    {
      "id": "Model_Parameterizations",
      "type": "subnode",
      "parent": "Bias_Variance_Tradoff",
      "description": "Impact of parameter choices on test error decomposition."
    },
    {
      "id": "Bias-Variance Tradeoff",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Formalization and discussion of bias-variance tradeoff in machine learning models."
    },
    {
      "id": "Double Descent Phenomenon",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Describes the unexpected decrease in test error after an initial increase with model complexity or data size."
    },
    {
      "id": "Training and Test Datasets",
      "type": "subnode",
      "parent": "Bias-Variance Tradeoff",
      "description": "Illustrates the use of training and test datasets to evaluate model performance."
    },
    {
      "id": "Linear Regression Example",
      "type": "subnode",
      "parent": "Bias-Variance Tradeoff",
      "description": "Provides an example using linear regression models on a quadratic function dataset."
    },
    {
      "id": "Model Complexity",
      "type": "subnode",
      "parent": "Bias-Variance Tradeoff",
      "description": "Discusses the impact of model complexity on underfitting and overfitting."
    },
    {
      "id": "Machine_Learning_Bias_Variance_Tradeoff",
      "type": "major",
      "parent": null,
      "description": "Exploration of bias and variance in machine learning hypothesis classes."
    },
    {
      "id": "Linear_Model_Failure",
      "type": "subnode",
      "parent": "Machine_Learning_Bias_Variance_Tradeoff",
      "description": "Failure of linear models to capture data structure despite large training sets."
    },
    {
      "id": "Bias_Definition",
      "type": "subnode",
      "parent": "Machine_Learning_Bias_Variance_Tradeoff",
      "description": "Definition and implications of model bias in the context of fitting errors."
    },
    {
      "id": "5th_Degree_Polynomial_Failure",
      "type": "subnode",
      "parent": "Machine_Learning_Bias_Variance_Tradeoff",
      "description": "Failure of 5th-degree polynomials to generalize despite capturing training data well."
    },
    {
      "id": "Generalization_Error",
      "type": "subnode",
      "parent": "Machine_Learning_Bias_Variance_Tradeoff",
      "description": "Probability that a hypothesis will misclassify new data drawn from the same distribution."
    },
    {
      "id": "PolynomialFitting",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Discussion on fitting polynomials to data sets."
    },
    {
      "id": "Variance",
      "type": "subnode",
      "parent": "PolynomialFitting",
      "description": "Description of variance in model fitting procedures."
    },
    {
      "id": "BiasVsVarianceTradeoff",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Discussion on the trade-off between bias and variance in models."
    },
    {
      "id": "Test Error Decomposition",
      "type": "subnode",
      "parent": "Bias-Variance Tradeoff",
      "description": "Decomposes test error into bias and variance components."
    },
    {
      "id": "Mathematical Decomposition for Regression",
      "type": "major",
      "parent": null,
      "description": "Formal mathematical description of bias-variance tradeoff in regression problems."
    },
    {
      "id": "BiasVarianceTradeoff",
      "type": "subnode",
      "parent": "RegressionProblems",
      "description": "Exploration of the trade-off between model complexity and error types."
    },
    {
      "id": "TrainingDataset",
      "type": "subnode",
      "parent": "BiasVarianceTradeoff",
      "description": "Description of a training dataset for regression problems."
    },
    {
      "id": "TestExample",
      "type": "subnode",
      "parent": "BiasVarianceTradeoff",
      "description": "Explanation of how to use a test example in the context of model evaluation."
    },
    {
      "id": "MSE",
      "type": "subnode",
      "parent": "BiasVarianceTradeoff",
      "description": "Definition and calculation of Mean Squared Error (MSE) for evaluating models."
    },
    {
      "id": "Claim8.1.1",
      "type": "subnode",
      "parent": "BiasVarianceTradeoff",
      "description": "Mathematical claim used to decompose MSE into bias and variance terms."
    },
    {
      "id": "MSEDecomposition",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Breakdown of Mean Squared Error into bias and variance components."
    },
    {
      "id": "AverageModel",
      "type": "subnode",
      "parent": "MSEDecomposition",
      "description": "Definition and properties of the average model in machine learning theory."
    },
    {
      "id": "BiasTerm",
      "type": "subnode",
      "parent": "MSEDecomposition",
      "description": "Explanation of bias as a component of error due to model limitations."
    },
    {
      "id": "VarianceTerm",
      "type": "subnode",
      "parent": "MSEDecomposition",
      "description": "Explanation of variance as the variability in predictions across different datasets."
    },
    {
      "id": "Machine_Learning_Bias_Variance",
      "type": "major",
      "parent": null,
      "description": "Overview of bias and variance in machine learning models."
    },
    {
      "id": "Bias_Term",
      "type": "subnode",
      "parent": "Machine_Learning_Bias_Variance",
      "description": "Explains the concept of bias term in model approximation."
    },
    {
      "id": "Variance_Term",
      "type": "subnode",
      "parent": "Machine_Learning_Bias_Variance",
      "description": "Describes how variance captures errors due to dataset randomness."
    },
    {
      "id": "Noise_Prediction_Impact",
      "type": "subnode",
      "parent": "Machine_Learning_Bias_Variance",
      "description": "Discusses the impact of noise prediction limitations."
    },
    {
      "id": "Bias_Variance_Classification",
      "type": "subnode",
      "parent": "Machine_Learning_Bias_Variance",
      "description": "Notes on bias-variance decomposition for classification problems."
    },
    {
      "id": "Double_Descent_Phenomenon",
      "type": "major",
      "parent": null,
      "description": "Introduction to the double descent phenomenon in machine learning models."
    },
    {
      "id": "Model_Wise_Double_Descent",
      "type": "subnode",
      "parent": "Double_Descent_Phenomenon",
      "description": "Explains model-wise double descent observed in various ML models."
    },
    {
      "id": "Model-wise Double Descent",
      "type": "subnode",
      "parent": "Double Descent Phenomenon",
      "description": "Test error decreases, increases, then decreases again as model parameters exceed training data size."
    },
    {
      "id": "Sample-wise Double Descent",
      "type": "subnode",
      "parent": "Double Descent Phenomenon",
      "description": "Similar to model-wise but considers the effect of increasing sample size on test error."
    },
    {
      "id": "Overparameterized Models",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Models with more parameters than necessary, often used in deep learning."
    },
    {
      "id": "Historical Context",
      "type": "subnode",
      "parent": "Double Descent Phenomenon",
      "description": "Early work by Opper and recent popularization by Belkin et al., Hastie et al."
    },
    {
      "id": "Optimal Regularization",
      "type": "subnode",
      "parent": "Model-wise Double Descent",
      "description": "Improves performance by tuning regularization parameters effectively."
    },
    {
      "id": "Implicit Regularization",
      "type": "subnode",
      "parent": "Double Descent Phenomenon",
      "description": "Effect of optimizers like gradient descent in overparameterized models."
    },
    {
      "id": "Regularization Techniques",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Techniques to prevent overfitting such as L2 regularization."
    },
    {
      "id": "Parameter Count vs. Model Norm",
      "type": "subnode",
      "parent": "Model Complexity",
      "description": "Comparison between using number of parameters and norm as measures of model complexity."
    },
    {
      "id": "Linear Regression Setup",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Setup for linear regression with specific dataset and input/output configurations."
    },
    {
      "id": "Sample Complexity Bounds",
      "type": "major",
      "parent": null,
      "description": "Theoretical bounds on the number of samples required for learning algorithms to generalize well."
    },
    {
      "id": "Model Selection Methods",
      "type": "subnode",
      "parent": "Sample Complexity Bounds",
      "description": "Methods for selecting the appropriate model complexity based on training data performance."
    },
    {
      "id": "Generalization Error",
      "type": "subnode",
      "parent": "Sample Complexity Bounds",
      "description": "Discussion on how well a model performs on unseen data compared to training set accuracy."
    },
    {
      "id": "Learning Theory Proofs",
      "type": "major",
      "parent": null,
      "description": "Conditions and lemmas to prove learning algorithms work well."
    },
    {
      "id": "Union Bound Lemma",
      "type": "subnode",
      "parent": "Learning Theory Proofs",
      "description": "Probability bound for union of events."
    },
    {
      "id": "Hoeffding Inequality",
      "type": "subnode",
      "parent": "Learning Theory Proofs",
      "description": "Bound on deviation between sample mean and true probability."
    },
    {
      "id": "Training Set",
      "type": "subnode",
      "parent": "Binary Classification",
      "description": "Set of input-output pairs used for training the model."
    },
    {
      "id": "Binary_Classification",
      "type": "subnode",
      "parent": "Machine_Learning_Basics",
      "description": "Classification where labels are binary (0 or 1)."
    },
    {
      "id": "Hypothesis",
      "type": "subnode",
      "parent": "Binary_Classification",
      "description": "A function that maps input data to predicted labels."
    },
    {
      "id": "Training_Error",
      "type": "subnode",
      "parent": "Binary_Classification",
      "description": "Error calculated on the training dataset used to optimize model parameters."
    },
    {
      "id": "PAC_Assumptions",
      "type": "subnode",
      "parent": "Binary_Classification",
      "description": "Probably approximately correct assumptions in learning theory."
    },
    {
      "id": "Linear_Classification",
      "type": "subnode",
      "parent": "Machine_Learning_Basics",
      "description": "Classification using linear functions to separate data into classes."
    },
    {
      "id": "Empirical_Risk_Minimization",
      "type": "subnode",
      "parent": "Linear_Classification",
      "description": "Process of selecting a hypothesis with the smallest training error from a set of hypotheses."
    },
    {
      "id": "Hypothesis_Class",
      "type": "subnode",
      "parent": "Machine_Learning",
      "description": "Set of all classifiers considered by a learning algorithm, abstracting from specific parameterization."
    },
    {
      "id": "Finite_Hypothesis_Class",
      "type": "subnode",
      "parent": "Empirical_Risk_Minimization",
      "description": "Case where hypothesis class consists of finite number of hypotheses for easier analysis."
    },
    {
      "id": "Hoeffding_Inequality",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Statistical tool used to bound the probability that the observed error deviates from true error by more than a specified amount."
    },
    {
      "id": "Bernoulli_Random_Variables",
      "type": "subnode",
      "parent": "Generalization_Error",
      "description": "Random variables indicating whether a hypothesis misclassifies an example drawn from distribution D."
    },
    {
      "id": "Uniform_Convergence",
      "type": "major",
      "parent": "Generalization_Error_Bound",
      "description": "Property ensuring that training error closely approximates generalization error for all hypotheses in the hypothesis space."
    },
    {
      "id": "Training_Error_Generalization_Error_Difference",
      "type": "subnode",
      "parent": "Uniform_Convergence",
      "description": "Difference between training and generalization errors for a hypothesis set"
    },
    {
      "id": "Union_Bound_Application",
      "type": "subnode",
      "parent": "Uniform_Convergence",
      "description": "Application of the union bound to extend individual hypothesis error bounds to all hypotheses"
    },
    {
      "id": "Probability_Error_Bounds",
      "type": "subnode",
      "parent": "Training_Error_Generalization_Error_Difference",
      "description": "Bounds on the probability that training error differs from generalization error by more than gamma"
    },
    {
      "id": "Quantities_of_Interest",
      "type": "major",
      "parent": null,
      "description": "Three key quantities in machine learning: n (sample size), γ (error margin), and δ (probability of error)"
    },
    {
      "id": "Sample_Size_Calculation",
      "type": "subnode",
      "parent": "Quantities_of_Interest",
      "description": "Calculation to determine necessary sample size for a given probability of error bound"
    },
    {
      "id": "Error_Margin_Determination",
      "type": "subnode",
      "parent": "Quantities_of_Interest",
      "description": "Determining the margin γ based on desired confidence level and sample size"
    },
    {
      "id": "Machine_Learning_Theory",
      "type": "major",
      "parent": null,
      "description": "Theoretical foundations of machine learning including bounds and complexity."
    },
    {
      "id": "Generalization_Error_Bound",
      "type": "subnode",
      "parent": "Machine_Learning_Theory",
      "description": "Bound on how much worse ˆh can be compared to h* in terms of generalization error."
    },
    {
      "id": "Sample_Complexity",
      "type": "subnode",
      "parent": "Machine_Learning_Theory",
      "description": "Number of samples needed to achieve a certain level of performance with high probability."
    },
    {
      "id": "Hypothesis_Space_Size",
      "type": "subnode",
      "parent": "Sample_Complexity",
      "description": "The number of hypotheses impacts the sample complexity logarithmically."
    },
    {
      "id": "Best_Hypothesis",
      "type": "subnode",
      "parent": "Machine_Learning_Theory",
      "description": "The hypothesis with the lowest generalization error in the hypothesis space."
    },
    {
      "id": "Machine_Learning_Theorem",
      "type": "major",
      "parent": null,
      "description": "A theorem relating uniform convergence and generalization error in machine learning."
    },
    {
      "id": "Uniform_Convergence_Assumption",
      "type": "subnode",
      "parent": "Machine_Learning_Theorem",
      "description": "Assumption that the empirical risk is close to true risk with high probability."
    },
    {
      "id": "Bias_Variance_Tradeoff",
      "type": "subnode",
      "parent": "Machine_Learning_Theorem",
      "description": "Discusses the reconciliation of modern ML practice with classical bias-variance trade-off concepts."
    },
    {
      "id": "Hypothesis_Class_Switching",
      "type": "subnode",
      "parent": "Machine_Learning_Bias_Variance_Tradeoff",
      "description": "Discussion on switching from Ω to a larger hypothesis class Ω'."
    },
    {
      "id": "Bias_Decrease",
      "type": "subnode",
      "parent": "Hypothesis_Class_Switching",
      "description": "Explanation of how bias decreases when moving to a larger hypothesis class."
    },
    {
      "id": "Variance_Increase",
      "type": "subnode",
      "parent": "Hypothesis_Class_Switching",
      "description": "Discussion on the increase in variance with a larger hypothesis class."
    },
    {
      "id": "Sample_Complexity_Bound",
      "type": "subnode",
      "parent": "Machine_Learning_Bias_Variance_Tradeoff",
      "description": "Derivation of sample complexity bound for finite Ω classes."
    },
    {
      "id": "Infinite_Hypothesis_Classes",
      "type": "subnode",
      "parent": "Machine_Learning_Bias_Variance_Tradeoff",
      "description": "Consideration of hypothesis classes parameterized by real numbers and their implications."
    },
    {
      "id": "Hypothesis_Class_Size",
      "type": "subnode",
      "parent": "Sample_Complexity",
      "description": "Size of the hypothesis class in terms of parameters and bits used."
    },
    {
      "id": "Parameterization_Impact",
      "type": "subnode",
      "parent": "Sample_Complexity",
      "description": "Effect of model parameterization on sample complexity and learning guarantees."
    },
    {
      "id": "HypothesisClassParameterization",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Exploration of how different parameterizations can represent the same hypothesis class."
    },
    {
      "id": "LinearClassifierDefinition",
      "type": "subnode",
      "parent": "HypothesisClassParameterization",
      "description": "Definition and representation of linear classifiers with varying parameters."
    },
    {
      "id": "ShatteringConcept",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Explanation of the concept of shattering in hypothesis classes."
    },
    {
      "id": "VCDimensionIntroduction",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Definition and importance of Vapnik-Chervonenkis dimension for a hypothesis class."
    },
    {
      "id": "VC Dimension",
      "type": "major",
      "parent": null,
      "description": "Measure of the capacity of a statistical classification algorithm"
    },
    {
      "id": "Shattering",
      "type": "subnode",
      "parent": "VC Dimension",
      "description": "A set is shattered if every possible labeling can be achieved by some hypothesis in H"
    },
    {
      "id": "Vapnik's Theorem",
      "type": "major",
      "parent": null,
      "description": "Theorem linking VC dimension to uniform convergence with high probability"
    },
    {
      "id": "Uniform Convergence",
      "type": "subnode",
      "parent": "Vapnik's Theorem",
      "description": "Asymptotic property ensuring that empirical risk approximates true risk"
    },
    {
      "id": "Corollary of Vapnik's Theorem",
      "type": "major",
      "parent": null,
      "description": "Number of training examples needed for learning well is linear in VC dimension"
    },
    {
      "id": "Chapter9",
      "type": "major",
      "parent": null,
      "description": "Regularization and model selection in machine learning"
    },
    {
      "id": "ModelComplexity",
      "type": "subnode",
      "parent": "Regularization",
      "description": "Measured by number of parameters or function of parameters"
    },
    {
      "id": "RegularizerFunction",
      "type": "subnode",
      "parent": "Regularization",
      "description": "Additional term added to training loss to control complexity"
    },
    {
      "id": "TrainingLoss",
      "type": "subnode",
      "parent": "Regularization",
      "description": "Cost function that includes the regularizer term"
    },
    {
      "id": "RegularizationParameter",
      "type": "subnode",
      "parent": "Regularization",
      "description": "Controls the impact of regularization on model complexity"
    },
    {
      "id": "Regularized Loss Function",
      "type": "major",
      "parent": null,
      "description": "Combination of loss and regularizer to balance model fit and complexity."
    },
    {
      "id": "Loss J(θ)",
      "type": "subnode",
      "parent": "Regularized Loss Function",
      "description": "Measures how well the model fits the training data."
    },
    {
      "id": "Regularizer R(θ)",
      "type": "subnode",
      "parent": "Regularized Loss Function",
      "description": "Penalizes model complexity to prevent overfitting."
    },
    {
      "id": "Regularization Parameter λ",
      "type": "subnode",
      "parent": "Regularized Loss Function",
      "description": "Controls the trade-off between loss and regularizer."
    },
    {
      "id": "L2 Regularization",
      "type": "subnode",
      "parent": "Regularizer R(θ)",
      "description": "Encourages small model weights to reduce complexity."
    },
    {
      "id": "Weight Decay",
      "type": "subnode",
      "parent": "L2 Regularization",
      "description": "In deep learning, L2 regularization is equivalent to decaying the weights during training."
    },
    {
      "id": "Sparsity Inducing Regularization",
      "type": "subnode",
      "parent": "Regularizer R(θ)",
      "description": "Promotes models with fewer non-zero parameters based on prior belief of sparsity."
    },
    {
      "id": "Regularization in Machine Learning",
      "type": "major",
      "parent": null,
      "description": "Techniques to prevent overfitting by adding constraints on model parameters."
    },
    {
      "id": "Sparsity Regularization",
      "type": "subnode",
      "parent": "Regularization in Machine Learning",
      "description": "Encourages models to have fewer non-zero parameter values."
    },
    {
      "id": "L1 Norm (LASSO)",
      "type": "subnode",
      "parent": "Sparsity Regularization",
      "description": "Promotes sparsity by penalizing the absolute value of parameters."
    },
    {
      "id": "L2 Norm Regularization",
      "type": "subnode",
      "parent": "Regularization in Machine Learning",
      "description": "Penalizes the square of parameter values to reduce model complexity."
    },
    {
      "id": "Gradient Descent Incompatibility",
      "type": "subnode",
      "parent": "Sparsity Regularization",
      "description": "L1 norm is not differentiable, making it incompatible with gradient descent methods."
    },
    {
      "id": "Kernel Methods Compatibility",
      "type": "subnode",
      "parent": "Regularization in Machine Learning",
      "description": "L2 regularization works well with kernel methods due to compatibility issues with L1."
    },
    {
      "id": "Deep Learning Regularization Techniques",
      "type": "subnode",
      "parent": "Regularization in Machine Learning",
      "description": "Includes weight decay, dropout, data augmentation, and more."
    },
    {
      "id": "Regularization in Deep Learning",
      "type": "major",
      "parent": null,
      "description": "Overview of regularization techniques and concepts in deep learning."
    },
    {
      "id": "Explicit Regularization Techniques",
      "type": "subnode",
      "parent": "Regularization in Deep Learning",
      "description": "Techniques such as weight decay, dropout, data augmentation, spectral norm regularization, and Lipschitz regularization."
    },
    {
      "id": "Implicit Regularization Effect",
      "type": "subnode",
      "parent": "Regularization in Deep Learning",
      "description": "The impact of optimizers on model parameters beyond explicit regularization."
    },
    {
      "id": "Global Minima Diversity",
      "type": "subnode",
      "parent": "Implicit Regularization Effect",
      "description": "Different optimizers converge to different global minima with varying generalization performance."
    },
    {
      "id": "Optimizer Impact",
      "type": "subnode",
      "parent": "Implicit Regularization Effect",
      "description": "Optimizers can bias towards certain types of global minima, affecting model generalization."
    },
    {
      "id": "Global Minima",
      "type": "subnode",
      "parent": "Optimizers",
      "description": "Different global minima can have varying test performance."
    },
    {
      "id": "Learning Rate Schedules",
      "type": "subnode",
      "parent": "Optimizers",
      "description": "Impact of learning rate schedules on model generalization."
    },
    {
      "id": "Model Selection via Cross Validation",
      "type": "major",
      "parent": null,
      "description": "Process of selecting among different models using cross validation."
    },
    {
      "id": "Model Selection",
      "type": "major",
      "parent": null,
      "description": "Process of choosing the best model for a given task."
    },
    {
      "id": "Cross Validation",
      "type": "subnode",
      "parent": "Model Selection",
      "description": "Techniques for selecting and evaluating models in machine learning."
    },
    {
      "id": "Polynomial Regression Model",
      "type": "subnode",
      "parent": "Model Selection",
      "description": "Regression model using polynomial functions of the input variables."
    },
    {
      "id": "Bias and Variance Tradeoff",
      "type": "subnode",
      "parent": "Cross Validation",
      "description": "Balancing underfitting (high bias) and overfitting (high variance)."
    },
    {
      "id": "Model Set M",
      "type": "subnode",
      "parent": "Model Selection",
      "description": "Finite set of models to choose from."
    },
    {
      "id": "SVM",
      "type": "subnode",
      "parent": "Model Set M",
      "description": "Support Vector Machine for classification and regression analysis."
    },
    {
      "id": "Neural Network",
      "type": "subnode",
      "parent": "Model Set M",
      "description": "Artificial neural network for machine learning tasks."
    },
    {
      "id": "EmpiricalRiskMinimization",
      "type": "subnode",
      "parent": "MachineLearningAlgorithms",
      "description": "Process of minimizing empirical risk for model selection."
    },
    {
      "id": "CrossValidation",
      "type": "major",
      "parent": null,
      "description": "Technique to estimate the generalization error of models."
    },
    {
      "id": "HoldOutCrossValidation",
      "type": "subnode",
      "parent": "CrossValidation",
      "description": "Method involving splitting data into training and validation sets for model selection."
    },
    {
      "id": "TrainingSetS",
      "type": "subnode",
      "parent": "EmpiricalRiskMinimization",
      "description": "Dataset used to train models in empirical risk minimization."
    },
    {
      "id": "HypothesesHi",
      "type": "subnode",
      "parent": "EmpiricalRiskMinimization",
      "description": "Set of hypotheses generated from training different models on the dataset."
    },
    {
      "id": "TrainingError",
      "type": "subnode",
      "parent": "EmpiricalRiskMinimization",
      "description": "Measure of error based on the training set used for model selection."
    },
    {
      "id": "DegreeOfPolynomial",
      "type": "subnode",
      "parent": "EmpiricalRiskMinimization",
      "description": "Example parameter affecting model complexity and generalization ability."
    },
    {
      "id": "S_train",
      "type": "subnode",
      "parent": "HoldOutCrossValidation",
      "description": "Subset of the training data used for training models in cross validation."
    },
    {
      "id": "S_cv",
      "type": "subnode",
      "parent": "HoldOutCrossValidation",
      "description": "Subset of the training data used to validate model performance."
    },
    {
      "id": "ValidationError",
      "type": "subnode",
      "parent": "HoldOutCrossValidation",
      "description": "Measure of error based on validation set, estimating generalization ability of models."
    },
    {
      "id": "Machine_Learning_Techniques",
      "type": "major",
      "parent": null,
      "description": "Techniques used in machine learning for model evaluation and selection."
    },
    {
      "id": "Model_Selection",
      "type": "subnode",
      "parent": "Machine_Learning_Techniques",
      "description": "Process of choosing the best model based on validation set performance."
    },
    {
      "id": "Validation_Set_Size",
      "type": "subnode",
      "parent": "Model_Selection",
      "description": "Determining an appropriate size for the validation dataset."
    },
    {
      "id": "Cross_Validation",
      "type": "subnode",
      "parent": "Model_Selection",
      "description": "Technique to evaluate models by partitioning data into subsets."
    },
    {
      "id": "Hold_Out_Cross_Validation",
      "type": "subnode",
      "parent": "Cross_Validation",
      "description": "Method where a portion of the dataset is held out for validation purposes."
    },
    {
      "id": "k_Fold_Cross_Validation",
      "type": "subnode",
      "parent": "Cross_Validation",
      "description": "Technique that splits data into k partitions and iterates over these subsets for training and validation."
    },
    {
      "id": "Machine_Learning_Challenges",
      "type": "major",
      "parent": null,
      "description": "Challenges in machine learning when data is scarce."
    },
    {
      "id": "Leave_One_Out_Cross_Validation",
      "type": "subnode",
      "parent": "k_Fold_Cross_Validation",
      "description": "Special case of cross validation where one data point is left out for testing."
    },
    {
      "id": "Training_and_Testing_Process",
      "type": "subnode",
      "parent": "k_Fold_Cross_Validation",
      "description": "Process of training models on subsets and testing on remaining data."
    },
    {
      "id": "Leave-One-Out CV",
      "type": "subnode",
      "parent": "Cross Validation",
      "description": "A method of cross validation where one training example is held out at a time."
    },
    {
      "id": "Bayesian Statistics",
      "type": "major",
      "parent": null,
      "description": "An approach to statistics that treats parameters as random variables with prior distributions."
    },
    {
      "id": "MLE",
      "type": "subnode",
      "parent": "Bayesian Statistics",
      "description": "Maximum likelihood estimation for parameter fitting without considering θ as a random variable."
    },
    {
      "id": "Prior Distribution",
      "type": "subnode",
      "parent": "Bayesian Statistics",
      "description": "A probability distribution that represents prior beliefs about the parameters before observing data."
    },
    {
      "id": "Bayesian Machine Learning",
      "type": "major",
      "parent": null,
      "description": "Predictions made using posterior distributions on parameters."
    },
    {
      "id": "Posterior Distribution",
      "type": "subnode",
      "parent": "Bayesian Machine Learning",
      "description": "Probability distribution over parameters given the data."
    },
    {
      "id": "Bayes' Theorem",
      "type": "subnode",
      "parent": "Posterior Distribution",
      "description": "Formula for calculating posterior from likelihood and prior."
    },
    {
      "id": "Likelihood Function",
      "type": "subnode",
      "parent": "Bayes' Theorem",
      "description": "Function to maximize for parameter estimation"
    },
    {
      "id": "Prediction on New Data",
      "type": "subnode",
      "parent": "Posterior Distribution",
      "description": "Making predictions using posterior distribution for new inputs."
    },
    {
      "id": "Expected Value Prediction",
      "type": "subnode",
      "parent": "Prediction on New Data",
      "description": "Predicting the expected value of y given x and S."
    },
    {
      "id": "Fully Bayesian Prediction",
      "type": "subnode",
      "parent": "Bayesian Machine Learning",
      "description": "Averaging predictions over posterior distribution p(θ|S)."
    },
    {
      "id": "Computational Challenges",
      "type": "subnode",
      "parent": "Fully Bayesian Prediction",
      "description": "Difficulty in computing high-dimensional integrals for the posterior."
    },
    {
      "id": "BayesianInference",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Techniques for estimating parameters using prior knowledge and data."
    },
    {
      "id": "PosteriorApproximation",
      "type": "subnode",
      "parent": "BayesianInference",
      "description": "Methods to approximate the posterior distribution when exact computation is infeasible."
    },
    {
      "id": "MAPEstimate",
      "type": "subnode",
      "parent": "PosteriorApproximation",
      "description": "Finding a point estimate for parameters that maximizes the posterior probability."
    },
    {
      "id": "MLEvsMAP",
      "type": "subnode",
      "parent": "MAPEstimate",
      "description": "Comparison between maximum likelihood and MAP estimates, highlighting the role of prior distributions."
    },
    {
      "id": "PriorDistributions",
      "type": "subnode",
      "parent": "PosteriorApproximation",
      "description": "Selection of appropriate priors for parameter estimation, such as normal distribution with mean 0."
    },
    {
      "id": "UnsupervisedLearning",
      "type": "major",
      "parent": null,
      "description": "Techniques that learn from data without labeled responses."
    },
    {
      "id": "Clustering",
      "type": "subnode",
      "parent": "UnsupervisedLearning",
      "description": "Grouping a set of objects in such a way that objects in the same group are more similar to each other than to those in other groups."
    },
    {
      "id": "KMeansAlgorithm",
      "type": "subnode",
      "parent": "Clustering",
      "description": "A method for partitioning data into clusters based on minimizing the sum of squared distances from points to cluster centers."
    },
    {
      "id": "k-means_algorithm",
      "type": "major",
      "parent": null,
      "description": "Clustering algorithm that partitions data into k clusters."
    },
    {
      "id": "distortion_function",
      "type": "subnode",
      "parent": "k-means_algorithm",
      "description": "Measures sum of squared distances between examples and cluster centroids."
    },
    {
      "id": "centroid_initialization",
      "type": "subnode",
      "parent": "k-means_algorithm",
      "description": "Randomly selects k training examples as initial centroids."
    },
    {
      "id": "inner_loop_steps",
      "type": "subnode",
      "parent": "k-means_algorithm",
      "description": "Assigns each example to closest centroid and updates centroid positions."
    },
    {
      "id": "coordinate_descent",
      "type": "subnode",
      "parent": "distortion_function",
      "description": "Optimization technique used in k-means for minimizing distortion function."
    },
    {
      "id": "k-means Algorithm",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Clustering algorithm that partitions data into k clusters."
    },
    {
      "id": "Distortion Function J",
      "type": "subnode",
      "parent": "k-means Algorithm",
      "description": "Function measuring the quality of clustering; aims to minimize this value."
    },
    {
      "id": "Convergence in k-means",
      "type": "subnode",
      "parent": "k-means Algorithm",
      "description": "Process by which cluster centroids and assignments stabilize over iterations."
    },
    {
      "id": "EM Algorithms",
      "type": "major",
      "parent": null,
      "description": "Techniques for density estimation using iterative expectation-maximization process."
    },
    {
      "id": "Mixture of Gaussians",
      "type": "subnode",
      "parent": "EM Algorithms",
      "description": "Modeling data with a combination of Gaussian distributions to capture complex patterns."
    },
    {
      "id": "Unsupervised Learning",
      "type": "major",
      "parent": null,
      "description": "Learning from data without labels"
    },
    {
      "id": "Mixture of Gaussians Model",
      "type": "subnode",
      "parent": "Unsupervised Learning",
      "description": "Model using multiple Gaussian distributions for clustering"
    },
    {
      "id": "Joint Distribution",
      "type": "subnode",
      "parent": "Mixture of Gaussians Model",
      "description": "Distribution combining latent and observed variables"
    },
    {
      "id": "Latent Variables",
      "type": "subnode",
      "parent": "Joint Distribution",
      "description": "Hidden random variables influencing data generation"
    },
    {
      "id": "Parameter Estimation",
      "type": "subnode",
      "parent": "Mixture of Gaussians Model",
      "description": "Estimating parameters φ, μ, and Σ from data likelihood"
    },
    {
      "id": "Closed Form Solution",
      "type": "subnode",
      "parent": "Parameter Estimation",
      "description": "Infeasibility of deriving parameters directly from likelihood"
    },
    {
      "id": "DensityEstimation",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Techniques for estimating probability density functions from data."
    },
    {
      "id": "GaussianMixtureModel",
      "type": "subnode",
      "parent": "DensityEstimation",
      "description": "A probabilistic model that assumes all the data points are generated from a mixture of several Gaussian distributions with unknown parameters."
    },
    {
      "id": "EMAlgorithm",
      "type": "subnode",
      "parent": "GaussianMixtureModel",
      "description": "Iterative method for finding maximum likelihood or maximum a posteriori estimates of parameters in statistical models, where the model depends on unobserved latent variables."
    },
    {
      "id": "EM_Algorithm",
      "type": "major",
      "parent": "Optimization_Techniques",
      "description": "Iterative method for finding maximum likelihood or maximum a posteriori estimates in statistical models with latent variables."
    },
    {
      "id": "E_Step",
      "type": "subnode",
      "parent": "EM_Algorithm",
      "description": "Calculates posterior probabilities of latent variables given data and current parameter estimates."
    },
    {
      "id": "M_Step",
      "type": "subnode",
      "parent": "EM_Algorithm",
      "description": "Updates parameters to maximize the expected log-likelihood found in E-step."
    },
    {
      "id": "Gaussian_Mixture_Models",
      "type": "subnode",
      "parent": "E_Step",
      "description": "Model that uses Gaussian distributions to represent subpopulations within an overall population."
    },
    {
      "id": "Soft_Assignments",
      "type": "subnode",
      "parent": "EM_Algorithm",
      "description": "Assign probabilities to each latent variable rather than hard assignments."
    },
    {
      "id": "K_Means_Clustering",
      "type": "subnode",
      "parent": "EM_Algorithm",
      "description": "Algorithm for partitioning data into clusters, similar in concept but uses hard assignments."
    },
    {
      "id": "Convergence_Issues",
      "type": "subnode",
      "parent": "EM_Algorithm",
      "description": "Discussion on convergence properties of fitted value iteration compared to traditional value iteration."
    },
    {
      "id": "EM_Algorithm_Generalization",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "General view of the EM algorithm for estimation problems with latent variables."
    },
    {
      "id": "Jensens_Inequality",
      "type": "major",
      "parent": null,
      "description": "A fundamental inequality used in convex analysis and probability theory."
    },
    {
      "id": "Convex_Functions",
      "type": "subnode",
      "parent": "Jensens_Inequality",
      "description": "Functions where the second derivative is non-negative or positive definite for vector inputs."
    },
    {
      "id": "Theorem_Jensens_Inequality",
      "type": "subnode",
      "parent": "Jensens_Inequality",
      "description": "Statement of Jensen's inequality and its implications on expectations and random variables."
    },
    {
      "id": "Jensen's Inequality",
      "type": "major",
      "parent": null,
      "description": "Inequality relating expected values of convex functions and their arguments."
    },
    {
      "id": "Convex Function",
      "type": "subnode",
      "parent": "Jensen's Inequality",
      "description": "A function where the line segment between any two points on the graph lies above or on the graph."
    },
    {
      "id": "Concave Function",
      "type": "subnode",
      "parent": "Jensen's Inequality",
      "description": "Opposite of a convex function, lying below the line segments connecting its points."
    },
    {
      "id": "E[f(X)] vs f(E[X])",
      "type": "subnode",
      "parent": "Jensen's Inequality",
      "description": "Relationship between expected value of a function and function of an expected value for convex/concave functions."
    },
    {
      "id": "EM Algorithm",
      "type": "major",
      "parent": "Variational Inference",
      "description": "Iterative method to find maximum likelihood estimates in probabilistic models with latent variables."
    },
    {
      "id": "OptimizationChallenges",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Difficulties in optimizing parameters due to non-convex problems."
    },
    {
      "id": "EMAlgorithmIntroduction",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Efficient method for maximum likelihood estimation using E-step and M-step."
    },
    {
      "id": "LatentVariables",
      "type": "subnode",
      "parent": "EMAlgorithmIntroduction",
      "description": "Use of latent variables to simplify optimization problems."
    },
    {
      "id": "EStepMStepProcess",
      "type": "subnode",
      "parent": "EMAlgorithmIntroduction",
      "description": "Iterative process of constructing a lower-bound and optimizing it."
    },
    {
      "id": "SingleExampleOptimization",
      "type": "subnode",
      "parent": "LikelihoodFunction",
      "description": "Simplifying the problem by considering optimization for a single example first."
    },
    {
      "id": "SummationNotEssential",
      "type": "subnode",
      "parent": "SingleExampleOptimization",
      "description": "Explanation that summation is not essential and can be added later."
    },
    {
      "id": "ProbabilityDistributions",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Discussion on probability distributions used in ML models."
    },
    {
      "id": "JensensInequality",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Explanation of Jensen's inequality and its application in deriving lower bounds."
    },
    {
      "id": "LogLikelihoodBound",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Derivation of a lower bound for log likelihood using specific distributions Q."
    },
    {
      "id": "EvidenceLowerBoundELBO",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Definition and importance of ELBO in machine learning models."
    },
    {
      "id": "Log_Likelihood_Optimization",
      "type": "subnode",
      "parent": "EM_Algorithm",
      "description": "Process of maximizing the log-likelihood function to estimate parameters in probabilistic models."
    },
    {
      "id": "Evidence_Lower_Bound_(ELBO)",
      "type": "subnode",
      "parent": "Log_Likelihood_Optimization",
      "description": "Objective function used in variational inference and EM algorithm to approximate posterior distributions."
    },
    {
      "id": "Multiple_Examples_Consideration",
      "type": "subnode",
      "parent": "Evidence_Lower_Bound_(ELBO)",
      "description": "Extension of ELBO for multiple training examples, summing over individual example bounds."
    },
    {
      "id": "ExpectationMaximizationAlgorithm",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Iterative method for finding maximum likelihood or maximum a posteriori estimates of parameters in statistical models."
    },
    {
      "id": "EStep",
      "type": "subnode",
      "parent": "ExpectationMaximizationAlgorithm",
      "description": "Calculates the expected value of the log-likelihood, with respect to the current estimate of the hidden variables, given data and parameters."
    },
    {
      "id": "MStep",
      "type": "subnode",
      "parent": "ExpectationMaximizationAlgorithm",
      "description": "The M-step maximizes the expected log-likelihood found in the E-step with respect to model parameters."
    },
    {
      "id": "LogLikelihoodImprovement",
      "type": "subnode",
      "parent": "ExpectationMaximizationAlgorithm",
      "description": "Explanation of how EM monotonically improves the log-likelihood at each iteration."
    },
    {
      "id": "ELBO",
      "type": "subnode",
      "parent": "EStep",
      "description": "Evidence Lower Bound (ELBO) is used in the E-step to approximate the posterior distribution."
    },
    {
      "id": "ELBOExplanation",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Detailed explanation of Evidence Lower Bound (ELBO) definition and its various forms."
    },
    {
      "id": "AlternativeFormulationsOfELBO",
      "type": "subnode",
      "parent": "ELBOExplanation",
      "description": "Different mathematical formulations of ELBO including expectations and KL divergence."
    },
    {
      "id": "MarginalDistributionIndependence",
      "type": "subnode",
      "parent": "AlternativeFormulationsOfELBO",
      "description": "Discussion on the independence of marginal distribution from parameter theta in ELBO maximization."
    },
    {
      "id": "ConditionalLikelihoodSimplification",
      "type": "subnode",
      "parent": "MarginalDistributionIndependence",
      "description": "Explanation that maximizing conditional likelihood simplifies optimization problems compared to joint likelihood."
    },
    {
      "id": "EMAlgorithmOverview",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Introduction and overview of the Expectation-Maximization (EM) algorithm."
    },
    {
      "id": "MixtureOfGaussiansExample",
      "type": "subnode",
      "parent": "EMAlgorithmOverview",
      "description": "Application of EM to fitting parameters in a mixture of Gaussian distributions example."
    },
    {
      "id": "PhiParameterUpdate",
      "type": "subnode",
      "parent": "MStep",
      "description": "Specific derivation of the update rule for parameters φ_j in the M-step."
    },
    {
      "id": "MuParameterUpdate",
      "type": "subnode",
      "parent": "MStep",
      "description": "Update rule for μ_l parameter during M-step iteration."
    },
    {
      "id": "SigmaParameterUpdate",
      "type": "subnode",
      "parent": "MStep",
      "description": "Exercise left to reader: update rule for Σ_j parameter during M-step iteration."
    },
    {
      "id": "MStepUpdateRule",
      "type": "subnode",
      "parent": "ExpectationMaximizationAlgorithm",
      "description": "Derivation and update rule for the M-step in EM algorithm, focusing on parameter updates."
    },
    {
      "id": "LagrangianConstruction",
      "type": "subnode",
      "parent": "PhiParameterUpdate",
      "description": "Use of Lagrangian to handle constraints during parameter updates."
    },
    {
      "id": "VariationalInference",
      "type": "major",
      "parent": null,
      "description": "Techniques for approximating probability distributions in complex models using variational methods."
    },
    {
      "id": "VariationalAutoEncoder",
      "type": "subnode",
      "parent": "VariationalInference",
      "description": "Family of algorithms extending EM to more complex models parameterized by neural networks."
    },
    {
      "id": "Variational_Autoencoder",
      "type": "subnode",
      "parent": "Machine_Learning",
      "description": "A type of neural network architecture used to learn latent variable models."
    },
    {
      "id": "EM_Algorithms",
      "type": "subnode",
      "parent": "Machine_Learning",
      "description": "Expectation-Maximization algorithm for parameter estimation in statistical models."
    },
    {
      "id": "Variational_Inference",
      "type": "subnode",
      "parent": "Machine_Learning",
      "description": "Technique to approximate posterior distributions in Bayesian inference."
    },
    {
      "id": "Reparametrization_Trick",
      "type": "subnode",
      "parent": "Variational_Autoencoder",
      "description": "Method for sampling from a distribution using differentiable transformations."
    },
    {
      "id": "Posterior_Distribution",
      "type": "subnode",
      "parent": "Machine_Learning",
      "description": "Probability distribution of an unobserved variable given observed data."
    },
    {
      "id": "Variational Inference",
      "type": "major",
      "parent": null,
      "description": "Approximating true posterior distribution using a family of Q distributions."
    },
    {
      "id": "ELBO Lower Bound",
      "type": "subnode",
      "parent": "Variational Inference",
      "description": "Expected lower bound on log-likelihood used in variational inference optimization."
    },
    {
      "id": "Mean Field Assumption",
      "type": "subnode",
      "parent": "Variational Inference",
      "description": "Assumption that Q distribution can be decomposed into independent coordinates for discrete latent variables."
    },
    {
      "id": "Continuous Latent Variables",
      "type": "subnode",
      "parent": "Variational Inference",
      "description": "Handling continuous latent variables requires additional techniques beyond mean field assumptions."
    },
    {
      "id": "Latent_Variables",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Continuous latent variables used in probabilistic models."
    },
    {
      "id": "Gaussian_Distribution_Qi",
      "type": "subnode",
      "parent": "Latent_Variables",
      "description": "Distribution Qi modeled as a Gaussian with mean and variance functions of input x."
    },
    {
      "id": "Mean_and_Variance_Functions",
      "type": "subnode",
      "parent": "Gaussian_Distribution_Qi",
      "description": "Functions q(x;phi) and v(x;psi) that determine the mean and diagonal covariance matrix for Qi."
    },
    {
      "id": "Encoder_Decoder_Networks",
      "type": "subnode",
      "parent": "Variational_Autoencoder",
      "description": "Components q and v (encoder) and g(z;theta) (decoder) in variational autoencoders."
    },
    {
      "id": "ELBO_Optimization",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Process of optimizing the evidence lower bound for probabilistic models."
    },
    {
      "id": "ELBOOptimization",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Details on optimizing the Evidence Lower Bound (ELBO) in variational inference."
    },
    {
      "id": "QFormRequirements",
      "type": "subnode",
      "parent": "ELBOOptimization",
      "description": "Conditions and requirements for the form of Q_i in ELBO optimization."
    },
    {
      "id": "EfficientEvaluationOfELBO",
      "type": "subnode",
      "parent": "ELBOOptimization",
      "description": "Methods to efficiently evaluate the value of ELBO given fixed Q and theta."
    },
    {
      "id": "GaussianDistributionQ_i",
      "type": "subnode",
      "parent": "EfficientEvaluationOfELBO",
      "description": "Properties of Gaussian distribution for Q_i in efficient evaluation."
    },
    {
      "id": "GradientAscentOptimization",
      "type": "subnode",
      "parent": "ELBOOptimization",
      "description": "Using gradient ascent to optimize the ELBO parameters phi, psi, and theta."
    },
    {
      "id": "ELBO_Gradient_Computation",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Computation of the gradient of the ELBO with respect to parameters."
    },
    {
      "id": "Gradient_Simple_Case",
      "type": "subnode",
      "parent": "ELBO_Gradient_Computation",
      "description": "Simple case where computing gradients is straightforward."
    },
    {
      "id": "Complexity_in_Computing_Gradients",
      "type": "subnode",
      "parent": "ELBO_Gradient_Computation",
      "description": "Challenges in computing gradients over parameters phi and psi due to dependency on sampling distribution Q_i."
    },
    {
      "id": "Reparameterization_Trick",
      "type": "subnode",
      "parent": "Complexity_in_Computing_Gradients",
      "description": "Technique used to simplify gradient computation by re-parameterizing the random variable z^{(i)}."
    },
    {
      "id": "Mathematical_Formulation_Reparametrization",
      "type": "subnode",
      "parent": "Reparameterization_Trick",
      "description": "Detailed mathematical formulation of the re-parametrization trick for simplifying gradient calculations."
    },
    {
      "id": "Gradient_Estimation",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Estimating the gradient of a policy's log probability with respect to parameters."
    },
    {
      "id": "PCA_Method",
      "type": "major",
      "parent": null,
      "description": "Principal Components Analysis for dimensionality reduction in datasets."
    },
    {
      "id": "Dataset_Analysis",
      "type": "subnode",
      "parent": "PCA_Method",
      "description": "Analysis of dataset attributes using PCA."
    },
    {
      "id": "RedundancyDetection",
      "type": "major",
      "parent": null,
      "description": "Identifying and removing redundant data attributes in machine learning."
    },
    {
      "id": "CarAttributesExample",
      "type": "subnode",
      "parent": "RedundancyDetection",
      "description": "Illustration of redundancy with car speed measurements in mph and kph."
    },
    {
      "id": "PilotSurveyExample",
      "type": "subnode",
      "parent": "RedundancyDetection",
      "description": "Illustration of correlation between piloting skill and enjoyment among RC helicopter pilots."
    },
    {
      "id": "PCAAlgorithm",
      "type": "major",
      "parent": null,
      "description": "Principal Component Analysis for dimensionality reduction in data sets."
    },
    {
      "id": "DataNormalization",
      "type": "subnode",
      "parent": "PCAAlgorithm",
      "description": "Preprocessing step to normalize features before PCA application."
    },
    {
      "id": "Mean Normalization",
      "type": "subnode",
      "parent": "Normalization Techniques",
      "description": "Subtracting the mean from each feature to center the data around zero."
    },
    {
      "id": "Variance Scaling",
      "type": "subnode",
      "parent": "Normalization Techniques",
      "description": "Dividing by standard deviation to ensure unit variance for comparable attributes."
    },
    {
      "id": "Data Rescaling",
      "type": "subnode",
      "parent": "Normalization Techniques",
      "description": "Adjusting data scale when features are already on the same level."
    },
    {
      "id": "Major Axis of Variation",
      "type": "major",
      "parent": null,
      "description": "Finding the direction that maximizes variance for projected data."
    },
    {
      "id": "Projection Direction",
      "type": "subnode",
      "parent": "Major Axis of Variation",
      "description": "Selecting a unit vector to project data onto maximizing retained variance."
    },
    {
      "id": "PrincipalComponentAnalysis",
      "type": "major",
      "parent": null,
      "description": "Technique for reducing dimensionality while retaining variance."
    },
    {
      "id": "ProjectionOntoDirectionU",
      "type": "subnode",
      "parent": "PrincipalComponentAnalysis",
      "description": "Process of projecting data points onto a unit vector u to maximize variance."
    },
    {
      "id": "VarianceMaximization",
      "type": "subnode",
      "parent": "ProjectionOntoDirectionU",
      "description": "Objective is to find direction u that maximizes the variance of projections."
    },
    {
      "id": "EmpiricalCovarianceMatrix",
      "type": "subnode",
      "parent": "PrincipalComponentAnalysis",
      "description": "Matrix representing data covariance, used in PCA calculations."
    },
    {
      "id": "LagrangeMultipliersMethod",
      "type": "subnode",
      "parent": "VarianceMaximization",
      "description": "Mathematical method to solve optimization problems with constraints."
    },
    {
      "id": "PrincipalEigenvector",
      "type": "subnode",
      "parent": "VarianceMaximization",
      "description": "Direction u that maximizes variance, found as eigenvector of covariance matrix."
    },
    {
      "id": "kDimensionalSubspace",
      "type": "subnode",
      "parent": "PrincipalComponentAnalysis",
      "description": "Generalization to projecting data into k-dimensional subspace using top k eigenvectors."
    },
    {
      "id": "Principal Component Analysis (PCA)",
      "type": "major",
      "parent": "Dimensionality Reduction Techniques",
      "description": "A statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components."
    },
    {
      "id": "Dimensionality Reduction",
      "type": "subnode",
      "parent": "Principal Component Analysis (PCA)",
      "description": "Process of converting high-dimensional data into a lower-dimensional space."
    },
    {
      "id": "Eigenvectors and Eigenvalues",
      "type": "subnode",
      "parent": "Principal Component Analysis (PCA)",
      "description": "Key concepts in PCA for finding principal components."
    },
    {
      "id": "Orthogonal Basis",
      "type": "subnode",
      "parent": "Principal Component Analysis (PCA)",
      "description": "Set of orthogonal vectors used to represent data in a lower-dimensional space."
    },
    {
      "id": "Approximation Error Minimization",
      "type": "subnode",
      "parent": "Principal Component Analysis (PCA)",
      "description": "Method for deriving PCA by minimizing projection errors onto k-dimensional subspace."
    },
    {
      "id": "Data Visualization",
      "type": "subnode",
      "parent": "Principal Component Analysis (PCA)",
      "description": "Application of PCA to visualize high-dimensional data in 2D or 3D."
    },
    {
      "id": "Compression",
      "type": "subnode",
      "parent": "Principal Component Analysis (PCA)",
      "description": "Use of PCA for compressing high-dimensional data into lower dimensions."
    },
    {
      "id": "Dimensionality Reduction Techniques",
      "type": "major",
      "parent": null,
      "description": "Techniques to reduce the number of random variables under consideration."
    },
    {
      "id": "Plotting Similarity",
      "type": "subnode",
      "parent": "Principal Component Analysis (PCA)",
      "description": "Visualizing data points in PCA space to identify similar and clustered groups."
    },
    {
      "id": "Dimension Reduction Before Supervised Learning",
      "type": "subnode",
      "parent": "Principal Component Analysis (PCA)",
      "description": "Preprocessing step that reduces dimensionality before applying supervised learning algorithms."
    },
    {
      "id": "Noise Reduction",
      "type": "subnode",
      "parent": "Principal Component Analysis (PCA)",
      "description": "Using PCA to estimate intrinsic features from noisy data, such as estimating piloting skill from noisy measurements."
    },
    {
      "id": "Eigenfaces Method",
      "type": "subnode",
      "parent": "Principal Component Analysis (PCA)",
      "description": "Application of PCA in face recognition by reducing the dimensionality of image vectors."
    },
    {
      "id": "Independent Components Analysis (ICA)",
      "type": "major",
      "parent": null,
      "description": "A computational method for separating a multivariate signal into independent, non-Gaussian components assuming that the recorded signals are linear mixtures of some unknown latent variables."
    },
    {
      "id": "Machine_Learning_Topics",
      "type": "major",
      "parent": null,
      "description": "Various topics in machine learning."
    },
    {
      "id": "ICA",
      "type": "subnode",
      "parent": "Machine_Learning_Topics",
      "description": "Independent Component Analysis (ICA) for separating mixed signals."
    },
    {
      "id": "Cocktail_Party_Problem",
      "type": "subnode",
      "parent": "ICA",
      "description": "Example problem of ICA where multiple speakers' voices are separated from microphone recordings."
    },
    {
      "id": "Mixing_Matrix_A",
      "type": "subnode",
      "parent": "ICA",
      "description": "Matrix A that mixes independent sources into observed data."
    },
    {
      "id": "Unmixing_Matrix_W",
      "type": "subnode",
      "parent": "ICA",
      "description": "Inverse of mixing matrix used to recover original sources from mixed signals."
    },
    {
      "id": "ICA_Ambiguities",
      "type": "subnode",
      "parent": "ICA",
      "description": "Discussion on the ambiguities in recovering the unmixing matrix W without prior knowledge."
    },
    {
      "id": "ICA Ambiguities",
      "type": "major",
      "parent": null,
      "description": "Discusses inherent ambiguities in ICA recovery"
    },
    {
      "id": "Permutation Matrix",
      "type": "subnode",
      "parent": "ICA Ambiguities",
      "description": "Matrix that permutes the coordinates of a vector"
    },
    {
      "id": "Scaling Ambiguity",
      "type": "subnode",
      "parent": "ICA Ambiguities",
      "description": "Ambiguity in determining correct scaling factors for sources"
    },
    {
      "id": "Volume Adjustment",
      "type": "subnode",
      "parent": "Scaling Ambiguity",
      "description": "Adjusting volume does not affect the identification of sources"
    },
    {
      "id": "Scaling_Impact",
      "type": "subnode",
      "parent": "ICA_Ambiguities",
      "description": "Explains the effect of scaling a speaker's speech signal."
    },
    {
      "id": "Sign_Ignorance",
      "type": "subnode",
      "parent": "ICA_Ambiguities",
      "description": "Notes that sign changes in signals are irrelevant."
    },
    {
      "id": "Non_Gaussian_Sources",
      "type": "subnode",
      "parent": "ICA_Ambiguities",
      "description": "States that non-Gaussian sources resolve ambiguities."
    },
    {
      "id": "Gaussian_Data_Issue",
      "type": "subnode",
      "parent": "ICA_Ambiguities",
      "description": "Describes the problem with Gaussian data in ICA."
    },
    {
      "id": "Mixing_Matrix_Rotation",
      "type": "subnode",
      "parent": "Gaussian_Data_Issue",
      "description": "Explains how rotation of mixing matrix affects Gaussian data."
    },
    {
      "id": "ICAOnGaussianData",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Discussion on the limitations of Independent Component Analysis (ICA) when applied to Gaussian data due to rotational symmetry."
    },
    {
      "id": "RotationalSymmetry",
      "type": "subnode",
      "parent": "ICAOnGaussianData",
      "description": "Explanation that multivariate standard normal distribution is rotationally symmetric, making ICA impossible on Gaussian data."
    },
    {
      "id": "NonGaussianDataRecovery",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Discussion on the possibility of recovering independent sources from non-Gaussian data using sufficient data."
    },
    {
      "id": "LinearTransformationsEffect",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Explanation of how linear transformations affect densities in random variables."
    },
    {
      "id": "Density Transformation",
      "type": "major",
      "parent": null,
      "description": "Transformation of density functions under linear transformations."
    },
    {
      "id": "1D Example",
      "type": "subnode",
      "parent": "Density Transformation",
      "description": "Example in one dimension illustrating the transformation formula."
    },
    {
      "id": "General Case",
      "type": "subnode",
      "parent": "Density Transformation",
      "description": "Generalization to vector-valued distributions and higher dimensions."
    },
    {
      "id": "Volume Calculation",
      "type": "subnode",
      "parent": "General Case",
      "description": "Calculation of volume changes under linear transformations."
    },
    {
      "id": "ICA Algorithm",
      "type": "major",
      "parent": null,
      "description": "Derivation and interpretation of an ICA algorithm based on maximum likelihood estimation."
    },
    {
      "id": "Bell and Sejnowski's Method",
      "type": "subnode",
      "parent": "ICA Algorithm",
      "description": "Description of the ICA method by Bell and Sejnowski."
    },
    {
      "id": "ICAConcepts",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Introduction to Independent Component Analysis (ICA) principles and applications."
    },
    {
      "id": "JointDistributionModeling",
      "type": "subnode",
      "parent": "ICAConcepts",
      "description": "Description of modeling joint distribution as a product of marginals for independent sources."
    },
    {
      "id": "CumulativeDistributionFunction",
      "type": "subnode",
      "parent": "MaximumLikelihoodEstimation",
      "description": "Definition and properties of cumulative distribution function (CDF)."
    },
    {
      "id": "DataPreprocessing",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Process of preparing data for analysis by normalizing or transforming it."
    },
    {
      "id": "LogisticFunctionDerivative",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "The derivative of the logistic function and its properties."
    },
    {
      "id": "ModelParameters",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Description of model parameters such as matrix W."
    },
    {
      "id": "LogLikelihoodCalculation",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Computation of log likelihood for a given set of data and parameters."
    },
    {
      "id": "ConvergenceAndSourceRecovery",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Process after convergence to recover original sources from data."
    },
    {
      "id": "IndependenceAssumption",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Discussion on the independence assumption of training examples and its implications."
    },
    {
      "id": "Stochastic Gradient Ascent",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Optimization technique used for minimizing loss functions in machine learning models."
    },
    {
      "id": "Self-supervised Learning",
      "type": "major",
      "parent": null,
      "description": "Learning paradigm where the model learns from unlabelled data using implicit supervision from the input itself."
    },
    {
      "id": "Foundation Models",
      "type": "subnode",
      "parent": "Self-supervised Learning",
      "description": "Models pre-trained on large datasets that can be adapted to various downstream tasks with limited labeled data."
    },
    {
      "id": "Pretraining and Adaptation",
      "type": "subnode",
      "parent": "Foundation Models",
      "description": "Two-phase process involving training a model on unlabeled data followed by adapting it for specific tasks."
    },
    {
      "id": "Transfer_Learning",
      "type": "subnode",
      "parent": "Machine_Learning",
      "description": "Technique where a pre-trained model is adapted to new tasks."
    },
    {
      "id": "Pretraining_Phase",
      "type": "subnode",
      "parent": "Transfer_Learning",
      "description": "Initial training phase on large, unlabeled datasets."
    },
    {
      "id": "Adaptation_Phase",
      "type": "subnode",
      "parent": "Transfer_Learning",
      "description": "Customizing the pre-trained model for specific tasks."
    },
    {
      "id": "Unlabeled_Dataset",
      "type": "subnode",
      "parent": "Pretraining_Phase",
      "description": "Dataset used in initial training without labels."
    },
    {
      "id": "Labeled_Task_Dataset",
      "type": "subnode",
      "parent": "Adaptation_Phase",
      "description": "Dataset with labeled data for specific tasks."
    },
    {
      "id": "Model_Parameter_Theta",
      "type": "subnode",
      "parent": "Pretraining_Phase",
      "description": "Parameters of the model during pretraining."
    },
    {
      "id": "Embedding_Features",
      "type": "subnode",
      "parent": "Pretraining_Phase",
      "description": "Representations learned by the model from data."
    },
    {
      "id": "Self_Supervised_Loss",
      "type": "subnode",
      "parent": "Pretraining_Phase",
      "description": "Loss function using data point itself as supervision."
    },
    {
      "id": "Machine_Learning_Adaptation",
      "type": "major",
      "parent": null,
      "description": "Overview of machine learning adaptation techniques."
    },
    {
      "id": "Downstream_Task_Dataset",
      "type": "subnode",
      "parent": "Machine_Learning_Adaptation",
      "description": "Labeled dataset for a specific task with varying sizes."
    },
    {
      "id": "Zero_Shot_Learning",
      "type": "subnode",
      "parent": "Downstream_Task_Dataset",
      "description": "Scenario where no labeled data is available for the downstream task."
    },
    {
      "id": "Few_Shot_Learning",
      "type": "subnode",
      "parent": "Downstream_Task_Dataset",
      "description": "Situation with a small number of labeled examples, typically between 1 and 50."
    },
    {
      "id": "Adaptation_Algorithm",
      "type": "subnode",
      "parent": "Machine_Learning_Adaptation",
      "description": "Process that modifies a pretrained model to fit the downstream task."
    },
    {
      "id": "Linear_Probe_Method",
      "type": "subnode",
      "parent": "Adaptation_Algorithm",
      "description": "Uses a linear head on top of fixed pretrained model representations for prediction."
    },
    {
      "id": "Finetuning_Method",
      "type": "subnode",
      "parent": "Adaptation_Algorithm",
      "description": "Modifies both the downstream task parameters and the pretrained model parameters."
    },
    {
      "id": "Language_Problems_Adaptation",
      "type": "subnode",
      "parent": "Machine_Learning_Adaptation",
      "description": "Specific adaptation methods for language tasks discussed in 14.3.2."
    },
    {
      "id": "Machine_Learning_Adaptation_Methods",
      "type": "major",
      "parent": null,
      "description": "Methods for adapting pre-trained models to new tasks."
    },
    {
      "id": "Finetuning_Pretrained_Models",
      "type": "subnode",
      "parent": "Machine_Learning_Adaptation_Methods",
      "description": "Adjusting both weights and pretrained model parameters during downstream task training."
    },
    {
      "id": "Linear_Head_Initiation",
      "type": "subnode",
      "parent": "Finetuning_Pretrained_Models",
      "description": "Random initialization of the linear head weight vector w."
    },
    {
      "id": "Optimization_Objective",
      "type": "subnode",
      "parent": "Finetuning_Pretrained_Models",
      "description": "Objective function to minimize loss for downstream tasks."
    },
    {
      "id": "Pretraining_Methods_Computer_Vision",
      "type": "major",
      "parent": null,
      "description": "Techniques used in computer vision for pre-training models."
    },
    {
      "id": "Supervised_Pretraining",
      "type": "subnode",
      "parent": "Pretraining_Methods_Computer_Vision",
      "description": "Training with large labeled datasets to learn representations."
    },
    {
      "id": "Contrastive_Learning",
      "type": "subnode",
      "parent": "Pretraining_Methods_Computer_Vision",
      "description": "Self-supervised method using unlabeled data for representation learning."
    },
    {
      "id": "Self-Supervised Learning",
      "type": "major",
      "parent": null,
      "description": "Method using unlabeled data for pretraining."
    },
    {
      "id": "Representation Function",
      "type": "subnode",
      "parent": "Self-Supervised Learning",
      "description": "Function φθ(∘) maps images to representations."
    },
    {
      "id": "Positive Pair",
      "type": "subnode",
      "parent": "Representation Function",
      "description": "Augmentations of the same image, semantically related."
    },
    {
      "id": "Negative Pair",
      "type": "subnode",
      "parent": "Representation Function",
      "description": "Randomly selected augmentations from different images."
    },
    {
      "id": "Data Augmentation",
      "type": "subnode",
      "parent": "Self-Supervised Learning",
      "description": "Technique to generate variations of an image for training."
    },
    {
      "id": "Supervised Contrastive Algorithms",
      "type": "subnode",
      "parent": "Self-Supervised Learning",
      "description": "Use labeled data and class similarity for pretraining."
    },
    {
      "id": "ContrastiveLearning",
      "type": "subnode",
      "parent": "MachineLearning",
      "description": "Technique used to learn representations by contrasting positive and negative pairs."
    },
    {
      "id": "SIMCLRAlgorithm",
      "type": "subnode",
      "parent": "ContrastiveLearning",
      "description": "Specific algorithm based on contrastive learning for unsupervised representation learning."
    },
    {
      "id": "AugmentationTechniques",
      "type": "subnode",
      "parent": "SIMCLRAlgorithm",
      "description": "Methods for creating variations of input data to improve model robustness."
    },
    {
      "id": "PositivePairs",
      "type": "subnode",
      "parent": "ContrastiveLearning",
      "description": "Data pairs that should be close in the learned representation space."
    },
    {
      "id": "NegativePairs",
      "type": "subnode",
      "parent": "ContrastiveLearning",
      "description": "Data pairs that should be far apart in the learned representation space."
    },
    {
      "id": "Pretrained_Models",
      "type": "major",
      "parent": null,
      "description": "Overview of pretrained models in machine learning, particularly focusing on language processing."
    },
    {
      "id": "Natural_Language_Processing",
      "type": "subnode",
      "parent": "Pretrained_Models",
      "description": "Application of pretraining techniques to natural language processing tasks."
    },
    {
      "id": "Language_Models",
      "type": "subnode",
      "parent": "Natural_Language_Processing",
      "description": "Probabilistic models representing the probability distribution over sequences of words in a document."
    },
    {
      "id": "ConditionalProbabilityModeling",
      "type": "subnode",
      "parent": "MachineLearningModels",
      "description": "Modeling conditional probabilities in sequence prediction tasks."
    },
    {
      "id": "ParameterizedFunction",
      "type": "subnode",
      "parent": "ConditionalProbabilityModeling",
      "description": "Using parameterized functions to model conditional probabilities."
    },
    {
      "id": "Embeddings",
      "type": "subnode",
      "parent": "ParameterizedFunction",
      "description": "Introduction of word embeddings for numerical input handling."
    },
    {
      "id": "TransformerModel",
      "type": "subnode",
      "parent": "MachineLearningModels",
      "description": "Overview and use of the Transformer model in sequence prediction tasks."
    },
    {
      "id": "InputOutputInterface",
      "type": "subnode",
      "parent": "TransformerModel",
      "description": "Description of how input sequences are transformed into output logits using a Transformer model."
    },
    {
      "id": "Transformer_Models",
      "type": "major",
      "parent": null,
      "description": "Models that map inputs to outputs using a blackbox function."
    },
    {
      "id": "Conditional_Probability",
      "type": "subnode",
      "parent": "Transformer_Models",
      "description": "Probability of the next input given previous inputs."
    },
    {
      "id": "Training_Transformer",
      "type": "subnode",
      "parent": "Transformer_Models",
      "description": "Minimizing negative log-likelihood to train the model parameters."
    },
    {
      "id": "Autoregressive_Decoding",
      "type": "subnode",
      "parent": "Transformer_Models",
      "description": "Generating text sequentially using the conditional distribution from previous tokens."
    },
    {
      "id": "LanguageModels",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Models that generate text based on learned patterns from large datasets."
    },
    {
      "id": "TemperatureParameter",
      "type": "subnode",
      "parent": "ConditionalProbability",
      "description": "A parameter that adjusts the randomness or determinism of generated text."
    },
    {
      "id": "TextGeneration",
      "type": "subnode",
      "parent": "LanguageModels",
      "description": "The process of generating sequences of tokens based on learned probabilities."
    },
    {
      "id": "AdaptiveSampling",
      "type": "subnode",
      "parent": "ConditionalProbability",
      "description": "Adjusting the sampling method to control randomness in text generation."
    },
    {
      "id": "ModelAdaptation",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Techniques for adapting pretrained models to new tasks without fine-tuning."
    },
    {
      "id": "Finetuning",
      "type": "subnode",
      "parent": "ModelAdaptation",
      "description": "Adjusting model parameters based on a specific task's dataset."
    },
    {
      "id": "ZeroShotLearning",
      "type": "subnode",
      "parent": "ModelAdaptation",
      "description": "Using pretrained models for tasks without additional training data."
    },
    {
      "id": "InContextLearning",
      "type": "subnode",
      "parent": "ModelAdaptation",
      "description": "A method where the model learns from a small number of examples provided during inference."
    },
    {
      "id": "Machine_Learning_Adaptation_Techniques",
      "type": "major",
      "parent": null,
      "description": "Techniques for adapting machine learning models to new tasks without additional training data."
    },
    {
      "id": "Zero-Shot_Adaptation",
      "type": "subnode",
      "parent": "Machine_Learning_Adaptation_Techniques",
      "description": "Method where no input-output pairs from downstream tasks are available."
    },
    {
      "id": "In-Context_Learning",
      "type": "subnode",
      "parent": "Machine_Learning_Adaptation_Techniques",
      "description": "Approach for few-shot settings using a small number of labeled examples to guide model predictions."
    },
    {
      "id": "Task_Formulation",
      "type": "subnode",
      "parent": "Zero-Shot_Adaptation",
      "description": "Formulating tasks as questions or cloze tests for language problems."
    },
    {
      "id": "Prompting_Strategy",
      "type": "subnode",
      "parent": "In-Context_Learning",
      "description": "Strategy of using labeled examples to construct prompts for guiding model predictions."
    },
    {
      "id": "Model_Optimization",
      "type": "subnode",
      "parent": "Machine_Learning_Adaptation_Techniques",
      "description": "Optimizing parameters in a pretrained model for new tasks."
    },
    {
      "id": "Reinforcement Learning",
      "type": "major",
      "parent": "Machine Learning",
      "description": "Type of machine learning where an agent learns to make decisions based on rewards and punishments."
    },
    {
      "id": "Sequential Decision Making",
      "type": "subnode",
      "parent": "Reinforcement Learning",
      "description": "Decision making in sequences without explicit supervision."
    },
    {
      "id": "Markov Decision Processes (MDP)",
      "type": "subnode",
      "parent": "Reinforcement Learning",
      "description": "Formal framework for modeling decision-making situations in reinforcement learning."
    },
    {
      "id": "States",
      "type": "subnode",
      "parent": "Markov Decision Processes (MDP)",
      "description": "Set of all possible conditions or configurations in an environment."
    },
    {
      "id": "Actions",
      "type": "subnode",
      "parent": "Markov Decision Processes (MDP)",
      "description": "Set of all possible actions that can be taken in a given state."
    },
    {
      "id": "State Transition Probabilities",
      "type": "subnode",
      "parent": "Markov Decision Processes (MDP)",
      "description": "Probabilities associated with moving from one state to another based on an action."
    },
    {
      "id": "Discount Factor",
      "type": "subnode",
      "parent": "Markov Decision Processes (MDP)",
      "description": "Parameter that determines the importance of future rewards relative to immediate ones."
    },
    {
      "id": "Reward Function",
      "type": "subnode",
      "parent": "Markov Decision Processes (MDP)",
      "description": "Function mapping state-action pairs or states to real numbers representing rewards."
    },
    {
      "id": "Markov Decision Process (MDP)",
      "type": "major",
      "parent": null,
      "description": "Framework for modeling decision-making in situations where outcomes are partly random and partly under the control of a decision maker."
    },
    {
      "id": "State Transition",
      "type": "subnode",
      "parent": "Markov Decision Process (MDP)",
      "description": "Random transition from one state to another based on action taken."
    },
    {
      "id": "Action Selection",
      "type": "subnode",
      "parent": "Markov Decision Process (MDP)",
      "description": "Choosing actions in each state according to a policy."
    },
    {
      "id": "Total Payoff",
      "type": "subnode",
      "parent": "Markov Decision Process (MDP)",
      "description": "Cumulative reward over time, discounted by γ^t."
    },
    {
      "id": "Discount Factor (γ)",
      "type": "subnode",
      "parent": "Total Payoff",
      "description": "Factor that discounts future rewards based on their temporal distance from the present."
    },
    {
      "id": "Policy",
      "type": "major",
      "parent": null,
      "description": "Function mapping states to actions in reinforcement learning."
    },
    {
      "id": "Value Function",
      "type": "subnode",
      "parent": "Policy",
      "description": "Expected sum of discounted rewards for starting in state s and following policy π."
    },
    {
      "id": "Policy Execution",
      "type": "major",
      "parent": null,
      "description": "Process of selecting actions based on a policy in given states."
    },
    {
      "id": "Bellman Equations",
      "type": "subnode",
      "parent": "Value Function",
      "description": "Set of equations used to solve for value function V^π(s) in finite-state MDPs."
    },
    {
      "id": "Immediate Reward",
      "type": "subnode",
      "parent": "Bellman Equations",
      "description": "Reward received immediately upon entering a state s."
    },
    {
      "id": "Future Discounted Rewards",
      "type": "subnode",
      "parent": "Bellman Equations",
      "description": "Expected sum of discounted rewards after the first step in an MDP from state s."
    },
    {
      "id": "Policy Evaluation",
      "type": "subnode",
      "parent": "Value Function",
      "description": "Process of calculating value function given a fixed policy."
    },
    {
      "id": "Optimal Value Function",
      "type": "subnode",
      "parent": "Value Function",
      "description": "Best possible expected sum of discounted rewards using any policy."
    },
    {
      "id": "Bellman's Equation",
      "type": "subnode",
      "parent": "Value Function",
      "description": "Equation defining the optimal value function recursively."
    },
    {
      "id": "Optimal Policy",
      "type": "subnode",
      "parent": "Value Function",
      "description": "Policy that maximizes the expected sum of discounted rewards for all states."
    },
    {
      "id": "Optimal_Policy",
      "type": "subnode",
      "parent": "Machine_Learning",
      "description": "Policy π* that maximizes the value function for all states in an MDP."
    },
    {
      "id": "MDP_Finite_State_Space",
      "type": "subnode",
      "parent": "Machine_Learning",
      "description": "Markov Decision Process with a finite set of states and actions."
    },
    {
      "id": "Value_Iteration",
      "type": "subnode",
      "parent": "MDP_Finite_State_Space",
      "description": "Algorithm used to find the optimal policy in reinforcement learning through iterative application of Bellman's equation."
    },
    {
      "id": "Synchronous_Update",
      "type": "subnode",
      "parent": "Value_Iteration",
      "description": "Update method where all state values are updated simultaneously before overwriting."
    },
    {
      "id": "Asynchronous_Update",
      "type": "subnode",
      "parent": "Value_Iteration",
      "description": "Update method where each state value is updated one at a time in sequence."
    },
    {
      "id": "ValueIterationAlgorithm",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Iterative algorithm to find optimal value function in an MDP."
    },
    {
      "id": "PolicyIterationAlgorithm",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Alternative method for finding the optimal policy by iteratively improving policies and their corresponding value functions."
    },
    {
      "id": "ConvergenceOfAlgorithms",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Conditions under which value iteration and policy iteration converge to optimal solutions."
    },
    {
      "id": "ComparisonValuePolicyIteration",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Discussion on the relative merits of value iteration versus policy iteration in different contexts."
    },
    {
      "id": "BellmanEquations",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Set of equations used to solve for optimal policies and values in MDPs."
    },
    {
      "id": "Policy_Iteration",
      "type": "subnode",
      "parent": "Machine_Learning_Algorithms",
      "description": "Alternative method to find optimal policies through successive policy improvements"
    },
    {
      "id": "Learning_Model_for_MDP",
      "type": "major",
      "parent": null,
      "description": "Discussion on learning state transition probabilities and rewards in MDPs from data."
    },
    {
      "id": "Inverted_Pendulum_Problem",
      "type": "subnode",
      "parent": "Learning_Model_for_MDP",
      "description": "Example problem used to illustrate the process of estimating MDP parameters from experience."
    },
    {
      "id": "State_Transition_Probabilities",
      "type": "subnode",
      "parent": "Learning_Model_for_MDP",
      "description": "Estimation method for state transition probabilities using maximum likelihood estimation based on observed data."
    },
    {
      "id": "MDP_Model_Learning",
      "type": "major",
      "parent": null,
      "description": "Process of learning state transition probabilities and rewards in an MDP"
    },
    {
      "id": "Expected_Immediate_Rewards",
      "type": "subnode",
      "parent": "MDP_Model_Learning",
      "description": "Calculation of average rewards observed in a particular state"
    },
    {
      "id": "Optimization_Techniques",
      "type": "subnode",
      "parent": "MDP_Model_Learning",
      "description": "Techniques for improving the efficiency of learning algorithms in MDPs"
    },
    {
      "id": "Continuous_State_MDPs",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Discussion on Markov Decision Processes with infinite state spaces, such as those involving continuous variables."
    },
    {
      "id": "Discretization_Method",
      "type": "subnode",
      "parent": "Continuous_State_MDPs",
      "description": "Technique to convert continuous-state MDPs into discrete-state MDPs for easier computation and algorithm application."
    },
    {
      "id": "Discretization in MDPs",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Process of converting continuous state space into discrete states for easier computation."
    },
    {
      "id": "Value Iteration",
      "type": "subnode",
      "parent": "Discretization in MDPs",
      "description": "Algorithm to find optimal value function in a discrete state Markov Decision Process."
    },
    {
      "id": "Policy Iteration",
      "type": "subnode",
      "parent": "Discretization in MDPs",
      "description": "Algorithm to find optimal policy in a discrete state Markov Decision Process."
    },
    {
      "id": "Supervised Learning",
      "type": "major",
      "parent": null,
      "description": "Learning from labeled data to predict outcomes for new inputs."
    },
    {
      "id": "Piecewise Constant Representation",
      "type": "subnode",
      "parent": "Discretization in MDPs",
      "description": "Representation that assumes constant value within each discrete state interval."
    },
    {
      "id": "Curse of Dimensionality",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Exponential increase in volume required to represent the state space as dimensionality increases."
    },
    {
      "id": "StateRepresentation",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Methods for representing states in machine learning models."
    },
    {
      "id": "GridCellRepresentation",
      "type": "subnode",
      "parent": "StateRepresentation",
      "description": "Using grid cells to represent continuous state spaces, requiring fine discretization."
    },
    {
      "id": "CurseOfDimensionality",
      "type": "subnode",
      "parent": "StateRepresentation",
      "description": "Exponential increase in discrete states with dimensionality, limiting scalability of discretization methods."
    },
    {
      "id": "ValueFunctionApproximation",
      "type": "major",
      "parent": "MachineLearningOverview",
      "description": "Techniques for approximating the value function in reinforcement learning using supervised learning methods."
    },
    {
      "id": "ModelOrSimulator",
      "type": "subnode",
      "parent": "ValueFunctionApproximation",
      "description": "Use of models or simulators to approximate value functions in continuous-state MDPs."
    },
    {
      "id": "Model Creation Methods",
      "type": "major",
      "parent": null,
      "description": "Different methods to create a model for state transitions in an MDP."
    },
    {
      "id": "Physics Simulation",
      "type": "subnode",
      "parent": "Model Creation Methods",
      "description": "Using physical laws or software to simulate system behavior."
    },
    {
      "id": "Off-the-Shelf Software",
      "type": "subnode",
      "parent": "Physics Simulation",
      "description": "Utilizing existing physics simulation tools like Open Dynamics Engine."
    },
    {
      "id": "Learning from Data",
      "type": "subnode",
      "parent": "Model Creation Methods",
      "description": "Inferring state transition probabilities from collected data in an MDP."
    },
    {
      "id": "Data Collection Process",
      "type": "subnode",
      "parent": "Learning from Data",
      "description": "Executing trials and observing state sequences to learn model parameters."
    },
    {
      "id": "LinearModelPrediction",
      "type": "subnode",
      "parent": "MachineLearningModels",
      "description": "Using linear models to predict the next state based on current state and action."
    },
    {
      "id": "DeterministicModel",
      "type": "subnode",
      "parent": "MachineLearningModels",
      "description": "Predicting exact outcomes given inputs in a deterministic manner."
    },
    {
      "id": "StochasticModel",
      "type": "subnode",
      "parent": "MachineLearningModels",
      "description": "Introducing randomness to model predictions with noise terms."
    },
    {
      "id": "NonLinearFeatureMapping",
      "type": "subnode",
      "parent": "MachineLearningModels",
      "description": "Using non-linear transformations of states and actions for more complex models."
    },
    {
      "id": "LossFunctions",
      "type": "subnode",
      "parent": "MachineLearningModels",
      "description": "Different functions used to measure the error in predictions, like L2 norm."
    },
    {
      "id": "Non-linear Feature Mappings",
      "type": "subnode",
      "parent": "Machine Learning Models",
      "description": "Feature mappings that transform states and actions into non-linear representations."
    },
    {
      "id": "MDP Simulators",
      "type": "subnode",
      "parent": "Machine Learning Models",
      "description": "Simulations of Markov Decision Processes using learned models."
    },
    {
      "id": "Fitted Value Iteration",
      "type": "major",
      "parent": null,
      "description": "Algorithm for approximating the value function in continuous state MDPs."
    },
    {
      "id": "Continuous State Space",
      "type": "subnode",
      "parent": "Fitted Value Iteration",
      "description": "The state space is modeled as a continuous set of real numbers."
    },
    {
      "id": "Discrete Action Space",
      "type": "subnode",
      "parent": "Fitted Value Iteration",
      "description": "Action space consists of a small, finite number of discrete actions."
    },
    {
      "id": "Value Function Approximation",
      "type": "subnode",
      "parent": "Fitted Value Iteration",
      "description": "Approximating the value function using supervised learning algorithms like linear regression."
    },
    {
      "id": "FittedValueIteration",
      "type": "subnode",
      "parent": "ValueFunctionApproximation",
      "description": "Algorithm that uses linear regression to approximate the value function over a finite sample of states."
    },
    {
      "id": "SupervisedLearning",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Process of training models on labeled data to predict outcomes for new inputs."
    },
    {
      "id": "StateSampling",
      "type": "subnode",
      "parent": "FittedValueIteration",
      "description": "Random selection of states to approximate the value function over a finite sample."
    },
    {
      "id": "ActionEvaluation",
      "type": "subnode",
      "parent": "FittedValueIteration",
      "description": "Process of evaluating actions in sampled states to estimate future rewards and state values."
    },
    {
      "id": "Supervised_Learning",
      "type": "subnode",
      "parent": "Machine_Learning",
      "description": "Learning with labeled training data to predict outcomes for new inputs."
    },
    {
      "id": "Linear_Regression",
      "type": "subnode",
      "parent": "Supervised_Learning",
      "description": "Statistical method for modeling relationships between a dependent variable and one or more independent variables."
    },
    {
      "id": "Fitted_Value_Iteration",
      "type": "subnode",
      "parent": "Machine_Learning",
      "description": "Technique used in reinforcement learning to approximate value functions using regression methods."
    },
    {
      "id": "Deterministic_Simulator",
      "type": "subnode",
      "parent": "Fitted_Value_Iteration",
      "description": "Simplification when using a deterministic model for the Markov Decision Process (MDP)."
    },
    {
      "id": "Policy_Definition",
      "type": "subnode",
      "parent": "Machine_Learning",
      "description": "Definition of policy based on approximated value function in reinforcement learning context."
    },
    {
      "id": "Expectation_Computation",
      "type": "subnode",
      "parent": "Value_Iteration",
      "description": "Process of approximating expectations in reinforcement learning algorithms."
    },
    {
      "id": "Gaussian_Noise_Model",
      "type": "subnode",
      "parent": "Expectation_Computation",
      "description": "Model involving deterministic function and Gaussian noise."
    },
    {
      "id": "Approximation_Methods",
      "type": "subnode",
      "parent": "Value_Iteration",
      "description": "Techniques for approximating expectations in value iteration."
    },
    {
      "id": "Linear_System_Solver",
      "type": "subnode",
      "parent": "Policy_Iteration",
      "description": "Method used to compute policy evaluation in policy iteration."
    },
    {
      "id": "Bellman_Updates",
      "type": "subnode",
      "parent": "Policy_Iteration",
      "description": "Iterative process for evaluating policies using Bellman equations."
    },
    {
      "id": "Algorithm 6",
      "type": "subnode",
      "parent": "Machine Learning Algorithms",
      "description": "Variant of policy iteration that uses value evaluation procedure VE."
    },
    {
      "id": "Procedure VE",
      "type": "subnode",
      "parent": "Algorithm 6",
      "description": "Evaluation function used in Algorithm 6 to update state values."
    },
    {
      "id": "Option 1 and Option 2",
      "type": "subnode",
      "parent": "Procedure VE",
      "description": "Two initialization options for the value evaluation procedure VE."
    },
    {
      "id": "Chapter_15_Summary",
      "type": "major",
      "parent": null,
      "description": "Summary of Chapter 15 on MDPs and iterative methods."
    },
    {
      "id": "k_steps_update_frequency",
      "type": "subnode",
      "parent": "Chapter_15_Summary",
      "description": "Discussion on optimal update frequency for k steps in iterative processes."
    },
    {
      "id": "Policy_Iteration_Speedup",
      "type": "subnode",
      "parent": "Chapter_15_Summary",
      "description": "Explanation of policy iteration's efficiency advantage over value iteration."
    },
    {
      "id": "Value_Iteration_Preference",
      "type": "subnode",
      "parent": "Chapter_15_Summary",
      "description": "Conditions under which value iteration is preferred over policy iteration."
    },
    {
      "id": "Chapter_16_LQR_DDP_LQG",
      "type": "major",
      "parent": null,
      "description": "Introduction to Chapter 16 on Linear Quadratic Regulator (LQR), Differential Dynamic Programming (DDP) and Linear Quadratic Gaussian (LQG)."
    },
    {
      "id": "Finite_Horizon_MDPs",
      "type": "subnode",
      "parent": "Chapter_16_LQR_DDP_LQG",
      "description": "Introduction to finite-horizon MDPs in a general setting."
    },
    {
      "id": "Optimal_Bellman_Equation",
      "type": "subnode",
      "parent": "Finite_Horizon_MDPs",
      "description": "Definition of the optimal Bellman equation for finding the optimal value function and policy."
    },
    {
      "id": "General_Setting_Equations",
      "type": "subnode",
      "parent": "Finite_Horizon_MDPs",
      "description": "Formulation of equations that apply to both discrete and continuous state spaces."
    },
    {
      "id": "ExpectationRewriting",
      "type": "major",
      "parent": null,
      "description": "Rewriting expectations for finite and continuous states."
    },
    {
      "id": "RewardsDependency",
      "type": "subnode",
      "parent": "ExpectationRewriting",
      "description": "Rewards depend on both states and actions."
    },
    {
      "id": "OptimalActionComputation",
      "type": "subnode",
      "parent": "RewardsDependency",
      "description": "Computing optimal action considering state-action rewards and future value expectations."
    },
    {
      "id": "FiniteHorizonMDP",
      "type": "major",
      "parent": null,
      "description": "Definition of a finite horizon Markov Decision Process (MDP)."
    },
    {
      "id": "TimeHorizonTuple",
      "type": "subnode",
      "parent": "FiniteHorizonMDP",
      "description": "Defining MDP with time horizon T."
    },
    {
      "id": "PayoffDefinition",
      "type": "subnode",
      "parent": "FiniteHorizonMDP",
      "description": "Summation of rewards over a finite number of steps without discount factor."
    },
    {
      "id": "DiscountFactorImpact",
      "type": "subnode",
      "parent": "FiniteHorizonMDP",
      "description": "Explanation for the absence of discount factor in finite horizon MDPs."
    },
    {
      "id": "NonStationaryOptimalPolicy",
      "type": "major",
      "parent": null,
      "description": "Optimal policy changes over time in a finite horizon setting."
    },
    {
      "id": "Non-Stationary Policies",
      "type": "major",
      "parent": null,
      "description": "Policies that change over time in finite-horizon MDPs."
    },
    {
      "id": "Finite Horizon MDP Dynamics",
      "type": "subnode",
      "parent": "Non-Stationary Policies",
      "description": "Dynamics of an MDP with a changing policy over time steps."
    },
    {
      "id": "Time Dependent Transition Probabilities",
      "type": "subnode",
      "parent": "Finite Horizon MDP Dynamics",
      "description": "Transition probabilities that vary based on the current time step and state-action pair."
    },
    {
      "id": "Optimal Policy in Finite Horizon",
      "type": "subnode",
      "parent": "Non-Stationary Policies",
      "description": "The optimal policy varies depending on remaining steps to goal attainment."
    },
    {
      "id": "Value Function for Non-Stationary MDPs",
      "type": "subnode",
      "parent": "Finite Horizon MDP Dynamics",
      "description": "Definition of the value function considering time-varying policies and states."
    },
    {
      "id": "Reinforcement_Learning",
      "type": "subnode",
      "parent": "Machine_Learning",
      "description": "Field of machine learning concerned with how software agents ought to take actions in an environment."
    },
    {
      "id": "Value_Functions",
      "type": "subnode",
      "parent": "Reinforcement_Learning",
      "description": "Function that estimates the expected cumulative reward from a given state under a policy."
    },
    {
      "id": "Policy_Evaluation",
      "type": "subnode",
      "parent": "Value_Functions",
      "description": "Process of estimating the value function for a fixed policy in reinforcement learning."
    },
    {
      "id": "Bellman_Equation",
      "type": "subnode",
      "parent": "Value_Functions",
      "description": "Equation used to express the relationship between the value of a state and its successor states."
    },
    {
      "id": "Dynamic_Programming",
      "type": "subnode",
      "parent": "Reinforcement_Learning",
      "description": "Technique for solving complex problems by breaking them down into simpler subproblems."
    },
    {
      "id": "Machine_Learning_Overview",
      "type": "major",
      "parent": null,
      "description": "General overview of machine learning concepts and techniques."
    },
    {
      "id": "Bellman_Equations",
      "type": "subnode",
      "parent": "Value_Iteration",
      "description": "Equations used to describe the relationship between value functions at different time steps."
    },
    {
      "id": "Geometric_Convergence",
      "type": "subnode",
      "parent": "Value_Iteration",
      "description": "Rate of convergence in value iteration, characterized by a geometric progression."
    },
    {
      "id": "Linear_Quadratic_Regulation_(LQR)",
      "type": "major",
      "parent": null,
      "description": "A special case of finite-horizon setting where exact solutions are tractable."
    },
    {
      "id": "Continuous_Model_Assumptions",
      "type": "subnode",
      "parent": "Linear_Quadratic_Regulation_(LQR)",
      "description": "Assumptions about the state and action spaces in a continuous setting."
    },
    {
      "id": "Linear_Transitions",
      "type": "subnode",
      "parent": "Continuous_Model_Assumptions",
      "description": "Model of system dynamics with linear transitions between states."
    },
    {
      "id": "Gaussian_Noise",
      "type": "subnode",
      "parent": "Linear_Transitions",
      "description": "Assumption about noise in the transition model, specifically Gaussian with zero mean."
    },
    {
      "id": "Quadratic_Rewards",
      "type": "subnode",
      "parent": "Continuous_Model_Assumptions",
      "description": "Rewards are modeled as quadratic functions of state and action variables."
    },
    {
      "id": "LQRModelAssumptions",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Assumptions made in the Linear Quadratic Regulator (LQR) model for optimal control problems."
    },
    {
      "id": "LQRAlgorithmSteps",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Two-step process of estimating and applying the LQR algorithm to find an optimal policy."
    },
    {
      "id": "Step1Estimation",
      "type": "subnode",
      "parent": "LQRAlgorithmSteps",
      "description": "First step involves collecting data and using linear regression to estimate model parameters."
    },
    {
      "id": "Step2OptimalPolicy",
      "type": "subnode",
      "parent": "LQRAlgorithmSteps",
      "description": "Second step uses dynamic programming to derive the optimal policy given known or estimated parameters."
    },
    {
      "id": "DynamicProgrammingApplication",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Application of dynamic programming in solving for the optimal value function in LQR problems."
    },
    {
      "id": "Optimal_Value_Function",
      "type": "major",
      "parent": null,
      "description": "Definition and properties of the optimal value function in a quadratic form."
    },
    {
      "id": "Quadratic_Form",
      "type": "subnode",
      "parent": "Optimal_Value_Function",
      "description": "The optimal value function is expressed as a quadratic function of state."
    },
    {
      "id": "Dynamics_Model",
      "type": "subnode",
      "parent": "Optimal_Value_Function",
      "description": "Model dynamics and their integration into the value function calculation."
    },
    {
      "id": "Linear_Policy_Derivation",
      "type": "subnode",
      "parent": "Optimal_Policy",
      "description": "Process to derive the linear form of the optimal policy from quadratic value functions."
    },
    {
      "id": "Optimal Policy in LQR",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Discussion on optimal policy formulation in Linear Quadratic Regulator (LQR)."
    },
    {
      "id": "Discrete Ricatti Equations",
      "type": "subnode",
      "parent": "Optimal Policy in LQR",
      "description": "Equations used to update Φ_t and Ψ_t iteratively."
    },
    {
      "id": "Independence of Noise",
      "type": "subnode",
      "parent": "Optimal Policy in LQR",
      "description": "Explanation that optimal policy does not depend on noise but cost function value does."
    },
    {
      "id": "LQR Algorithm Steps",
      "type": "subnode",
      "parent": "Optimal Policy in LQR",
      "description": "Steps involved in implementing the Linear Quadratic Regulator algorithm."
    },
    {
      "id": "Efficiency Improvement",
      "type": "subnode",
      "parent": "LQR Algorithm Steps",
      "description": "Method to optimize algorithm performance by updating only Φ_t."
    },
    {
      "id": "Non-linear Dynamics and LQR",
      "type": "major",
      "parent": null,
      "description": "Exploration of how non-linear systems can be approximated using Linear Quadratic Regulator methods."
    },
    {
      "id": "Inverted Pendulum Example",
      "type": "subnode",
      "parent": "Non-linear Dynamics and LQR",
      "description": "Illustration of applying LQR to the inverted pendulum problem."
    },
    {
      "id": "Inverted_Pendulum_Model",
      "type": "subnode",
      "parent": "Dynamics_and_Control_Systems",
      "description": "A model used to illustrate control theory principles."
    },
    {
      "id": "State_Transitions",
      "type": "subnode",
      "parent": "Inverted_Pendulum_Model",
      "description": "Describes how states change over time in the inverted pendulum system."
    },
    {
      "id": "Linearization_of_Dynamics",
      "type": "major",
      "parent": "Differential_Dynamic_Programming_(DDP)",
      "description": "Process of approximating nonlinear dynamics with linear equations around each point on the nominal trajectory."
    },
    {
      "id": "Taylor_Expansion_Method",
      "type": "subnode",
      "parent": "Linearization_of_Dynamics",
      "description": "Uses Taylor series to approximate functions around a point."
    },
    {
      "id": "LQR_Assumptions",
      "type": "subnode",
      "parent": "Linearization_of_Dynamics",
      "description": "Connection between linearized dynamics and Linear Quadratic Regulator assumptions."
    },
    {
      "id": "Differential_Dynamic_Programming",
      "type": "major",
      "parent": null,
      "description": "Optimization technique for nonlinear systems aiming to stay near a target state."
    },
    {
      "id": "Optimization_Methods",
      "type": "subnode",
      "parent": "Machine_Learning",
      "description": "Techniques used to find optimal solutions in machine learning problems."
    },
    {
      "id": "Differential_Dynamic_Programming_(DDP)",
      "type": "subnode",
      "parent": "Optimization_Methods",
      "description": "Method for trajectory optimization that discretizes the path and uses linear approximations around each point."
    },
    {
      "id": "Nominal_Trajectory",
      "type": "subnode",
      "parent": "Differential_Dynamic_Programming_(DDP)",
      "description": "Initial approximation of the desired trajectory using a simple controller."
    },
    {
      "id": "Rewriting_Dynamics",
      "type": "subnode",
      "parent": "Linearization_of_Dynamics",
      "description": "Expressing the system's dynamics in a linear form using matrices A and B for state and action respectively."
    },
    {
      "id": "Reward_Function_Approximation",
      "type": "subnode",
      "parent": "Differential_Dynamic_Programming_(DDP)",
      "description": "Approximating the reward function around each point on the trajectory with Taylor expansion."
    },
    {
      "id": "Optimization_Frameworks",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Frameworks used for optimization in machine learning problems."
    },
    {
      "id": "Linear_Quadratic_Regulator_(LQR)",
      "type": "subnode",
      "parent": "Optimization_Frameworks",
      "description": "A control strategy that minimizes a quadratic cost function over time."
    },
    {
      "id": "Hessian_Matrix",
      "type": "subnode",
      "parent": "Optimization_Frameworks",
      "description": "Matrix of second-order partial derivatives used in optimization problems."
    },
    {
      "id": "LQR_Framework_Applications",
      "type": "subnode",
      "parent": "Linear_Quadratic_Regulator_(LQR)",
      "description": "Application of LQR to find optimal policies and controllers."
    },
    {
      "id": "Trajectory_Generation",
      "type": "subnode",
      "parent": "Optimization_Frameworks",
      "description": "Process of generating new trajectories using the derived policy in an iterative manner."
    },
    {
      "id": "Linear_Quadratic_Gaussian_(LQG)",
      "type": "major",
      "parent": null,
      "description": "Extension of LQR to handle systems with partial state observability."
    },
    {
      "id": "Observation vs State",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Discussion on the difference between observation and state in real-world problems."
    },
    {
      "id": "Partially Observable MDPs (POMDP)",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Introduction to POMDP as a tool for modeling partially observable environments."
    },
    {
      "id": "Belief State",
      "type": "subnode",
      "parent": "Partially Observable MDPs (POMDP)",
      "description": "Maintaining belief state based on observations in POMDP framework."
    },
    {
      "id": "LQR Extension to POMDP",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Extension of Linear Quadratic Regulator (LQR) to Partially Observable MDPs."
    },
    {
      "id": "Kalman Filter",
      "type": "subnode",
      "parent": "LQR Extension to POMDP",
      "description": "A recursive algorithm that estimates the state of a system over time using a series of measurements."
    },
    {
      "id": "Step 1",
      "type": "subnode",
      "parent": "Kalman Filter",
      "description": "Initial step where the system dynamics are defined without action dependence."
    },
    {
      "id": "Gaussian Distributions",
      "type": "major",
      "parent": null,
      "description": "Statistical distributions used in Kalman filter for modeling uncertainties."
    },
    {
      "id": "Predict Step",
      "type": "subnode",
      "parent": "Kalman Filter",
      "description": "Computes the distribution of the next state given current observations."
    },
    {
      "id": "Update Step",
      "type": "subnode",
      "parent": "Kalman Filter",
      "description": "Step to update the predicted state with new observations."
    },
    {
      "id": "LQR Algorithm",
      "type": "major",
      "parent": null,
      "description": "Linear Quadratic Regulator algorithm used for control problems."
    },
    {
      "id": "Computational Efficiency",
      "type": "subnode",
      "parent": "Kalman Filter",
      "description": "Efficient computation of state estimates over time using Kalman filter."
    },
    {
      "id": "Belief States Update",
      "type": "subnode",
      "parent": "Kalman Filter",
      "description": "Combination of predict and update steps to refine belief states over time."
    },
    {
      "id": "Gaussian Distribution",
      "type": "subnode",
      "parent": "Predict Step",
      "description": "The distribution used in the predict step is Gaussian with mean s_t|t and covariance Sigma_t|t."
    },
    {
      "id": "Kalman Gain (K_t)",
      "type": "subnode",
      "parent": "Update Step",
      "description": "A matrix that determines how much new measurements should be trusted over previous estimates."
    },
    {
      "id": "Backward Pass (LQR Updates)",
      "type": "subnode",
      "parent": "Kalman Filter",
      "description": "Computes Psi_t, Psi_t and L_t to refine the optimal policy based on previous estimates."
    },
    {
      "id": "Chapter_17_Policy_Gradient_REINFORCE",
      "type": "major",
      "parent": null,
      "description": "Model-free algorithm that optimizes policy parameters without value functions."
    },
    {
      "id": "Finite_Horizon_Case",
      "type": "subnode",
      "parent": "Chapter_17_Policy_Gradient_REINFORCE",
      "description": "Assumption for trajectory length in REINFORCE method."
    },
    {
      "id": "Randomized_Policy",
      "type": "subnode",
      "parent": "Chapter_17_Policy_Gradient_REINFORCE",
      "description": "REINFORCE applies to learning policies that output actions probabilistically."
    },
    {
      "id": "Transition_Probabilities_Sampling",
      "type": "subnode",
      "parent": "Chapter_17_Policy_Gradient_REINFORCE",
      "description": "Sampling from transition probabilities is sufficient for REINFORCE, no need for analytical form."
    },
    {
      "id": "Reward_Function_Querying",
      "type": "subnode",
      "parent": "Chapter_17_Policy_Gradient_REINFORCE",
      "description": "REINFORCE queries reward function at state-action pairs without needing its analytical form."
    },
    {
      "id": "Expected_Total_Payoff_Optimization",
      "type": "subnode",
      "parent": "Chapter_17_Policy_Gradient_REINFORCE",
      "description": "Optimizing expected total payoff over policy parameters in finite horizon setting."
    },
    {
      "id": "Policy Gradient Methods",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Techniques for optimizing policies in reinforcement learning."
    },
    {
      "id": "Gradient Ascent",
      "type": "subnode",
      "parent": "Policy Gradient Methods",
      "description": "Optimization technique to maximize the expected reward function."
    },
    {
      "id": "Reward Function Estimation",
      "type": "subnode",
      "parent": "Policy Gradient Methods",
      "description": "Challenges in estimating gradients without knowing the exact form of the reward function."
    },
    {
      "id": "Variational Auto-Encoder (VAE)",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Technique for learning latent variable models by optimizing variational lower bound."
    },
    {
      "id": "Re-parametrization Technique",
      "type": "subnode",
      "parent": "Variational Auto-Encoder (VAE)",
      "description": "Method to compute gradients through random variables in VAEs."
    },
    {
      "id": "REINFORCE Algorithm",
      "type": "subnode",
      "parent": "Policy Gradient Methods",
      "description": "Algorithm for estimating policy gradients using likelihood ratio methods."
    },
    {
      "id": "GradientEstimation",
      "type": "major",
      "parent": null,
      "description": "Overview of estimating gradients in machine learning."
    },
    {
      "id": "PolicyGradients",
      "type": "subnode",
      "parent": "GradientEstimation",
      "description": "Methods for computing policy gradients using samples."
    },
    {
      "id": "SampleBasedEstimator",
      "type": "subnode",
      "parent": "PolicyGradients",
      "description": "Using empirical samples to estimate the gradient of expected values."
    },
    {
      "id": "LogProbabilityDerivative",
      "type": "subnode",
      "parent": "PolicyGradients",
      "description": "Computing the derivative of log probability with respect to policy parameters."
    },
    {
      "id": "AnalyticalFormula",
      "type": "subnode",
      "parent": "LogProbabilityDerivative",
      "description": "Deriving an analytical formula for π_θ(a|s)."
    },
    {
      "id": "AutoDifferentiation",
      "type": "subnode",
      "parent": "LogProbabilityDerivative",
      "description": "Using automatic differentiation to compute gradients."
    },
    {
      "id": "Policy_Gradient_Theorem",
      "type": "major",
      "parent": null,
      "description": "Fundamental theorem in reinforcement learning for policy optimization."
    },
    {
      "id": "Log_Probability_Ratio",
      "type": "subnode",
      "parent": "Policy_Gradient_Theorem",
      "description": "Ratio of the probabilities of an action given a state according to two different policies, often used in reinforcement learning algorithms."
    },
    {
      "id": "Vanilla_REINFORCE_Algorithm",
      "type": "subnode",
      "parent": "Policy_Gradient_Theorem",
      "description": "Basic algorithm for learning policies using policy gradients."
    },
    {
      "id": "Empirical_Sample_Trajectories",
      "type": "subnode",
      "parent": "Gradient_Estimation",
      "description": "Using sample trajectories to estimate gradients in reinforcement learning."
    },
    {
      "id": "Trajectory_Probability_Change",
      "type": "subnode",
      "parent": "Log_Probability_Ratio",
      "description": "Change in trajectory probability due to policy parameter changes."
    },
    {
      "id": "PolicyGradientMethods",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Techniques that optimize policies in reinforcement learning by using gradients of the performance measure with respect to policy parameters."
    },
    {
      "id": "TrajectoryProbability",
      "type": "subnode",
      "parent": "PolicyGradientMethods",
      "description": "The probability of a trajectory given a policy and its importance in gradient calculation."
    },
    {
      "id": "RewardFunction",
      "type": "subnode",
      "parent": "PolicyGradientMethods",
      "description": "A function that assigns a scalar value to each possible outcome, used to evaluate the quality of actions taken by an agent."
    },
    {
      "id": "ExpectationCalculation",
      "type": "subnode",
      "parent": "PolicyGradientMethods",
      "description": "Calculating expectations over trajectories under a policy and their implications for gradient estimation."
    },
    {
      "id": "SimplificationOfFormulas",
      "type": "subnode",
      "parent": "PolicyGradientMethods",
      "description": "Simplifying complex formulas to more manageable expressions based on certain assumptions or properties."
    },
    {
      "id": "Policy_Gradient_Methods",
      "type": "subnode",
      "parent": "Reinforcement_Learning",
      "description": "Techniques for optimizing the policy directly by taking gradients of the performance measure with respect to the parameters of the policy."
    },
    {
      "id": "Law_of_Total_Expectation",
      "type": "subnode",
      "parent": "Policy_Gradient_Methods",
      "description": "Theorem that states how to compute expectations under a probability distribution by conditioning on another random variable."
    },
    {
      "id": "Value_Function_Estimation",
      "type": "subnode",
      "parent": "Policy_Gradient_Methods",
      "description": "Estimating the expected cumulative reward from a state or action under a policy."
    },
    {
      "id": "Gradient Estimation",
      "type": "subnode",
      "parent": "Policy Gradient Methods",
      "description": "Estimating gradients of policy parameters using trajectories."
    },
    {
      "id": "Baseline Function",
      "type": "subnode",
      "parent": "Gradient Estimation",
      "description": "Function used to reduce variance in gradient estimation."
    },
    {
      "id": "Algorithm 7",
      "type": "subnode",
      "parent": "Gradient Estimation",
      "description": "Vanilla policy gradient algorithm with baseline function."
    },
    {
      "id": "Bibliography References",
      "type": "major",
      "parent": null,
      "description": "References to academic papers related to machine learning concepts."
    },
    {
      "id": "Machine_Learning_Papers",
      "type": "major",
      "parent": null,
      "description": "Collection of influential papers in machine learning and statistics."
    },
    {
      "id": "Double_Descent_Weak_Features",
      "type": "subnode",
      "parent": "Machine_Learning_Papers",
      "description": "Explores double descent phenomenon in machine learning models using weak features."
    },
    {
      "id": "Variational_Inference_Review",
      "type": "subnode",
      "parent": "Machine_Learning_Papers",
      "description": "Provides a comprehensive review of variational inference for statisticians."
    },
    {
      "id": "Foundation_Models",
      "type": "subnode",
      "parent": "Machine_Learning_Papers",
      "description": "Discusses the opportunities and risks associated with foundation models in AI."
    },
    {
      "id": "Contrastive_Learning_Visual_Representations",
      "type": "subnode",
      "parent": "Machine_Learning_Papers",
      "description": "Introduces a simple framework for contrastive learning of visual representations."
    },
    {
      "id": "BERT_Model",
      "type": "subnode",
      "parent": "Machine_Learning_Papers",
      "description": "Describes the pre-training of deep bidirectional transformers for language understanding."
    },
    {
      "id": "Implicit_Bias_Noise_Covariance",
      "type": "subnode",
      "parent": "Machine_Learning_Papers",
      "description": "Analyzes the implicit bias introduced by noise covariance in machine learning models."
    },
    {
      "id": "High_Dimensional_Statistics",
      "type": "subnode",
      "parent": "Machine_Learning_Papers",
      "description": "Surveys unexpected phenomena in high-dimensional statistical analysis and modeling."
    },
    {
      "id": "Machine Learning Papers",
      "type": "major",
      "parent": null,
      "description": "Collection of papers related to machine learning and statistical learning theory."
    },
    {
      "id": "Implicit Bias in Machine Learning",
      "type": "subnode",
      "parent": "Machine Learning Papers",
      "description": "Studies on the implicit bias introduced by different methods in high-dimensional settings."
    },
    {
      "id": "High-Dimensional Interpolation",
      "type": "subnode",
      "parent": "Machine Learning Papers",
      "description": "Research on interpolation properties of machine learning models in high dimensions."
    },
    {
      "id": "Deep Residual Learning",
      "type": "subnode",
      "parent": "Machine Learning Papers",
      "description": "Work on deep residual networks for image recognition tasks."
    },
    {
      "id": "Statistical Learning Textbook",
      "type": "subnode",
      "parent": "Machine Learning Papers",
      "description": "Textbook covering fundamental concepts and techniques in statistical learning."
    },
    {
      "id": "Optimization Methods",
      "type": "subnode",
      "parent": "Machine Learning Papers",
      "description": "Methods for stochastic optimization, including Adam and other advanced algorithms."
    },
    {
      "id": "Variational Autoencoders",
      "type": "subnode",
      "parent": "Machine Learning Papers",
      "description": "Research on variational Bayesian methods applied to auto-encoding models."
    },
    {
      "id": "Model-Based Reinforcement Learning",
      "type": "subnode",
      "parent": "Machine Learning Papers",
      "description": "Framework for model-based reinforcement learning with theoretical guarantees."
    },
    {
      "id": "Generalization Error Analysis",
      "type": "subnode",
      "parent": "Machine Learning Papers",
      "description": "Analysis of generalization error in random features regression and linear models."
    },
    {
      "id": "Learning Theory Fundamentals",
      "type": "subnode",
      "parent": "Machine Learning Papers",
      "description": "Fundamental concepts in learning theory, including statistical mechanics approaches."
    },
    {
      "id": "double_descent",
      "type": "major",
      "parent": null,
      "description": "Phenomenon in machine learning where performance initially improves, then worsens, and finally improves again as model complexity increases."
    },
    {
      "id": "statistical_mechanics_of_learning",
      "type": "subnode",
      "parent": "double_descent",
      "description": "Application of statistical mechanics principles to understand learning processes in neural networks."
    },
    {
      "id": "generalization",
      "type": "subnode",
      "parent": "statistical_mechanics_of_learning",
      "description": "Concept within statistical mechanics of learning focusing on how well a model performs on unseen data."
    },
    {
      "id": "learning_to_generalize",
      "type": "subnode",
      "parent": "double_descent",
      "description": "Study of methods and theories for improving the generalization ability of machine learning models."
    }
  ],
  "edges": [
    {
      "from": "Zero-Shot_Adaptation",
      "to": "Task_Formulation",
      "relationship": "related_to"
    },
    {
      "from": "Regularization Parameter λ",
      "to": "Regularized Loss Function",
      "relationship": "subtopic"
    },
    {
      "from": "LogisticRegression",
      "to": "NewtonMethod",
      "relationship": "has_algorithm"
    },
    {
      "from": "Machine_Learning_Algorithms",
      "to": "Value_Iteration",
      "relationship": "contains"
    },
    {
      "from": "MLPArchitecture",
      "to": "NonlinearActivationModule",
      "relationship": "depends_on"
    },
    {
      "from": "MachineLearningModels",
      "to": "OrdinaryLeastSquares",
      "relationship": "contains"
    },
    {
      "from": "EmpiricalRiskMinimization",
      "to": "TrainingError",
      "relationship": "subtopic"
    },
    {
      "from": "Major Axis of Variation",
      "to": "Projection Direction",
      "relationship": "has_subtopic"
    },
    {
      "from": "ExponentialFamilyDistributions",
      "to": "SufficientStatistic",
      "relationship": "has_subtopic"
    },
    {
      "from": "Logistic_Regression_Gradient_Descent",
      "to": "Machine_Learning_Concepts",
      "relationship": "subtopic"
    },
    {
      "from": "SchoolQuality",
      "to": "DerivedFeatures",
      "relationship": "subtopic"
    },
    {
      "from": "Optimization Problem",
      "to": "Objective Function",
      "relationship": "defines"
    },
    {
      "from": "Learning a model for an MDP",
      "to": "Continuous state MDPs",
      "relationship": "contains"
    },
    {
      "from": "Differentiable Circuit",
      "to": "Gradient Computation",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Variational Auto-Encoder (VAE)",
      "relationship": "related_to"
    },
    {
      "from": "GradientAscentRule",
      "to": "ConvergenceAndSourceRecovery",
      "relationship": "depends_on"
    },
    {
      "from": "ValueIterationAlgorithm",
      "to": "ConvergenceOfAlgorithms",
      "relationship": "depends_on"
    },
    {
      "from": "Equation_7.53_Usefulness",
      "to": "Derivations_Section_7.4.3",
      "relationship": "related_to"
    },
    {
      "from": "LogLikelihoodImprovement",
      "to": "ExpectationMaximizationAlgorithm",
      "relationship": "related_to"
    },
    {
      "from": "Natural_Language_Processing",
      "to": "Language_Models",
      "relationship": "contains"
    },
    {
      "from": "IntermediateVariables",
      "to": "BackwardPass",
      "relationship": "follows"
    },
    {
      "from": "Primal_Dual_Pairing",
      "to": "Dual_Problem_Solution",
      "relationship": "subtopic"
    },
    {
      "from": "PolicyGradientMethods",
      "to": "TrajectoryProbability",
      "relationship": "depends_on"
    },
    {
      "from": "TwoLayerNN",
      "to": "PriorKnowledge",
      "relationship": "contains"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Functional_Margin",
      "relationship": "subtopic"
    },
    {
      "from": "PoissonDistributionModeling",
      "to": "MachineLearningOverview",
      "relationship": "depends_on"
    },
    {
      "from": "ResNetArchitecture",
      "to": "BatchNormalization",
      "relationship": "subtopic"
    },
    {
      "from": "LQR, DDP and LQG",
      "to": "Finite-horizon MDPs",
      "relationship": "has_subtopic"
    },
    {
      "from": "IndependenceAssumption",
      "to": "MachineLearningOverview",
      "relationship": "related_to"
    },
    {
      "from": "Sparsity Regularization",
      "to": "Gradient Descent Incompatibility",
      "relationship": "related_to"
    },
    {
      "from": "Deep Learning Introduction",
      "to": "Machine Learning Overview",
      "relationship": "related_to"
    },
    {
      "from": "Functional_Margin",
      "to": "Confidence_and_Prediction",
      "relationship": "depends_on"
    },
    {
      "from": "SIMCLRAlgorithm",
      "to": "LossFunction",
      "relationship": "depends_on"
    },
    {
      "from": "Support Vector Machines (SVMs)",
      "to": "Geometric Margin",
      "relationship": "subtopic"
    },
    {
      "from": "Optimization_Frameworks",
      "to": "Linear_Quadratic_Regulator_(LQR)",
      "relationship": "subtopic"
    },
    {
      "from": "Value_Iteration",
      "to": "Bellman_Equations",
      "relationship": "depends_on"
    },
    {
      "from": "GaussianDistribution",
      "to": "StandardNormalDistribution",
      "relationship": "subtopic"
    },
    {
      "from": "Probability_Error_Bounds",
      "to": "Training_Error_Generalization_Error_Difference",
      "relationship": "related_to"
    },
    {
      "from": "KernelsAsSimilarityMetrics",
      "to": "MachineLearningConcepts",
      "relationship": "subtopic"
    },
    {
      "from": "DynamicProgrammingApplication",
      "to": "MachineLearningOverview",
      "relationship": "related_to"
    },
    {
      "from": "RegularizerFunction",
      "to": "TrainingLoss",
      "relationship": "part_of"
    },
    {
      "from": "Model Creation Methods",
      "to": "Learning from Data",
      "relationship": "has_subtopic"
    },
    {
      "from": "Linear_Quadratic_Regulation_(LQR)",
      "to": "Continuous_Model_Assumptions",
      "relationship": "subtopic"
    },
    {
      "from": "NewWordDetection",
      "to": "NaiveBayesFilter",
      "relationship": "subtopic"
    },
    {
      "from": "Necessary_Conditions",
      "to": "Positive_Semi_Definite",
      "relationship": "depends_on"
    },
    {
      "from": "Stochastic Gradient Descent (SGD)",
      "to": "Hyperparameters",
      "relationship": "depends_on"
    },
    {
      "from": "Differential_Dynamic_Programming_(DDP)",
      "to": "Nominal_Trajectory",
      "relationship": "depends_on"
    },
    {
      "from": "Best_Hypothesis",
      "to": "Machine_Learning_Theory",
      "relationship": "related_to"
    },
    {
      "from": "NeurIPS20xxSubmission",
      "to": "MachineLearningConferences",
      "relationship": "related_to"
    },
    {
      "from": "Bernoulli_Random_Variables",
      "to": "Generalization_Error",
      "relationship": "depends_on"
    },
    {
      "from": "ELBO_Gradient_Computation",
      "to": "Complexity_in_Computing_Gradients",
      "relationship": "subtopic_of"
    },
    {
      "from": "Optimizers",
      "to": "Initialization",
      "relationship": "related_to"
    },
    {
      "from": "Discrete Action Space",
      "to": "Fitted Value Iteration",
      "relationship": "subtopic"
    },
    {
      "from": "ModelAdaptation",
      "to": "ZeroShotLearning",
      "relationship": "subtopic"
    },
    {
      "from": "Classification Problem",
      "to": "Spam Classification Example",
      "relationship": "example_of"
    },
    {
      "from": "FeatureMapsAndKernels",
      "to": "KernelFunctionDefinition",
      "relationship": "subtopic"
    },
    {
      "from": "Parameter Estimation",
      "to": "Likelihood Function",
      "relationship": "depends_on"
    },
    {
      "from": "Machine_Learning_Adaptation_Techniques",
      "to": "Zero-Shot_Adaptation",
      "relationship": "subtopic"
    },
    {
      "from": "Feature_Vector",
      "to": "Vocabulary",
      "relationship": "composed_of"
    },
    {
      "from": "MachineLearningBasics",
      "to": "TrainingSetExamples",
      "relationship": "has_subtopic"
    },
    {
      "from": "Generalization_Error_Bound",
      "to": "Sample_Complexity",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning",
      "to": "Generalization",
      "relationship": "related_to"
    },
    {
      "from": "EMAlgorithmIntroduction",
      "to": "LatentVariables",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningConcepts",
      "to": "StateRepresentation",
      "relationship": "depends_on"
    },
    {
      "from": "Double Descent Phenomenon",
      "to": "Model-wise Double Descent",
      "relationship": "subtopic"
    },
    {
      "from": "Optimization_Techniques",
      "to": "MDP_Model_Learning",
      "relationship": "subtopic"
    },
    {
      "from": "Supervised Learning Problem",
      "to": "Regression",
      "relationship": "has_subtopic"
    },
    {
      "from": "LagrangianFormulation",
      "to": "DualProblemDerivation",
      "relationship": "subtopic"
    },
    {
      "from": "distortion_function",
      "to": "coordinate_descent",
      "relationship": "related_to"
    },
    {
      "from": "Backpropagation",
      "to": "Modules",
      "relationship": "subtopic"
    },
    {
      "from": "EMAlgorithmOverview",
      "to": "MixtureOfGaussiansExample",
      "relationship": "contains"
    },
    {
      "from": "Self_Supervised_Loss",
      "to": "Pretraining_Phase",
      "relationship": "depends_on"
    },
    {
      "from": "Double Descent Phenomenon",
      "to": "Model-wise Double Descent",
      "relationship": "contains"
    },
    {
      "from": "Policy Gradient Methods",
      "to": "Gradient Estimation",
      "relationship": "subtopic_of"
    },
    {
      "from": "Reparametrization_Trick",
      "to": "Variational_Autoencoder",
      "relationship": "technique_used_in"
    },
    {
      "from": "Adaptation_Phase",
      "to": "Transfer_Learning",
      "relationship": "subtopic"
    },
    {
      "from": "ActivationFunctions",
      "to": "SoftplusFunction",
      "relationship": "subtopic"
    },
    {
      "from": "Unsupervised learning",
      "to": "Independent components analysis",
      "relationship": "has_subtopic"
    },
    {
      "from": "Policy Evaluation",
      "to": "Value Function",
      "relationship": "depends_on"
    },
    {
      "from": "Bias-Variance Tradeoff",
      "to": "Training and Test Datasets",
      "relationship": "depends_on"
    },
    {
      "from": "Machine_Learning_Optimization",
      "to": "Lagrange_Duality",
      "relationship": "related_to"
    },
    {
      "from": "Sample-wise Double Descent",
      "to": "Optimal Regularization",
      "relationship": "depends_on"
    },
    {
      "from": "7 Deep learning",
      "to": "7.3 Modules in Modern Neural Networks",
      "relationship": "has_subtopic"
    },
    {
      "from": "ActivationFunctions",
      "to": "ReLUFunction",
      "relationship": "subtopic"
    },
    {
      "from": "NaiveBayesClassifier",
      "to": "MaximumLikelihoodEstimation",
      "relationship": "depends_on"
    },
    {
      "from": "NeuralNetworks",
      "to": "ActivationFunctions",
      "relationship": "depends_on"
    },
    {
      "from": "Chapter_15_Summary",
      "to": "k_steps_update_frequency",
      "relationship": "subtopic"
    },
    {
      "from": "EfficientEvaluationOfELBO",
      "to": "GaussianDistributionQ_i",
      "relationship": "contains"
    },
    {
      "from": "EM algorithms",
      "to": "General EM algorithms",
      "relationship": "has_subtopic"
    },
    {
      "from": "Backpropagation",
      "to": "General strategy of backpropagation",
      "relationship": "has_subtopic"
    },
    {
      "from": "1.2 The normal equations",
      "to": "1.2.1 Matrix derivatives",
      "relationship": "has_subtopic"
    },
    {
      "from": "Optimization_Methods",
      "to": "Differential_Dynamic_Programming_(DDP)",
      "relationship": "subtopic"
    },
    {
      "from": "Policy_Iteration",
      "to": "Bellman_Updates",
      "relationship": "alternative_to"
    },
    {
      "from": "MultiClassClassification",
      "to": "NegativeLogLikelihoodLossMulticlass",
      "relationship": "subtopic"
    },
    {
      "from": "Cross Validation",
      "to": "Bias and Variance Tradeoff",
      "relationship": "depends_on"
    },
    {
      "from": "Backpropagation",
      "to": "Back-propagation for MLPs",
      "relationship": "has_subtopic"
    },
    {
      "from": "TransformerModel",
      "to": "InputOutputInterface",
      "relationship": "contains"
    },
    {
      "from": "2 Classification and logistic regression",
      "to": "2.2 Digression: the perceptron learning algorithm",
      "relationship": "has_subtopic"
    },
    {
      "from": "Model Complexity",
      "to": "Parameter Count vs. Model Norm",
      "relationship": "has_subtopic"
    },
    {
      "from": "Variational_Autoencoder",
      "to": "EM_Algorithms",
      "relationship": "extends"
    },
    {
      "from": "Machine Learning Algorithms",
      "to": "Neural Networks",
      "relationship": "related_to"
    },
    {
      "from": "Neural Networks",
      "to": "Backpropagation",
      "relationship": "has_subtopic"
    },
    {
      "from": "NeurIPS20xxSubmission",
      "to": "NewWordDetection",
      "relationship": "depends_on"
    },
    {
      "from": "Loss_Functions",
      "to": "Training_Loss",
      "relationship": "subtopic"
    },
    {
      "from": "MStep",
      "to": "SigmaParameterUpdate",
      "relationship": "related_to"
    },
    {
      "from": "ICA_Ambiguities",
      "to": "Sign_Ignorance",
      "relationship": "has_subtopic"
    },
    {
      "from": "MachineLearningModels",
      "to": "LogisticRegression",
      "relationship": "related_to"
    },
    {
      "from": "3 Generalized linear models",
      "to": "3.2 Constructing GLMs",
      "relationship": "has_subtopic"
    },
    {
      "from": "MSEDecomposition",
      "to": "AverageModel",
      "relationship": "subtopic_of"
    },
    {
      "from": "GaussianDistribution",
      "to": "MeanOfGaussian",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Double Descent Phenomenon",
      "relationship": "subtopic"
    },
    {
      "from": "ResNetOverview",
      "to": "ResNetComposition",
      "relationship": "subtopic"
    },
    {
      "from": "JensensInequality",
      "to": "LogLikelihoodBound",
      "relationship": "related_to"
    },
    {
      "from": "Chain_Rule_Application",
      "to": "Gradient_Computation",
      "relationship": "depends_on"
    },
    {
      "from": "Transformer_Models",
      "to": "Training_Transformer",
      "relationship": "has_subtopic"
    },
    {
      "from": "ScalingInvariantProperty",
      "to": "MM_Wb",
      "relationship": "subtopic"
    },
    {
      "from": "Simplified_1D_Convolution",
      "to": "Bias_Scalar",
      "relationship": "depends_on"
    },
    {
      "from": "Reinforcement_Learning",
      "to": "Value_Functions",
      "relationship": "depends_on"
    },
    {
      "from": "Implicit_Bias_Noise_Covariance",
      "to": "Machine_Learning_Papers",
      "relationship": "belongs_to"
    },
    {
      "from": "Sufficient Conditions for Kernels",
      "to": "Feature Mapping",
      "relationship": "related_to"
    },
    {
      "from": "7.4 Backpropagation",
      "to": "7.4.3 Backward functions for basic modules",
      "relationship": "has_subtopic"
    },
    {
      "from": "MachineLearningOverview",
      "to": "FeatureMapsAndKernels",
      "relationship": "depends_on"
    },
    {
      "from": "Multi-classClassification",
      "to": "ResponseVariable",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Backpropagation",
      "relationship": "related_to"
    },
    {
      "from": "Reinforcement learning",
      "to": "Continuous state MDPs",
      "relationship": "has_subtopic"
    },
    {
      "from": "Normalization Techniques",
      "to": "Layer Normalization (LN)",
      "relationship": "contains"
    },
    {
      "from": "Dual_Problem_Solution",
      "to": "Convexity_Conditions",
      "relationship": "depends_on"
    },
    {
      "from": "Step1Estimation",
      "to": "LQRAlgorithmSteps",
      "relationship": "subtopic"
    },
    {
      "from": "Convolutional_Neural_Networks",
      "to": "1D_Convolution",
      "relationship": "contains"
    },
    {
      "from": "Dual_Problem_Solution",
      "to": "KKT_Conditions",
      "relationship": "related_to"
    },
    {
      "from": "Cross_Validation",
      "to": "Hold_Out_Cross_Validation",
      "relationship": "specific_type_of"
    },
    {
      "from": "Discretization in MDPs",
      "to": "Value Iteration",
      "relationship": "depends_on"
    },
    {
      "from": "Machine_Learning_Topics",
      "to": "ICA",
      "relationship": "contains"
    },
    {
      "from": "Principal Component Analysis (PCA)",
      "to": "Eigenfaces Method",
      "relationship": "subtopic_of"
    },
    {
      "from": "Regularization and model selection",
      "to": "Implicit regularization effect (optional reading)",
      "relationship": "has_subtopic"
    },
    {
      "from": "Dual Problem",
      "to": "Relationship Between Primal and Dual",
      "relationship": "related_to"
    },
    {
      "from": "ConditionalProbabilityModeling",
      "to": "ParameterizedFunction",
      "relationship": "contains"
    },
    {
      "from": "Conditional_Distribution_Modeling",
      "to": "Hypothesis_Functions",
      "relationship": "has_subtopic"
    },
    {
      "from": "Linear_Quadratic_Regulator_(LQR)",
      "to": "LQR_Framework_Applications",
      "relationship": "subtopic"
    },
    {
      "from": "ICA Ambiguities",
      "to": "Scaling Ambiguity",
      "relationship": "related_to"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Test_Data_Set",
      "relationship": "subtopic"
    },
    {
      "from": "Differential_Dynamic_Programming_(DDP)",
      "to": "Linearization_of_Dynamics",
      "relationship": "subtopic"
    },
    {
      "from": "Value_Iteration",
      "to": "Geometric_Convergence",
      "relationship": "related_to"
    },
    {
      "from": "LMSUpdateRule",
      "to": "GradientDescentAlgorithm",
      "relationship": "related_to"
    },
    {
      "from": "Normalization Techniques",
      "to": "Mean Normalization",
      "relationship": "has_subtopic"
    },
    {
      "from": "Double_Descent_Phenomenon",
      "to": "Model_Wise_Double_Descent",
      "relationship": "contains"
    },
    {
      "from": "Regularization and Non-Separable Case",
      "to": "Outlier Sensitivity",
      "relationship": "related_to"
    },
    {
      "from": "Support_Vector_Machines_SVM",
      "to": "Dual_Problem_Formulation",
      "relationship": "contains"
    },
    {
      "from": "TwoLayerNN",
      "to": "SoftplusFunction",
      "relationship": "depends_on"
    },
    {
      "from": "Mixture of Gaussians Model",
      "to": "Parameter Estimation",
      "relationship": "subtopic"
    },
    {
      "from": "Dimensionality Reduction Techniques",
      "to": "Independent Components Analysis (ICA)",
      "relationship": "related_to"
    },
    {
      "from": "Fitted_Value_Iteration",
      "to": "Deterministic_Simulator",
      "relationship": "subtopic"
    },
    {
      "from": "GaussianDiscriminantAnalysis(GDA)",
      "to": "LikelihoodFunction",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning",
      "to": "Matricization Approach",
      "relationship": "depends_on"
    },
    {
      "from": "LMS_Update_Rule",
      "to": "Single_Training_Example",
      "relationship": "subtopic"
    },
    {
      "from": "RegressionProblems",
      "to": "TrainingDataset",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Overfitting",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Gradient Computation",
      "relationship": "depends_on"
    },
    {
      "from": "Update Step",
      "to": "Kalman Gain (K_t)",
      "relationship": "has_component"
    },
    {
      "from": "Newton's Method",
      "to": "Gradient Descent",
      "relationship": "compared_with"
    },
    {
      "from": "Jensens_Inequality",
      "to": "Machine_Learning_Concepts",
      "relationship": "related_to"
    },
    {
      "from": "Logistic Regression",
      "to": "Hessian Matrix",
      "relationship": "depends_on"
    },
    {
      "from": "Convolutional_Layers",
      "to": "Parameter_Sharing",
      "relationship": "subtopic_of"
    },
    {
      "from": "MachineLearningOverview",
      "to": "KernelTrick",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Bias_Variance_Tradeoff",
      "to": "Bias_Definition",
      "relationship": "subtopic"
    },
    {
      "from": "Activation_Functions",
      "to": "ReLU",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Algorithms",
      "to": "Laplace Smoothing",
      "relationship": "related_to"
    },
    {
      "from": "Reward_Function_Querying",
      "to": "Chapter_17_Policy_Gradient_REINFORCE",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningConcepts",
      "to": "GeneralizedLinearModels",
      "relationship": "related_to"
    },
    {
      "from": "Machine_Learning",
      "to": "Neural_Networks",
      "relationship": "related_to"
    },
    {
      "from": "PolicyGradientMethods",
      "to": "SimplificationOfFormulas",
      "relationship": "leads_to"
    },
    {
      "from": "MachineLearningOverview",
      "to": "EMAlgorithmIntroduction",
      "relationship": "related_to"
    },
    {
      "from": "Pretraining_Methods_Computer_Vision",
      "to": "Supervised_Pretraining",
      "relationship": "subtopic_of"
    },
    {
      "from": "Pretraining_Methods_Computer_Vision",
      "to": "Contrastive_Learning",
      "relationship": "subtopic_of"
    },
    {
      "from": "ELBOOptimization",
      "to": "GradientAscentOptimization",
      "relationship": "subtopic"
    },
    {
      "from": "Backpropagation",
      "to": "Chain Rule",
      "relationship": "depends_on"
    },
    {
      "from": "FittedValueIteration",
      "to": "ActionEvaluation",
      "relationship": "subtopic"
    },
    {
      "from": "Test_Error_Training_Error",
      "to": "Generalization_Gap",
      "relationship": "related_to"
    },
    {
      "from": "KKT Conditions",
      "to": "Support Vector Machine (SVM)",
      "relationship": "related_to"
    },
    {
      "from": "Zero-Shot_Adaptation",
      "to": "Language_Models",
      "relationship": "depends_on"
    },
    {
      "from": "1 Linear regression",
      "to": "1.4 Locally weighted linear regression (optional reading)",
      "relationship": "has_subtopic"
    },
    {
      "from": "ActivationFunctions",
      "to": "GELUFunction",
      "relationship": "subtopic"
    },
    {
      "from": "Generalization and regularization",
      "to": "Generalization",
      "relationship": "has_subtopic"
    },
    {
      "from": "PolicyGradientMethods",
      "to": "ExpectationCalculation",
      "relationship": "contains"
    },
    {
      "from": "Machine_Learning",
      "to": "Empirical_Risk_Minimization",
      "relationship": "depends_on"
    },
    {
      "from": "Kalman Filter",
      "to": "Forward Pass",
      "relationship": "includes"
    },
    {
      "from": "Linear Regression",
      "to": "Features Selection",
      "relationship": "depends_on"
    },
    {
      "from": "Design Matrix",
      "to": "Least Squares Revisited",
      "relationship": "subtopic"
    },
    {
      "from": "OptimalActionComputation",
      "to": "RewardsDependency",
      "relationship": "subtopic"
    },
    {
      "from": "Nonlinear Activation Module",
      "to": "MLP Composition",
      "relationship": "depends_on"
    },
    {
      "from": "Kalman Filter",
      "to": "Backward Pass (LQR Updates)",
      "relationship": "includes"
    },
    {
      "from": "BackwardFunctionsBasics",
      "to": "LossFunctionBackward",
      "relationship": "has_subtopic"
    },
    {
      "from": "Reinforcement learning",
      "to": "Connections between Policy and Value Iteration (Optional)",
      "relationship": "has_subtopic"
    },
    {
      "from": "Optimal Policy in LQR",
      "to": "Discrete Ricatti Equations",
      "relationship": "depends_on"
    },
    {
      "from": "Primal Problem",
      "to": "Objective Function Primal",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Backpropagation Algorithm",
      "relationship": "depends_on"
    },
    {
      "from": "Efficient_Update",
      "to": "Constraints_Satisfaction",
      "relationship": "depends_on"
    },
    {
      "from": "LogisticRegression",
      "to": "LinearRegression",
      "relationship": "compared_to"
    },
    {
      "from": "MachineLearningOverview",
      "to": "KernelFunctions",
      "relationship": "contains"
    },
    {
      "from": "ParallelismGPUs",
      "to": "VectorizationInNN",
      "relationship": "related_to"
    },
    {
      "from": "Machine_Learning_Backward_Functions",
      "to": "Cross_Entropy_Loss_Backward",
      "relationship": "subtopic"
    },
    {
      "from": "Linear_Classification",
      "to": "Empirical_Risk_Minimization",
      "relationship": "subtopic"
    },
    {
      "from": "NeuralNetworksInspiration",
      "to": "BiologicalSimilarity",
      "relationship": "contains"
    },
    {
      "from": "HoldOutCrossValidation",
      "to": "ValidationError",
      "relationship": "subtopic"
    },
    {
      "from": "SingleNeuronNN",
      "to": "ReLUFunction",
      "relationship": "depends_on"
    },
    {
      "from": "Support_Vector_Machines_SVM",
      "to": "Linear_Constraints",
      "relationship": "has_subtopic"
    },
    {
      "from": "PrimalProblem",
      "to": "GeneralizedLagrangian",
      "relationship": "subtopic"
    },
    {
      "from": "BinaryFeatures",
      "to": "NaiveBayesAlgorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Reinforcement_Learning",
      "to": "Dynamic_Programming",
      "relationship": "depends_on"
    },
    {
      "from": "NeuralNetworks",
      "to": "WeightMatrices",
      "relationship": "depends_on"
    },
    {
      "from": "ELBOOptimization",
      "to": "QFormRequirements",
      "relationship": "subtopic"
    },
    {
      "from": "Functional Margins",
      "to": "Machine Learning Concepts",
      "relationship": "subtopic"
    },
    {
      "from": "Learning_Settings",
      "to": "Training_Distribution",
      "relationship": "depends_on"
    },
    {
      "from": "LinearRegression",
      "to": "LocallyWeightedLinearRegression",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Double Descent Phenomenon",
      "relationship": "has_subtopic"
    },
    {
      "from": "Binary_Classification",
      "to": "Generalization_Error",
      "relationship": "subtopic"
    },
    {
      "from": "Transfer_Learning",
      "to": "Machine_Learning",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Backward_Propagation",
      "to": "Chain_Rule_Interpretation",
      "relationship": "subtopic"
    },
    {
      "from": "Kernels_in_Machine_Learning",
      "to": "Algorithm_5.11",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningOverview",
      "to": "BackpropagationDiscussion",
      "relationship": "has_subtopic"
    },
    {
      "from": "VariationalAutoEncoder",
      "to": "VariationalInference",
      "relationship": "subtopic"
    },
    {
      "from": "ELBO_Gradient_Computation",
      "to": "Gradient_Simple_Case",
      "relationship": "subtopic_of"
    },
    {
      "from": "Gradient_Estimation",
      "to": "Reparameterization_Trick",
      "relationship": "depends_on"
    },
    {
      "from": "StateRepresentation",
      "to": "CurseOfDimensionality",
      "relationship": "has_subtopic"
    },
    {
      "from": "Softmax_Function",
      "to": "Probability_Vector_Output",
      "relationship": "subtopic"
    },
    {
      "from": "Support Vector Machine (SVM)",
      "to": "Optimization Problem in SVM",
      "relationship": "depends_on"
    },
    {
      "from": "EmpiricalRiskMinimization",
      "to": "HypothesesHi",
      "relationship": "subtopic"
    },
    {
      "from": "Dual Problem",
      "to": "Theta D",
      "relationship": "related_to"
    },
    {
      "from": "Machine_Learning_Theory",
      "to": "Sample_Complexity",
      "relationship": "subtopic"
    },
    {
      "from": "Foundation Models",
      "to": "Machine Learning Overview",
      "relationship": "subtopic"
    },
    {
      "from": "Regularization",
      "to": "ModelComplexity",
      "relationship": "depends_on"
    },
    {
      "from": "Policy Gradient Methods",
      "to": "Reward Function Estimation",
      "relationship": "contains"
    },
    {
      "from": "Expectation_Computation",
      "to": "Gaussian_Noise_Model",
      "relationship": "subtopic"
    },
    {
      "from": "Sample_Size_Calculation",
      "to": "Quantities_of_Interest",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Adaptation",
      "to": "Language_Problems_Adaptation",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Discretization in MDPs",
      "relationship": "related_to"
    },
    {
      "from": "k-means Algorithm",
      "to": "Convergence in k-means",
      "relationship": "related_to"
    },
    {
      "from": "MachineLearningModels",
      "to": "LogisticRegression",
      "relationship": "contains"
    },
    {
      "from": "MachineLearningModels",
      "to": "GaussianDiscriminantAnalysis",
      "relationship": "related_to"
    },
    {
      "from": "BERT_Model",
      "to": "Machine_Learning_Papers",
      "relationship": "belongs_to"
    },
    {
      "from": "Class Priors",
      "to": "Bayesian Classification",
      "relationship": "depends_on"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Partial_Derivatives",
      "relationship": "contains"
    },
    {
      "from": "double_descent",
      "to": "learning_to_generalize",
      "relationship": "related_to"
    },
    {
      "from": "Continuous State Space",
      "to": "Fitted Value Iteration",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningOverview",
      "to": "Backpropagation",
      "relationship": "subtopic"
    },
    {
      "from": "Variational_Autoencoder",
      "to": "Neural_Networks",
      "relationship": "uses"
    },
    {
      "from": "MachineLearningConcepts",
      "to": "LikelihoodFunction",
      "relationship": "contains"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Population_Distribution",
      "relationship": "related_to"
    },
    {
      "from": "Kalman Filter",
      "to": "Belief States Update",
      "relationship": "includes"
    },
    {
      "from": "Bellman's Equation",
      "to": "Optimal Value Function",
      "relationship": "defines"
    },
    {
      "from": "MachineLearningConcepts",
      "to": "MSEDecomposition",
      "relationship": "has_subtopic"
    },
    {
      "from": "Inverted_Pendulum_Model",
      "to": "State_Transitions",
      "relationship": "subtopic"
    },
    {
      "from": "Exponential_Family_Distributions",
      "to": "Canonical_Response_Function",
      "relationship": "has_subtopic"
    },
    {
      "from": "MachineLearningConcepts",
      "to": "ActivationFunctions",
      "relationship": "related_to"
    },
    {
      "from": "GradientDescentAlgorithm",
      "to": "StochasticGradientDescent",
      "relationship": "subtopic"
    },
    {
      "from": "Evidence_Lower_Bound_(ELBO)",
      "to": "Log_Likelihood_Optimization",
      "relationship": "depends_on"
    },
    {
      "from": "MachineLearningOverview",
      "to": "PolicyGradientMethods",
      "relationship": "contains"
    },
    {
      "from": "MDP_Finite_State_Space",
      "to": "Machine_Learning",
      "relationship": "subtopic"
    },
    {
      "from": "LikelihoodFunction",
      "to": "MaximumLikelihoodEstimation",
      "relationship": "leads_to"
    },
    {
      "from": "LossFunction",
      "to": "CrossEntropyLoss",
      "relationship": "has_subtopic"
    },
    {
      "from": "Model_Selection",
      "to": "Validation_Set_Size",
      "relationship": "depends_on"
    },
    {
      "from": "EM algorithms",
      "to": "Mixture of Gaussians revisited",
      "relationship": "has_subtopic"
    },
    {
      "from": "Simplified_1D_Convolution",
      "to": "Matrix_Multiplication",
      "relationship": "related_to"
    },
    {
      "from": "MachineLearningOverview",
      "to": "LayerNormalization",
      "relationship": "contains"
    },
    {
      "from": "FeatureMapsAndKernels",
      "to": "AlgorithmEfficiency",
      "relationship": "subtopic"
    },
    {
      "from": "Dual_Problem_Formulation",
      "to": "KKT_Conditions",
      "relationship": "related_to"
    },
    {
      "from": "GeneralizedLinearModels",
      "to": "Assumption2",
      "relationship": "has_assumption"
    },
    {
      "from": "Support_Vector_Machines_SVM",
      "to": "Sequential_Minimal_Optimization_SMO",
      "relationship": "depends_on"
    },
    {
      "from": "MachineLearningOverview",
      "to": "NeuralNetworksComposition",
      "relationship": "has_subtopic"
    },
    {
      "from": "Mathematical Decomposition for Regression",
      "to": "Bias-Variance Tradeoff",
      "relationship": "depends_on"
    },
    {
      "from": "Support Vector Machines (SVM)",
      "to": "Optimal Parameters Calculation",
      "relationship": "depends_on"
    },
    {
      "from": "Generalization Error Analysis",
      "to": "Machine Learning Papers",
      "relationship": "subtopic"
    },
    {
      "from": "Future Discounted Rewards",
      "to": "Bellman Equations",
      "relationship": "related_to"
    },
    {
      "from": "Bayesian Machine Learning",
      "to": "Training Set",
      "relationship": "depends_on"
    },
    {
      "from": "Neural Networks",
      "to": "Modules in Modern Neural Networks",
      "relationship": "has_subtopic"
    },
    {
      "from": "PerceptronAlgorithm",
      "to": "DecisionBoundary",
      "relationship": "depends_on"
    },
    {
      "from": "ValueFunctionApproximation",
      "to": "ModelOrSimulator",
      "relationship": "has_subtopic"
    },
    {
      "from": "PolynomialKernels",
      "to": "ComputationalEfficiency",
      "relationship": "contains"
    },
    {
      "from": "ELBOOptimization",
      "to": "EfficientEvaluationOfELBO",
      "relationship": "subtopic"
    },
    {
      "from": "SingleTrainingExampleCase",
      "to": "PartialDerivativeCalculation",
      "relationship": "subtopic"
    },
    {
      "from": "ModelAdaptation",
      "to": "InContextLearning",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Backpropagation",
      "to": "Chain_Rule_Application",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningOverview",
      "to": "Conv1D-S",
      "relationship": "subtopic"
    },
    {
      "from": "EM_Algorithm",
      "to": "K_Means_Clustering",
      "relationship": "related_to"
    },
    {
      "from": "MachineLearningOverview",
      "to": "UnsupervisedLearning",
      "relationship": "depends_on"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Conditional_Distribution_Modeling",
      "relationship": "has_subtopic"
    },
    {
      "from": "Gradient Computation",
      "to": "Chain Rule",
      "relationship": "subtopic"
    },
    {
      "from": "Bias-variance tradeoff",
      "to": "A mathematical decomposition (for regression)",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine_Learning_Theorem",
      "to": "Uniform_Convergence_Assumption",
      "relationship": "subtopic"
    },
    {
      "from": "Neural Networks",
      "to": "Vectorization over training examples",
      "relationship": "has_subtopic"
    },
    {
      "from": "Compression",
      "to": "Principal Component Analysis (PCA)",
      "relationship": "subtopic"
    },
    {
      "from": "General EM algorithms",
      "to": "Other interpretation of ELBO",
      "relationship": "has_subtopic"
    },
    {
      "from": "Linear_Transitions",
      "to": "Gaussian_Noise",
      "relationship": "related_to"
    },
    {
      "from": "BernoulliDistribution",
      "to": "SufficientStatisticForBernoulli",
      "relationship": "has_subtopic"
    },
    {
      "from": "FamilySize",
      "to": "DerivedFeatures",
      "relationship": "subtopic"
    },
    {
      "from": "Discretization in MDPs",
      "to": "Piecewise Constant Representation",
      "relationship": "has_property"
    },
    {
      "from": "Dual_Complementarity",
      "to": "Support_Vectors",
      "relationship": "leads_to"
    },
    {
      "from": "Optimization Problem",
      "to": "Parameter C",
      "relationship": "controls"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Stochastic Gradient Ascent",
      "relationship": "contains"
    },
    {
      "from": "Optimization_Problems",
      "to": "KKT_Conditions",
      "relationship": "depends_on"
    },
    {
      "from": "Pretrained large language models",
      "to": "Open up the blackbox of Transformers",
      "relationship": "has_subtopic"
    },
    {
      "from": "Support_Vector_Machines_SVM",
      "to": "Convex_Quadratic_Objective",
      "relationship": "has_subtopic"
    },
    {
      "from": "LinearRegression",
      "to": "GradientDescent",
      "relationship": "related_to"
    },
    {
      "from": "Normalization Techniques",
      "to": "Scale-Invariant Property",
      "relationship": "contains"
    },
    {
      "from": "Regularizer R(θ)",
      "to": "Regularized Loss Function",
      "relationship": "related_to"
    },
    {
      "from": "Reinforcement learning",
      "to": "Value iteration and policy iteration",
      "relationship": "has_subtopic"
    },
    {
      "from": "Learning_Settings",
      "to": "Domain_Shift",
      "relationship": "related_to"
    },
    {
      "from": "Double Descent Phenomenon",
      "to": "Sample-wise Double Descent",
      "relationship": "contains"
    },
    {
      "from": "EvidenceLowerBoundELBO",
      "to": "EMAlgorithm",
      "relationship": "related_to"
    },
    {
      "from": "7 Deep learning",
      "to": "7.1 Supervised learning with non-linear models",
      "relationship": "has_subtopic"
    },
    {
      "from": "Discount Factor (γ)",
      "to": "Total Payoff",
      "relationship": "related_to"
    },
    {
      "from": "MachineLearningOverview",
      "to": "NeuralNetworks",
      "relationship": "depends_on"
    },
    {
      "from": "Linear_Policy_Derivation",
      "to": "Optimal_Value_Function",
      "relationship": "depends_on"
    },
    {
      "from": "MatrixMultiplicationModule",
      "to": "NeuralNetworksComposition",
      "relationship": "depends_on"
    },
    {
      "from": "ActivationFunctions",
      "to": "SigmoidFunction",
      "relationship": "subtopic"
    },
    {
      "from": "Unsupervised learning",
      "to": "Clustering and the k-means algorithm",
      "relationship": "has_subtopic"
    },
    {
      "from": "Geometric Margins",
      "to": "Machine Learning Concepts",
      "relationship": "subtopic"
    },
    {
      "from": "Foundation Models",
      "to": "Pretraining and Adaptation",
      "relationship": "subtopic"
    },
    {
      "from": "Lagrange_Duality",
      "to": "Dual_Form_Optimization",
      "relationship": "has_subtopic"
    },
    {
      "from": "Regularization",
      "to": "BiasVarianceTradeoff",
      "relationship": "related_to"
    },
    {
      "from": "Negative_Log_Likelihood",
      "to": "Cross_Entropy_Loss",
      "relationship": "subtopic"
    },
    {
      "from": "Regularization in Deep Learning",
      "to": "Implicit Regularization Effect",
      "relationship": "contains"
    },
    {
      "from": "InitializationThetaZero",
      "to": "IterativeUpdateRule",
      "relationship": "subtopic"
    },
    {
      "from": "Variational Auto-Encoder (VAE)",
      "to": "Re-parametrization Technique",
      "relationship": "contains"
    },
    {
      "from": "Generative_Modeling",
      "to": "Naive_Bayes_Classifier",
      "relationship": "has_subtopic"
    },
    {
      "from": "LMS_Update_Rule",
      "to": "Error_Term",
      "relationship": "depends_on"
    },
    {
      "from": "TrainingSetExamples",
      "to": "MatrixNotation",
      "relationship": "depends_on"
    },
    {
      "from": "Volume Calculation",
      "to": "General Case",
      "relationship": "subtopic"
    },
    {
      "from": "Kernel Matrix Properties",
      "to": "Sufficient Conditions for Kernels",
      "relationship": "subtopic"
    },
    {
      "from": "LogisticRegression",
      "to": "NegativeLikelihoodLoss",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Algorithms",
      "to": "Backpropagation Algorithm",
      "relationship": "depends_on"
    },
    {
      "from": "Backpropagation for MLPs",
      "to": "Forward Pass",
      "relationship": "depends_on"
    },
    {
      "from": "Neural_Networks",
      "to": "Stacking_Neurons",
      "relationship": "subtopic"
    },
    {
      "from": "MStep",
      "to": "PhiParameterUpdate",
      "relationship": "contains"
    },
    {
      "from": "Scaling Ambiguity",
      "to": "Volume Adjustment",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningAlgorithms",
      "to": "CrossValidation",
      "relationship": "related_to"
    },
    {
      "from": "Bias-Variance Tradeoff",
      "to": "Double Descent Phenomenon",
      "relationship": "related_to"
    },
    {
      "from": "MLPArchitecture",
      "to": "MatrixMultiplicationModule",
      "relationship": "depends_on"
    },
    {
      "from": "BackwardFunctionsBasics",
      "to": "ActivationFunctions",
      "relationship": "has_subtopic"
    },
    {
      "from": "LogLikelihood",
      "to": "GradientAscent",
      "relationship": "subtopic"
    },
    {
      "from": "Necessary_Conditions",
      "to": "Symmetry_Property",
      "relationship": "depends_on"
    },
    {
      "from": "Latent_Variables",
      "to": "Gaussian_Distribution_Qi",
      "relationship": "depends_on"
    },
    {
      "from": "MachineLearningConcepts",
      "to": "BackwardFunctionWb",
      "relationship": "has_subtopic"
    },
    {
      "from": "Naive Bayes Algorithm",
      "to": "Discretization",
      "relationship": "subtopic"
    },
    {
      "from": "Mean Vector",
      "to": "Multivariate Normal Distribution",
      "relationship": "related_to"
    },
    {
      "from": "Conditional_Distribution_Modeling",
      "to": "Bernoulli_Distribution",
      "relationship": "has_subtopic"
    },
    {
      "from": "Alpha_Parameters",
      "to": "Bounds_L_H",
      "relationship": "related_to"
    },
    {
      "from": "Optimization Problem in SVM",
      "to": "Functional Margin",
      "relationship": "related_to"
    },
    {
      "from": "LogLikelihoodCalculation",
      "to": "GradientAscentRule",
      "relationship": "subtopic"
    },
    {
      "from": "PilotSurveyExample",
      "to": "RedundancyDetection",
      "relationship": "subtopic"
    },
    {
      "from": "Value_Function_Estimation",
      "to": "Policy_Gradient_Methods",
      "relationship": "subtopic"
    },
    {
      "from": "Logistic_Regression_Gradient_Descent",
      "to": "Perceptron_Algorithm",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Model Complexity",
      "relationship": "has_subtopic"
    },
    {
      "from": "Posterior Distribution",
      "to": "Bayes' Theorem",
      "relationship": "depends_on"
    },
    {
      "from": "L1 Regularization",
      "to": "Optimization Problem",
      "relationship": "depends_on"
    },
    {
      "from": "Machine_Learning_Algorithms",
      "to": "SMO_Algorithm",
      "relationship": "has_subtopic"
    },
    {
      "from": "Embedding_Features",
      "to": "Pretraining_Phase",
      "relationship": "subtopic"
    },
    {
      "from": "ContrastiveLearning",
      "to": "SIMCLRAlgorithm",
      "relationship": "example_of"
    },
    {
      "from": "ICA Ambiguities",
      "to": "Permutation Matrix",
      "relationship": "depends_on"
    },
    {
      "from": "NonLinearModel",
      "to": "TrainingExamples",
      "relationship": "depends_on"
    },
    {
      "from": "Machine_Learning_Basics",
      "to": "Linear_Classification",
      "relationship": "subtopic"
    },
    {
      "from": "VC Dimension",
      "to": "Shattering",
      "relationship": "defines"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Naive_Bayes_Classifier",
      "relationship": "subtopic"
    },
    {
      "from": "Foundation_Models",
      "to": "Machine_Learning_Papers",
      "relationship": "belongs_to"
    },
    {
      "from": "Self-Supervised Learning",
      "to": "Supervised Contrastive Algorithms",
      "relationship": "subtopic"
    },
    {
      "from": "MultiClassClassification",
      "to": "SoftmaxFunction",
      "relationship": "depends_on"
    },
    {
      "from": "GaussianDistribution",
      "to": "DensityExamples",
      "relationship": "subtopic"
    },
    {
      "from": "Continuous Latent Variables",
      "to": "Variational Inference",
      "relationship": "subtopic"
    },
    {
      "from": "I Supervised learning",
      "to": "2 Classification and logistic regression",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Support Vector Machines (SVMs)",
      "relationship": "contains"
    },
    {
      "from": "EStep",
      "to": "ExpectationMaximizationAlgorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Topic",
      "to": "Text_Classification",
      "relationship": "has_subtopic"
    },
    {
      "from": "High_Dimensional_Statistics",
      "to": "Machine_Learning_Papers",
      "relationship": "belongs_to"
    },
    {
      "from": "Optimal_Margin_Classifier",
      "to": "Lagrange_Duality",
      "relationship": "depends_on"
    },
    {
      "from": "Few_Shot_Learning",
      "to": "Machine_Learning_Papers",
      "relationship": "belongs_to"
    },
    {
      "from": "HiddenUnits",
      "to": "NeuralNetworks",
      "relationship": "related_to"
    },
    {
      "from": "Probability_Estimation",
      "to": "Maximum_Likelihood_Estimates",
      "relationship": "depends_on"
    },
    {
      "from": "PolicyGradientMethods",
      "to": "RewardFunction",
      "relationship": "related_to"
    },
    {
      "from": "OptimizationMethods",
      "to": "NewtonMethod",
      "relationship": "contains"
    },
    {
      "from": "TotalParametersConv1D",
      "to": "LinearMappingComparison",
      "relationship": "related_to"
    },
    {
      "from": "Regularization in Machine Learning",
      "to": "Kernel Methods Compatibility",
      "relationship": "contains"
    },
    {
      "from": "Markov Decision Processes (MDP)",
      "to": "States",
      "relationship": "component_of"
    },
    {
      "from": "Statistical Learning Textbook",
      "to": "Machine Learning Papers",
      "relationship": "subtopic"
    },
    {
      "from": "Least-Squares Cost Function J",
      "to": "Error Term Distribution",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning",
      "to": "Text_Classification",
      "relationship": "has_subtopic"
    },
    {
      "from": "MachineLearningConcepts",
      "to": "ELBOExplanation",
      "relationship": "contains"
    },
    {
      "from": "FeatureMappingPhiX",
      "to": "RuntimeAndMemoryEfficiency",
      "relationship": "depends_on"
    },
    {
      "from": "LMS_Update_Rule",
      "to": "Widrow_Hoff_Learning_Rule",
      "relationship": "related_to"
    },
    {
      "from": "Multivariate Normal Distribution",
      "to": "Gaussian Discriminant Analysis (GDA)",
      "relationship": "depends_on"
    },
    {
      "from": "ELBOExplanation",
      "to": "AlternativeFormulationsOfELBO",
      "relationship": "subtopic"
    },
    {
      "from": "Representation Function",
      "to": "Self-Supervised Learning",
      "relationship": "subtopic"
    },
    {
      "from": "Policy_Gradient_Theorem",
      "to": "Vanilla_REINFORCE_Algorithm",
      "relationship": "has_subtopic"
    },
    {
      "from": "MachineLearningConcepts",
      "to": "LeastSquaresRegression",
      "relationship": "contains"
    },
    {
      "from": "ICA_Ambiguities",
      "to": "Gaussian_Data_Issue",
      "relationship": "has_subtopic"
    },
    {
      "from": "Learning Theory Proofs",
      "to": "Binary Classification",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningConcepts",
      "to": "EMAlgorithmOverview",
      "relationship": "contains"
    },
    {
      "from": "Gradient Estimation",
      "to": "Algorithm 7",
      "relationship": "describes"
    },
    {
      "from": "Non-linear Feature Mappings",
      "to": "Machine Learning Models",
      "relationship": "subtopic"
    },
    {
      "from": "LanguageModels",
      "to": "ConditionalProbability",
      "relationship": "depends_on"
    },
    {
      "from": "Feature Mapping",
      "to": "Linear Function Over Features",
      "relationship": "subtopic"
    },
    {
      "from": "Text_Classification",
      "to": "Spam_Filtering",
      "relationship": "example_of"
    },
    {
      "from": "Optimization_Frameworks",
      "to": "Hessian_Matrix",
      "relationship": "depends_on"
    },
    {
      "from": "Regularization and Non-Separable Case",
      "to": "Lagrangian Formulation",
      "relationship": "leads_to"
    },
    {
      "from": "2 Classification and logistic regression",
      "to": "2.4 Another algorithm for maximizing λ(θ)",
      "relationship": "has_subtopic"
    },
    {
      "from": "DiscriminativeLearning",
      "to": "PerceptronAlgorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Deep Learning Packages",
      "relationship": "related_to"
    },
    {
      "from": "CarAttributesExample",
      "to": "RedundancyDetection",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Regularization Techniques",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine_Learning_Bias_Variance_Tradeoff",
      "to": "Infinite_Hypothesis_Classes",
      "relationship": "subtopic"
    },
    {
      "from": "Notational Consistency",
      "to": "Multi-layer Neural Networks",
      "relationship": "subtopic"
    },
    {
      "from": "Value Function Approximation",
      "to": "Fitted Value Iteration",
      "relationship": "subtopic"
    },
    {
      "from": "Downstream_Task_Dataset",
      "to": "Few_Shot_Learning",
      "relationship": "subtopic"
    },
    {
      "from": "KernelFunctions",
      "to": "FeatureMapping",
      "relationship": "related_to"
    },
    {
      "from": "PosteriorApproximation",
      "to": "MAPEstimate",
      "relationship": "subtopic"
    },
    {
      "from": "Normalization Techniques",
      "to": "Data Rescaling",
      "relationship": "has_subtopic"
    },
    {
      "from": "Learning_Model_for_MDP",
      "to": "State_Transition_Probabilities",
      "relationship": "contains"
    },
    {
      "from": "Continuous state MDPs",
      "to": "Discretization",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningConcepts",
      "to": "VCDimensionIntroduction",
      "relationship": "subtopic"
    },
    {
      "from": "LogProbabilityDerivative",
      "to": "AnalyticalFormula",
      "relationship": "depends_on"
    },
    {
      "from": "BatchGradientDescent",
      "to": "FeatureMapPhi",
      "relationship": "related_to"
    },
    {
      "from": "Generalization",
      "to": "Bias-variance tradeoff",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine Learning",
      "to": "Reinforcement Learning",
      "relationship": "has_subtopic"
    },
    {
      "from": "SupervisedLearning",
      "to": "LinearRegression",
      "relationship": "related_to"
    },
    {
      "from": "MachineLearningConcepts",
      "to": "MaximumLikelihoodEstimation",
      "relationship": "contains"
    },
    {
      "from": "Normal_Equations_Method",
      "to": "Matrix_Derivatives",
      "relationship": "has_subtopic"
    },
    {
      "from": "Conditional Probability p(x|y)",
      "to": "Bayesian Classification",
      "relationship": "depends_on"
    },
    {
      "from": "Spam_Filtering",
      "to": "Feature_Vector",
      "relationship": "uses"
    },
    {
      "from": "MachineLearningOverview",
      "to": "ValueFunctionApproximation",
      "relationship": "has_subtopic"
    },
    {
      "from": "Optimization_Problems",
      "to": "Primal_Dual_Pairing",
      "relationship": "subtopic"
    },
    {
      "from": "UnsupervisedLearning",
      "to": "Clustering",
      "relationship": "subtopic"
    },
    {
      "from": "I Supervised learning",
      "to": "4 Generative learning algorithms",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine_Learning_Algorithms",
      "to": "Policy_Iteration",
      "relationship": "contains"
    },
    {
      "from": "Objective Function",
      "to": "Slack Variables",
      "relationship": "includes"
    },
    {
      "from": "EM_Algorithm",
      "to": "M_Step",
      "relationship": "subtopic"
    },
    {
      "from": "Backpropagation for MLPs",
      "to": "Gradient Calculation",
      "relationship": "depends_on"
    },
    {
      "from": "LogisticRegression",
      "to": "Robustness",
      "relationship": "subtopic"
    },
    {
      "from": "MatrixAlgebra",
      "to": "VectorizationInNN",
      "relationship": "subtopic"
    },
    {
      "from": "Loss_Functions",
      "to": "Test_Error",
      "relationship": "subtopic"
    },
    {
      "from": "PropertiesOfKernels",
      "to": "FeatureMapsAndKernels",
      "relationship": "related_to"
    },
    {
      "from": "LogisticRegression",
      "to": "GradientAscentRule",
      "relationship": "contains"
    },
    {
      "from": "GaussianKernel",
      "to": "KernelsAsSimilarityMetrics",
      "relationship": "subtopic"
    },
    {
      "from": "Gaussian_Data_Issue",
      "to": "Mixing_Matrix_Rotation",
      "relationship": "has_subtopic"
    },
    {
      "from": "Functional Margins",
      "to": "Geometric Margins",
      "relationship": "related_to"
    },
    {
      "from": "MachineLearningModels",
      "to": "MultinomialEventModel",
      "relationship": "has_subtopic"
    },
    {
      "from": "ConstrainedOptimization",
      "to": "PrimalProblem",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningConcepts",
      "to": "MultivariateNormalDistribution",
      "relationship": "contains"
    },
    {
      "from": "LogisticRegression",
      "to": "LogisticLossFunction",
      "relationship": "contains"
    },
    {
      "from": "Sequential Minimal Optimization (SMO) Algorithm",
      "to": "Support Vector Machines (SVM)",
      "relationship": "subtopic"
    },
    {
      "from": "Empirical_Risk_Minimization",
      "to": "Machine_Learning_Concepts",
      "relationship": "subtopic"
    },
    {
      "from": "LQR Extension to POMDP",
      "to": "Machine Learning Concepts",
      "relationship": "subtopic"
    },
    {
      "from": "Reinforcement Learning and Control",
      "to": "Reinforcement learning",
      "relationship": "has_subtopic"
    },
    {
      "from": "MachineLearningOverview",
      "to": "SupervisedLearning",
      "relationship": "related_to"
    },
    {
      "from": "EM_Algorithm",
      "to": "Soft_Assignments",
      "relationship": "related_to"
    },
    {
      "from": "k-means Algorithm",
      "to": "Distortion Function J",
      "relationship": "depends_on"
    },
    {
      "from": "Deep Residual Learning",
      "to": "Machine Learning Papers",
      "relationship": "subtopic"
    },
    {
      "from": "Model-wise Double Descent",
      "to": "Optimal Regularization",
      "relationship": "related_to"
    },
    {
      "from": "Finite_Horizon_MDPs",
      "to": "Optimal_Bellman_Equation",
      "relationship": "subtopic"
    },
    {
      "from": "Optimization_Problems",
      "to": "Primal_Dual_Equivalence",
      "relationship": "related_to"
    },
    {
      "from": "Sparsity Regularization",
      "to": "L1 Norm (LASSO)",
      "relationship": "depends_on"
    },
    {
      "from": "MachineLearningModels",
      "to": "TransformerModel",
      "relationship": "contains"
    },
    {
      "from": "Logistic Regression",
      "to": "Classification Problem",
      "relationship": "related_to"
    },
    {
      "from": "Machine_Learning",
      "to": "Supervised_Learning",
      "relationship": "contains"
    },
    {
      "from": "MachineLearningOverview",
      "to": "BayesianInference",
      "relationship": "depends_on"
    },
    {
      "from": "Bayesian Machine Learning",
      "to": "Posterior Distribution",
      "relationship": "subtopic"
    },
    {
      "from": "Hypothesis_Class_Switching",
      "to": "Bias_Decrease",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Techniques",
      "to": "Model_Selection",
      "relationship": "has_subtopic"
    },
    {
      "from": "Optimizers",
      "to": "Gradient Descent (GD)",
      "relationship": "subtopic"
    },
    {
      "from": "LossFunction",
      "to": "GradientCalculation",
      "relationship": "has_subtopic"
    },
    {
      "from": "HousingPricePrediction",
      "to": "DerivedFeatures",
      "relationship": "depends_on"
    },
    {
      "from": "EM_Algorithm",
      "to": "Log_Likelihood_Optimization",
      "relationship": "subtopic"
    },
    {
      "from": "ModulesInMLP",
      "to": "ParameterizationOfModules",
      "relationship": "depends_on"
    },
    {
      "from": "Cross Validation",
      "to": "Leave-One-Out CV",
      "relationship": "has_subtopic"
    },
    {
      "from": "Data Visualization",
      "to": "Principal Component Analysis (PCA)",
      "relationship": "subtopic"
    },
    {
      "from": "7 Deep learning",
      "to": "7.2 Neural networks",
      "relationship": "has_subtopic"
    },
    {
      "from": "PartialDerivativeCalculation",
      "to": "GradientDescentAlgorithm",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningConcepts",
      "to": "FeatureExtraction",
      "relationship": "has_subtopic"
    },
    {
      "from": "Support Vector Machines (SVMs)",
      "to": "Notation for SVMs",
      "relationship": "subtopic"
    },
    {
      "from": "ModelAdaptation",
      "to": "Finetuning",
      "relationship": "subtopic"
    },
    {
      "from": "7.4 Backpropagation",
      "to": "7.4.1 Preliminaries on partial derivatives",
      "relationship": "has_subtopic"
    },
    {
      "from": "4.1 Gaussian discriminant analysis",
      "to": "4.1.1 The multivariate normal distribution",
      "relationship": "has_subtopic"
    },
    {
      "from": "Primal Problem",
      "to": "Relationship Between Primal and Dual",
      "relationship": "related_to"
    },
    {
      "from": "Theorem 7.4.1",
      "to": "Backpropagation",
      "relationship": "related_to"
    },
    {
      "from": "Machine_Learning_Challenges",
      "to": "k_Fold_Cross_Validation",
      "relationship": "has_subtopic"
    },
    {
      "from": "Gradient Calculation",
      "to": "Matrix Derivatives",
      "relationship": "subtopic"
    },
    {
      "from": "ExponentialFamilyDistributions",
      "to": "NaturalParameter",
      "relationship": "has_component"
    },
    {
      "from": "Text_Classification",
      "to": "Feature_Vector_Selection",
      "relationship": "subtopic"
    },
    {
      "from": "State_Transition_Probabilities",
      "to": "MDP_Model_Learning",
      "relationship": "depends_on"
    },
    {
      "from": "Support Vector Machines (SVM)",
      "to": "Machine Learning Overview",
      "relationship": "depends_on"
    },
    {
      "from": "Convex_Functions",
      "to": "Jensens_Inequality",
      "relationship": "subtopic"
    },
    {
      "from": "Generalization_Error",
      "to": "Machine_Learning_Concepts",
      "relationship": "subtopic"
    },
    {
      "from": "Locally Weighted Linear Regression",
      "to": "Value Function Approximation",
      "relationship": "related_to"
    },
    {
      "from": "EM Algorithm",
      "to": "Convex Function",
      "relationship": "related_to"
    },
    {
      "from": "ParameterEstimation",
      "to": "NaiveBayesAlgorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Softmax_Function",
      "to": "Logits",
      "relationship": "subtopic"
    },
    {
      "from": "Backpropagation Algorithm",
      "to": "Intermediate Values Storage",
      "relationship": "related_to"
    },
    {
      "from": "LinearRegression",
      "to": "HypothesisFunction",
      "relationship": "has_subtopic"
    },
    {
      "from": "MachineLearningOverview",
      "to": "BinaryClassificationProblem",
      "relationship": "contains"
    },
    {
      "from": "CrossValidation",
      "to": "HoldOutCrossValidation",
      "relationship": "subtopic"
    },
    {
      "from": "Quadratic_Form",
      "to": "Dynamics_Model",
      "relationship": "depends_on"
    },
    {
      "from": "Machine_Learning_Bias_Variance_Tradeoff",
      "to": "Linear_Model_Failure",
      "relationship": "subtopic"
    },
    {
      "from": "Algorithm 6",
      "to": "Procedure VE",
      "relationship": "depends_on"
    },
    {
      "from": "Step 1",
      "to": "Gaussian Distributions",
      "relationship": "depends_on"
    },
    {
      "from": "Finetuning_Pretrained_Models",
      "to": "Optimization_Objective",
      "relationship": "related_to"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Negative_Log_Likelihood",
      "relationship": "related_to"
    },
    {
      "from": "Transition_Probabilities_Sampling",
      "to": "Chapter_17_Policy_Gradient_REINFORCE",
      "relationship": "subtopic"
    },
    {
      "from": "Variational Autoencoders",
      "to": "Machine Learning Papers",
      "relationship": "subtopic"
    },
    {
      "from": "GeneralizedLinearModels",
      "to": "Assumption3",
      "relationship": "has_assumption"
    },
    {
      "from": "Regularization in Deep Learning",
      "to": "Explicit Regularization Techniques",
      "relationship": "contains"
    },
    {
      "from": "EM Algorithm",
      "to": "Variational Inference",
      "relationship": "related_to"
    },
    {
      "from": "1.2 The normal equations",
      "to": "1.2.2 Least squares revisited",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine_Learning_Backward_Functions",
      "to": "Squared_Loss_Backward",
      "relationship": "subtopic"
    },
    {
      "from": "PayoffDefinition",
      "to": "FiniteHorizonMDP",
      "relationship": "subtopic"
    },
    {
      "from": "BackwardFunctionWb",
      "to": "VectorizedNotation",
      "relationship": "has_subtopic"
    },
    {
      "from": "ICA_Ambiguities",
      "to": "Scaling_Impact",
      "relationship": "has_subtopic"
    },
    {
      "from": "Continuous state MDPs",
      "to": "Discretization",
      "relationship": "has_subtopic"
    },
    {
      "from": "ICA",
      "to": "Mixing_Matrix_A",
      "relationship": "component_of"
    },
    {
      "from": "SingleExampleOptimization",
      "to": "SummationNotEssential",
      "relationship": "depends_on"
    },
    {
      "from": "MachineLearningConcepts",
      "to": "LinearTransformationsEffect",
      "relationship": "contains"
    },
    {
      "from": "DataPreprocessing",
      "to": "LogisticFunctionDerivative",
      "relationship": "depends_on"
    },
    {
      "from": "4.2 Naive bayes (Option Reading)",
      "to": "4.2.1 Laplace smoothing",
      "relationship": "has_subtopic"
    },
    {
      "from": "Double Descent Phenomenon",
      "to": "Implicit Regularization",
      "relationship": "related_to"
    },
    {
      "from": "NewtonMethod",
      "to": "FisherScoring",
      "relationship": "special_case_of"
    },
    {
      "from": "Multiple_Examples_Consideration",
      "to": "Evidence_Lower_Bound_(ELBO)",
      "relationship": "subtopic"
    },
    {
      "from": "ProbabilisticModeling",
      "to": "LikelihoodFunction",
      "relationship": "subtopic"
    },
    {
      "from": "MStepUpdateRule",
      "to": "ExpectationMaximizationAlgorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Expectation_Computation",
      "to": "Deterministic_Simulator",
      "relationship": "subtopic"
    },
    {
      "from": "statistical_mechanics_of_learning",
      "to": "generalization",
      "relationship": "subtopic"
    },
    {
      "from": "Naive Bayes Algorithm",
      "to": "Binary Features",
      "relationship": "depends_on"
    },
    {
      "from": "HypothesisClassParameterization",
      "to": "LinearClassifierDefinition",
      "relationship": "depends_on"
    },
    {
      "from": "Model_Selection",
      "to": "Cross_Validation",
      "relationship": "includes"
    },
    {
      "from": "Eigenvectors and Eigenvalues",
      "to": "Principal Component Analysis (PCA)",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Algorithms",
      "to": "Value Iteration",
      "relationship": "related_to"
    },
    {
      "from": "Kernel_Functions",
      "to": "Necessary_Conditions",
      "relationship": "subtopic"
    },
    {
      "from": "Objective_Function_W",
      "to": "Maximization_Process",
      "relationship": "subtopic"
    },
    {
      "from": "IntermediateVariables",
      "to": "ForwardPass",
      "relationship": "contains"
    },
    {
      "from": "Machine_Learning_Optimization",
      "to": "Stochastic_Gradient_Descent",
      "relationship": "has_subtopic"
    },
    {
      "from": "Reinforcement Learning",
      "to": "Markov Decision Processes (MDP)",
      "relationship": "defines_formalism"
    },
    {
      "from": "Hypothesis_Space_Size",
      "to": "Sample_Complexity",
      "relationship": "subtopic"
    },
    {
      "from": "Reparameterization_Trick",
      "to": "Mathematical_Formulation_Reparametrization",
      "relationship": "subtopic_of"
    },
    {
      "from": "Asynchronous_Update",
      "to": "Value_Iteration",
      "relationship": "subtopic"
    },
    {
      "from": "Finite Horizon MDP Dynamics",
      "to": "Time Dependent Transition Probabilities",
      "relationship": "subtopic"
    },
    {
      "from": "GaussianDiscriminantAnalysis",
      "to": "ModelAssumptions",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningModels",
      "to": "GaussianDistribution",
      "relationship": "related_to"
    },
    {
      "from": "Maximize_Geometric_Margin",
      "to": "Optimal_Margin_Classifier",
      "relationship": "subtopic"
    },
    {
      "from": "KernelMethods",
      "to": "KernelTrick",
      "relationship": "has_subtopic"
    },
    {
      "from": "Cross_Validation",
      "to": "k_Fold_Cross_Validation",
      "relationship": "alternative_method"
    },
    {
      "from": "GaussianMixtureModel",
      "to": "EMAlgorithm",
      "relationship": "subtopic"
    },
    {
      "from": "LogisticRegression",
      "to": "ProbabilityPrediction",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Overparameterized Models",
      "relationship": "related_to"
    },
    {
      "from": "I Supervised learning",
      "to": "3 Generalized linear models",
      "relationship": "has_subtopic"
    },
    {
      "from": "Labeled_Task_Dataset",
      "to": "Adaptation_Phase",
      "relationship": "depends_on"
    },
    {
      "from": "Finding Roots",
      "to": "Maximizing Functions",
      "relationship": "related_to"
    },
    {
      "from": "Markov Decision Processes (MDP)",
      "to": "Actions",
      "relationship": "component_of"
    },
    {
      "from": "Machine_Learning_Bias_Variance_Tradeoff",
      "to": "Sample_Complexity_Bound",
      "relationship": "subtopic"
    },
    {
      "from": "Procedure VE",
      "to": "Option 1 and Option 2",
      "relationship": "related_to"
    },
    {
      "from": "Kernels_in_Machine_Learning",
      "to": "Kernel_Function_K",
      "relationship": "related_to"
    },
    {
      "from": "MachineLearningOverview",
      "to": "FeatureMapping",
      "relationship": "related_to"
    },
    {
      "from": "Finite_Horizon_Case",
      "to": "Chapter_17_Policy_Gradient_REINFORCE",
      "relationship": "subtopic"
    },
    {
      "from": "NaturalParameterForBernoulli",
      "to": "SigmoidFunction",
      "relationship": "depends_on"
    },
    {
      "from": "Variational_Autoencoder",
      "to": "Encoder_Decoder_Networks",
      "relationship": "subtopic"
    },
    {
      "from": "NeuralNetworksInspiration",
      "to": "ParametersTheta",
      "relationship": "contains"
    },
    {
      "from": "Loss Function",
      "to": "Negative Log-Likelihood",
      "relationship": "subtopic"
    },
    {
      "from": "Locally Weighted Linear Regression",
      "to": "Machine Learning Models",
      "relationship": "subtopic"
    },
    {
      "from": "Dimensionality Reduction Techniques",
      "to": "Principal Component Analysis (PCA)",
      "relationship": "contains"
    },
    {
      "from": "SIMCLRAlgorithm",
      "to": "AugmentationTechniques",
      "relationship": "uses"
    },
    {
      "from": "BatchGradientDescent",
      "to": "BetaUpdateEquation",
      "relationship": "depends_on"
    },
    {
      "from": "Gaussian Discriminant Analysis (GDA)",
      "to": "Bayesian Classification",
      "relationship": "subtopic"
    },
    {
      "from": "GaussianDiscriminantAnalysis",
      "to": "AsymptoticEfficiency",
      "relationship": "subtopic"
    },
    {
      "from": "Self-Supervised Learning",
      "to": "Data Augmentation",
      "relationship": "depends_on"
    },
    {
      "from": "BinaryClassification",
      "to": "LogisticFunction",
      "relationship": "depends_on"
    },
    {
      "from": "GaussianDiscriminantAnalysis(GDA)",
      "to": "DecisionBoundary",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine_Learning_Bias_Variance",
      "to": "Bias_Term",
      "relationship": "contains"
    },
    {
      "from": "Sample Complexity Bounds",
      "to": "Generalization Error",
      "relationship": "subtopic"
    },
    {
      "from": "TwoLayerNN",
      "to": "GELUFunction",
      "relationship": "depends_on"
    },
    {
      "from": "Discretization in MDPs",
      "to": "Policy Iteration",
      "relationship": "depends_on"
    },
    {
      "from": "GeometricMargin",
      "to": "FunctionalMargin",
      "relationship": "depends_on"
    },
    {
      "from": "Regularization and Non-Separable Case",
      "to": "L1 Regularization",
      "relationship": "has_subtopic"
    },
    {
      "from": "LayerNormalization",
      "to": "AffineTransformation",
      "relationship": "related_to"
    },
    {
      "from": "Independent components analysis",
      "to": "ICA ambiguities",
      "relationship": "has_subtopic"
    },
    {
      "from": "SMO_Algorithm",
      "to": "Heuristic_Selection",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Neural Networks",
      "relationship": "depends_on"
    },
    {
      "from": "Jensen's Inequality",
      "to": "E[f(X)] vs f(E[X])",
      "relationship": "subtopic"
    },
    {
      "from": "Regularization and model selection",
      "to": "Model selection via cross validation",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine_Learning_Bias_Variance",
      "to": "Noise_Prediction_Impact",
      "relationship": "contains"
    },
    {
      "from": "TwoLayerNN",
      "to": "TanhFunction",
      "relationship": "depends_on"
    },
    {
      "from": "Support_Vector_Machines",
      "to": "Optimal_Margin_Classifier",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "EM Algorithms",
      "relationship": "related_to"
    },
    {
      "from": "MachineLearningOverview",
      "to": "ELBOOptimization",
      "relationship": "contains"
    },
    {
      "from": "Finite Horizon MDP Dynamics",
      "to": "Value Function for Non-Stationary MDPs",
      "relationship": "subtopic"
    },
    {
      "from": "From non-linear dynamics to LQR",
      "to": "Differential Dynamic Programming (DDP)",
      "relationship": "subtopic"
    },
    {
      "from": "4.1 Gaussian discriminant analysis",
      "to": "4.1.2 The Gaussian discriminant analysis model",
      "relationship": "has_subtopic"
    },
    {
      "from": "Conditional Probabilistic Models",
      "to": "Exponential Family Distribution",
      "relationship": "depends_on"
    },
    {
      "from": "Binary_Classification",
      "to": "Training_Error",
      "relationship": "subtopic"
    },
    {
      "from": "Model-Based Reinforcement Learning",
      "to": "Machine Learning Papers",
      "relationship": "subtopic"
    },
    {
      "from": "Generalization",
      "to": "Sample complexity bounds (optional readings)",
      "relationship": "has_subtopic"
    },
    {
      "from": "TextGeneration",
      "to": "ConditionalProbability",
      "relationship": "depends_on"
    },
    {
      "from": "EM algorithms",
      "to": "EM for mixture of Gaussians",
      "relationship": "has_subtopic"
    },
    {
      "from": "GeneralizedLinearModels",
      "to": "ExponentialFamilyDistributions",
      "relationship": "contains"
    },
    {
      "from": "State Transition",
      "to": "Markov Decision Process (MDP)",
      "relationship": "depends_on"
    },
    {
      "from": "Backward_Function_Linear_Map",
      "to": "Jacobian_Matrix_Transpose",
      "relationship": "depends_on"
    },
    {
      "from": "Naive Bayes Algorithm",
      "to": "Feature Representation",
      "relationship": "depends_on"
    },
    {
      "from": "EM_Algorithm",
      "to": "Convergence_Issues",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Algorithms",
      "to": "Kernel_Trick",
      "relationship": "contains"
    },
    {
      "from": "Machine_Learning_Optimization",
      "to": "Normal_Equations_Method",
      "relationship": "related_to"
    },
    {
      "from": "MachineLearningAlgorithms",
      "to": "DiscriminativeLearning",
      "relationship": "contains"
    },
    {
      "from": "Regularization",
      "to": "Overfitting",
      "relationship": "addresses"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Linear Regression Setup",
      "relationship": "depends_on"
    },
    {
      "from": "EM algorithms",
      "to": "Jensen's inequality",
      "relationship": "has_subtopic"
    },
    {
      "from": "MachineLearningConcepts",
      "to": "GaussianDiscriminantAnalysisModel",
      "relationship": "contains"
    },
    {
      "from": "Machine_Learning",
      "to": "Policy_Definition",
      "relationship": "related_to"
    },
    {
      "from": "RegressionProblems",
      "to": "MeanSquaredLoss",
      "relationship": "subtopic"
    },
    {
      "from": "KernelTrick",
      "to": "MachineLearningConcepts",
      "relationship": "related_to"
    },
    {
      "from": "ICA_Ambiguities",
      "to": "Non_Gaussian_Sources",
      "relationship": "has_subtopic"
    },
    {
      "from": "Self-supervised learning and foundation models",
      "to": "Pretrained large language models",
      "relationship": "has_subtopic"
    },
    {
      "from": "4 Generative learning algorithms",
      "to": "4.2 Naive bayes (Option Reading)",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Differential_Dynamic_Programming",
      "relationship": "related_to"
    },
    {
      "from": "Machine_Learning_Optimization",
      "to": "Batch_Gradient_Descent",
      "relationship": "has_subtopic"
    },
    {
      "from": "ReLU Activation Function",
      "to": "Multi-layer Neural Networks",
      "relationship": "subtopic"
    },
    {
      "from": "Convolutional_Layers",
      "to": "Efficiency_of_Convolution",
      "relationship": "subtopic_of"
    },
    {
      "from": "WeightsInLWLR",
      "to": "BandwidthParameter",
      "relationship": "related_to"
    },
    {
      "from": "Machine_Learning_Backward_Functions",
      "to": "Efficiency_of_Backward_Pass",
      "relationship": "related_to"
    },
    {
      "from": "MDP Simulators",
      "to": "Machine Learning Models",
      "relationship": "subtopic"
    },
    {
      "from": "Representation Function",
      "to": "Negative Pair",
      "relationship": "related_to"
    },
    {
      "from": "Machine_Learning_Optimization",
      "to": "Coordinate_Ascend_Method",
      "relationship": "subtopic"
    },
    {
      "from": "ReLUActivation",
      "to": "NeuralNetworks",
      "relationship": "subtopic"
    },
    {
      "from": "AdaptiveSampling",
      "to": "TemperatureParameter",
      "relationship": "related_to"
    },
    {
      "from": "GeneralizedLinearModels",
      "to": "Assumption1",
      "relationship": "has_assumption"
    },
    {
      "from": "Principal Component Analysis (PCA)",
      "to": "Noise Reduction",
      "relationship": "subtopic_of"
    },
    {
      "from": "MachineLearningOverview",
      "to": "ResNetArchitecture",
      "relationship": "contains"
    },
    {
      "from": "Exponential_Family_Distributions",
      "to": "Canonical_Link_Function",
      "relationship": "has_subtopic"
    },
    {
      "from": "Uniform_Convergence",
      "to": "Generalization_Error_Bound",
      "relationship": "related_to"
    },
    {
      "from": "Unsupervised Learning",
      "to": "Mixture of Gaussians Model",
      "relationship": "depends_on"
    },
    {
      "from": "Unsupervised learning",
      "to": "Principal components analysis",
      "relationship": "has_subtopic"
    },
    {
      "from": "Markov Decision Processes (MDP)",
      "to": "Discount Factor",
      "relationship": "component_of"
    },
    {
      "from": "Machine_Learning_Basics",
      "to": "LMS_Algorithm",
      "relationship": "depends_on"
    },
    {
      "from": "Machine_Learning_Adaptation_Methods",
      "to": "Finetuning_Pretrained_Models",
      "relationship": "has_subtopic"
    },
    {
      "from": "Backpropagation",
      "to": "Backward functions for basic modules",
      "relationship": "has_subtopic"
    },
    {
      "from": "LogisticRegression",
      "to": "HypothesisFunction",
      "relationship": "uses"
    },
    {
      "from": "NeuralNetworks",
      "to": "SingleNeuronNN",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Backpropagation",
      "to": "Efficient_Backward_Propagation",
      "relationship": "related_to"
    },
    {
      "from": "Variational_Inference_Review",
      "to": "Machine_Learning_Papers",
      "relationship": "belongs_to"
    },
    {
      "from": "FittedValueIteration",
      "to": "StateSampling",
      "relationship": "depends_on"
    },
    {
      "from": "MachineLearningConcepts",
      "to": "BiasVsVarianceTradeoff",
      "relationship": "subtopic"
    },
    {
      "from": "1 Linear regression",
      "to": "1.3 Probabilistic interpretation",
      "relationship": "has_subtopic"
    },
    {
      "from": "5 Kernel methods",
      "to": "5.3 LMS with the kernel trick",
      "relationship": "has_subtopic"
    },
    {
      "from": "E_Step",
      "to": "Gaussian_Mixture_Models",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Overview",
      "to": "Non-Parametric Algorithms",
      "relationship": "contains"
    },
    {
      "from": "5 Kernel methods",
      "to": "5.1 Feature maps",
      "relationship": "has_subtopic"
    },
    {
      "from": "Model Creation Methods",
      "to": "Physics Simulation",
      "relationship": "has_subtopic"
    },
    {
      "from": "MachineLearningModels",
      "to": "DeterministicModel",
      "relationship": "subtopic"
    },
    {
      "from": "Optimal_Margin_Classifier",
      "to": "Machine_Learning_Concepts",
      "relationship": "related_to"
    },
    {
      "from": "KKT_Conditions",
      "to": "Dual_Complementarity",
      "relationship": "subtopic"
    },
    {
      "from": "MAPEstimate",
      "to": "MLEvsMAP",
      "relationship": "related_to"
    },
    {
      "from": "ProbabilityDistributions",
      "to": "JensensInequality",
      "relationship": "depends_on"
    },
    {
      "from": "MachineLearningConcepts",
      "to": "ShatteringConcept",
      "relationship": "related_to"
    },
    {
      "from": "Support_Vector_Machines_SVM",
      "to": "Optimal_Margin_Classifier",
      "relationship": "leads_to"
    },
    {
      "from": "Necessary_Conditions",
      "to": "Kernel_Matrix",
      "relationship": "related_to"
    },
    {
      "from": "SingleNeuronNN",
      "to": "HousingPricePrediction",
      "relationship": "related_to"
    },
    {
      "from": "Machine_Learning_Backward_Functions",
      "to": "Vectorized_Notation_Backward_Func",
      "relationship": "subtopic"
    },
    {
      "from": "Log_Probability_Ratio",
      "to": "Gradient_Estimation",
      "relationship": "depends_on"
    },
    {
      "from": "MachineLearningModels",
      "to": "BinaryClassification",
      "relationship": "subtopic"
    },
    {
      "from": "Value_Iteration",
      "to": "MDP_Model_Learning",
      "relationship": "related_to"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Support_Vector_Machines_SVM",
      "relationship": "contains"
    },
    {
      "from": "GenerativeLearning",
      "to": "ConditionalProbabilityModel",
      "relationship": "contains"
    },
    {
      "from": "Machine_Learning",
      "to": "Feature_Engineering",
      "relationship": "depends_on"
    },
    {
      "from": "MachineLearningOverview",
      "to": "KernelMethods",
      "relationship": "contains"
    },
    {
      "from": "MachineLearningOverview",
      "to": "ICAConcepts",
      "relationship": "contains"
    },
    {
      "from": "MachineLearningModels",
      "to": "LossFunctions",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Adaptation_Techniques",
      "to": "In-Context_Learning",
      "relationship": "subtopic"
    },
    {
      "from": "LinearRegression",
      "to": "FeatureSelection",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningModels",
      "to": "GeneralizedLinearModels",
      "relationship": "contains"
    },
    {
      "from": "Predict Step",
      "to": "Update Step",
      "relationship": "related_to"
    },
    {
      "from": "FullyConnectedNN",
      "to": "IntermediateVariables",
      "relationship": "depends_on"
    },
    {
      "from": "FeatureMapsAndKernels",
      "to": "InnerProductCalculation",
      "relationship": "subtopic"
    },
    {
      "from": "Computational Efficiency",
      "to": "Kalman Filter",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Algorithms",
      "to": "Initialization",
      "relationship": "subtopic"
    },
    {
      "from": "Training_Transformer",
      "to": "Cross_Entropy_Loss",
      "relationship": "related_to"
    },
    {
      "from": "Probabilistic Interpretation of Linear Regression",
      "to": "Least-Squares Cost Function J",
      "relationship": "related_to"
    },
    {
      "from": "BernoulliDistribution",
      "to": "NaturalParameterForBernoulli",
      "relationship": "has_subtopic"
    },
    {
      "from": "Adaptation_Algorithm",
      "to": "Linear_Probe_Method",
      "relationship": "has_subtopic"
    },
    {
      "from": "Hoeffding_Inequality",
      "to": "Machine_Learning_Concepts",
      "relationship": "subtopic"
    },
    {
      "from": "LogisticRegression",
      "to": "LogitFunction",
      "relationship": "depends_on"
    },
    {
      "from": "MachineLearningOverview",
      "to": "LocallyWeightedLinearRegression",
      "relationship": "related_to"
    },
    {
      "from": "ELBO Lower Bound",
      "to": "Variational Inference",
      "relationship": "depends_on"
    },
    {
      "from": "ICAConcepts",
      "to": "MaximumLikelihoodEstimation",
      "relationship": "subtopic"
    },
    {
      "from": "Support_Vector_Machines_SVMs",
      "to": "SVM_Dual_Problem",
      "relationship": "depends_on"
    },
    {
      "from": "DiscriminativeLearning",
      "to": "LogisticRegression",
      "relationship": "subtopic"
    },
    {
      "from": "Sample_Complexity",
      "to": "Hypothesis_Class_Size",
      "relationship": "depends_on"
    },
    {
      "from": "I Supervised learning",
      "to": "6 Support vector machines",
      "relationship": "has_subtopic"
    },
    {
      "from": "Training_Error_Generalization_Error_Difference",
      "to": "Uniform_Convergence",
      "relationship": "depends_on"
    },
    {
      "from": "BernoulliEventModel",
      "to": "NaiveBayes",
      "relationship": "related_to"
    },
    {
      "from": "Reinforcement learning",
      "to": "Markov decision processes",
      "relationship": "has_subtopic"
    },
    {
      "from": "LinearRegression",
      "to": "FittingTheta",
      "relationship": "depends_on"
    },
    {
      "from": "Non-linear Dynamics and LQR",
      "to": "Inverted Pendulum Example",
      "relationship": "example"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Geometric_Margins",
      "relationship": "subtopic"
    },
    {
      "from": "Randomized_Policy",
      "to": "Chapter_17_Policy_Gradient_REINFORCE",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Overview",
      "to": "Parametric Algorithms",
      "relationship": "contains"
    },
    {
      "from": "TwoLayerNN",
      "to": "SigmoidFunction",
      "relationship": "depends_on"
    },
    {
      "from": "Bayes' Theorem",
      "to": "Prior Distribution",
      "relationship": "related_to"
    },
    {
      "from": "PolicyGradients",
      "to": "SampleBasedEstimator",
      "relationship": "has_subtopic"
    },
    {
      "from": "7.4 Backpropagation",
      "to": "7.4.2 General strategy of backpropagation",
      "relationship": "has_subtopic"
    },
    {
      "from": "Optimal Value Function",
      "to": "Value Function",
      "relationship": "subtopic"
    },
    {
      "from": "Optimal_Policy",
      "to": "Machine_Learning",
      "relationship": "subtopic"
    },
    {
      "from": "SVM",
      "to": "Model Set M",
      "relationship": "subtopic"
    },
    {
      "from": "Deep_Learning",
      "to": "Neural_Networks",
      "relationship": "depends_on"
    },
    {
      "from": "Machine_Learning",
      "to": "Deep_Learning",
      "relationship": "subtopic"
    },
    {
      "from": "Classification Problem",
      "to": "Binary Classification",
      "relationship": "subtopic"
    },
    {
      "from": "Dimensionality Reduction",
      "to": "Principal Component Analysis (PCA)",
      "relationship": "depends_on"
    },
    {
      "from": "Machine_Learning",
      "to": "Optimization_Techniques",
      "relationship": "related_to"
    },
    {
      "from": "TwoLayerNN",
      "to": "IdentityFunction",
      "relationship": "related_to"
    },
    {
      "from": "3.2 Constructing GLMs",
      "to": "3.2.2 Logistic regression",
      "relationship": "has_subtopic"
    },
    {
      "from": "k-means_algorithm",
      "to": "inner_loop_steps",
      "relationship": "has_subtopic"
    },
    {
      "from": "ELBO",
      "to": "EStep",
      "relationship": "depends_on"
    },
    {
      "from": "LQR Algorithm Steps",
      "to": "Efficiency Improvement",
      "relationship": "subtopic"
    },
    {
      "from": "Optimal Parameters Calculation",
      "to": "Prediction with Inner Products",
      "relationship": "subtopic"
    },
    {
      "from": "GenerativeLearning",
      "to": "ClassPriors",
      "relationship": "contains"
    },
    {
      "from": "BackwardFunctionWb",
      "to": "EfficiencyConsiderations",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine_Learning_Theorem",
      "to": "Bias_Variance_Tradeoff",
      "relationship": "subtopic"
    },
    {
      "from": "LogisticFunction",
      "to": "DerivativeOfSigmoid",
      "relationship": "has_derivative"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Softmax_Function",
      "relationship": "depends_on"
    },
    {
      "from": "Matricization Approach",
      "to": "Data Matrix Representation",
      "relationship": "subtopic"
    },
    {
      "from": "Generalization",
      "to": "Training Loss Function",
      "relationship": "depends_on"
    },
    {
      "from": "Fitted_Value_Iteration",
      "to": "Convergence_Issues",
      "relationship": "related_to"
    },
    {
      "from": "EmpiricalRiskMinimization",
      "to": "TrainingSetS",
      "relationship": "subtopic"
    },
    {
      "from": "SupportVectorsConcept",
      "to": "KernelTrickIntroduction",
      "relationship": "related_to"
    },
    {
      "from": "Expected_Total_Payoff_Optimization",
      "to": "Chapter_17_Policy_Gradient_REINFORCE",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Adaptation_Techniques",
      "to": "Model_Optimization",
      "relationship": "related_to"
    },
    {
      "from": "Contrastive_Learning_Visual_Representations",
      "to": "Machine_Learning_Papers",
      "relationship": "belongs_to"
    },
    {
      "from": "Physics Simulation",
      "to": "Off-the-Shelf Software",
      "relationship": "related_to"
    },
    {
      "from": "IterativeUpdateRule",
      "to": "LinearCombinationRepresentation",
      "relationship": "subtopic"
    },
    {
      "from": "Cost_Function",
      "to": "Ordinary_Least_Squares",
      "relationship": "related_to"
    },
    {
      "from": "Characterization_of_Kernels",
      "to": "Example_Kernel_Functions",
      "relationship": "example"
    },
    {
      "from": "Kernel Examples",
      "to": "String Classification Example",
      "relationship": "subtopic"
    },
    {
      "from": "ModelParameters",
      "to": "LogLikelihoodCalculation",
      "relationship": "related_to"
    },
    {
      "from": "Sample_Complexity",
      "to": "Parameterization_Impact",
      "relationship": "depends_on"
    },
    {
      "from": "Learning a model for an MDP",
      "to": "Connections between Policy and Value Iteration (Optional)",
      "relationship": "contains"
    },
    {
      "from": "Double Descent Phenomenon",
      "to": "Sample-wise Double Descent",
      "relationship": "subtopic"
    },
    {
      "from": "LinearRegression",
      "to": "CostFunction",
      "relationship": "has_subtopic"
    },
    {
      "from": "Value_Iteration",
      "to": "MDP_Finite_State_Space",
      "relationship": "subtopic"
    },
    {
      "from": "Policy_Iteration",
      "to": "Linear_System_Solver",
      "relationship": "uses"
    },
    {
      "from": "MachineLearningBasics",
      "to": "GaussianDistribution",
      "relationship": "contains"
    },
    {
      "from": "Variational_Inference",
      "to": "Variational_Autoencoder",
      "relationship": "component_of"
    },
    {
      "from": "HoldOutCrossValidation",
      "to": "S_train",
      "relationship": "subtopic"
    },
    {
      "from": "Step2OptimalPolicy",
      "to": "LQRAlgorithmSteps",
      "relationship": "subtopic"
    },
    {
      "from": "RuntimeAndMemoryEfficiency",
      "to": "InitializationThetaZero",
      "relationship": "subtopic"
    },
    {
      "from": "Negative Log-Likelihood",
      "to": "Cross-Entropy Loss",
      "relationship": "related_to"
    },
    {
      "from": "MachineLearningModels",
      "to": "ConditionalProbabilityModeling",
      "relationship": "contains"
    },
    {
      "from": "Machine_Learning_Theory",
      "to": "Empirical_Risk_Minimization",
      "relationship": "related_to"
    },
    {
      "from": "Support_Vector_Machines_SVMs",
      "to": "SMO_Algorithm",
      "relationship": "related_to"
    },
    {
      "from": "LogisticLossFunction",
      "to": "NegativeLogLikelihood",
      "relationship": "depends_on"
    },
    {
      "from": "Feature_Maps",
      "to": "Linear_Model",
      "relationship": "subtopic"
    },
    {
      "from": "ComparisonValuePolicyIteration",
      "to": "MachineLearningConcepts",
      "relationship": "subtopic"
    },
    {
      "from": "Linear_Model_Failure",
      "to": "Bias_Definition",
      "relationship": "depends_on"
    },
    {
      "from": "Observation vs State",
      "to": "Partially Observable MDPs (POMDP)",
      "relationship": "depends_on"
    },
    {
      "from": "EM_Algorithm",
      "to": "E_Step",
      "relationship": "subtopic"
    },
    {
      "from": "Gradient Estimation",
      "to": "Baseline Function",
      "relationship": "introduces"
    },
    {
      "from": "Log_Probability_Ratio",
      "to": "Policy_Gradient_Methods",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Loss Function",
      "relationship": "depends_on"
    },
    {
      "from": "Total Parameters Count",
      "to": "Multi-layer Neural Networks",
      "relationship": "subtopic"
    },
    {
      "from": "Dual_Problem_Solution",
      "to": "Feasibility_Constraints",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Policy Gradient Methods",
      "relationship": "contains"
    },
    {
      "from": "GaussianDiscriminantAnalysisModel",
      "to": "BernoulliDistribution",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningOverview",
      "to": "Conv2D-S",
      "relationship": "subtopic"
    },
    {
      "from": "Function_Representation",
      "to": "Cost_Function",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "ELBO_Gradient_Computation",
      "relationship": "has_subtopic"
    },
    {
      "from": "Loss Functions",
      "to": "Logistic Loss",
      "relationship": "subtopic"
    },
    {
      "from": "1D Example",
      "to": "Density Transformation",
      "relationship": "depends_on"
    },
    {
      "from": "Uniform_Convergence_Assumption",
      "to": "Generalization_Error_Bound",
      "relationship": "depends_on"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Value_Iteration",
      "relationship": "depends_on"
    },
    {
      "from": "VectorizationTechniques",
      "to": "Broadcasting",
      "relationship": "subtopic"
    },
    {
      "from": "Theorem_Jensens_Inequality",
      "to": "Jensens_Inequality",
      "relationship": "subtopic"
    },
    {
      "from": "Jensen's Inequality",
      "to": "Convex Function",
      "relationship": "subtopic"
    },
    {
      "from": "Binary_Classification",
      "to": "Training_Set",
      "relationship": "depends_on"
    },
    {
      "from": "ICAOnGaussianData",
      "to": "RotationalSymmetry",
      "relationship": "depends_on"
    },
    {
      "from": "Support_Vector_Machines_SVM",
      "to": "SMO_Algorithm",
      "relationship": "contains"
    },
    {
      "from": "MachineLearningOverview",
      "to": "TwoLayerNN",
      "relationship": "contains"
    },
    {
      "from": "MachineLearningModels",
      "to": "DecisionBoundaries",
      "relationship": "depends_on"
    },
    {
      "from": "Feature Discovery",
      "to": "Black Box Nature",
      "relationship": "subtopic"
    },
    {
      "from": "BinaryClassificationProblem",
      "to": "LossFunctionFormulation",
      "relationship": "related_to"
    },
    {
      "from": "Mixture of Gaussians Model",
      "to": "Joint Distribution",
      "relationship": "subtopic"
    },
    {
      "from": "Coordinate_Ascend_Algorithm",
      "to": "Unconstrained_Optimization_Problem",
      "relationship": "subtopic"
    },
    {
      "from": "Overfitting_Underfitting",
      "to": "Test_Error_Training_Error",
      "relationship": "depends_on"
    },
    {
      "from": "LayerNormalization",
      "to": "TransformerArchitecture",
      "relationship": "related_to"
    },
    {
      "from": "Loss Functions",
      "to": "Cross-Entropy Loss",
      "relationship": "subtopic"
    },
    {
      "from": "Expected_Immediate_Rewards",
      "to": "MDP_Model_Learning",
      "relationship": "depends_on"
    },
    {
      "from": "Cross_Entropy_Loss",
      "to": "Softmax_Cross_Entropy_Loss",
      "relationship": "is_a_specific_type_of"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "ELBO_Optimization",
      "relationship": "depends_on"
    },
    {
      "from": "Pretraining_Phase",
      "to": "Transfer_Learning",
      "relationship": "subtopic"
    },
    {
      "from": "BinaryClassificationProblem",
      "to": "MLPModelDefinition",
      "relationship": "depends_on"
    },
    {
      "from": "OrdinaryLeastSquares",
      "to": "ExponentialFamilyDistribution",
      "relationship": "uses_distribution"
    },
    {
      "from": "ParameterizedModel",
      "to": "SoftmaxFunction",
      "relationship": "uses"
    },
    {
      "from": "Learning_Model_for_MDP",
      "to": "Inverted_Pendulum_Problem",
      "relationship": "example_of"
    },
    {
      "from": "ConditionalProbability",
      "to": "JointLikelihood",
      "relationship": "depends_on"
    },
    {
      "from": "Exponential_Family_Distributions",
      "to": "Logistic_Function",
      "relationship": "related_to"
    },
    {
      "from": "Gradient Computation",
      "to": "Theorem 7.4.1",
      "relationship": "subtopic"
    },
    {
      "from": "Dual Problem",
      "to": "Objective Function Dual",
      "relationship": "depends_on"
    },
    {
      "from": "Bias-Variance Tradeoff",
      "to": "Underfitting",
      "relationship": "depends_on"
    },
    {
      "from": "Union_Bound_Application",
      "to": "Uniform_Convergence",
      "relationship": "subtopic"
    },
    {
      "from": "4.1 Gaussian discriminant analysis",
      "to": "4.1.3 Discussion: GDA and logistic regression",
      "relationship": "has_subtopic"
    },
    {
      "from": "PrimalProblem",
      "to": "ThetaP",
      "relationship": "subtopic"
    },
    {
      "from": "7.4 Backpropagation",
      "to": "7.4.4 Back-propagation for MLPs",
      "relationship": "has_subtopic"
    },
    {
      "from": "Policy",
      "to": "Value Function",
      "relationship": "depends_on"
    },
    {
      "from": "Value_Functions",
      "to": "Bellman_Equation",
      "relationship": "related_to"
    },
    {
      "from": "Least-Squares Cost Function J",
      "to": "Target Variables and Inputs Relation",
      "relationship": "subtopic"
    },
    {
      "from": "Neural Network",
      "to": "Model Set M",
      "relationship": "subtopic"
    },
    {
      "from": "k_Fold_Cross_Validation",
      "to": "Leave_One_Out_Cross_Validation",
      "relationship": "special_case_of"
    },
    {
      "from": "LQR, DDP and LQG",
      "to": "Linear Quadratic Gaussian (LQG)",
      "relationship": "subtopic"
    },
    {
      "from": "Convolutional Layers",
      "to": "1-D Convolution (Conv1D)",
      "relationship": "contains"
    },
    {
      "from": "MultinomialEventModel",
      "to": "ParameterEstimation",
      "relationship": "has_subtopic"
    },
    {
      "from": "Non-Stationary Policies",
      "to": "Optimal Policy in Finite Horizon",
      "relationship": "subtopic"
    },
    {
      "from": "Target Vector",
      "to": "Least Squares Revisited",
      "relationship": "subtopic"
    },
    {
      "from": "Finetuning_Pretrained_Models",
      "to": "Linear_Head_Initiation",
      "relationship": "depends_on"
    },
    {
      "from": "MStep",
      "to": "MuParameterUpdate",
      "relationship": "contains"
    },
    {
      "from": "Backpropagation",
      "to": "Auto-differentiation",
      "relationship": "related_to"
    },
    {
      "from": "Learning Theory Proofs",
      "to": "Hoeffding Inequality",
      "relationship": "depends_on"
    },
    {
      "from": "SpeedPerspective",
      "to": "VectorizationInNN",
      "relationship": "depends_on"
    },
    {
      "from": "Principal Component Analysis (PCA)",
      "to": "Plotting Similarity",
      "relationship": "subtopic_of"
    },
    {
      "from": "Policy_Iteration",
      "to": "MDP_Model_Learning",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Models",
      "to": "Support Vector Machines (SVM)",
      "relationship": "contains"
    },
    {
      "from": "CostFunctionJ",
      "to": "GradientDescentAlgorithm",
      "relationship": "related_to"
    },
    {
      "from": "7 Deep learning",
      "to": "7.4 Backpropagation",
      "relationship": "has_subtopic"
    },
    {
      "from": "double_descent",
      "to": "statistical_mechanics_of_learning",
      "relationship": "depends_on"
    },
    {
      "from": "Principal Component Analysis (PCA)",
      "to": "Dimension Reduction Before Supervised Learning",
      "relationship": "subtopic_of"
    },
    {
      "from": "Support Vector Machines (SVMs)",
      "to": "Functional Margin",
      "relationship": "subtopic"
    },
    {
      "from": "ConstrainedOptimization",
      "to": "LagrangeMultipliers",
      "relationship": "depends_on"
    },
    {
      "from": "Training_Error",
      "to": "Generalization_Error_Bound",
      "relationship": "depends_on"
    },
    {
      "from": "ClassificationProblem",
      "to": "LogisticRegression",
      "relationship": "subtopic_of"
    },
    {
      "from": "Bias_Variance_Tradeoff",
      "to": "Machine_Learning_Papers",
      "relationship": "belongs_to"
    },
    {
      "from": "Bias-Variance Tradeoff",
      "to": "Model Complexity",
      "relationship": "explains"
    },
    {
      "from": "DensityEstimation",
      "to": "GaussianMixtureModel",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Overview",
      "to": "Locally Weighted Linear Regression",
      "relationship": "contains"
    },
    {
      "from": "Optimization Methods",
      "to": "Machine Learning Papers",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Bias_Variance_Tradeoff",
      "to": "5th_Degree_Polynomial_Failure",
      "relationship": "subtopic"
    },
    {
      "from": "TemperatureParameter",
      "to": "ConditionalProbability",
      "relationship": "subtopic"
    },
    {
      "from": "GaussianDiscriminantAnalysisModel",
      "to": "MultivariateNormalForClasses",
      "relationship": "subtopic"
    },
    {
      "from": "ICA",
      "to": "Unmixing_Matrix_W",
      "relationship": "component_of"
    },
    {
      "from": "MachineLearningOverview",
      "to": "GradientDescentAlgorithm",
      "relationship": "related_to"
    },
    {
      "from": "Posterior Distribution",
      "to": "Prediction on New Data",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Chain_Rule",
      "relationship": "contains"
    },
    {
      "from": "Pretrained_Models",
      "to": "Natural_Language_Processing",
      "relationship": "contains"
    },
    {
      "from": "3 Generalized linear models",
      "to": "3.1 The exponential family",
      "relationship": "has_subtopic"
    },
    {
      "from": "ContrastiveLearning",
      "to": "PositivePairs",
      "relationship": "has_subtopic"
    },
    {
      "from": "Finite_Horizon_MDPs",
      "to": "General_Setting_Equations",
      "relationship": "subtopic"
    },
    {
      "from": "EventModelsForTextClassification",
      "to": "BernoulliEventModel",
      "relationship": "contains"
    },
    {
      "from": "Prediction on New Data",
      "to": "Expected Value Prediction",
      "relationship": "depends_on"
    },
    {
      "from": "MachineLearningOverview",
      "to": "NeuralNetworksInspiration",
      "relationship": "contains"
    },
    {
      "from": "Learning from Data",
      "to": "Data Collection Process",
      "relationship": "depends_on"
    },
    {
      "from": "Functional_Margin",
      "to": "Scaling_Issue",
      "relationship": "related_to"
    },
    {
      "from": "Optimizers",
      "to": "Implicit Regularization",
      "relationship": "depends_on"
    },
    {
      "from": "MultivariateNormalDistribution",
      "to": "MeanVectorMovement",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Gradient_Estimation",
      "relationship": "contains"
    },
    {
      "from": "GenerativeLearning",
      "to": "BayesRule",
      "relationship": "depends_on"
    },
    {
      "from": "DistanceToBoundary",
      "to": "DecisionBoundary",
      "relationship": "subtopic"
    },
    {
      "from": "KernelMethods",
      "to": "FeatureMaps",
      "relationship": "subtopic"
    },
    {
      "from": "Newton's Method",
      "to": "Maximizing Functions",
      "relationship": "related_to"
    },
    {
      "from": "Normalization Techniques",
      "to": "Batch Normalization (BN)",
      "relationship": "contains"
    },
    {
      "from": "BiasVarianceTradeoff",
      "to": "MSE",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningModels",
      "to": "NonLinearFeatureMapping",
      "relationship": "subtopic"
    },
    {
      "from": "Objective_Function_W",
      "to": "Quadratic_Formulation",
      "relationship": "subtopic"
    },
    {
      "from": "Chapter_15_Summary",
      "to": "Value_Iteration_Preference",
      "relationship": "subtopic"
    },
    {
      "from": "Action Selection",
      "to": "Markov Decision Process (MDP)",
      "relationship": "depends_on"
    },
    {
      "from": "Approximation Error Minimization",
      "to": "Principal Component Analysis (PCA)",
      "relationship": "subtopic"
    },
    {
      "from": "Sequential Decision Making",
      "to": "Reinforcement Learning",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Double Descent Phenomenon",
      "relationship": "related_to"
    },
    {
      "from": "Neural Networks",
      "to": "MLPs (Multilayer Perceptrons)",
      "relationship": "subtopic"
    },
    {
      "from": "HoldOutCrossValidation",
      "to": "S_cv",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Variational_Autoencoder",
      "relationship": "related_to"
    },
    {
      "from": "Layer Normalization (LN)",
      "to": "Equation 7.43",
      "relationship": "described_by"
    },
    {
      "from": "StateRepresentation",
      "to": "GridCellRepresentation",
      "relationship": "has_subtopic"
    },
    {
      "from": "GradientAscentRule",
      "to": "StochasticGradientAscent",
      "relationship": "contains"
    },
    {
      "from": "Dynamic_Programming",
      "to": "Value_Iteration",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearning",
      "to": "Multi-classClassification",
      "relationship": "has_subtopic"
    },
    {
      "from": "GaussianDistribution",
      "to": "CovarianceMatrix",
      "relationship": "subtopic"
    },
    {
      "from": "PolynomialFitting",
      "to": "Overfitting",
      "relationship": "depends_on"
    },
    {
      "from": "RewardsDependency",
      "to": "ExpectationRewriting",
      "relationship": "depends_on"
    },
    {
      "from": "KernelTrickIntroduction",
      "to": "LagrangianFormulation",
      "relationship": "depends_on"
    },
    {
      "from": "Linear_Functions",
      "to": "Parameters_Weights",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningConcepts",
      "to": "ICAOnGaussianData",
      "relationship": "contains"
    },
    {
      "from": "ICA",
      "to": "ICA_Ambiguities",
      "relationship": "subtopic"
    },
    {
      "from": "Normalization Techniques",
      "to": "Group Normalization",
      "relationship": "contains"
    },
    {
      "from": "Vapnik's Theorem",
      "to": "Uniform Convergence",
      "relationship": "implies"
    },
    {
      "from": "Mini-batch Stochastic Gradient Descent",
      "to": "Stochastic Gradient Descent (SGD)",
      "relationship": "variant_of"
    },
    {
      "from": "Bell and Sejnowski's Method",
      "to": "ICA Algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Chain_Rule",
      "to": "Scalar_Variable_J",
      "relationship": "subtopic"
    },
    {
      "from": "Text_Classification",
      "to": "Generative_Modeling",
      "relationship": "subtopic"
    },
    {
      "from": "Differential_Dynamic_Programming_(DDP)",
      "to": "Reward_Function_Approximation",
      "relationship": "related_to"
    },
    {
      "from": "MachineLearningAlgorithms",
      "to": "EmpiricalRiskMinimization",
      "relationship": "depends_on"
    },
    {
      "from": "Linear Regression",
      "to": "Housing Example Dataset",
      "relationship": "uses_example"
    },
    {
      "from": "Unsupervised learning",
      "to": "EM algorithms",
      "relationship": "has_subtopic"
    },
    {
      "from": "MSEDecomposition",
      "to": "VarianceTerm",
      "relationship": "subtopic_of"
    },
    {
      "from": "Transformer_Models",
      "to": "Conditional_Probability",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Curse of Dimensionality",
      "relationship": "related_to"
    },
    {
      "from": "Efficiency_of_Backward_Pass",
      "to": "Vectorized_Notation_Backward_Func",
      "relationship": "depends_on"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Training_Data_Set",
      "relationship": "subtopic"
    },
    {
      "from": "Law_of_Total_Expectation",
      "to": "Log_Probability_Ratio",
      "relationship": "depends_on"
    },
    {
      "from": "JointLikelihood",
      "to": "MaximumLikelihoodEstimation",
      "relationship": "related_to"
    },
    {
      "from": "TwoLayerNN",
      "to": "ReLUFunction",
      "relationship": "depends_on"
    },
    {
      "from": "Weight Matrices and Biases",
      "to": "Multi-layer Neural Networks",
      "relationship": "subtopic"
    },
    {
      "from": "Gaussian_Distribution_Qi",
      "to": "Mean_and_Variance_Functions",
      "relationship": "subtopic"
    },
    {
      "from": "Optimizers",
      "to": "Global Minima",
      "relationship": "depends_on"
    },
    {
      "from": "LayerActivations",
      "to": "VectorizationTechniques",
      "relationship": "related_to"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Empirical_Distribution",
      "relationship": "related_to"
    },
    {
      "from": "ProjectionOntoDirectionU",
      "to": "VarianceMaximization",
      "relationship": "depends_on"
    },
    {
      "from": "Regularization in Machine Learning",
      "to": "Sparsity Regularization",
      "relationship": "contains"
    },
    {
      "from": "LayerNormalization",
      "to": "LN-S",
      "relationship": "depends_on"
    },
    {
      "from": "Optimal Parameters w and b",
      "to": "Dual Optimization Problem",
      "relationship": "subtopic"
    },
    {
      "from": "Adaptation_Algorithm",
      "to": "Finetuning_Method",
      "relationship": "has_subtopic"
    },
    {
      "from": "BiasVarianceTradeoff",
      "to": "Claim8.1.1",
      "relationship": "depends_on"
    },
    {
      "from": "Optimizers",
      "to": "Pretraining_Phase",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Models",
      "to": "Regularization and Non-Separable Data",
      "relationship": "related_to"
    },
    {
      "from": "NeuralNetworks",
      "to": "InputFeatures",
      "relationship": "depends_on"
    },
    {
      "from": "MachineLearningBasics",
      "to": "MatrixNotation",
      "relationship": "has_subtopic"
    },
    {
      "from": "LinearRegression",
      "to": "NormalEquations",
      "relationship": "subtopic"
    },
    {
      "from": "MultivariateNormalDistribution",
      "to": "CovarianceMatrixImpact",
      "relationship": "subtopic"
    },
    {
      "from": "Unsupervised learning",
      "to": "Self-supervised learning and foundation models",
      "relationship": "related_to"
    },
    {
      "from": "Bias-Variance Tradeoff",
      "to": "Model Complexity",
      "relationship": "related_to"
    },
    {
      "from": "Kalman Filter",
      "to": "Belief State",
      "relationship": "related_to"
    },
    {
      "from": "Continuous_State_MDPs",
      "to": "Discretization_Method",
      "relationship": "subtopic"
    },
    {
      "from": "PolicyGradients",
      "to": "LogProbabilityDerivative",
      "relationship": "has_subtopic"
    },
    {
      "from": "Independent components analysis",
      "to": "ICA algorithm",
      "relationship": "has_subtopic"
    },
    {
      "from": "Bellman Equations",
      "to": "Value Function",
      "relationship": "subtopic"
    },
    {
      "from": "Total Payoff",
      "to": "Markov Decision Process (MDP)",
      "relationship": "subtopic"
    },
    {
      "from": "Binary Classification",
      "to": "Training Set",
      "relationship": "contains"
    },
    {
      "from": "Non-Stationary Policies",
      "to": "Finite Horizon MDP Dynamics",
      "relationship": "depends_on"
    },
    {
      "from": "Markov Decision Processes (MDP)",
      "to": "State Transition Probabilities",
      "relationship": "component_of"
    },
    {
      "from": "GradientEstimation",
      "to": "PolicyGradients",
      "relationship": "has_subtopic"
    },
    {
      "from": "Bias-Variance Tradeoff",
      "to": "Test Error Decomposition",
      "relationship": "subtopic"
    },
    {
      "from": "SMO_Algorithm",
      "to": "KKT_Conditions",
      "relationship": "has_subtopic"
    },
    {
      "from": "Regularization and model selection",
      "to": "Bayesian statistics and regularization",
      "relationship": "has_subtopic"
    },
    {
      "from": "RegularizerFunction",
      "to": "RegularizationParameter",
      "relationship": "depends_on"
    },
    {
      "from": "MachineLearningModels",
      "to": "ExponentialFamilyDistributions",
      "relationship": "subtopic"
    },
    {
      "from": "Batch_Gradient_Descent",
      "to": "Gradient_Descent",
      "relationship": "related_to"
    },
    {
      "from": "Bayesian Machine Learning",
      "to": "Fully Bayesian Prediction",
      "relationship": "subtopic"
    },
    {
      "from": "Learning Theory Fundamentals",
      "to": "Machine Learning Papers",
      "relationship": "subtopic"
    },
    {
      "from": "Total Number of Neurons",
      "to": "Multi-layer Neural Networks",
      "relationship": "subtopic"
    },
    {
      "from": "Sufficient Conditions for Kernels",
      "to": "Mercer's Theorem",
      "relationship": "depends_on"
    },
    {
      "from": "MachineLearningOverview",
      "to": "OptimizationChallenges",
      "relationship": "depends_on"
    },
    {
      "from": "LinearRegression",
      "to": "Overfitting",
      "relationship": "subtopic"
    },
    {
      "from": "Lagrangian Function",
      "to": "Dual Optimization Problem",
      "relationship": "depends_on"
    },
    {
      "from": "LogisticRegression",
      "to": "ExponentialFamilyDistribution",
      "relationship": "uses_distribution"
    },
    {
      "from": "Bias-Variance Tradeoff",
      "to": "Linear Regression Example",
      "relationship": "illustrates"
    },
    {
      "from": "Posterior Distribution p(y|x)",
      "to": "Bayesian Classification",
      "relationship": "related_to"
    },
    {
      "from": "Backpropagation",
      "to": "Gradient Computation",
      "relationship": "depends_on"
    },
    {
      "from": "MachineLearningOverview",
      "to": "ModulesInPractice",
      "relationship": "has_subtopic"
    },
    {
      "from": "FeatureMapping",
      "to": "HighDimensionalFeatures",
      "relationship": "depends_on"
    },
    {
      "from": "MachineLearningModels",
      "to": "Vectorization",
      "relationship": "related_to"
    },
    {
      "from": "Machine_Learning",
      "to": "Hypothesis_Class",
      "relationship": "related_to"
    },
    {
      "from": "Markov Decision Processes (MDP)",
      "to": "Reward Function",
      "relationship": "component_of"
    },
    {
      "from": "Independent components analysis",
      "to": "Densities and linear transformations",
      "relationship": "has_subtopic"
    },
    {
      "from": "ActivationFunctions",
      "to": "LeakyReLUFunction",
      "relationship": "subtopic"
    },
    {
      "from": "Probabilistic Interpretation of Linear Regression",
      "to": "Regression Problem",
      "relationship": "depends_on"
    },
    {
      "from": "5 Kernel methods",
      "to": "5.4 Properties of kernels",
      "relationship": "has_subtopic"
    },
    {
      "from": "Regularization and Non-Separable Case",
      "to": "Linear Separability Assumption",
      "relationship": "depends_on"
    },
    {
      "from": "OptimizationChallenges",
      "to": "LikelihoodFunction",
      "relationship": "subtopic"
    },
    {
      "from": "MStep",
      "to": "ExpectationMaximizationAlgorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Downstream_Task_Dataset",
      "to": "Zero_Shot_Learning",
      "relationship": "subtopic"
    },
    {
      "from": "NonStationaryOptimalPolicy",
      "to": "FiniteHorizonMDP",
      "relationship": "follows_from"
    },
    {
      "from": "Kernel Examples",
      "to": "Digit Recognition Problem",
      "relationship": "subtopic"
    },
    {
      "from": "KernelFunctions",
      "to": "PolynomialKernels",
      "relationship": "subtopic"
    },
    {
      "from": "Generative_Modeling",
      "to": "Naive_Bayes_Assumption",
      "relationship": "has_subtopic"
    },
    {
      "from": "PCA_Method",
      "to": "Dataset_Analysis",
      "relationship": "contains"
    },
    {
      "from": "LinearCombinationRepresentation",
      "to": "CoefficientUpdateRule",
      "relationship": "subtopic"
    },
    {
      "from": "Single_Neuron_Model",
      "to": "Bias_and_Weights",
      "relationship": "depends_on"
    },
    {
      "from": "TotalParametersConv1D",
      "to": "ParameterTensorRepresentation",
      "relationship": "related_to"
    },
    {
      "from": "Support Vector Machines",
      "to": "Regularization and Non-Separable Case",
      "relationship": "has_subtopic"
    },
    {
      "from": "Supervised Learning Problem",
      "to": "Classification",
      "relationship": "has_subtopic"
    },
    {
      "from": "LogisticRegressionAsGLM",
      "to": "BernoulliDistribution",
      "relationship": "depends_on"
    },
    {
      "from": "Deep_Learning",
      "to": "Feature_Maps",
      "relationship": "related_to"
    },
    {
      "from": "Multi_Class_Classification",
      "to": "Machine_Learning_Concepts",
      "relationship": "subtopic"
    },
    {
      "from": "GaussianDiscriminantAnalysis(GDA)",
      "to": "ParametersEstimation",
      "relationship": "has_subtopic"
    },
    {
      "from": "Value_Functions",
      "to": "Policy_Evaluation",
      "relationship": "subtopic"
    },
    {
      "from": "Support_Vector_Machines",
      "to": "SMO_Algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Predict Step",
      "to": "Gaussian Distribution",
      "relationship": "depends_on"
    },
    {
      "from": "Orthogonal Basis",
      "to": "Principal Component Analysis (PCA)",
      "relationship": "subtopic"
    },
    {
      "from": "Self-supervised Learning",
      "to": "Foundation Models",
      "relationship": "subtopic"
    },
    {
      "from": "NaiveBayes",
      "to": "DiscreteFeatures",
      "relationship": "contains"
    },
    {
      "from": "ParameterEstimation",
      "to": "NaiveBayesFilter",
      "relationship": "subtopic"
    },
    {
      "from": "Bias_Variance_Tradoff",
      "to": "Model_Parameterizations",
      "relationship": "depends_on"
    },
    {
      "from": "Error_Margin_Determination",
      "to": "Quantities_of_Interest",
      "relationship": "subtopic"
    },
    {
      "from": "Regularization in Machine Learning",
      "to": "L2 Norm Regularization",
      "relationship": "contains"
    },
    {
      "from": "Implicit Regularization Effect",
      "to": "Optimizer Impact",
      "relationship": "contains"
    },
    {
      "from": "Overfitting",
      "to": "Variance",
      "relationship": "depends_on"
    },
    {
      "from": "BayesianInference",
      "to": "PosteriorApproximation",
      "relationship": "subtopic"
    },
    {
      "from": "FeatureExtraction",
      "to": "StringFeatureExtraction",
      "relationship": "has_subtopic"
    },
    {
      "from": "House Price Prediction Example",
      "to": "Feature Discovery",
      "relationship": "related_to"
    },
    {
      "from": "Confidence in Predictions",
      "to": "Machine Learning Concepts",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningOverview",
      "to": "NaiveBayesClassifier",
      "relationship": "contains"
    },
    {
      "from": "AlternativeFormulationsOfELBO",
      "to": "MarginalDistributionIndependence",
      "relationship": "subtopic"
    },
    {
      "from": "I Supervised learning",
      "to": "5 Kernel methods",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Linear_Quadratic_Gaussian_(LQG)",
      "relationship": "next_topic"
    },
    {
      "from": "GradientCalculation",
      "to": "StochasticGradientDescent",
      "relationship": "leads_to"
    },
    {
      "from": "Generalization",
      "to": "The double descent phenomenon",
      "relationship": "has_subtopic"
    },
    {
      "from": "I Supervised learning",
      "to": "1 Linear regression",
      "relationship": "has_subtopic"
    },
    {
      "from": "Backpropagation",
      "to": "Preliminaries on partial derivatives",
      "relationship": "has_subtopic"
    },
    {
      "from": "EventModelsForTextClassification",
      "to": "MultinomialEventModel",
      "relationship": "contains"
    },
    {
      "from": "Gaussian_Mixture_Models",
      "to": "Posterior_Distribution",
      "relationship": "has_analytical_solution_for"
    },
    {
      "from": "LinearRegressionOptimization",
      "to": "GradientDescentConvergence",
      "relationship": "depends_on"
    },
    {
      "from": "TwoLayerNetwork",
      "to": "VectorizationInNN",
      "relationship": "subtopic"
    },
    {
      "from": "Decision Boundary",
      "to": "Machine Learning Concepts",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningConcepts",
      "to": "KernelMethods",
      "relationship": "has_subtopic"
    },
    {
      "from": "LinearModelPrediction",
      "to": "StochasticModel",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Double Descent Phenomenon",
      "relationship": "contains"
    },
    {
      "from": "Complexity_in_Computing_Gradients",
      "to": "Reparameterization_Trick",
      "relationship": "has_subtopic"
    },
    {
      "from": "RegressionProblems",
      "to": "TestExample",
      "relationship": "related_to"
    },
    {
      "from": "Chain_Rule",
      "to": "Vectorized_Notation",
      "relationship": "subtopic"
    },
    {
      "from": "Chapter_16_LQR_DDP_LQG",
      "to": "Finite_Horizon_MDPs",
      "relationship": "subtopic"
    },
    {
      "from": "1D_Convolution",
      "to": "Simplified_1D_Convolution",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningConcepts",
      "to": "NonGaussianDataRecovery",
      "relationship": "contains"
    },
    {
      "from": "Scaling_Parameters",
      "to": "Machine_Learning_Concepts",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Optimal Policy in LQR",
      "relationship": "contains"
    },
    {
      "from": "Parameter Estimation",
      "to": "Closed Form Solution",
      "relationship": "related_to"
    },
    {
      "from": "3.2 Constructing GLMs",
      "to": "3.2.1 Ordinary least squares",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine_Learning_Optimization",
      "to": "Support_Vector_Machines_SVMs",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Algorithms",
      "to": "Support_Vector_Machines_SVM",
      "relationship": "related_to"
    },
    {
      "from": "ExpectationMaximizationAlgorithm",
      "to": "EStep",
      "relationship": "subtopic"
    },
    {
      "from": "Continuous_Model_Assumptions",
      "to": "Linear_Transitions",
      "relationship": "depends_on"
    },
    {
      "from": "Neural_Networks",
      "to": "Single_Neuron_Model",
      "relationship": "subtopic"
    },
    {
      "from": "Geometric_Margin",
      "to": "Machine_Learning_Concepts",
      "relationship": "subtopic"
    },
    {
      "from": "RegressionProblems",
      "to": "LeastSquareCostFunction",
      "relationship": "subtopic"
    },
    {
      "from": "Linearization_of_Dynamics",
      "to": "Taylor_Expansion_Method",
      "relationship": "subtopic"
    },
    {
      "from": "General Case",
      "to": "Density Transformation",
      "relationship": "subtopic"
    },
    {
      "from": "Sparsity Inducing Regularization",
      "to": "Regularizer R(θ)",
      "relationship": "subtopic"
    },
    {
      "from": "Joint Distribution",
      "to": "Latent Variables",
      "relationship": "related_to"
    },
    {
      "from": "ResNetOverview",
      "to": "ResidualBlockDefinition",
      "relationship": "subtopic"
    },
    {
      "from": "2 Classification and logistic regression",
      "to": "2.1 Logistic regression",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Vectorization Over Training Examples",
      "relationship": "related_to"
    },
    {
      "from": "5th_Degree_Polynomial_Failure",
      "to": "Generalization_Error",
      "relationship": "related_to"
    },
    {
      "from": "Kernels_in_Machine_Learning",
      "to": "Feature_Map_Phi",
      "relationship": "depends_on"
    },
    {
      "from": "Conditional_Probability",
      "to": "Softmax_Function",
      "relationship": "depends_on"
    },
    {
      "from": "Loss Function",
      "to": "Average Loss Function",
      "relationship": "subtopic"
    },
    {
      "from": "LQR Algorithm",
      "to": "Kalman Filter",
      "relationship": "subtopic"
    },
    {
      "from": "TwoLayerNN",
      "to": "LeakyReLUFunction",
      "relationship": "depends_on"
    },
    {
      "from": "Implicit Regularization Effect",
      "to": "Global Minima Diversity",
      "relationship": "contains"
    },
    {
      "from": "EMAlgorithmIntroduction",
      "to": "EStepMStepProcess",
      "relationship": "subtopic"
    },
    {
      "from": "MaximumLikelihoodEstimation",
      "to": "CumulativeDistributionFunction",
      "relationship": "depends_on"
    },
    {
      "from": "Supervised Learning",
      "to": "Linear Regression",
      "relationship": "subtopic"
    },
    {
      "from": "Separating Hyperplane",
      "to": "Decision Boundary",
      "relationship": "subtopic"
    },
    {
      "from": "ExponentialFamilyDistributions",
      "to": "SufficientStatistic",
      "relationship": "has_component"
    },
    {
      "from": "From non-linear dynamics to LQR",
      "to": "Linearization of dynamics",
      "relationship": "subtopic"
    },
    {
      "from": "MLPModelDefinition",
      "to": "ModulesInMLP",
      "relationship": "contains"
    },
    {
      "from": "Newton's Method",
      "to": "Finding Roots",
      "relationship": "depends_on"
    },
    {
      "from": "MaximumLikelihoodEstimation",
      "to": "LaplaceSmoothing",
      "relationship": "related_to"
    },
    {
      "from": "Continuous state MDPs",
      "to": "Value function approximation",
      "relationship": "subtopic"
    },
    {
      "from": "UnitVectorW",
      "to": "VectorW",
      "relationship": "related_to"
    },
    {
      "from": "4.2 Naive bayes (Option Reading)",
      "to": "4.2.2 Event models for text classification",
      "relationship": "has_subtopic"
    },
    {
      "from": "LogisticRegression",
      "to": "TotalLossFunction",
      "relationship": "subtopic"
    },
    {
      "from": "Value Function",
      "to": "Policy Execution",
      "relationship": "depends_on"
    },
    {
      "from": "Belief State",
      "to": "Partially Observable MDPs (POMDP)",
      "relationship": "subtopic"
    },
    {
      "from": "LayerNormalization",
      "to": "LearnableParameters",
      "relationship": "has"
    },
    {
      "from": "k_Fold_Cross_Validation",
      "to": "Training_and_Testing_Process",
      "relationship": "includes"
    },
    {
      "from": "Unlabeled_Dataset",
      "to": "Pretraining_Phase",
      "relationship": "related_to"
    },
    {
      "from": "4 Generative learning algorithms",
      "to": "4.1 Gaussian discriminant analysis",
      "relationship": "has_subtopic"
    },
    {
      "from": "PhiParameterUpdate",
      "to": "MStepUpdateRule",
      "relationship": "subtopic"
    },
    {
      "from": "Double_Descent_Weak_Features",
      "to": "Machine_Learning_Papers",
      "relationship": "belongs_to"
    },
    {
      "from": "MaximumLikelihoodEstimation",
      "to": "SigmoidFunction",
      "relationship": "related_to"
    },
    {
      "from": "LikelihoodFunction",
      "to": "LogLikelihood",
      "relationship": "subtopic"
    },
    {
      "from": "Binary_Classification",
      "to": "Hypothesis",
      "relationship": "related_to"
    },
    {
      "from": "Backward_Function_Linear_Map",
      "to": "Equation_7.53_Usefulness",
      "relationship": "subtopic"
    },
    {
      "from": "ExpectationMaximizationAlgorithm",
      "to": "MachineLearningOverview",
      "relationship": "depends_on"
    },
    {
      "from": "Optimization Problem in SVM",
      "to": "Scaling Constraint",
      "relationship": "subtopic"
    },
    {
      "from": "LQR, DDP and LQG",
      "to": "From non-linear dynamics to LQR",
      "relationship": "subtopic"
    },
    {
      "from": "Bayesian Statistics",
      "to": "MLE",
      "relationship": "contrasts_with"
    },
    {
      "from": "Value_Iteration",
      "to": "Expectation_Computation",
      "relationship": "depends_on"
    },
    {
      "from": "LQRModelAssumptions",
      "to": "MachineLearningOverview",
      "relationship": "depends_on"
    },
    {
      "from": "Empirical_Risk_Minimization",
      "to": "Training_Error",
      "relationship": "depends_on"
    },
    {
      "from": "Hypothesis_Class_Switching",
      "to": "Variance_Increase",
      "relationship": "subtopic"
    },
    {
      "from": "FullyConnectedNN",
      "to": "ReLUActivation",
      "relationship": "uses"
    },
    {
      "from": "PrincipalComponentAnalysis",
      "to": "kDimensionalSubspace",
      "relationship": "related_to"
    },
    {
      "from": "MachineLearning",
      "to": "ExpectationMaximizationAlgorithm",
      "relationship": "contains"
    },
    {
      "from": "Machine_Learning_Optimization",
      "to": "Support_Vector_Machines_SVM",
      "relationship": "contains"
    },
    {
      "from": "Double Descent Phenomenon",
      "to": "Historical Context",
      "relationship": "depends_on"
    },
    {
      "from": "GLMAssumptions",
      "to": "GeneralizedLinearModels",
      "relationship": "subtopic"
    },
    {
      "from": "DiscriminativeLearning",
      "to": "ConditionalDistribution",
      "relationship": "subtopic"
    },
    {
      "from": "Kernel_Functions",
      "to": "Sufficient_Conditions",
      "relationship": "subtopic"
    },
    {
      "from": "Logistic Regression",
      "to": "Model Set M",
      "relationship": "subtopic"
    },
    {
      "from": "PrincipalComponentAnalysis",
      "to": "EmpiricalCovarianceMatrix",
      "relationship": "related_to"
    },
    {
      "from": "MachineLearningModels",
      "to": "BernoulliEventModel",
      "relationship": "related_to"
    },
    {
      "from": "FeatureMapping",
      "to": "MachineLearningConcepts",
      "relationship": "depends_on"
    },
    {
      "from": "Alpha Updates",
      "to": "Sequential Minimal Optimization (SMO) Algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Bayesian Statistics",
      "to": "Prior Distribution",
      "relationship": "includes"
    },
    {
      "from": "VarianceMaximization",
      "to": "PrincipalEigenvector",
      "relationship": "subtopic"
    },
    {
      "from": "ActivationFunctions",
      "to": "BackwardFunctionActivations",
      "relationship": "has_subtopic"
    },
    {
      "from": "Self-supervised learning and foundation models",
      "to": "Pretraining methods in computer vision",
      "relationship": "has_subtopic"
    },
    {
      "from": "PolynomialFitting",
      "to": "Variance",
      "relationship": "related_to"
    },
    {
      "from": "Mean Field Assumption",
      "to": "Variational Inference",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Inverted_Pendulum_Model",
      "relationship": "related_to"
    },
    {
      "from": "Model_Parameter_Theta",
      "to": "Pretraining_Phase",
      "relationship": "related_to"
    },
    {
      "from": "Feature Mapping",
      "to": "Feature Map Definition",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning",
      "to": "Deep Learning Packages",
      "relationship": "related_to"
    },
    {
      "from": "Loss J(θ)",
      "to": "Regularized Loss Function",
      "relationship": "depends_on"
    },
    {
      "from": "Machine_Learning_Adaptation_Methods",
      "to": "Pretraining_Methods_Computer_Vision",
      "relationship": "has_subtopic"
    },
    {
      "from": "MachineLearningConcepts",
      "to": "SupportVectorMachines",
      "relationship": "related_to"
    },
    {
      "from": "Learning Theory Proofs",
      "to": "Union Bound Lemma",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Algorithms",
      "to": "Policy Iteration",
      "relationship": "related_to"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Convolutional_Layers",
      "relationship": "has_subtopic"
    },
    {
      "from": "NeuralNetworks",
      "to": "BiasVectors",
      "relationship": "depends_on"
    },
    {
      "from": "MachineLearningModels",
      "to": "RegressionProblems",
      "relationship": "subtopic"
    },
    {
      "from": "Jensen's Inequality",
      "to": "Concave Function",
      "relationship": "subtopic"
    },
    {
      "from": "BernoulliDistribution",
      "to": "LogPartitionFunctionForBernoulli",
      "relationship": "has_subtopic"
    },
    {
      "from": "LQRAlgorithmSteps",
      "to": "MachineLearningOverview",
      "relationship": "subtopic"
    },
    {
      "from": "LikelihoodFunction",
      "to": "SingleExampleOptimization",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Bias_Variance",
      "to": "Bias_Variance_Classification",
      "relationship": "contains"
    },
    {
      "from": "MachineLearningConcepts",
      "to": "LanguageModels",
      "relationship": "has_subtopic"
    },
    {
      "from": "LeastSquaresRegression",
      "to": "MaximumLikelihoodEstimation",
      "relationship": "related_to"
    },
    {
      "from": "Fully Bayesian Prediction",
      "to": "Computational Challenges",
      "relationship": "related_to"
    },
    {
      "from": "MachineLearning",
      "to": "ContrastiveLearning",
      "relationship": "has_subtopic"
    },
    {
      "from": "MachineLearningModels",
      "to": "FullyConnectedNN",
      "relationship": "subtopic"
    },
    {
      "from": "Implicit Bias in Machine Learning",
      "to": "High-Dimensional Interpolation",
      "relationship": "related_to"
    },
    {
      "from": "Alpha_Parameters",
      "to": "Constraint_Equation",
      "relationship": "depends_on"
    },
    {
      "from": "Quantities_of_Interest",
      "to": "Uniform_Convergence",
      "relationship": "related_to"
    },
    {
      "from": "Policy Gradient Methods",
      "to": "Gradient Ascent",
      "relationship": "subtopic"
    },
    {
      "from": "Linearization_of_Dynamics",
      "to": "LQR_Assumptions",
      "relationship": "related_to"
    },
    {
      "from": "Bayes' Theorem",
      "to": "Likelihood Function",
      "relationship": "related_to"
    },
    {
      "from": "1 Linear regression",
      "to": "1.1 LMS algorithm",
      "relationship": "has_subtopic"
    },
    {
      "from": "Convolutional_Layers",
      "to": "Conv1D_Channel_Variants",
      "relationship": "subtopic_of"
    },
    {
      "from": "VectorW",
      "to": "DecisionBoundary",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Algorithms",
      "to": "Algorithm 6",
      "relationship": "subtopic"
    },
    {
      "from": "Optimal_Policy",
      "to": "Quadratic_Form",
      "relationship": "related_to"
    },
    {
      "from": "Linearization_of_Dynamics",
      "to": "Rewriting_Dynamics",
      "relationship": "subtopic"
    },
    {
      "from": "Bias-Variance Tradeoff",
      "to": "Overfitting",
      "relationship": "depends_on"
    },
    {
      "from": "Chapter9",
      "to": "Regularization",
      "relationship": "contains"
    },
    {
      "from": "ParameterizedFunction",
      "to": "Embeddings",
      "relationship": "contains"
    },
    {
      "from": "Representation Function",
      "to": "Positive Pair",
      "relationship": "related_to"
    },
    {
      "from": "Self-supervised learning and foundation models",
      "to": "Pretraining and adaptation",
      "relationship": "has_subtopic"
    },
    {
      "from": "Layer Normalization (LN)",
      "to": "Equation 7.44-7.47",
      "relationship": "described_by"
    },
    {
      "from": "ExpectationMaximizationAlgorithm",
      "to": "MStep",
      "relationship": "subtopic"
    },
    {
      "from": "ActivationFunctions",
      "to": "IdentityFunction",
      "relationship": "subtopic"
    },
    {
      "from": "LagrangianConstruction",
      "to": "PhiParameterUpdate",
      "relationship": "depends_on"
    },
    {
      "from": "Perceptron_Algorithm",
      "to": "Machine_Learning_Concepts",
      "relationship": "subtopic"
    },
    {
      "from": "ActivationFunctions",
      "to": "TanhFunction",
      "relationship": "subtopic"
    },
    {
      "from": "Stacking_Neurons",
      "to": "Complex_Neural_Networks",
      "relationship": "related_to"
    },
    {
      "from": "LayerNormalization",
      "to": "LN-SModule",
      "relationship": "subtopic"
    },
    {
      "from": "5 Kernel methods",
      "to": "5.2 LMS (least mean squares) with features",
      "relationship": "has_subtopic"
    },
    {
      "from": "Stochastic_Gradient_Descent",
      "to": "Learning_Rate_Decay",
      "relationship": "has_subtopic"
    },
    {
      "from": "MachineLearningModels",
      "to": "GaussianDiscriminantAnalysis(GDA)",
      "relationship": "has_subtopic"
    },
    {
      "from": "Optimization Problem in SVM",
      "to": "Non-Convex Constraint",
      "relationship": "depends_on"
    },
    {
      "from": "Continuous state MDPs",
      "to": "Value function approximation",
      "relationship": "has_subtopic"
    },
    {
      "from": "ProbabilisticModeling",
      "to": "ConditionalProbabilityDistribution",
      "relationship": "depends_on"
    },
    {
      "from": "LocallyWeightedLinearRegression",
      "to": "WeightsInLWLR",
      "relationship": "has_subcomponent"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Underfitting",
      "relationship": "related_to"
    },
    {
      "from": "Pretrained large language models",
      "to": "Zero-shot learning and in-context learning",
      "relationship": "has_subtopic"
    },
    {
      "from": "PolicyIterationAlgorithm",
      "to": "BellmanEquations",
      "relationship": "related_to"
    },
    {
      "from": "MachineLearningAlgorithms",
      "to": "GenerativeLearning",
      "relationship": "contains"
    },
    {
      "from": "LinearRegressionOptimization",
      "to": "BatchGradientDescentExample",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Bias_Variance",
      "to": "Variance_Term",
      "relationship": "contains"
    },
    {
      "from": "Simplified_1D_Convolution",
      "to": "Filter_Vector",
      "relationship": "depends_on"
    },
    {
      "from": "Synchronous_Update",
      "to": "Value_Iteration",
      "relationship": "subtopic"
    },
    {
      "from": "LinearModelPrediction",
      "to": "DeterministicModel",
      "relationship": "depends_on"
    },
    {
      "from": "Machine_Learning_Backward_Functions",
      "to": "Logistic_Loss_Backward",
      "relationship": "subtopic"
    },
    {
      "from": "Linear Function Over Features",
      "to": "Cubic Function Representation",
      "relationship": "depends_on"
    },
    {
      "from": "Functional_Margin",
      "to": "Function_Margin_of_S",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Algorithms",
      "to": "Support_Vector_Machines",
      "relationship": "contains"
    },
    {
      "from": "MultiClassClassification",
      "to": "Logits",
      "relationship": "related_to"
    },
    {
      "from": "Value_Iteration",
      "to": "Approximation_Methods",
      "relationship": "depends_on"
    },
    {
      "from": "Policy Gradient Methods",
      "to": "Value Function",
      "relationship": "relates_to"
    },
    {
      "from": "Matricization Approach",
      "to": "Conversion Between Representations",
      "relationship": "subtopic"
    },
    {
      "from": "Naive Bayes Algorithm",
      "to": "Multinomial Distribution",
      "relationship": "related_to"
    },
    {
      "from": "LayerArchitecture",
      "to": "TwoLayerNN",
      "relationship": "subtopic"
    },
    {
      "from": "Matrix Multiplication Module",
      "to": "MLP Composition",
      "relationship": "depends_on"
    },
    {
      "from": "Coordinate_Ascend_Algorithm",
      "to": "Gradient_Ascend_Newtons_Method",
      "relationship": "related_to"
    },
    {
      "from": "Machine_Learning_Adaptation",
      "to": "Adaptation_Algorithm",
      "relationship": "has_subtopic"
    },
    {
      "from": "LogProbabilityDerivative",
      "to": "AutoDifferentiation",
      "relationship": "related_to"
    },
    {
      "from": "MachineLearningConcepts",
      "to": "LocallyWeightedLinearRegression",
      "relationship": "contains"
    },
    {
      "from": "NeuralNetworks",
      "to": "RegressionProblem",
      "relationship": "subtopic"
    },
    {
      "from": "Model Set M",
      "to": "Model Selection",
      "relationship": "subtopic"
    },
    {
      "from": "ICA",
      "to": "Cocktail_Party_Problem",
      "relationship": "example_of"
    },
    {
      "from": "Multi-classClassification",
      "to": "MultinomialDistribution",
      "relationship": "related_to"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Overfitting_Underfitting",
      "relationship": "subtopic"
    },
    {
      "from": "Regularization in Machine Learning",
      "to": "Deep Learning Regularization Techniques",
      "relationship": "contains"
    },
    {
      "from": "MaximumLikelihoodEstimation",
      "to": "LogLikelihoodFunction",
      "relationship": "related_to"
    },
    {
      "from": "Probability_Estimation",
      "to": "Laplace_Smoothing",
      "relationship": "related_to"
    },
    {
      "from": "L2 Regularization",
      "to": "Regularizer R(θ)",
      "relationship": "subtopic"
    },
    {
      "from": "Lagrange_Duality",
      "to": "Kernels_High_Dimensional_Spaces",
      "relationship": "has_subtopic"
    },
    {
      "from": "VarianceMaximization",
      "to": "LagrangeMultipliersMethod",
      "relationship": "subtopic"
    },
    {
      "from": "In-Context_Learning",
      "to": "Prompting_Strategy",
      "relationship": "subtopic"
    },
    {
      "from": "Weight Decay",
      "to": "L2 Regularization",
      "relationship": "related_to"
    },
    {
      "from": "Reinforcement learning",
      "to": "Learning a model for an MDP",
      "relationship": "has_subtopic"
    },
    {
      "from": "Clustering",
      "to": "KMeansAlgorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Loss_Functions",
      "relationship": "contains"
    },
    {
      "from": "Optimization_Frameworks",
      "to": "Trajectory_Generation",
      "relationship": "related_to"
    },
    {
      "from": "FeatureMapPhi",
      "to": "InnerProductEfficiency",
      "relationship": "subtopic"
    },
    {
      "from": "EM_Algorithm_Generalization",
      "to": "Machine_Learning_Concepts",
      "relationship": "subtopic"
    },
    {
      "from": "Optimal Parameters Calculation",
      "to": "Dual Form Optimization",
      "relationship": "subtopic"
    },
    {
      "from": "DataNormalization",
      "to": "PCAAlgorithm",
      "relationship": "depends_on"
    },
    {
      "from": "WalkableNeighborhood",
      "to": "DerivedFeatures",
      "relationship": "subtopic"
    },
    {
      "from": "ExponentialFamilyDistributions",
      "to": "NaturalParameter",
      "relationship": "has_subtopic"
    },
    {
      "from": "MachineLearning",
      "to": "ClassificationProblem",
      "relationship": "has_subtopic"
    },
    {
      "from": "DiscountFactorImpact",
      "to": "FiniteHorizonMDP",
      "relationship": "related_to"
    },
    {
      "from": "BackwardFunctionsBasics",
      "to": "MatrixMultiplicationModule",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Convolutional_Neural_Networks",
      "relationship": "contains"
    },
    {
      "from": "Polynomial Regression Model",
      "to": "Model Selection",
      "relationship": "related_to"
    },
    {
      "from": "Function_Representation",
      "to": "Linear_Functions",
      "relationship": "subtopic"
    },
    {
      "from": "Regularization and model selection",
      "to": "Regularization",
      "relationship": "has_subtopic"
    },
    {
      "from": "Feature_Vector",
      "to": "Stop_Words",
      "relationship": "related_to"
    },
    {
      "from": "MachineLearningOverview",
      "to": "LinearRegression",
      "relationship": "depends_on"
    },
    {
      "from": "GeneralizedLinearModels",
      "to": "MachineLearningOverview",
      "relationship": "subtopic"
    },
    {
      "from": "Conditional_Distribution_Modeling",
      "to": "Exponential_Family_Distributions",
      "relationship": "has_subtopic"
    },
    {
      "from": "VariationalInference",
      "to": "MachineLearningOverview",
      "relationship": "related_to"
    },
    {
      "from": "Dual_Problem_Formulation",
      "to": "Lagrange_Multipliers",
      "relationship": "depends_on"
    },
    {
      "from": "k-means_algorithm",
      "to": "centroid_initialization",
      "relationship": "depends_on"
    },
    {
      "from": "LogisticRegression",
      "to": "ModelAssumptions",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningModels",
      "to": "ClassificationModelAssumptions",
      "relationship": "depends_on"
    },
    {
      "from": "Policy_Gradient_Theorem",
      "to": "Log_Probability_Ratio",
      "relationship": "has_subtopic"
    },
    {
      "from": "Chapter_15_Summary",
      "to": "Policy_Iteration_Speedup",
      "relationship": "subtopic"
    },
    {
      "from": "ExponentialFamilyDistributions",
      "to": "LogPartitionFunction",
      "relationship": "has_subtopic"
    },
    {
      "from": "Sample Complexity Bounds",
      "to": "Model Selection Methods",
      "relationship": "subtopic"
    },
    {
      "from": "LQR, DDP and LQG",
      "to": "Finite-horizon MDPs",
      "relationship": "subtopic"
    },
    {
      "from": "Binary_Classification",
      "to": "PAC_Assumptions",
      "relationship": "related_to"
    },
    {
      "from": "Gradient_Estimation",
      "to": "Empirical_Sample_Trajectories",
      "relationship": "has_subtopic"
    },
    {
      "from": "Jacobian_Matrix_Transpose",
      "to": "Complexity_of_Jacobian_Matrices",
      "relationship": "related_to"
    },
    {
      "from": "Optimizers",
      "to": "Stochastic Gradient Descent (SGD)",
      "relationship": "related_to"
    },
    {
      "from": "Continuous_Model_Assumptions",
      "to": "Quadratic_Rewards",
      "relationship": "subtopic"
    },
    {
      "from": "Support_Vector_Machines",
      "to": "Kernels_in_SVMs",
      "relationship": "subtopic"
    },
    {
      "from": "Loss_Functions",
      "to": "Cross_Entropy_Loss",
      "relationship": "has_subtopic"
    },
    {
      "from": "LinearRegression",
      "to": "Underfitting",
      "relationship": "subtopic"
    },
    {
      "from": "TimeHorizonTuple",
      "to": "FiniteHorizonMDP",
      "relationship": "subtopic"
    },
    {
      "from": "NeuralNetworks",
      "to": "ClassificationProblem",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Theorem",
      "to": "Generalization_Error_Bound",
      "relationship": "subtopic"
    },
    {
      "from": "MarginalDistributionIndependence",
      "to": "ConditionalLikelihoodSimplification",
      "relationship": "subtopic"
    },
    {
      "from": "II Deep learning",
      "to": "7 Deep learning",
      "relationship": "has_subtopic"
    },
    {
      "from": "Regularization",
      "to": "RegularizerFunction",
      "relationship": "contains"
    },
    {
      "from": "LQR, DDP and LQG",
      "to": "Linear Quadratic Regulation (LQR)",
      "relationship": "subtopic"
    },
    {
      "from": "Neural_Networks",
      "to": "Activation_Functions",
      "relationship": "depends_on"
    },
    {
      "from": "Transformer_Models",
      "to": "Autoregressive_Decoding",
      "relationship": "has_subtopic"
    },
    {
      "from": "Optimal Policy in LQR",
      "to": "Independence of Noise",
      "relationship": "related_to"
    },
    {
      "from": "LossFunctionFormulation",
      "to": "IntermediateVariables",
      "relationship": "leads_to"
    },
    {
      "from": "Corollary of Vapnik's Theorem",
      "to": "Vapnik's Theorem",
      "relationship": "derives_from"
    },
    {
      "from": "ContrastiveLearning",
      "to": "NegativePairs",
      "relationship": "has_subtopic"
    },
    {
      "from": "Supervised_Learning",
      "to": "Linear_Regression",
      "relationship": "depends_on"
    },
    {
      "from": "MachineLearningOverview",
      "to": "LogisticRegression",
      "relationship": "contains"
    },
    {
      "from": "EmpiricalRiskMinimization",
      "to": "DegreeOfPolynomial",
      "relationship": "related_to"
    },
    {
      "from": "EM Algorithms",
      "to": "Mixture of Gaussians",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Continuous_State_MDPs",
      "relationship": "related_to"
    },
    {
      "from": "Optimizers",
      "to": "Learning Rate Schedules",
      "relationship": "related_to"
    },
    {
      "from": "MachineLearningBasics",
      "to": "LayerGeneralization",
      "relationship": "has_subtopic"
    },
    {
      "from": "Loss_Functions",
      "to": "Gradient_Computation",
      "relationship": "has_subtopic"
    },
    {
      "from": "ResNetArchitecture",
      "to": "ConvolutionalLayers",
      "relationship": "subtopic"
    },
    {
      "from": "SMO_Algorithm",
      "to": "Efficient_Update",
      "relationship": "has_subtopic"
    },
    {
      "from": "2 Classification and logistic regression",
      "to": "2.3 Multi-class classification",
      "relationship": "has_subtopic"
    },
    {
      "from": "MachineLearningModels",
      "to": "GDA",
      "relationship": "contains"
    },
    {
      "from": "Non-linear Feature Mappings",
      "to": "MDP Simulators",
      "relationship": "depends_on"
    },
    {
      "from": "MachineLearningModels",
      "to": "LinearModelPrediction",
      "relationship": "subtopic"
    },
    {
      "from": "ClassificationModelAssumptions",
      "to": "LikelihoodFunction",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningModels",
      "to": "GLMConstruction",
      "relationship": "subtopic"
    },
    {
      "from": "Support_Vector_Machines",
      "to": "Margins_Intuition",
      "relationship": "subtopic"
    },
    {
      "from": "Other Activation Functions",
      "to": "Multi-layer Neural Networks",
      "relationship": "related_to"
    },
    {
      "from": "Policy Gradient Methods",
      "to": "REINFORCE Algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Error Term Distribution",
      "to": "Conditional Probability of y given x",
      "relationship": "subtopic"
    },
    {
      "from": "PosteriorApproximation",
      "to": "PriorDistributions",
      "relationship": "subtopic"
    },
    {
      "from": "ConvergenceOfAlgorithms",
      "to": "PolicyIterationAlgorithm",
      "relationship": "depends_on"
    },
    {
      "from": "Matrix Derivatives",
      "to": "Least Squares Revisited",
      "relationship": "depends_on"
    },
    {
      "from": "1 Linear regression",
      "to": "1.2 The normal equations",
      "relationship": "has_subtopic"
    },
    {
      "from": "Primal Problem",
      "to": "Theta P",
      "relationship": "related_to"
    },
    {
      "from": "Sample Complexity Bounds",
      "to": "Bias-Variance Tradeoff",
      "relationship": "subtopic"
    },
    {
      "from": "Spam_Filtering",
      "to": "Training_Set",
      "relationship": "depends_on"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Bias_Variance_Tradoff",
      "relationship": "subtopic"
    },
    {
      "from": "Conv1D-S",
      "to": "TotalParametersConv1D",
      "relationship": "depends_on"
    },
    {
      "from": "Naive_Bayes_Classifier",
      "to": "Event_Models_Text_Classification",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Probabilistic_Model",
      "relationship": "depends_on"
    },
    {
      "from": "Covariance Matrix",
      "to": "Multivariate Normal Distribution",
      "relationship": "related_to"
    },
    {
      "from": "Backpropagation",
      "to": "Loss Function Composition",
      "relationship": "subtopic"
    },
    {
      "from": "Supervised Learning with Non-Linear Models",
      "to": "Deep Learning Introduction",
      "relationship": "subtopic"
    },
    {
      "from": "ICAConcepts",
      "to": "JointDistributionModeling",
      "relationship": "subtopic"
    },
    {
      "from": "Immediate Reward",
      "to": "Bellman Equations",
      "relationship": "related_to"
    },
    {
      "from": "MachineLearningOverview",
      "to": "LinearRegression",
      "relationship": "has_subtopic"
    },
    {
      "from": "Learning_Settings",
      "to": "Test_Distribution",
      "relationship": "depends_on"
    },
    {
      "from": "Log_Probability_Ratio",
      "to": "Trajectory_Probability_Change",
      "relationship": "related_to"
    },
    {
      "from": "MultinomialDistribution",
      "to": "ParameterizedModel",
      "relationship": "has_subtopic"
    },
    {
      "from": "Conv2D-S",
      "to": "TotalParametersConv2D",
      "relationship": "depends_on"
    },
    {
      "from": "MachineLearningModels",
      "to": "StochasticModel",
      "relationship": "subtopic"
    },
    {
      "from": "ValueFunctionApproximation",
      "to": "FittedValueIteration",
      "relationship": "subtopic"
    },
    {
      "from": "Newton's Method",
      "to": "Logistic Regression",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Loss_Functions",
      "relationship": "has_subtopic"
    },
    {
      "from": "Optimization Problem in SVM",
      "to": "Geometric Margin",
      "relationship": "related_to"
    },
    {
      "from": "Normalization Techniques",
      "to": "Variance Scaling",
      "relationship": "has_subtopic"
    },
    {
      "from": "LMS with Features",
      "to": "Gradient Descent Update",
      "relationship": "subtopic"
    },
    {
      "from": "LearningRate",
      "to": "GradientDescentAlgorithm",
      "relationship": "depends_on"
    },
    {
      "from": "ValueFunctionApproximation",
      "to": "FeatureMapping",
      "relationship": "related_to"
    },
    {
      "from": "Empirical_Risk_Minimization",
      "to": "Finite_Hypothesis_Class",
      "relationship": "subtopic"
    },
    {
      "from": "Backpropagation Algorithm",
      "to": "Gradient Calculation",
      "relationship": "subtopic"
    },
    {
      "from": "StochasticGradientDescent",
      "to": "SGDAlgorithm",
      "relationship": "subtopic"
    },
    {
      "from": "MSEDecomposition",
      "to": "BiasTerm",
      "relationship": "subtopic_of"
    },
    {
      "from": "FullyConnectedNN",
      "to": "Parameterization",
      "relationship": "depends_on"
    },
    {
      "from": "JensensInequality",
      "to": "EvidenceLowerBoundELBO",
      "relationship": "depends_on"
    },
    {
      "from": "Optimal Policy",
      "to": "Optimal Value Function",
      "relationship": "related_to"
    }
  ]
}