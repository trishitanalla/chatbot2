
<!DOCTYPE html>
<html>
<head>
    <title>KG1 Visualization (Vis.js Interactive)</title>
    <script type="text/javascript" src="https://unpkg.com/vis-network/standalone/umd/vis-network.min.js"></script>
    <style type="text/css">
        body { font-family: sans-serif; margin: 0; display: flex; height: 100vh; }
        #networkContainer { flex-grow: 1; height: 100%; border-right: 1px solid lightgray; position: relative; }
        #mynetwork { width: 100%; height: 100%; }
        #detailsContainer { width: 300px; height: 100%; overflow-y: auto; padding: 15px; box-sizing: border-box; background-color: #f8f8f8; }
        #detailsContainer h3 { margin-top: 0; border-bottom: 1px solid #ccc; padding-bottom: 5px; }
        #detailsContainer p { font-size: 0.9em; line-height: 1.4; }
        .loading-overlay {
            position: absolute; top: 0; left: 0; width: 100%; height: 100%;
            background-color: rgba(255, 255, 255, 0.8); z-index: 10;
            display: flex; justify-content: center; align-items: center; font-size: 1.2em;
        }
    </style>
</head>
<body>
    <div id="networkContainer">
        <div id="mynetwork"></div>
        <div id="loadingOverlay" class="loading-overlay">Loading & Initial Layout...</div>
    </div>
    <div id="detailsContainer">
        <h3>Node Details</h3>
        <div id="nodeDetails">Select a node to see details.</div>
    </div>

    <script type="text/javascript">
        // --- Embedded Data (Generated by Python) ---
        const nodesData = [{"id": "Supervised Learning", "label": "Supervised Learning", "title": "<b>Supervised Learning</b> (major)<hr>Learning process where a model is trained on labeled data to predict outcomes.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Learning process where a model is trained on labeled data to predict outcomes.", "node_type": "major"}, {"id": "Regression Problem", "label": "Regression Problem", "title": "<b>Regression Problem</b> (subnode)<hr>Statistical approach to predicting a continuous outcome variable based on one or more predictor variables.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Statistical approach to predicting a continuous outcome variable based on one or more predictor variables.", "node_type": "subnode"}, {"id": "Classification Problem", "label": "Classification Problem", "title": "<b>Classification Problem</b> (subnode)<hr>Type of supervised learning where the target variable takes on discrete values.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Type of supervised learning where the target variable takes on discrete values.", "node_type": "subnode"}, {"id": "Hypothesis", "label": "Hypothesis", "title": "<b>Hypothesis</b> (subnode)<hr>Function learned by a model to predict outcomes based on input data.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Function learned by a model to predict outcomes based on input data.", "node_type": "subnode"}, {"id": "Linear Regression", "label": "Linear Regression", "title": "<b>Linear Regression</b> (major)<hr>Technique for modeling the relationship between a scalar dependent variable y and one or more explanatory variables X.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Technique for modeling the relationship between a scalar dependent variable y and one or more explanatory variables X.", "node_type": "major"}, {"id": "Feature Selection", "label": "Feature Selection", "title": "<b>Feature Selection</b> (subnode)<hr>Process of selecting which features to include in a model to improve performance.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Process of selecting which features to include in a model to improve performance.", "node_type": "subnode"}, {"id": "Machine Learning Basics", "label": "Machine Learning Basics", "title": "<b>Machine Learning Basics</b> (major)<hr>Fundamental concepts in machine learning including training and test datasets.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Fundamental concepts in machine learning including training and test datasets.", "node_type": "major"}, {"id": "Gradient Descent", "label": "Gradient Descent", "title": "<b>Gradient Descent</b> (subnode)<hr>Optimization algorithm used to minimize a function by iteratively moving towards the minimum value of that function.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Optimization algorithm used to minimize a function by iteratively moving towards the minimum value of that function.", "node_type": "subnode"}, {"id": "LMS Update Rule", "label": "LMS Update Rule", "title": "<b>LMS Update Rule</b> (subnode)<hr>Specific update rule derived from cost function for a single training example.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Specific update rule derived from cost function for a single training example.", "node_type": "subnode"}, {"id": "Widrow-Hoff Learning Rule", "label": "Widrow-Hoff Learning Rule", "title": "<b>Widrow-Hoff Learning Rule</b> (subnode)<hr>Alternative name for the LMS update rule, used in adaptive filters and neural networks.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Alternative name for the LMS update rule, used in adaptive filters and neural networks.", "node_type": "subnode"}, {"id": "Error Term", "label": "Error Term", "title": "<b>Error Term</b> (subnode)<hr>Difference between actual output and predicted value that guides parameter updates.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Difference between actual output and predicted value that guides parameter updates.", "node_type": "subnode"}, {"id": "Batch Gradient Descent", "label": "Batch Gradient Descent", "title": "<b>Batch Gradient Descent</b> (subnode)<hr>Version of gradient descent that uses the entire dataset to make a single update.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Version of gradient descent that uses the entire dataset to make a single update.", "node_type": "subnode"}, {"id": "Matrix Derivatives", "label": "Matrix Derivatives", "title": "<b>Matrix Derivatives</b> (major)<hr>Derivation of function f with respect to matrix A.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Derivation of function f with respect to matrix A.", "node_type": "major"}, {"id": "Gradient Calculation", "label": "Gradient Calculation", "title": "<b>Gradient Calculation</b> (subnode)<hr>Process of calculating gradients for parameters to update model weights.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Process of calculating gradients for parameters to update model weights.", "node_type": "subnode"}, {"id": "Least Squares Revisited", "label": "Least Squares Revisited", "title": "<b>Least Squares Revisited</b> (major)<hr>Revisiting least squares using matrix derivatives.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Revisiting least squares using matrix derivatives.", "node_type": "major"}, {"id": "Design Matrix", "label": "Design Matrix", "title": "<b>Design Matrix</b> (subnode)<hr>Matrix containing training examples' input values in its rows.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Matrix containing training examples' input values in its rows.", "node_type": "subnode"}, {"id": "Vector y", "label": "Vector y", "title": "<b>Vector y</b> (subnode)<hr>n-dimensional vector with target values from the training set.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "n-dimensional vector with target values from the training set.", "node_type": "subnode"}, {"id": "Machine Learning Algorithms", "label": "Machine Learning Algorithms", "title": "<b>Machine Learning Algorithms</b> (major)<hr>Overview of machine learning algorithms including gradient descent and stochastic methods.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Overview of machine learning algorithms including gradient descent and stochastic methods.", "node_type": "major"}, {"id": "Locally Weighted Linear Regression (LWLR)", "label": "Locally Weighted Linear Regression (LWLR)", "title": "<b>Locally Weighted Linear Regression (LWLR)</b> (subnode)<hr>A variant of linear regression where weights are assigned to data points based on their proximity to the point being predicted.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "A variant of linear regression where weights are assigned to data points based on their proximity to the point being predicted.", "node_type": "subnode"}, {"id": "Weight Calculation", "label": "Weight Calculation", "title": "<b>Weight Calculation</b> (subnode)<hr>Determines how much influence each training example has on the prediction at a given query point.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Determines how much influence each training example has on the prediction at a given query point.", "node_type": "subnode"}, {"id": "Bandwidth Parameter (\u03c4)", "label": "Bandwidth Parameter (\u03c4)", "title": "<b>Bandwidth Parameter (\u03c4)</b> (subnode)<hr>Controls the rate of decay in weight as distance from the query point increases.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Controls the rate of decay in weight as distance from the query point increases.", "node_type": "subnode"}, {"id": "Learning a model for an MDP", "label": "Learning a model for an MDP", "title": "<b>Learning a model for an MDP</b> (major)<hr>Techniques to estimate the transition dynamics of an environment from data.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Techniques to estimate the transition dynamics of an environment from data.", "node_type": "major"}, {"id": "Continuous state MDPs", "label": "Continuous state MDPs", "title": "<b>Continuous state MDPs</b> (subnode)<hr>Handling environments with a continuous state space in reinforcement learning.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Handling environments with a continuous state space in reinforcement learning.", "node_type": "subnode"}, {"id": "Discretization", "label": "Discretization", "title": "<b>Discretization</b> (subnode)<hr>Approaches to converting continuous states into discrete representations for RL algorithms.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Approaches to converting continuous states into discrete representations for RL algorithms.", "node_type": "subnode"}, {"id": "Value function approximation", "label": "Value function approximation", "title": "<b>Value function approximation</b> (subnode)<hr>Methods for estimating value functions in environments with a large or infinite number of states.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Methods for estimating value functions in environments with a large or infinite number of states.", "node_type": "subnode"}, {"id": "Connections between Policy and Value Iteration (Optional)", "label": "Connections between Policy and Value Iteration (Optional)", "title": "<b>Connections between Policy and Value Iteration (Optional)</b> (subnode)<hr>Theoretical connections and relationships between policy iteration and value iteration methods.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Theoretical connections and relationships between policy iteration and value iteration methods.", "node_type": "subnode"}, {"id": "LQR, DDP and LQG", "label": "LQR, DDP and LQG", "title": "<b>LQR, DDP and LQG</b> (major)<hr>Control theory concepts applied to reinforcement learning problems.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Control theory concepts applied to reinforcement learning problems.", "node_type": "major"}, {"id": "Finite-horizon MDPs", "label": "Finite-horizon MDPs", "title": "<b>Finite-horizon MDPs</b> (subnode)<hr>Analysis of Markov decision processes with a fixed time horizon.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Analysis of Markov decision processes with a fixed time horizon.", "node_type": "subnode"}, {"id": "Linear Quadratic Regulation (LQR)", "label": "Linear Quadratic Regulation (LQR)", "title": "<b>Linear Quadratic Regulation (LQR)</b> (subnode)<hr>Special case of finite-horizon setting in reinforcement learning with linear transitions and quadratic rewards.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Special case of finite-horizon setting in reinforcement learning with linear transitions and quadratic rewards.", "node_type": "subnode"}, {"id": "From non-linear dynamics to LQR", "label": "From non-linear dynamics to LQR", "title": "<b>From non-linear dynamics to LQR</b> (subnode)<hr>Approaches to apply LQR in nonlinear dynamic environments", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Approaches to apply LQR in nonlinear dynamic environments", "node_type": "subnode"}, {"id": "Linearization of dynamics", "label": "Linearization of dynamics", "title": "<b>Linearization of dynamics</b> (subnode)<hr>Techniques for linearizing nonlinear system dynamics", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Techniques for linearizing nonlinear system dynamics", "node_type": "subnode"}, {"id": "Differential Dynamic Programming (DDP)", "label": "Differential Dynamic Programming (DDP)", "title": "<b>Differential Dynamic Programming (DDP)</b> (subnode)<hr>Optimization technique used for trajectory optimization in robotics and control systems.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Optimization technique used for trajectory optimization in robotics and control systems.", "node_type": "subnode"}, {"id": "Linear Quadratic Gaussian (LQG)", "label": "Linear Quadratic Gaussian (LQG)", "title": "<b>Linear Quadratic Gaussian (LQG)</b> (subnode)<hr>Combination of LQR with stochastic dynamics and noisy measurements", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Combination of LQR with stochastic dynamics and noisy measurements", "node_type": "subnode"}, {"id": "Policy Gradient (REINFORCE)", "label": "Policy Gradient (REINFORCE)", "title": "<b>Policy Gradient (REINFORCE)</b> (major)<hr>Model-free algorithm for learning randomized policies without value functions.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Model-free algorithm for learning randomized policies without value functions.", "node_type": "major"}, {"id": "Supervised Learning Introduction", "label": "Supervised Learning Introduction", "title": "<b>Supervised Learning Introduction</b> (major)<hr>Introduction to supervised learning concepts and examples", "shape": "star", "size": 25, "color": "#FF6347", "description": "Introduction to supervised learning concepts and examples", "node_type": "major"}, {"id": "Machine Learning Concepts", "label": "Machine Learning Concepts", "title": "<b>Machine Learning Concepts</b> (major)<hr>Overview of key concepts in machine learning including EM algorithm and variational inference.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Overview of key concepts in machine learning including EM algorithm and variational inference.", "node_type": "major"}, {"id": "Underfitting", "label": "Underfitting", "title": "<b>Underfitting</b> (subnode)<hr>Scenario where a model is too simple to capture the underlying pattern of the data.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Scenario where a model is too simple to capture the underlying pattern of the data.", "node_type": "subnode"}, {"id": "Overfitting", "label": "Overfitting", "title": "<b>Overfitting</b> (subnode)<hr>Situation where a model fits noise in training data rather than underlying pattern.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Situation where a model fits noise in training data rather than underlying pattern.", "node_type": "subnode"}, {"id": "Locally Weighted Linear Regression (LWR)", "label": "Locally Weighted Linear Regression (LWR)", "title": "<b>Locally Weighted Linear Regression (LWR)</b> (subnode)<hr>Regression algorithm that assigns more weight to nearby points when making predictions.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Regression algorithm that assigns more weight to nearby points when making predictions.", "node_type": "subnode"}, {"id": "Maximum Likelihood Estimation (MLE)", "label": "Maximum Likelihood Estimation (MLE)", "title": "<b>Maximum Likelihood Estimation (MLE)</b> (subnode)<hr>Process for estimating the parameters that maximize the probability of observing the training data.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Process for estimating the parameters that maximize the probability of observing the training data.", "node_type": "subnode"}, {"id": "Least Squares Regression", "label": "Least Squares Regression", "title": "<b>Least Squares Regression</b> (subnode)<hr>Method for fitting a line to data points by minimizing squared errors.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Method for fitting a line to data points by minimizing squared errors.", "node_type": "subnode"}, {"id": "Probabilistic Assumptions", "label": "Probabilistic Assumptions", "title": "<b>Probabilistic Assumptions</b> (subnode)<hr>Assumptions about the probability distribution of class labels given input features.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Assumptions about the probability distribution of class labels given input features.", "node_type": "subnode"}, {"id": "Cost Function J(\u03b8)", "label": "Cost Function J(\u03b8)", "title": "<b>Cost Function J(\u03b8)</b> (subnode)<hr>Function to be minimized, representing error between predictions and actual values.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Function to be minimized, representing error between predictions and actual values.", "node_type": "subnode"}, {"id": "Function Representation", "label": "Function Representation", "title": "<b>Function Representation</b> (subnode)<hr>How functions are represented and approximated in machine learning models.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "How functions are represented and approximated in machine learning models.", "node_type": "subnode"}, {"id": "Linear Function Approximation", "label": "Linear Function Approximation", "title": "<b>Linear Function Approximation</b> (subnode)<hr>Approximating the target function using linear equations with parameters.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Approximating the target function using linear equations with parameters.", "node_type": "subnode"}, {"id": "Parameters (Weights)", "label": "Parameters (Weights)", "title": "<b>Parameters (Weights)</b> (subnode)<hr>Coefficients that define the form of the linear approximation.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Coefficients that define the form of the linear approximation.", "node_type": "subnode"}, {"id": "Cost Function", "label": "Cost Function", "title": "<b>Cost Function</b> (subnode)<hr>Measures the performance of a machine learning model by quantifying the error between predicted and actual values.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Measures the performance of a machine learning model by quantifying the error between predicted and actual values.", "node_type": "subnode"}, {"id": "Ordinary Least Squares", "label": "Ordinary Least Squares", "title": "<b>Ordinary Least Squares</b> (subnode)<hr>A special case of GLM where target variable is continuous and modeled as Gaussian distribution.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "A special case of GLM where target variable is continuous and modeled as Gaussian distribution.", "node_type": "subnode"}, {"id": "LMS Algorithm", "label": "LMS Algorithm", "title": "<b>LMS Algorithm</b> (subnode)<hr>Iterative method to minimize the cost function by adjusting parameters.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Iterative method to minimize the cost function by adjusting parameters.", "node_type": "subnode"}, {"id": "Convex Function", "label": "Convex Function", "title": "<b>Convex Function</b> (subnode)<hr>Function with a single global minimum, ensuring gradient descent convergence.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Function with a single global minimum, ensuring gradient descent convergence.", "node_type": "subnode"}, {"id": "Stochastic Gradient Descent", "label": "Stochastic Gradient Descent", "title": "<b>Stochastic Gradient Descent</b> (subnode)<hr>Algorithm that uses random samples to update parameters, reducing computational load.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Algorithm that uses random samples to update parameters, reducing computational load.", "node_type": "subnode"}, {"id": "Optimization Problem", "label": "Optimization Problem", "title": "<b>Optimization Problem</b> (major)<hr>Mathematical problem of finding the best solution from all feasible solutions.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Mathematical problem of finding the best solution from all feasible solutions.", "node_type": "major"}, {"id": "Gradient Descent Methods", "label": "Gradient Descent Methods", "title": "<b>Gradient Descent Methods</b> (major)<hr>Techniques for minimizing cost function in machine learning.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Techniques for minimizing cost function in machine learning.", "node_type": "major"}, {"id": "Learning Rate Decay", "label": "Learning Rate Decay", "title": "<b>Learning Rate Decay</b> (subnode)<hr>Gradually decreases learning rate to ensure convergence.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Gradually decreases learning rate to ensure convergence.", "node_type": "subnode"}, {"id": "Normal Equations Method", "label": "Normal Equations Method", "title": "<b>Normal Equations Method</b> (major)<hr>Direct method for minimizing cost function without iteration.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Direct method for minimizing cost function without iteration.", "node_type": "major"}, {"id": "Least-Squares Cost Function", "label": "Least-Squares Cost Function", "title": "<b>Least-Squares Cost Function</b> (subnode)<hr>Method to find the best fit line by minimizing the sum of squared residuals.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Method to find the best fit line by minimizing the sum of squared residuals.", "node_type": "subnode"}, {"id": "Probabilistic Interpretation", "label": "Probabilistic Interpretation", "title": "<b>Probabilistic Interpretation</b> (subnode)<hr>Interpreting linear regression from a probabilistic perspective, assuming Gaussian noise.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Interpreting linear regression from a probabilistic perspective, assuming Gaussian noise.", "node_type": "subnode"}, {"id": "Invertibility of X^TX Matrix", "label": "Invertibility of X^TX Matrix", "title": "<b>Invertibility of X^TX Matrix</b> (subnode)<hr>Condition for the matrix to be invertible in least-squares solution.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Condition for the matrix to be invertible in least-squares solution.", "node_type": "subnode"}, {"id": "Gaussian Distribution Assumption", "label": "Gaussian Distribution Assumption", "title": "<b>Gaussian Distribution Assumption</b> (subnode)<hr>Assumes Qi is a Gaussian distribution with independent coordinates, mean governed by q(x;phi), variance by v(x;psi).", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Assumes Qi is a Gaussian distribution with independent coordinates, mean governed by q(x;phi), variance by v(x;psi).", "node_type": "subnode"}, {"id": "Normal Equations", "label": "Normal Equations", "title": "<b>Normal Equations</b> (subnode)<hr>Direct method for finding the optimal parameters in linear regression without iteration.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Direct method for finding the optimal parameters in linear regression without iteration.", "node_type": "subnode"}, {"id": "Probability Distribution", "label": "Probability Distribution", "title": "<b>Probability Distribution</b> (subnode)<hr>Distribution of y given x and parameters \u03b8.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Distribution of y given x and parameters \u03b8.", "node_type": "subnode"}, {"id": "Design Matrix X", "label": "Design Matrix X", "title": "<b>Design Matrix X</b> (subnode)<hr>Matrix containing all input variables x(i).", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Matrix containing all input variables x(i).", "node_type": "subnode"}, {"id": "Likelihood Function", "label": "Likelihood Function", "title": "<b>Likelihood Function</b> (subnode)<hr>Function that measures how likely a set of parameters is, given observed data.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Function that measures how likely a set of parameters is, given observed data.", "node_type": "subnode"}, {"id": "Independence Assumption", "label": "Independence Assumption", "title": "<b>Independence Assumption</b> (subnode)<hr>Assumption on \u03b5(i)'s leading to product form likelihood.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Assumption on \u03b5(i)'s leading to product form likelihood.", "node_type": "subnode"}, {"id": "Maximum Likelihood Estimation", "label": "Maximum Likelihood Estimation", "title": "<b>Maximum Likelihood Estimation</b> (subnode)<hr>Estimating parameters to maximize probability of observed data under model.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Estimating parameters to maximize probability of observed data under model.", "node_type": "subnode"}, {"id": "Log Likelihood", "label": "Log Likelihood", "title": "<b>Log Likelihood</b> (subnode)<hr>Expression for log likelihood in terms of joint probability distribution.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Expression for log likelihood in terms of joint probability distribution.", "node_type": "subnode"}, {"id": "Learning Rate (\u03b1)", "label": "Learning Rate (\u03b1)", "title": "<b>Learning Rate (\u03b1)</b> (subnode)<hr>Hyperparameter controlling the size of each step in gradient descent.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Hyperparameter controlling the size of each step in gradient descent.", "node_type": "subnode"}, {"id": "Update Rule", "label": "Update Rule", "title": "<b>Update Rule</b> (subnode)<hr>Rule for updating parameters in each iteration of gradient descent.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Rule for updating parameters in each iteration of gradient descent.", "node_type": "subnode"}, {"id": "Locally Weighted Linear Regression", "label": "Locally Weighted Linear Regression", "title": "<b>Locally Weighted Linear Regression</b> (subnode)<hr>A method for learning to estimate future states from current state-action pairs.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "A method for learning to estimate future states from current state-action pairs.", "node_type": "subnode"}, {"id": "Non-Parametric Algorithms", "label": "Non-Parametric Algorithms", "title": "<b>Non-Parametric Algorithms</b> (subnode)<hr>Algorithms where the amount of information needed grows with the dataset size.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Algorithms where the amount of information needed grows with the dataset size.", "node_type": "subnode"}, {"id": "Parametric Algorithms", "label": "Parametric Algorithms", "title": "<b>Parametric Algorithms</b> (subnode)<hr>Algorithms that have a fixed number of parameters regardless of data size.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Algorithms that have a fixed number of parameters regardless of data size.", "node_type": "subnode"}, {"id": "Binary Classification", "label": "Binary Classification", "title": "<b>Binary Classification</b> (subnode)<hr>Classification task where output is binary, using logistic function to predict probabilities.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Classification task where output is binary, using logistic function to predict probabilities.", "node_type": "subnode"}, {"id": "Logistic Regression", "label": "Logistic Regression", "title": "<b>Logistic Regression</b> (subnode)<hr>Statistical method for modeling binary dependent variables by estimating probabilities using a logistic function.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Statistical method for modeling binary dependent variables by estimating probabilities using a logistic function.", "node_type": "subnode"}, {"id": "Linear Regression Approach", "label": "Linear Regression Approach", "title": "<b>Linear Regression Approach</b> (subnode)<hr>Initial attempt to solve classification with linear regression.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Initial attempt to solve classification with linear regression.", "node_type": "subnode"}, {"id": "Logistic Function", "label": "Logistic Function", "title": "<b>Logistic Function</b> (subnode)<hr>Function used to convert logit values into probability estimates for classification tasks.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Function used to convert logit values into probability estimates for classification tasks.", "node_type": "subnode"}, {"id": "Derivative of Sigmoid", "label": "Derivative of Sigmoid", "title": "<b>Derivative of Sigmoid</b> (subnode)<hr>Calculation showing the derivative of the sigmoid function.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Calculation showing the derivative of the sigmoid function.", "node_type": "subnode"}, {"id": "Machine Learning Models", "label": "Machine Learning Models", "title": "<b>Machine Learning Models</b> (major)<hr>Overview of models used in machine learning including classification and regression.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Overview of models used in machine learning including classification and regression.", "node_type": "major"}, {"id": "Classification Model", "label": "Classification Model", "title": "<b>Classification Model</b> (subnode)<hr>A model that predicts categorical class labels based on input features.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "A model that predicts categorical class labels based on input features.", "node_type": "subnode"}, {"id": "Log-Likelihood", "label": "Log-Likelihood", "title": "<b>Log-Likelihood</b> (subnode)<hr>Natural logarithm of the likelihood function for easier computation and optimization.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Natural logarithm of the likelihood function for easier computation and optimization.", "node_type": "subnode"}, {"id": "Gradient Ascent", "label": "Gradient Ascent", "title": "<b>Gradient Ascent</b> (subnode)<hr>Optimization technique used to maximize a function by iteratively moving in the direction of steepest ascent.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Optimization technique used to maximize a function by iteratively moving in the direction of steepest ascent.", "node_type": "subnode"}, {"id": "Logistic Regression Derivation", "label": "Logistic Regression Derivation", "title": "<b>Logistic Regression Derivation</b> (subnode)<hr>Derivation of logistic regression formulae.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Derivation of logistic regression formulae.", "node_type": "subnode"}, {"id": "Perceptron Algorithm", "label": "Perceptron Algorithm", "title": "<b>Perceptron Algorithm</b> (subnode)<hr>Attempts to find a linear decision boundary between classes.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Attempts to find a linear decision boundary between classes.", "node_type": "subnode"}, {"id": "Multi-class Classification", "label": "Multi-class Classification", "title": "<b>Multi-class Classification</b> (subnode)<hr>Classification problem where response variable can take on multiple values.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Classification problem where response variable can take on multiple values.", "node_type": "subnode"}, {"id": "Classification and Logistic Regression", "label": "Classification and Logistic Regression", "title": "<b>Classification and Logistic Regression</b> (subnode)<hr>Classifying data into discrete categories using logistic regression.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Classifying data into discrete categories using logistic regression.", "node_type": "subnode"}, {"id": "Perceptron Learning Algorithm", "label": "Perceptron Learning Algorithm", "title": "<b>Perceptron Learning Algorithm</b> (subnode)<hr>Algorithm for learning linear classifiers.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Algorithm for learning linear classifiers.", "node_type": "subnode"}, {"id": "Maximizing l(theta)", "label": "Maximizing l(theta)", "title": "<b>Maximizing l(theta)</b> (subnode)<hr>Alternative algorithm for maximizing the likelihood function.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Alternative algorithm for maximizing the likelihood function.", "node_type": "subnode"}, {"id": "Generalized Linear Models", "label": "Generalized Linear Models", "title": "<b>Generalized Linear Models</b> (subnode)<hr>Extension of linear models to non-normal distributions.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Extension of linear models to non-normal distributions.", "node_type": "subnode"}, {"id": "Exponential Family", "label": "Exponential Family", "title": "<b>Exponential Family</b> (subnode)<hr>Family of probability distributions with common properties.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Family of probability distributions with common properties.", "node_type": "subnode"}, {"id": "Constructing GLMs", "label": "Constructing GLMs", "title": "<b>Constructing GLMs</b> (subnode)<hr>Methods for constructing generalized linear models.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Methods for constructing generalized linear models.", "node_type": "subnode"}, {"id": "Logistic Regression (GLM)", "label": "Logistic Regression (GLM)", "title": "<b>Logistic Regression (GLM)</b> (subnode)<hr>Application of logistic regression within generalized linear models framework.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Application of logistic regression within generalized linear models framework.", "node_type": "subnode"}, {"id": "Generative Learning Algorithms", "label": "Generative Learning Algorithms", "title": "<b>Generative Learning Algorithms</b> (subnode)<hr>Models that generate data based on underlying probability distributions.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Models that generate data based on underlying probability distributions.", "node_type": "subnode"}, {"id": "Gaussian Discriminant Analysis", "label": "Gaussian Discriminant Analysis", "title": "<b>Gaussian Discriminant Analysis</b> (subnode)<hr>Method for classification using Gaussian distributions.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Method for classification using Gaussian distributions.", "node_type": "subnode"}, {"id": "Multivariate Normal Distribution", "label": "Multivariate Normal Distribution", "title": "<b>Multivariate Normal Distribution</b> (subnode)<hr>A generalization of the one-dimensional normal distribution to higher dimensions.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "A generalization of the one-dimensional normal distribution to higher dimensions.", "node_type": "subnode"}, {"id": "GDA Model", "label": "GDA Model", "title": "<b>GDA Model</b> (subnode)<hr>Model for classification using multivariate normal distributions.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Model for classification using multivariate normal distributions.", "node_type": "subnode"}, {"id": "Discussion: GDA and Logistic Regression", "label": "Discussion: GDA and Logistic Regression", "title": "<b>Discussion: GDA and Logistic Regression</b> (subnode)<hr>Comparison between Gaussian discriminant analysis and logistic regression.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Comparison between Gaussian discriminant analysis and logistic regression.", "node_type": "subnode"}, {"id": "Naive Bayes", "label": "Naive Bayes", "title": "<b>Naive Bayes</b> (subnode)<hr>Simplified probabilistic classifier based on applying Bayes' theorem with strong independence assumptions.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Simplified probabilistic classifier based on applying Bayes' theorem with strong independence assumptions.", "node_type": "subnode"}, {"id": "Laplace Smoothing", "label": "Laplace Smoothing", "title": "<b>Laplace Smoothing</b> (subnode)<hr>Technique to improve Naive Bayes by handling zero probabilities for unseen events.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Technique to improve Naive Bayes by handling zero probabilities for unseen events.", "node_type": "subnode"}, {"id": "Event Models for Text Classification", "label": "Event Models for Text Classification", "title": "<b>Event Models for Text Classification</b> (subnode)<hr>Models used for classifying text documents based on word occurrences.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Models used for classifying text documents based on word occurrences.", "node_type": "subnode"}, {"id": "Kernel Methods", "label": "Kernel Methods", "title": "<b>Kernel Methods</b> (major)<hr>Techniques that allow algorithms to work in high-dimensional feature spaces without explicit computation of vectors.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Techniques that allow algorithms to work in high-dimensional feature spaces without explicit computation of vectors.", "node_type": "major"}, {"id": "Feature Maps", "label": "Feature Maps", "title": "<b>Feature Maps</b> (subnode)<hr>Transformation of input data into higher-dimensional space to fit more complex models.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Transformation of input data into higher-dimensional space to fit more complex models.", "node_type": "subnode"}, {"id": "LMS with Features", "label": "LMS with Features", "title": "<b>LMS with Features</b> (subnode)<hr>Gradient descent algorithm for fitting models using features.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Gradient descent algorithm for fitting models using features.", "node_type": "subnode"}, {"id": "LMS with Kernel Trick", "label": "LMS with Kernel Trick", "title": "<b>LMS with Kernel Trick</b> (subnode)<hr>Efficient computation of LMS using kernel functions.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Efficient computation of LMS using kernel functions.", "node_type": "subnode"}, {"id": "Properties of Kernels", "label": "Properties of Kernels", "title": "<b>Properties of Kernels</b> (subnode)<hr>Exploration of properties associated with kernel functions in machine learning.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Exploration of properties associated with kernel functions in machine learning.", "node_type": "subnode"}, {"id": "Support Vector Machines", "label": "Support Vector Machines", "title": "<b>Support Vector Machines</b> (major)<hr>Algorithm for classification that finds the hyperplane maximizing the margin between classes.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Algorithm for classification that finds the hyperplane maximizing the margin between classes.", "node_type": "major"}, {"id": "Margins: Intuition", "label": "Margins: Intuition", "title": "<b>Margins: Intuition</b> (subnode)<hr>Understanding the concept of margins in SVMs.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Understanding the concept of margins in SVMs.", "node_type": "subnode"}, {"id": "Notation (Optional)", "label": "Notation (Optional)", "title": "<b>Notation (Optional)</b> (subnode)<hr>Mathematical notation used in support vector machines.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Mathematical notation used in support vector machines.", "node_type": "subnode"}, {"id": "Functional and Geometric Margins", "label": "Functional and Geometric Margins", "title": "<b>Functional and Geometric Margins</b> (subnode)<hr>Definitions of functional and geometric margins in SVMs.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Definitions of functional and geometric margins in SVMs.", "node_type": "subnode"}, {"id": "Optimal Margin Classifier (Optional)", "label": "Optimal Margin Classifier (Optional)", "title": "<b>Optimal Margin Classifier (Optional)</b> (subnode)<hr>Finding the optimal margin classifier for linearly separable data.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Finding the optimal margin classifier for linearly separable data.", "node_type": "subnode"}, {"id": "Lagrange Duality (Optional)", "label": "Lagrange Duality (Optional)", "title": "<b>Lagrange Duality (Optional)</b> (subnode)<hr>Use of Lagrangian duality in solving optimization problems in SVMs.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Use of Lagrangian duality in solving optimization problems in SVMs.", "node_type": "subnode"}, {"id": "Dual Formulation (Optional)", "label": "Dual Formulation (Optional)", "title": "<b>Dual Formulation (Optional)</b> (subnode)<hr>Optimal margin classifier expressed in the dual form.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Optimal margin classifier expressed in the dual form.", "node_type": "subnode"}, {"id": "Regularization and Non-separable Case (Optional)", "label": "Regularization and Non-separable Case (Optional)", "title": "<b>Regularization and Non-separable Case (Optional)</b> (subnode)<hr>Handling non-separable data with regularization techniques.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Handling non-separable data with regularization techniques.", "node_type": "subnode"}, {"id": "SMO Algorithm (Optional)", "label": "SMO Algorithm (Optional)", "title": "<b>SMO Algorithm (Optional)</b> (subnode)<hr>Sequential minimal optimization algorithm for solving SVM problems.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Sequential minimal optimization algorithm for solving SVM problems.", "node_type": "subnode"}, {"id": "Coordinate Ascent", "label": "Coordinate Ascent", "title": "<b>Coordinate Ascent</b> (subnode)<hr>An iterative optimization algorithm that optimizes one variable at a time.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "An iterative optimization algorithm that optimizes one variable at a time.", "node_type": "subnode"}, {"id": "SMO Details", "label": "SMO Details", "title": "<b>SMO Details</b> (subnode)<hr>Detailed explanation of the sequential minimal optimization process.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Detailed explanation of the sequential minimal optimization process.", "node_type": "subnode"}, {"id": "Deep Learning", "label": "Deep Learning", "title": "<b>Deep Learning</b> (major)<hr>Subfield of machine learning using neural networks to learn representations from data.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Subfield of machine learning using neural networks to learn representations from data.", "node_type": "major"}, {"id": "Supervised Learning with Non-linear Models", "label": "Supervised Learning with Non-linear Models", "title": "<b>Supervised Learning with Non-linear Models</b> (subnode)<hr>Using non-linear models in supervised learning tasks.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Using non-linear models in supervised learning tasks.", "node_type": "subnode"}, {"id": "Neural Networks", "label": "Neural Networks", "title": "<b>Neural Networks</b> (subnode)<hr>Inspired by biological neural networks, used for complex function approximation.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Inspired by biological neural networks, used for complex function approximation.", "node_type": "subnode"}, {"id": "Modules in Modern Neural Networks", "label": "Modules in Modern Neural Networks", "title": "<b>Modules in Modern Neural Networks</b> (subnode)<hr>Introduces various building blocks and ways to combine them in modern neural networks.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Introduces various building blocks and ways to combine them in modern neural networks.", "node_type": "subnode"}, {"id": "Backpropagation", "label": "Backpropagation", "title": "<b>Backpropagation</b> (subnode)<hr>Algorithm for training multi-layered neural networks by adjusting weights based on error gradients.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Algorithm for training multi-layered neural networks by adjusting weights based on error gradients.", "node_type": "subnode"}, {"id": "Preliminaries on Partial Derivatives", "label": "Preliminaries on Partial Derivatives", "title": "<b>Preliminaries on Partial Derivatives</b> (subnode)<hr>Introduction to partial derivatives necessary for backpropagation.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Introduction to partial derivatives necessary for backpropagation.", "node_type": "subnode"}, {"id": "General Strategy of Backpropagation", "label": "General Strategy of Backpropagation", "title": "<b>General Strategy of Backpropagation</b> (subnode)<hr>Overview of the general strategy used in backpropagation algorithm.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Overview of the general strategy used in backpropagation algorithm.", "node_type": "subnode"}, {"id": "Backward Functions for Basic Modules", "label": "Backward Functions for Basic Modules", "title": "<b>Backward Functions for Basic Modules</b> (subnode)<hr>Derivation and implementation of backward functions for basic neural network modules.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Derivation and implementation of backward functions for basic neural network modules.", "node_type": "subnode"}, {"id": "Back-propagation for MLPs", "label": "Back-propagation for MLPs", "title": "<b>Back-propagation for MLPs</b> (subnode)<hr>Process of computing gradients through a multi-layer perceptron network to update weights during training.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Process of computing gradients through a multi-layer perceptron network to update weights during training.", "node_type": "subnode"}, {"id": "Stochastic Gradient Ascent Rule", "label": "Stochastic Gradient Ascent Rule", "title": "<b>Stochastic Gradient Ascent Rule</b> (subnode)<hr>Update rule for parameters using stochastic gradient ascent.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Update rule for parameters using stochastic gradient ascent.", "node_type": "subnode"}, {"id": "Logistic Loss Function", "label": "Logistic Loss Function", "title": "<b>Logistic Loss Function</b> (subnode)<hr>Function that measures the performance of a classification model where labels are binary values.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Function that measures the performance of a classification model where labels are binary values.", "node_type": "subnode"}, {"id": "Negative Log-Likelihood", "label": "Negative Log-Likelihood", "title": "<b>Negative Log-Likelihood</b> (subnode)<hr>Loss function measuring the dissimilarity between predicted and actual distributions.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Loss function measuring the dissimilarity between predicted and actual distributions.", "node_type": "subnode"}, {"id": "Logit", "label": "Logit", "title": "<b>Logit</b> (subnode)<hr>Linear combination of input features and parameters before applying the logistic function.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Linear combination of input features and parameters before applying the logistic function.", "node_type": "subnode"}, {"id": "Loss Functions", "label": "Loss Functions", "title": "<b>Loss Functions</b> (subnode)<hr>Functions used to measure the performance of a model during training and testing.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Functions used to measure the performance of a model during training and testing.", "node_type": "subnode"}, {"id": "Cross-Entropy Loss", "label": "Cross-Entropy Loss", "title": "<b>Cross-Entropy Loss</b> (subnode)<hr>Summation of negative log-likelihoods over training data for optimization.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Summation of negative log-likelihoods over training data for optimization.", "node_type": "subnode"}, {"id": "Softmax Function", "label": "Softmax Function", "title": "<b>Softmax Function</b> (subnode)<hr>Function used to convert logits into a probability vector for multi-class classification.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Function used to convert logits into a probability vector for multi-class classification.", "node_type": "subnode"}, {"id": "Machine Learning", "label": "Machine Learning", "title": "<b>Machine Learning</b> (major)<hr>Field of study that uses algorithms to make predictions or decisions based on data.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Field of study that uses algorithms to make predictions or decisions based on data.", "node_type": "major"}, {"id": "Classification", "label": "Classification", "title": "<b>Classification</b> (subnode)<hr>Technique for categorizing data into predefined classes.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Technique for categorizing data into predefined classes.", "node_type": "subnode"}, {"id": "Multinomial Distribution", "label": "Multinomial Distribution", "title": "<b>Multinomial Distribution</b> (subnode)<hr>Probability distribution over multiple outcomes.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Probability distribution over multiple outcomes.", "node_type": "subnode"}, {"id": "Logits", "label": "Logits", "title": "<b>Logits</b> (subnode)<hr>Inputs to the softmax function before transformation.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Inputs to the softmax function before transformation.", "node_type": "subnode"}, {"id": "Probability Vector", "label": "Probability Vector", "title": "<b>Probability Vector</b> (subnode)<hr>Output of softmax, a vector with nonnegative entries summing to 1.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Output of softmax, a vector with nonnegative entries summing to 1.", "node_type": "subnode"}, {"id": "Probabilistic Model", "label": "Probabilistic Model", "title": "<b>Probabilistic Model</b> (major)<hr>Model using softmax outputs as probabilities for classification tasks.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Model using softmax outputs as probabilities for classification tasks.", "node_type": "major"}, {"id": "Newton's Method", "label": "Newton's Method", "title": "<b>Newton's Method</b> (major)<hr>Optimization technique using second-order derivatives to find local minima or maxima.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Optimization technique using second-order derivatives to find local minima or maxima.", "node_type": "major"}, {"id": "Finding Roots", "label": "Finding Roots", "title": "<b>Finding Roots</b> (subnode)<hr>The process of determining the values of x where f(x) = 0.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "The process of determining the values of x where f(x) = 0.", "node_type": "subnode"}, {"id": "Maximizing Functions", "label": "Maximizing Functions", "title": "<b>Maximizing Functions</b> (subnode)<hr>Using Newton's method to find maxima by setting first derivative to zero.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Using Newton's method to find maxima by setting first derivative to zero.", "node_type": "subnode"}, {"id": "Multidimensional Generalization", "label": "Multidimensional Generalization", "title": "<b>Multidimensional Generalization</b> (subnode)<hr>Extending Newton's method to handle vector-valued functions in logistic regression.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Extending Newton's method to handle vector-valued functions in logistic regression.", "node_type": "subnode"}, {"id": "Hessian Matrix", "label": "Hessian Matrix", "title": "<b>Hessian Matrix</b> (subnode)<hr>A square matrix of second-order partial derivatives used for multidimensional optimization.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "A square matrix of second-order partial derivatives used for multidimensional optimization.", "node_type": "subnode"}, {"id": "Optimization Methods", "label": "Optimization Methods", "title": "<b>Optimization Methods</b> (subnode)<hr>Papers discussing optimization methods such as Adam and variational auto-encoding.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Papers discussing optimization methods such as Adam and variational auto-encoding.", "node_type": "subnode"}, {"id": "Fisher Scoring", "label": "Fisher Scoring", "title": "<b>Fisher Scoring</b> (subnode)<hr>A variant of Newton's method used in logistic regression.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "A variant of Newton's method used in logistic regression.", "node_type": "subnode"}, {"id": "Generalized Linear Models (GLMs)", "label": "Generalized Linear Models (GLMs)", "title": "<b>Generalized Linear Models (GLMs)</b> (major)<hr>Models that extend linear models to accommodate non-normal distributions and non-linear relationships.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Models that extend linear models to accommodate non-normal distributions and non-linear relationships.", "node_type": "major"}, {"id": "Exponential Family Distributions", "label": "Exponential Family Distributions", "title": "<b>Exponential Family Distributions</b> (subnode)<hr>A class of probability distributions that can be expressed in a specific form involving natural parameters and sufficient statistics.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "A class of probability distributions that can be expressed in a specific form involving natural parameters and sufficient statistics.", "node_type": "subnode"}, {"id": "Loss Function", "label": "Loss Function", "title": "<b>Loss Function</b> (subnode)<hr>Mathematical function used to measure the performance of a model and guide its training.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Mathematical function used to measure the performance of a model and guide its training.", "node_type": "subnode"}, {"id": "Cross Entropy Loss", "label": "Cross Entropy Loss", "title": "<b>Cross Entropy Loss</b> (subnode)<hr>A loss function commonly used in logistic regression and neural networks.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "A loss function commonly used in logistic regression and neural networks.", "node_type": "subnode"}, {"id": "Algorithm Application", "label": "Algorithm Application", "title": "<b>Algorithm Application</b> (subnode)<hr>Application of Newton's method in logistic regression for maximizing likelihood.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Application of Newton's method in logistic regression for maximizing likelihood.", "node_type": "subnode"}, {"id": "Modern Neural Networks", "label": "Modern Neural Networks", "title": "<b>Modern Neural Networks</b> (major)<hr>Overview of modern neural network architectures and training techniques.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Overview of modern neural network architectures and training techniques.", "node_type": "major"}, {"id": "Preliminaries on partial derivatives", "label": "Preliminaries on partial derivatives", "title": "<b>Preliminaries on partial derivatives</b> (subnode)<hr>Introduction to the mathematical concepts needed for backpropagation.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Introduction to the mathematical concepts needed for backpropagation.", "node_type": "subnode"}, {"id": "General strategy of backpropagation", "label": "General strategy of backpropagation", "title": "<b>General strategy of backpropagation</b> (subnode)<hr>Overview of how backpropagation works in neural networks.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Overview of how backpropagation works in neural networks.", "node_type": "subnode"}, {"id": "Backward functions for basic modules", "label": "Backward functions for basic modules", "title": "<b>Backward functions for basic modules</b> (subnode)<hr>Detailed explanation of backward propagation through simple network components.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Detailed explanation of backward propagation through simple network components.", "node_type": "subnode"}, {"id": "Vectorization over training examples", "label": "Vectorization over training examples", "title": "<b>Vectorization over training examples</b> (subnode)<hr>Techniques for optimizing the computation of gradients across multiple data points.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Techniques for optimizing the computation of gradients across multiple data points.", "node_type": "subnode"}, {"id": "Generalization and regularization", "label": "Generalization and regularization", "title": "<b>Generalization and regularization</b> (major)<hr>Strategies to improve model performance on unseen data through generalization and regularization techniques.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Strategies to improve model performance on unseen data through generalization and regularization techniques.", "node_type": "major"}, {"id": "Generalization", "label": "Generalization", "title": "<b>Generalization</b> (subnode)<hr>Concepts related to improving a model's ability to generalize from training data to new, unseen data.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Concepts related to improving a model's ability to generalize from training data to new, unseen data.", "node_type": "subnode"}, {"id": "Bias-variance tradeoff", "label": "Bias-variance tradeoff", "title": "<b>Bias-variance tradeoff</b> (subnode)<hr>Discussion on the balance between model complexity and error due to variance or bias.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Discussion on the balance between model complexity and error due to variance or bias.", "node_type": "subnode"}, {"id": "A mathematical decomposition (for regression)", "label": "A mathematical decomposition (for regression)", "title": "<b>A mathematical decomposition (for regression)</b> (subnode)<hr>Mathematical breakdown of the bias-variance tradeoff in a regression context.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Mathematical breakdown of the bias-variance tradeoff in a regression context.", "node_type": "subnode"}, {"id": "The double descent phenomenon", "label": "The double descent phenomenon", "title": "<b>The double descent phenomenon</b> (subnode)<hr>Phenomenon where model performance improves after reaching a peak due to overfitting.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Phenomenon where model performance improves after reaching a peak due to overfitting.", "node_type": "subnode"}, {"id": "Sample complexity bounds (optional readings)", "label": "Sample complexity bounds (optional readings)", "title": "<b>Sample complexity bounds (optional readings)</b> (subnode)<hr>Theoretical analysis of the number of samples needed for learning tasks.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Theoretical analysis of the number of samples needed for learning tasks.", "node_type": "subnode"}, {"id": "Regularization and model selection", "label": "Regularization and model selection", "title": "<b>Regularization and model selection</b> (major)<hr>Techniques to prevent overfitting by penalizing complex models or selecting optimal hyperparameters.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Techniques to prevent overfitting by penalizing complex models or selecting optimal hyperparameters.", "node_type": "major"}, {"id": "Regularization", "label": "Regularization", "title": "<b>Regularization</b> (subnode)<hr>Technique to prevent overfitting by adding a regularizer term to the loss function.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Technique to prevent overfitting by adding a regularizer term to the loss function.", "node_type": "subnode"}, {"id": "Implicit regularization effect (optional reading)", "label": "Implicit regularization effect (optional reading)", "title": "<b>Implicit regularization effect (optional reading)</b> (subnode)<hr>Exploration of how certain algorithms inherently regularize models without explicit penalties.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Exploration of how certain algorithms inherently regularize models without explicit penalties.", "node_type": "subnode"}, {"id": "Model selection via cross validation", "label": "Model selection via cross validation", "title": "<b>Model selection via cross validation</b> (subnode)<hr>Procedure for choosing the best model based on performance across different data splits.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Procedure for choosing the best model based on performance across different data splits.", "node_type": "subnode"}, {"id": "Bayesian statistics and regularization", "label": "Bayesian statistics and regularization", "title": "<b>Bayesian statistics and regularization</b> (subnode)<hr>Application of Bayesian methods to regularize models and improve generalization.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Application of Bayesian methods to regularize models and improve generalization.", "node_type": "subnode"}, {"id": "Unsupervised learning", "label": "Unsupervised learning", "title": "<b>Unsupervised learning</b> (major)<hr>Techniques for learning from data without labeled responses.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Techniques for learning from data without labeled responses.", "node_type": "major"}, {"id": "Clustering and the k-means algorithm", "label": "Clustering and the k-means algorithm", "title": "<b>Clustering and the k-means algorithm</b> (subnode)<hr>Introduction to clustering methods with a focus on the k-means algorithm.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Introduction to clustering methods with a focus on the k-means algorithm.", "node_type": "subnode"}, {"id": "EM algorithms", "label": "EM algorithms", "title": "<b>EM algorithms</b> (subnode)<hr>Expectation-Maximization techniques for parameter estimation in probabilistic models.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Expectation-Maximization techniques for parameter estimation in probabilistic models.", "node_type": "subnode"}, {"id": "EM for mixture of Gaussians", "label": "EM for mixture of Gaussians", "title": "<b>EM for mixture of Gaussians</b> (subnode)<hr>Application of EM to Gaussian Mixture Models (GMMs).", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Application of EM to Gaussian Mixture Models (GMMs).", "node_type": "subnode"}, {"id": "Jensen's inequality", "label": "Jensen's inequality", "title": "<b>Jensen's inequality</b> (subnode)<hr>Mathematical principle used in the derivation and understanding of EM algorithms.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Mathematical principle used in the derivation and understanding of EM algorithms.", "node_type": "subnode"}, {"id": "General EM algorithms", "label": "General EM algorithms", "title": "<b>General EM algorithms</b> (subnode)<hr>Overview of the general framework and applications of EM beyond GMMs.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Overview of the general framework and applications of EM beyond GMMs.", "node_type": "subnode"}, {"id": "Other interpretation of ELBO", "label": "Other interpretation of ELBO", "title": "<b>Other interpretation of ELBO</b> (subnode)<hr>Alternative perspectives on the Evidence Lower Bound (ELBO) in variational inference.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Alternative perspectives on the Evidence Lower Bound (ELBO) in variational inference.", "node_type": "subnode"}, {"id": "Mixture of Gaussians revisited", "label": "Mixture of Gaussians revisited", "title": "<b>Mixture of Gaussians revisited</b> (subnode)<hr>Re-examination and advanced topics related to Gaussian Mixture Models using EM.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Re-examination and advanced topics related to Gaussian Mixture Models using EM.", "node_type": "subnode"}, {"id": "Variational inference and variational auto-encoder (optional reading)", "label": "Variational inference and variational auto-encoder (optional reading)", "title": "<b>Variational inference and variational auto-encoder (optional reading)</b> (subnode)<hr>Advanced topic on probabilistic modeling with VAEs and VI.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Advanced topic on probabilistic modeling with VAEs and VI.", "node_type": "subnode"}, {"id": "Principal components analysis", "label": "Principal components analysis", "title": "<b>Principal components analysis</b> (subnode)<hr>Dimensionality reduction technique that projects data onto a lower-dimensional space.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Dimensionality reduction technique that projects data onto a lower-dimensional space.", "node_type": "subnode"}, {"id": "Independent components analysis", "label": "Independent components analysis", "title": "<b>Independent components analysis</b> (subnode)<hr>Technique for separating mixed signals into their independent sources.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Technique for separating mixed signals into their independent sources.", "node_type": "subnode"}, {"id": "ICA ambiguities", "label": "ICA ambiguities", "title": "<b>ICA ambiguities</b> (subnode)<hr>Discussion on the inherent limitations and challenges in ICA.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Discussion on the inherent limitations and challenges in ICA.", "node_type": "subnode"}, {"id": "Densities and linear transformations", "label": "Densities and linear transformations", "title": "<b>Densities and linear transformations</b> (subnode)<hr>Exploration of how densities change under linear transformations relevant to ICA.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Exploration of how densities change under linear transformations relevant to ICA.", "node_type": "subnode"}, {"id": "ICA algorithm", "label": "ICA algorithm", "title": "<b>ICA algorithm</b> (subnode)<hr>Detailed explanation of the Independent Components Analysis procedure.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Detailed explanation of the Independent Components Analysis procedure.", "node_type": "subnode"}, {"id": "Self-supervised learning and foundation models", "label": "Self-supervised learning and foundation models", "title": "<b>Self-supervised learning and foundation models</b> (major)<hr>Approaches to training large-scale models using self-supervision techniques.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Approaches to training large-scale models using self-supervision techniques.", "node_type": "major"}, {"id": "Pretraining and adaptation", "label": "Pretraining and adaptation", "title": "<b>Pretraining and adaptation</b> (subnode)<hr>Overview of pre-training methods and subsequent fine-tuning for specific tasks.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Overview of pre-training methods and subsequent fine-tuning for specific tasks.", "node_type": "subnode"}, {"id": "Pretraining methods in computer vision", "label": "Pretraining methods in computer vision", "title": "<b>Pretraining methods in computer vision</b> (subnode)<hr>Techniques used to train visual recognition systems without labeled data.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Techniques used to train visual recognition systems without labeled data.", "node_type": "subnode"}, {"id": "Pretrained large language models", "label": "Pretrained large language models", "title": "<b>Pretrained large language models</b> (subnode)<hr>Discussion on the development and applications of pre-trained language models.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Discussion on the development and applications of pre-trained language models.", "node_type": "subnode"}, {"id": "Open up the blackbox of Transformers", "label": "Open up the blackbox of Transformers", "title": "<b>Open up the blackbox of Transformers</b> (subnode)<hr>Exploration of the architecture and workings of Transformer-based models.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Exploration of the architecture and workings of Transformer-based models.", "node_type": "subnode"}, {"id": "Zero-shot learning and in-context learning", "label": "Zero-shot learning and in-context learning", "title": "<b>Zero-shot learning and in-context learning</b> (subnode)<hr>Capabilities of models to perform tasks without explicit training data.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Capabilities of models to perform tasks without explicit training data.", "node_type": "subnode"}, {"id": "Reinforcement Learning and Control", "label": "Reinforcement Learning and Control", "title": "<b>Reinforcement Learning and Control</b> (major)<hr>Techniques for agents to learn optimal behavior through interaction with an environment.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Techniques for agents to learn optimal behavior through interaction with an environment.", "node_type": "major"}, {"id": "Reinforcement learning", "label": "Reinforcement learning", "title": "<b>Reinforcement learning</b> (subnode)<hr>Introduction to the principles of reinforcement learning and its applications.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Introduction to the principles of reinforcement learning and its applications.", "node_type": "subnode"}, {"id": "Markov decision processes", "label": "Markov decision processes", "title": "<b>Markov decision processes</b> (subnode)<hr>Mathematical framework for modeling sequential decision-making problems.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Mathematical framework for modeling sequential decision-making problems.", "node_type": "subnode"}, {"id": "Value iteration and policy iteration", "label": "Value iteration and policy iteration", "title": "<b>Value iteration and policy iteration</b> (subnode)<hr>Algorithms for finding optimal policies in MDPs through value or policy updates.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Algorithms for finding optimal policies in MDPs through value or policy updates.", "node_type": "subnode"}, {"id": "Generalized Linear Model (GLM)", "label": "Generalized Linear Model (GLM)", "title": "<b>Generalized Linear Model (GLM)</b> (subnode)<hr>A method to model problems using exponential family distributions.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "A method to model problems using exponential family distributions.", "node_type": "subnode"}, {"id": "Poisson Distribution", "label": "Poisson Distribution", "title": "<b>Poisson Distribution</b> (subnode)<hr>Distribution used to model count data such as customer arrivals or page views.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Distribution used to model count data such as customer arrivals or page views.", "node_type": "subnode"}, {"id": "Conditional Distribution Assumption", "label": "Conditional Distribution Assumption", "title": "<b>Conditional Distribution Assumption</b> (subnode)<hr>Assumes y|x follows an exponential family distribution.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Assumes y|x follows an exponential family distribution.", "node_type": "subnode"}, {"id": "Prediction Goal", "label": "Prediction Goal", "title": "<b>Prediction Goal</b> (subnode)<hr>Aims to predict the expected value of T(y) given x.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Aims to predict the expected value of T(y) given x.", "node_type": "subnode"}, {"id": "Linear Relationship Assumption", "label": "Linear Relationship Assumption", "title": "<b>Linear Relationship Assumption</b> (subnode)<hr>Assumes a linear relationship between natural parameter and inputs.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Assumes a linear relationship between natural parameter and inputs.", "node_type": "subnode"}, {"id": "Generalized Linear Models (GLM)", "label": "Generalized Linear Models (GLM)", "title": "<b>Generalized Linear Models (GLM)</b> (subnode)<hr>Models that extend linear regression to accommodate non-normal distributions.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Models that extend linear regression to accommodate non-normal distributions.", "node_type": "subnode"}, {"id": "Bernoulli Distribution", "label": "Bernoulli Distribution", "title": "<b>Bernoulli Distribution</b> (subnode)<hr>Binary outcome probability distribution, an example of exponential family.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Binary outcome probability distribution, an example of exponential family.", "node_type": "subnode"}, {"id": "Gaussian Distribution", "label": "Gaussian Distribution", "title": "<b>Gaussian Distribution</b> (subnode)<hr>Describes the distribution of state at time t+1 given observations up to time t.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Describes the distribution of state at time t+1 given observations up to time t.", "node_type": "subnode"}, {"id": "Natural Parameter", "label": "Natural Parameter", "title": "<b>Natural Parameter</b> (subnode)<hr>Parameter \u03b7 in the exponential family distribution formula.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Parameter \u03b7 in the exponential family distribution formula.", "node_type": "subnode"}, {"id": "Sufficient Statistic", "label": "Sufficient Statistic", "title": "<b>Sufficient Statistic</b> (subnode)<hr>Statistic T(y) that summarizes data relevant to parameter estimation.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Statistic T(y) that summarizes data relevant to parameter estimation.", "node_type": "subnode"}, {"id": "Log Partition Function", "label": "Log Partition Function", "title": "<b>Log Partition Function</b> (subnode)<hr>Function a(\u03b7) ensuring the distribution sums/integrates to 1.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Function a(\u03b7) ensuring the distribution sums/integrates to 1.", "node_type": "subnode"}, {"id": "Natural Parameter for Bernoulli", "label": "Natural Parameter for Bernoulli", "title": "<b>Natural Parameter for Bernoulli</b> (subnode)<hr>\u03b7 = log(\u03c6/(1-\u03c6)), where \u03c6 is the mean of the distribution.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "\u03b7 = log(\u03c6/(1-\u03c6)), where \u03c6 is the mean of the distribution.", "node_type": "subnode"}, {"id": "Sufficient Statistic for Bernoulli", "label": "Sufficient Statistic for Bernoulli", "title": "<b>Sufficient Statistic for Bernoulli</b> (subnode)<hr>T(y) = y, indicating the outcome itself as sufficient statistic.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "T(y) = y, indicating the outcome itself as sufficient statistic.", "node_type": "subnode"}, {"id": "Log Partition Function for Bernoulli", "label": "Log Partition Function for Bernoulli", "title": "<b>Log Partition Function for Bernoulli</b> (subnode)<hr>a(\u03b7) = log(1 + e^\u03b7), ensuring normalization of distribution.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "a(\u03b7) = log(1 + e^\u03b7), ensuring normalization of distribution.", "node_type": "subnode"}, {"id": "Assumptions/Design Choices", "label": "Assumptions/Design Choices", "title": "<b>Assumptions/Design Choices</b> (subnode)<hr>Three foundational principles that lead to the derivation of GLMs.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Three foundational principles that lead to the derivation of GLMs.", "node_type": "subnode"}, {"id": "Response Variable", "label": "Response Variable", "title": "<b>Response Variable</b> (subnode)<hr>Continuous target variable in the context of GLM formulation.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Continuous target variable in the context of GLM formulation.", "node_type": "subnode"}, {"id": "Conditional Distribution", "label": "Conditional Distribution", "title": "<b>Conditional Distribution</b> (subnode)<hr>Modeling the conditional distribution of y given x and theta.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Modeling the conditional distribution of y given x and theta.", "node_type": "subnode"}, {"id": "Bayesian Classification", "label": "Bayesian Classification", "title": "<b>Bayesian Classification</b> (major)<hr>Classification method using Bayes' theorem to predict class membership probabilities.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Classification method using Bayes' theorem to predict class membership probabilities.", "node_type": "major"}, {"id": "Class Priors", "label": "Class Priors", "title": "<b>Class Priors</b> (subnode)<hr>Prior probability of each class before observing data.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Prior probability of each class before observing data.", "node_type": "subnode"}, {"id": "Conditional Probability", "label": "Conditional Probability", "title": "<b>Conditional Probability</b> (subnode)<hr>Probability of an event given that another event has occurred.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Probability of an event given that another event has occurred.", "node_type": "subnode"}, {"id": "Posterior Distribution", "label": "Posterior Distribution", "title": "<b>Posterior Distribution</b> (subnode)<hr>The posterior distribution of latent variables given observed data under current parameters.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "The posterior distribution of latent variables given observed data under current parameters.", "node_type": "subnode"}, {"id": "Gaussian Discriminant Analysis (GDA)", "label": "Gaussian Discriminant Analysis (GDA)", "title": "<b>Gaussian Discriminant Analysis (GDA)</b> (major)<hr>A probabilistic model that assumes the data is generated from a mixture of several Gaussian distributions with unknown parameters.", "shape": "star", "size": 25, "color": "#FF6347", "description": "A probabilistic model that assumes the data is generated from a mixture of several Gaussian distributions with unknown parameters.", "node_type": "major"}, {"id": "Conditional Distribution Modeling", "label": "Conditional Distribution Modeling", "title": "<b>Conditional Distribution Modeling</b> (subnode)<hr>Modeling the distribution of y given x.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Modeling the distribution of y given x.", "node_type": "subnode"}, {"id": "Canonical Response Function", "label": "Canonical Response Function", "title": "<b>Canonical Response Function</b> (subnode)<hr>Function giving the mean of a distribution as a function of its natural parameter.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Function giving the mean of a distribution as a function of its natural parameter.", "node_type": "subnode"}, {"id": "Canonical Link Function", "label": "Canonical Link Function", "title": "<b>Canonical Link Function</b> (subnode)<hr>Inverse of the canonical response function, mapping from expected value to natural parameter.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Inverse of the canonical response function, mapping from expected value to natural parameter.", "node_type": "subnode"}, {"id": "Discriminative Learning Algorithms", "label": "Discriminative Learning Algorithms", "title": "<b>Discriminative Learning Algorithms</b> (subnode)<hr>Algorithms that directly model the decision boundary between classes.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Algorithms that directly model the decision boundary between classes.", "node_type": "subnode"}, {"id": "Bayes Rule", "label": "Bayes Rule", "title": "<b>Bayes Rule</b> (subnode)<hr>Used to derive posterior distribution p(y|x) from p(x|y) and p(y).", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Used to derive posterior distribution p(y|x) from p(x|y) and p(y).", "node_type": "subnode"}, {"id": "Probability Distributions", "label": "Probability Distributions", "title": "<b>Probability Distributions</b> (subnode)<hr>Discussion on probability distributions used in ML models.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Discussion on probability distributions used in ML models.", "node_type": "subnode"}, {"id": "Mean", "label": "Mean", "title": "<b>Mean</b> (subnode)<hr>Definition and calculation of mean in Gaussian distribution.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Definition and calculation of mean in Gaussian distribution.", "node_type": "subnode"}, {"id": "Covariance", "label": "Covariance", "title": "<b>Covariance</b> (subnode)<hr>Explanation of covariance matrix and its significance.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Explanation of covariance matrix and its significance.", "node_type": "subnode"}, {"id": "Standard Normal Distribution", "label": "Standard Normal Distribution", "title": "<b>Standard Normal Distribution</b> (subnode)<hr>Definition and properties of standard normal distribution.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Definition and properties of standard normal distribution.", "node_type": "subnode"}, {"id": "Density Visualization", "label": "Density Visualization", "title": "<b>Density Visualization</b> (subnode)<hr>Examples showing how density changes with covariance matrix values.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Examples showing how density changes with covariance matrix values.", "node_type": "subnode"}, {"id": "Covariance Matrix", "label": "Covariance Matrix", "title": "<b>Covariance Matrix</b> (subnode)<hr>Matrix that describes the variance and covariance between random variables.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Matrix that describes the variance and covariance between random variables.", "node_type": "subnode"}, {"id": "Contour Plots", "label": "Contour Plots", "title": "<b>Contour Plots</b> (subnode)<hr>Graphical representation of density contours for different values of parameters.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Graphical representation of density contours for different values of parameters.", "node_type": "subnode"}, {"id": "Multivariate Normal Distributions for Classes", "label": "Multivariate Normal Distributions for Classes", "title": "<b>Multivariate Normal Distributions for Classes</b> (subnode)<hr>Represents different classes with separate mean vectors and a common covariance matrix.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Represents different classes with separate mean vectors and a common covariance matrix.", "node_type": "subnode"}, {"id": "Decision Boundaries", "label": "Decision Boundaries", "title": "<b>Decision Boundaries</b> (subnode)<hr>Boundaries that separate different classes in feature space.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Boundaries that separate different classes in feature space.", "node_type": "subnode"}, {"id": "Model Assumptions", "label": "Model Assumptions", "title": "<b>Model Assumptions</b> (subnode)<hr>Theoretical assumptions made by models about the data distribution.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Theoretical assumptions made by models about the data distribution.", "node_type": "subnode"}, {"id": "Model Parameters", "label": "Model Parameters", "title": "<b>Model Parameters</b> (subnode)<hr>Parameters include prior probability of spam, conditional probabilities of words given spam or non-spam", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Parameters include prior probability of spam, conditional probabilities of words given spam or non-spam", "node_type": "subnode"}, {"id": "Log-Likelihood Function", "label": "Log-Likelihood Function", "title": "<b>Log-Likelihood Function</b> (subnode)<hr>Function used to estimate parameters by maximizing likelihood of observed data given the model.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Function used to estimate parameters by maximizing likelihood of observed data given the model.", "node_type": "subnode"}, {"id": "Decision Boundary", "label": "Decision Boundary", "title": "<b>Decision Boundary</b> (subnode)<hr>Boundary in feature space where model predicts equal probabilities for both classes.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Boundary in feature space where model predicts equal probabilities for both classes.", "node_type": "subnode"}, {"id": "GDA (Generative Discriminative Approach)", "label": "GDA (Generative Discriminative Approach)", "title": "<b>GDA (Generative Discriminative Approach)</b> (subnode)<hr>Model that makes strong assumptions about data distribution.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Model that makes strong assumptions about data distribution.", "node_type": "subnode"}, {"id": "Robustness of Logistic Regression", "label": "Robustness of Logistic Regression", "title": "<b>Robustness of Logistic Regression</b> (subnode)<hr>Discusses the robust nature and performance in large datasets.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Discusses the robust nature and performance in large datasets.", "node_type": "subnode"}, {"id": "Naive Bayes (Discrete Features)", "label": "Naive Bayes (Discrete Features)", "title": "<b>Naive Bayes (Discrete Features)</b> (subnode)<hr>Algorithm for classification with discrete-valued features.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Algorithm for classification with discrete-valued features.", "node_type": "subnode"}, {"id": "Feature Vector Selection", "label": "Feature Vector Selection", "title": "<b>Feature Vector Selection</b> (subnode)<hr>Choosing relevant features for classification tasks.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Choosing relevant features for classification tasks.", "node_type": "subnode"}, {"id": "Generative Models", "label": "Generative Models", "title": "<b>Generative Models</b> (subnode)<hr>Models that generate data based on learned parameters.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Models that generate data based on learned parameters.", "node_type": "subnode"}, {"id": "Naive Bayes Classifier", "label": "Naive Bayes Classifier", "title": "<b>Naive Bayes Classifier</b> (subnode)<hr>A probabilistic classifier based on applying Bayes' theorem with strong (naive) independence assumptions between the features.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "A probabilistic classifier based on applying Bayes' theorem with strong (naive) independence assumptions between the features.", "node_type": "subnode"}, {"id": "Conditional Independence Assumption", "label": "Conditional Independence Assumption", "title": "<b>Conditional Independence Assumption</b> (subnode)<hr>Assumption that features are independent given the class label.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Assumption that features are independent given the class label.", "node_type": "subnode"}, {"id": "Text Classification", "label": "Text Classification", "title": "<b>Text Classification</b> (subnode)<hr>Process of categorizing text into predefined categories using machine learning techniques.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Process of categorizing text into predefined categories using machine learning techniques.", "node_type": "subnode"}, {"id": "Spam Filter", "label": "Spam Filter", "title": "<b>Spam Filter</b> (subnode)<hr>System that uses machine learning to classify emails as spam or non-spam.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "System that uses machine learning to classify emails as spam or non-spam.", "node_type": "subnode"}, {"id": "Training Set", "label": "Training Set", "title": "<b>Training Set</b> (subnode)<hr>Collection of examples used for training a model, each with features and labels.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Collection of examples used for training a model, each with features and labels.", "node_type": "subnode"}, {"id": "Feature Vector", "label": "Feature Vector", "title": "<b>Feature Vector</b> (subnode)<hr>Representation of an email as a vector where each dimension corresponds to the presence or absence of words in the vocabulary.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Representation of an email as a vector where each dimension corresponds to the presence or absence of words in the vocabulary.", "node_type": "subnode"}, {"id": "Vocabulary", "label": "Vocabulary", "title": "<b>Vocabulary</b> (subnode)<hr>Set of unique words used to represent emails in feature vectors.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Set of unique words used to represent emails in feature vectors.", "node_type": "subnode"}, {"id": "Stop Words", "label": "Stop Words", "title": "<b>Stop Words</b> (subnode)<hr>Commonly occurring words that are often excluded from the vocabulary due to their lack of discriminatory power.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Commonly occurring words that are often excluded from the vocabulary due to their lack of discriminatory power.", "node_type": "subnode"}, {"id": "Naive Bayes Algorithm", "label": "Naive Bayes Algorithm", "title": "<b>Naive Bayes Algorithm</b> (major)<hr>A probabilistic classifier based on applying Bayes' theorem with strong independence assumptions between the features.", "shape": "star", "size": 25, "color": "#FF6347", "description": "A probabilistic classifier based on applying Bayes' theorem with strong independence assumptions between the features.", "node_type": "major"}, {"id": "Bayesian Inference", "label": "Bayesian Inference", "title": "<b>Bayesian Inference</b> (subnode)<hr>Statistical method for updating the probability estimate for a hypothesis as more evidence or information becomes available.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Statistical method for updating the probability estimate for a hypothesis as more evidence or information becomes available.", "node_type": "subnode"}, {"id": "Parameter Estimation", "label": "Parameter Estimation", "title": "<b>Parameter Estimation</b> (subnode)<hr>Process of estimating parameters such as \u03c6\u208a|y=1 and \u03c6\u208a|y=0 from a training set.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Process of estimating parameters such as \u03c6\u208a|y=1 and \u03c6\u208a|y=0 from a training set.", "node_type": "subnode"}, {"id": "Binary Features", "label": "Binary Features", "title": "<b>Binary Features</b> (subnode)<hr>Features are binary-valued in the basic formulation of Naive Bayes.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Features are binary-valued in the basic formulation of Naive Bayes.", "node_type": "subnode"}, {"id": "Multinomial Features", "label": "Multinomial Features", "title": "<b>Multinomial Features</b> (subnode)<hr>Generalization to features with multiple discrete values.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Generalization to features with multiple discrete values.", "node_type": "subnode"}, {"id": "Spam Classification Example", "label": "Spam Classification Example", "title": "<b>Spam Classification Example</b> (subnode)<hr>Illustrative example of applying Laplace smoothing in email spam detection.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Illustrative example of applying Laplace smoothing in email spam detection.", "node_type": "subnode"}, {"id": "Machine Learning Conferences", "label": "Machine Learning Conferences", "title": "<b>Machine Learning Conferences</b> (major)<hr>Top machine learning conferences including NeurIPS.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Top machine learning conferences including NeurIPS.", "node_type": "major"}, {"id": "NeurIPS Conference", "label": "NeurIPS Conference", "title": "<b>NeurIPS Conference</b> (subnode)<hr>One of the top machine learning conferences with a submission deadline in May-June.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "One of the top machine learning conferences with a submission deadline in May-June.", "node_type": "subnode"}, {"id": "Naive Bayes Spam Filter", "label": "Naive Bayes Spam Filter", "title": "<b>Naive Bayes Spam Filter</b> (major)<hr>A probabilistic classifier used for spam detection based on Bayesian probability theory.", "shape": "star", "size": 25, "color": "#FF6347", "description": "A probabilistic classifier used for spam detection based on Bayesian probability theory.", "node_type": "major"}, {"id": "Zero Frequency Problem", "label": "Zero Frequency Problem", "title": "<b>Zero Frequency Problem</b> (subnode)<hr>Issue where the probability of an unseen event is estimated as zero based on finite training data.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Issue where the probability of an unseen event is estimated as zero based on finite training data.", "node_type": "subnode"}, {"id": "Probability Estimation", "label": "Probability Estimation", "title": "<b>Probability Estimation</b> (major)<hr>Discusses the statistical issues with estimating probabilities as zero.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Discusses the statistical issues with estimating probabilities as zero.", "node_type": "major"}, {"id": "Multinomial Random Variable", "label": "Multinomial Random Variable", "title": "<b>Multinomial Random Variable</b> (subnode)<hr>A random variable taking values in a finite set of outcomes.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "A random variable taking values in a finite set of outcomes.", "node_type": "subnode"}, {"id": "Maximum Likelihood Estimates", "label": "Maximum Likelihood Estimates", "title": "<b>Maximum Likelihood Estimates</b> (subnode)<hr>Estimates for parameters based on observed data, which can result in zero probabilities.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Estimates for parameters based on observed data, which can result in zero probabilities.", "node_type": "subnode"}, {"id": "Bernoulli Event Model", "label": "Bernoulli Event Model", "title": "<b>Bernoulli Event Model</b> (subnode)<hr>Model for text classification assuming binary presence or absence of words.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Model for text classification assuming binary presence or absence of words.", "node_type": "subnode"}, {"id": "Multinomial Event Model", "label": "Multinomial Event Model", "title": "<b>Multinomial Event Model</b> (subnode)<hr>Model for generating emails where each word is chosen independently from a multinomial distribution based on spam/non-spam classification", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Model for generating emails where each word is chosen independently from a multinomial distribution based on spam/non-spam classification", "node_type": "subnode"}, {"id": "Spam/Non-Spam Classification", "label": "Spam/Non-Spam Classification", "title": "<b>Spam/Non-Spam Classification</b> (subnode)<hr>Determining whether an email is spam or not before generating its content", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Determining whether an email is spam or not before generating its content", "node_type": "subnode"}, {"id": "Word Generation Process", "label": "Word Generation Process", "title": "<b>Word Generation Process</b> (subnode)<hr>Process of independently selecting each word from a multinomial distribution over words given the classification", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Process of independently selecting each word from a multinomial distribution over words given the classification", "node_type": "subnode"}, {"id": "Probability Calculation", "label": "Probability Calculation", "title": "<b>Probability Calculation</b> (subnode)<hr>Calculating overall probability of an email as product of probabilities for each word and spam/non-spam classification", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Calculating overall probability of an email as product of probabilities for each word and spam/non-spam classification", "node_type": "subnode"}, {"id": "Feature Map", "label": "Feature Map", "title": "<b>Feature Map</b> (subnode)<hr>Transformation from attributes to features.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Transformation from attributes to features.", "node_type": "subnode"}, {"id": "Linear Function Over Features", "label": "Linear Function Over Features", "title": "<b>Linear Function Over Features</b> (subnode)<hr>Rewriting a cubic function as a linear combination of feature variables.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Rewriting a cubic function as a linear combination of feature variables.", "node_type": "subnode"}, {"id": "Attributes", "label": "Attributes", "title": "<b>Attributes</b> (subnode)<hr>Original input values in machine learning problems.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Original input values in machine learning problems.", "node_type": "subnode"}, {"id": "Features Variables", "label": "Features Variables", "title": "<b>Features Variables</b> (subnode)<hr>New set of quantities derived from attributes using a feature map.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "New set of quantities derived from attributes using a feature map.", "node_type": "subnode"}, {"id": "Cubic Function Representation", "label": "Cubic Function Representation", "title": "<b>Cubic Function Representation</b> (subnode)<hr>Expressing cubic functions as linear combinations in higher dimensions.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Expressing cubic functions as linear combinations in higher dimensions.", "node_type": "subnode"}, {"id": "Gradient Descent Update Rule", "label": "Gradient Descent Update Rule", "title": "<b>Gradient Descent Update Rule</b> (subnode)<hr>Update rule for gradient descent in the context of high-dimensional features.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Update rule for gradient descent in the context of high-dimensional features.", "node_type": "subnode"}, {"id": "Feature Mapping", "label": "Feature Mapping", "title": "<b>Feature Mapping</b> (subnode)<hr>Transformation of input data into a higher-dimensional feature space for better model fitting.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Transformation of input data into a higher-dimensional feature space for better model fitting.", "node_type": "subnode"}, {"id": "Kernel Trick", "label": "Kernel Trick", "title": "<b>Kernel Trick</b> (subnode)<hr>Technique used in SVMs to handle non-linearly separable data by transforming it into a higher-dimensional space.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Technique used in SVMs to handle non-linearly separable data by transforming it into a higher-dimensional space.", "node_type": "subnode"}, {"id": "Computational Complexity", "label": "Computational Complexity", "title": "<b>Computational Complexity</b> (subnode)<hr>Discussion on computational challenges with high-dimensional feature mappings.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Discussion on computational challenges with high-dimensional feature mappings.", "node_type": "subnode"}, {"id": "Iterative Update Process", "label": "Iterative Update Process", "title": "<b>Iterative Update Process</b> (subnode)<hr>Process for updating coefficients in feature mapping iteratively.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Process for updating coefficients in feature mapping iteratively.", "node_type": "subnode"}, {"id": "Linear Combination Representation", "label": "Linear Combination Representation", "title": "<b>Linear Combination Representation</b> (subnode)<hr>Representation of the vector theta as a linear combination of transformed input vectors.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Representation of the vector theta as a linear combination of transformed input vectors.", "node_type": "subnode"}, {"id": "Feature Maps and Kernels", "label": "Feature Maps and Kernels", "title": "<b>Feature Maps and Kernels</b> (subnode)<hr>Discussion on feature maps and their corresponding kernel functions.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Discussion on feature maps and their corresponding kernel functions.", "node_type": "subnode"}, {"id": "Kernel Function Definition", "label": "Kernel Function Definition", "title": "<b>Kernel Function Definition</b> (subnode)<hr>Definition of the kernel function based on inner products.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Definition of the kernel function based on inner products.", "node_type": "subnode"}, {"id": "Inner Product Computation", "label": "Inner Product Computation", "title": "<b>Inner Product Computation</b> (subnode)<hr>Efficient computation of inner products between feature vectors in high-dimensional space.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Efficient computation of inner products between feature vectors in high-dimensional space.", "node_type": "subnode"}, {"id": "Algorithm Implementation", "label": "Algorithm Implementation", "title": "<b>Algorithm Implementation</b> (subnode)<hr>Steps to implement the algorithm using kernel functions.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Steps to implement the algorithm using kernel functions.", "node_type": "subnode"}, {"id": "Beta Update Equation", "label": "Beta Update Equation", "title": "<b>Beta Update Equation</b> (subnode)<hr>Equation describing how \beta_i is updated in each iteration.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Equation describing how \beta_i is updated in each iteration.", "node_type": "subnode"}, {"id": "Feature Map Phi", "label": "Feature Map Phi", "title": "<b>Feature Map Phi</b> (subnode)<hr>Mapping function that transforms input data into a higher-dimensional space.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Mapping function that transforms input data into a higher-dimensional space.", "node_type": "subnode"}, {"id": "Pre-computation Strategy", "label": "Pre-computation Strategy", "title": "<b>Pre-computation Strategy</b> (subnode)<hr>Technique to pre-calculate all pairwise inner products before the main loop starts.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Technique to pre-calculate all pairwise inner products before the main loop starts.", "node_type": "subnode"}, {"id": "Efficient Inner Product Calculation", "label": "Efficient Inner Product Calculation", "title": "<b>Efficient Inner Product Calculation</b> (subnode)<hr>Method for computing inner products without explicitly calculating feature maps.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Method for computing inner products without explicitly calculating feature maps.", "node_type": "subnode"}, {"id": "Kernels in Machine Learning", "label": "Kernels in Machine Learning", "title": "<b>Kernels in Machine Learning</b> (subnode)<hr>Introduction to kernel functions and their properties.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Introduction to kernel functions and their properties.", "node_type": "subnode"}, {"id": "Kernel Functions", "label": "Kernel Functions", "title": "<b>Kernel Functions</b> (subnode)<hr>Description and properties of kernel functions, their relation to feature maps.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Description and properties of kernel functions, their relation to feature maps.", "node_type": "subnode"}, {"id": "Explicit Definition of Kernels", "label": "Explicit Definition of Kernels", "title": "<b>Explicit Definition of Kernels</b> (subnode)<hr>How kernels can be defined explicitly or implicitly through feature maps.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "How kernels can be defined explicitly or implicitly through feature maps.", "node_type": "subnode"}, {"id": "Characterization of Valid Kernels", "label": "Characterization of Valid Kernels", "title": "<b>Characterization of Valid Kernels</b> (subnode)<hr>Criteria for determining if a function is a valid kernel corresponding to some feature map.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Criteria for determining if a function is a valid kernel corresponding to some feature map.", "node_type": "subnode"}, {"id": "Computational Efficiency", "label": "Computational Efficiency", "title": "<b>Computational Efficiency</b> (subnode)<hr>Efficiency of calculating kernel functions compared to direct computation in high-dimensional space.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Efficiency of calculating kernel functions compared to direct computation in high-dimensional space.", "node_type": "subnode"}, {"id": "Polynomial Kernels", "label": "Polynomial Kernels", "title": "<b>Polynomial Kernels</b> (subnode)<hr>Kernels that map data into polynomial feature spaces.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Kernels that map data into polynomial feature spaces.", "node_type": "subnode"}, {"id": "Kernels as Similarity Metrics", "label": "Kernels as Similarity Metrics", "title": "<b>Kernels as Similarity Metrics</b> (subnode)<hr>Using kernels to measure similarity between inputs.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Using kernels to measure similarity between inputs.", "node_type": "subnode"}, {"id": "Gaussian Kernel", "label": "Gaussian Kernel", "title": "<b>Gaussian Kernel</b> (subnode)<hr>A specific kernel function that measures similarity in an infinite dimensional space.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "A specific kernel function that measures similarity in an infinite dimensional space.", "node_type": "subnode"}, {"id": "Valid Kernels Conditions", "label": "Valid Kernels Conditions", "title": "<b>Valid Kernels Conditions</b> (subnode)<hr>Criteria for a function to be considered a valid kernel.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Criteria for a function to be considered a valid kernel.", "node_type": "subnode"}, {"id": "Feature Extraction for Strings", "label": "Feature Extraction for Strings", "title": "<b>Feature Extraction for Strings</b> (subnode)<hr>Techniques to extract features from variable-length string data.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Techniques to extract features from variable-length string data.", "node_type": "subnode"}, {"id": "Support Vector Machines (SVM)", "label": "Support Vector Machines (SVM)", "title": "<b>Support Vector Machines (SVM)</b> (subnode)<hr>A supervised learning model for classification and regression analysis.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "A supervised learning model for classification and regression analysis.", "node_type": "subnode"}, {"id": "Kernel Matrix Properties", "label": "Kernel Matrix Properties", "title": "<b>Kernel Matrix Properties</b> (major)<hr>Properties of the kernel matrix in machine learning.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Properties of the kernel matrix in machine learning.", "node_type": "major"}, {"id": "Sufficient Conditions for Valid Kernels", "label": "Sufficient Conditions for Valid Kernels", "title": "<b>Sufficient Conditions for Valid Kernels</b> (subnode)<hr>Conditions that are both necessary and sufficient for a function to be a valid kernel.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Conditions that are both necessary and sufficient for a function to be a valid kernel.", "node_type": "subnode"}, {"id": "Mercer's Theorem", "label": "Mercer's Theorem", "title": "<b>Mercer's Theorem</b> (subnode)<hr>Theorem stating necessary and sufficient conditions for a kernel to be valid.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Theorem stating necessary and sufficient conditions for a kernel to be valid.", "node_type": "subnode"}, {"id": "Testing Kernel Validity", "label": "Testing Kernel Validity", "title": "<b>Testing Kernel Validity</b> (subnode)<hr>Methods to test if a given function is a valid kernel.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Methods to test if a given function is a valid kernel.", "node_type": "subnode"}, {"id": "Examples of Kernels in Practice", "label": "Examples of Kernels in Practice", "title": "<b>Examples of Kernels in Practice</b> (major)<hr>Practical applications and examples of kernels in machine learning problems.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Practical applications and examples of kernels in machine learning problems.", "node_type": "major"}, {"id": "Necessary Conditions for Valid Kernels", "label": "Necessary Conditions for Valid Kernels", "title": "<b>Necessary Conditions for Valid Kernels</b> (subnode)<hr>Conditions a kernel must meet to correspond to some feature mapping.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Conditions a kernel must meet to correspond to some feature mapping.", "node_type": "subnode"}, {"id": "Symmetry Property", "label": "Symmetry Property", "title": "<b>Symmetry Property</b> (subnode)<hr>A valid kernel matrix is symmetric.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "A valid kernel matrix is symmetric.", "node_type": "subnode"}, {"id": "Positive Semi-Definiteness", "label": "Positive Semi-Definiteness", "title": "<b>Positive Semi-Definiteness</b> (subnode)<hr>A valid kernel matrix must be positive semi-definite.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "A valid kernel matrix must be positive semi-definite.", "node_type": "subnode"}, {"id": "Kernel Matrix", "label": "Kernel Matrix", "title": "<b>Kernel Matrix</b> (subnode)<hr>Matrix representation of a kernel function over a set of points.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Matrix representation of a kernel function over a set of points.", "node_type": "subnode"}, {"id": "Functional Margins", "label": "Functional Margins", "title": "<b>Functional Margins</b> (subnode)<hr>Measure of confidence for predictions based on distance from decision boundary.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Measure of confidence for predictions based on distance from decision boundary.", "node_type": "subnode"}, {"id": "Geometric Margins", "label": "Geometric Margins", "title": "<b>Geometric Margins</b> (subnode)<hr>Concept explaining the geometric interpretation of margins in machine learning models.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Concept explaining the geometric interpretation of margins in machine learning models.", "node_type": "subnode"}, {"id": "Support Vector Machines (SVMs)", "label": "Support Vector Machines (SVMs)", "title": "<b>Support Vector Machines (SVMs)</b> (subnode)<hr>A type of machine learning model used for classification and regression analysis.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "A type of machine learning model used for classification and regression analysis.", "node_type": "subnode"}, {"id": "Notation for SVMs", "label": "Notation for SVMs", "title": "<b>Notation for SVMs</b> (subnode)<hr>Introduction of notation using w and b parameters for linear classifiers.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Introduction of notation using w and b parameters for linear classifiers.", "node_type": "subnode"}, {"id": "Functional Margin", "label": "Functional Margin", "title": "<b>Functional Margin</b> (subnode)<hr>Measure indicating the confidence and correctness of predictions.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Measure indicating the confidence and correctness of predictions.", "node_type": "subnode"}, {"id": "Geometric Margin", "label": "Geometric Margin", "title": "<b>Geometric Margin</b> (subnode)<hr>Smallest margin between the decision boundary and closest data points in training set.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Smallest margin between the decision boundary and closest data points in training set.", "node_type": "subnode"}, {"id": "Confidence Measure Limitation", "label": "Confidence Measure Limitation", "title": "<b>Confidence Measure Limitation</b> (subnode)<hr>Explains why functional margin is not a reliable measure of confidence.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Explains why functional margin is not a reliable measure of confidence.", "node_type": "subnode"}, {"id": "Normalization Condition", "label": "Normalization Condition", "title": "<b>Normalization Condition</b> (subnode)<hr>Proposed normalization to improve the reliability of the functional margin.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Proposed normalization to improve the reliability of the functional margin.", "node_type": "subnode"}, {"id": "Function Margin with Training Set", "label": "Function Margin with Training Set", "title": "<b>Function Margin with Training Set</b> (subnode)<hr>Definition and calculation of function margin for a set of training examples.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Definition and calculation of function margin for a set of training examples.", "node_type": "subnode"}, {"id": "Margins", "label": "Margins", "title": "<b>Margins</b> (subnode)<hr>Concept of margins in SVMs to maximize the distance between classes.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Concept of margins in SVMs to maximize the distance between classes.", "node_type": "subnode"}, {"id": "Optimal Margin Classifier", "label": "Optimal Margin Classifier", "title": "<b>Optimal Margin Classifier</b> (subnode)<hr>Classifier that maximizes geometric margin to achieve confident predictions on linearly separable data sets.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Classifier that maximizes geometric margin to achieve confident predictions on linearly separable data sets.", "node_type": "subnode"}, {"id": "Lagrange Duality", "label": "Lagrange Duality", "title": "<b>Lagrange Duality</b> (subnode)<hr>Theory for transforming constrained optimization problems into dual form.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Theory for transforming constrained optimization problems into dual form.", "node_type": "subnode"}, {"id": "Kernels", "label": "Kernels", "title": "<b>Kernels</b> (subnode)<hr>Technique for handling non-linearly separable data in high-dimensional spaces.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Technique for handling non-linearly separable data in high-dimensional spaces.", "node_type": "subnode"}, {"id": "SMO Algorithm", "label": "SMO Algorithm", "title": "<b>SMO Algorithm</b> (subnode)<hr>Algorithm that updates two Lagrange multipliers simultaneously to solve the dual optimization problem.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Algorithm that updates two Lagrange multipliers simultaneously to solve the dual optimization problem.", "node_type": "subnode"}, {"id": "Vector w", "label": "Vector w", "title": "<b>Vector w</b> (subnode)<hr>Orthogonal to the decision boundary and points towards positive class.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Orthogonal to the decision boundary and points towards positive class.", "node_type": "subnode"}, {"id": "Distance to Decision Boundary", "label": "Distance to Decision Boundary", "title": "<b>Distance to Decision Boundary</b> (subnode)<hr>Calculated using vector projection and hyperplane equation.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Calculated using vector projection and hyperplane equation.", "node_type": "subnode"}, {"id": "Unit Vector w/||w||", "label": "Unit Vector w/||w||", "title": "<b>Unit Vector w/||w||</b> (subnode)<hr>Normalized version of vector w pointing in the same direction.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Normalized version of vector w pointing in the same direction.", "node_type": "subnode"}, {"id": "Model Parameters (w, b)", "label": "Model Parameters (w, b)", "title": "<b>Model Parameters (w, b)</b> (subnode)<hr>Parameters used to define the decision boundary in a linear classifier.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Parameters used to define the decision boundary in a linear classifier.", "node_type": "subnode"}, {"id": "Scaling Constraint on w", "label": "Scaling Constraint on w", "title": "<b>Scaling Constraint on w</b> (subnode)<hr>Arbitrary scaling constraints that can be imposed on parameter w without changing model behavior.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Arbitrary scaling constraints that can be imposed on parameter w without changing model behavior.", "node_type": "subnode"}, {"id": "Maximizing Geometric Margin", "label": "Maximizing Geometric Margin", "title": "<b>Maximizing Geometric Margin</b> (subnode)<hr>Process of finding decision boundary with maximum margin between positive and negative examples.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Process of finding decision boundary with maximum margin between positive and negative examples.", "node_type": "subnode"}, {"id": "Support Vector Machine (SVM)", "label": "Support Vector Machine (SVM)", "title": "<b>Support Vector Machine (SVM)</b> (major)<hr>Binary classification model maximizing margin between classes.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Binary classification model maximizing margin between classes.", "node_type": "major"}, {"id": "Non-Convex Constraint", "label": "Non-Convex Constraint", "title": "<b>Non-Convex Constraint</b> (subnode)<hr>Constraint that complicates optimization problem.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Constraint that complicates optimization problem.", "node_type": "subnode"}, {"id": "Scaling Constraint", "label": "Scaling Constraint", "title": "<b>Scaling Constraint</b> (subnode)<hr>Constraint to simplify the objective function.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Constraint to simplify the objective function.", "node_type": "subnode"}, {"id": "Objective Function Transformation", "label": "Objective Function Transformation", "title": "<b>Objective Function Transformation</b> (subnode)<hr>Transformation of the original problem into a convex one.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Transformation of the original problem into a convex one.", "node_type": "subnode"}, {"id": "Constrained Optimization", "label": "Constrained Optimization", "title": "<b>Constrained Optimization</b> (major)<hr>Optimization problems with equality and inequality constraints.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Optimization problems with equality and inequality constraints.", "node_type": "major"}, {"id": "Lagrange Multipliers", "label": "Lagrange Multipliers", "title": "<b>Lagrange Multipliers</b> (subnode)<hr>Variables used in the SMO algorithm to optimize the SVM's objective function.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Variables used in the SMO algorithm to optimize the SVM's objective function.", "node_type": "subnode"}, {"id": "Generalized Lagrangian", "label": "Generalized Lagrangian", "title": "<b>Generalized Lagrangian</b> (subnode)<hr>Function combining the objective and constraint functions with multipliers.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Function combining the objective and constraint functions with multipliers.", "node_type": "subnode"}, {"id": "Primal Problem", "label": "Primal Problem", "title": "<b>Primal Problem</b> (subnode)<hr>Optimization problem defined by maximizing over alpha and beta while minimizing over w.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Optimization problem defined by maximizing over alpha and beta while minimizing over w.", "node_type": "subnode"}, {"id": "\\(\\theta_{\\cal P}(w)\\)", "label": "\\(\\theta_{\\cal P}(w)\\)", "title": "<b>\\(\\theta_{\\cal P}(w)\\)</b> (subnode)<hr>Maximum value of the generalized Lagrangian for given \\(w\\).", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Maximum value of the generalized Lagrangian for given \\(w\\).", "node_type": "subnode"}, {"id": "Convex Quadratic Objective", "label": "Convex Quadratic Objective", "title": "<b>Convex Quadratic Objective</b> (subnode)<hr>Objective function in the form of a convex quadratic equation.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Objective function in the form of a convex quadratic equation.", "node_type": "subnode"}, {"id": "Linear Constraints", "label": "Linear Constraints", "title": "<b>Linear Constraints</b> (subnode)<hr>Constraints that are linear equations or inequalities.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Constraints that are linear equations or inequalities.", "node_type": "subnode"}, {"id": "Dual Formulation", "label": "Dual Formulation", "title": "<b>Dual Formulation</b> (subnode)<hr>Alternative formulation of the original problem that simplifies computation.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Alternative formulation of the original problem that simplifies computation.", "node_type": "subnode"}, {"id": "Optimization Problems", "label": "Optimization Problems", "title": "<b>Optimization Problems</b> (subnode)<hr>Formulation and solution of optimization problems in ML.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Formulation and solution of optimization problems in ML.", "node_type": "subnode"}, {"id": "KKT Conditions", "label": "KKT Conditions", "title": "<b>KKT Conditions</b> (subnode)<hr>Conditions ensuring optimality in constrained optimization problems", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Conditions ensuring optimality in constrained optimization problems", "node_type": "subnode"}, {"id": "Dual Complementarity Condition", "label": "Dual Complementarity Condition", "title": "<b>Dual Complementarity Condition</b> (subnode)<hr>Condition indicating active constraints in optimization problems.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Condition indicating active constraints in optimization problems.", "node_type": "subnode"}, {"id": "Support Vectors", "label": "Support Vectors", "title": "<b>Support Vectors</b> (subnode)<hr>Points that lie on the decision boundary and influence the optimal solution.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Points that lie on the decision boundary and influence the optimal solution.", "node_type": "subnode"}, {"id": "SVM Optimization Problem", "label": "SVM Optimization Problem", "title": "<b>SVM Optimization Problem</b> (subnode)<hr>Primal and dual forms of SVM optimization problem.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Primal and dual forms of SVM optimization problem.", "node_type": "subnode"}, {"id": "Dual Problem", "label": "Dual Problem", "title": "<b>Dual Problem</b> (major)<hr>Optimization problem where the order of max and min operations is reversed compared to the primal problem.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Optimization problem where the order of max and min operations is reversed compared to the primal problem.", "node_type": "major"}, {"id": "Objective Function Primal", "label": "Objective Function Primal", "title": "<b>Objective Function Primal</b> (subnode)<hr>Function \u03b8\u211aP(w) that represents the objective in the primal problem.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Function \u03b8\u211aP(w) that represents the objective in the primal problem.", "node_type": "subnode"}, {"id": "Objective Function Dual", "label": "Objective Function Dual", "title": "<b>Objective Function Dual</b> (subnode)<hr>Function \u03b8\u211aD(\u03b1,\u03b2) representing the dual optimization's objective.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Function \u03b8\u211aD(\u03b1,\u03b2) representing the dual optimization's objective.", "node_type": "subnode"}, {"id": "Primal Constraints", "label": "Primal Constraints", "title": "<b>Primal Constraints</b> (subnode)<hr>Constraints that w must satisfy in the primal problem.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Constraints that w must satisfy in the primal problem.", "node_type": "subnode"}, {"id": "Dual Constraints", "label": "Dual Constraints", "title": "<b>Dual Constraints</b> (subnode)<hr>Non-negativity constraints on \u03b1 for the dual problem.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Non-negativity constraints on \u03b1 for the dual problem.", "node_type": "subnode"}, {"id": "Lagrangian Function", "label": "Lagrangian Function", "title": "<b>Lagrangian Function</b> (major)<hr>Function used to derive dual problem in SVMs with L1 regularization.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Function used to derive dual problem in SVMs with L1 regularization.", "node_type": "major"}, {"id": "Value Primal Problem", "label": "Value Primal Problem", "title": "<b>Value Primal Problem</b> (subnode)<hr>Optimal value p* of the objective function for the primal problem.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Optimal value p* of the objective function for the primal problem.", "node_type": "subnode"}, {"id": "Value Dual Problem", "label": "Value Dual Problem", "title": "<b>Value Dual Problem</b> (subnode)<hr>Optimal value d* of the dual problem's objective function.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Optimal value d* of the dual problem's objective function.", "node_type": "subnode"}, {"id": "Machine Learning Theory", "label": "Machine Learning Theory", "title": "<b>Machine Learning Theory</b> (major)<hr>Theoretical foundations of machine learning including generalization and hypothesis classes.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Theoretical foundations of machine learning including generalization and hypothesis classes.", "node_type": "major"}, {"id": "Duality Gap", "label": "Duality Gap", "title": "<b>Duality Gap</b> (subnode)<hr>Difference between primal and dual problem solutions.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Difference between primal and dual problem solutions.", "node_type": "subnode"}, {"id": "Convex Functions", "label": "Convex Functions", "title": "<b>Convex Functions</b> (subnode)<hr>Functions where the line segment between any two points on the graph lies above or on the function.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Functions where the line segment between any two points on the graph lies above or on the function.", "node_type": "subnode"}, {"id": "Affine Constraints", "label": "Affine Constraints", "title": "<b>Affine Constraints</b> (subnode)<hr>Linear constraints that can be shifted by a constant.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Linear constraints that can be shifted by a constant.", "node_type": "subnode"}, {"id": "Feasibility Conditions", "label": "Feasibility Conditions", "title": "<b>Feasibility Conditions</b> (subnode)<hr>Conditions ensuring the existence of solutions satisfying all constraints.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Conditions ensuring the existence of solutions satisfying all constraints.", "node_type": "subnode"}, {"id": "Karush-Kuhn-Tucker (KKT) Conditions", "label": "Karush-Kuhn-Tucker (KKT) Conditions", "title": "<b>Karush-Kuhn-Tucker (KKT) Conditions</b> (subnode)<hr>Necessary conditions for a solution in nonlinear programming to be optimal.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Necessary conditions for a solution in nonlinear programming to be optimal.", "node_type": "subnode"}, {"id": "Dual Form of Problem", "label": "Dual Form of Problem", "title": "<b>Dual Form of Problem</b> (major)<hr>Formulation focusing on Lagrange multipliers to solve optimization problem.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Formulation focusing on Lagrange multipliers to solve optimization problem.", "node_type": "major"}, {"id": "Inner Products", "label": "Inner Products", "title": "<b>Inner Products</b> (subnode)<hr>Key to the kernel trick, expressed as (x^(i))^T x^(j).", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Key to the kernel trick, expressed as (x^(i))^T x^(j).", "node_type": "subnode"}, {"id": "Optimization Constraints", "label": "Optimization Constraints", "title": "<b>Optimization Constraints</b> (subnode)<hr>Constraints on optimization variables in machine learning models.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Constraints on optimization variables in machine learning models.", "node_type": "subnode"}, {"id": "Support Vectors Definition", "label": "Support Vectors Definition", "title": "<b>Support Vectors Definition</b> (subnode)<hr>Points with non-zero alpha values that define the decision boundary.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Points with non-zero alpha values that define the decision boundary.", "node_type": "subnode"}, {"id": "Lagrangian Optimization", "label": "Lagrangian Optimization", "title": "<b>Lagrangian Optimization</b> (major)<hr>Optimization involving Lagrangian function for SVMs", "shape": "star", "size": 25, "color": "#FF6347", "description": "Optimization involving Lagrangian function for SVMs", "node_type": "major"}, {"id": "Dual Problem Formulation", "label": "Dual Problem Formulation", "title": "<b>Dual Problem Formulation</b> (subnode)<hr>Formulating the dual problem from primal constraints and objective", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Formulating the dual problem from primal constraints and objective", "node_type": "subnode"}, {"id": "Optimal Parameters Alpha", "label": "Optimal Parameters Alpha", "title": "<b>Optimal Parameters Alpha</b> (subnode)<hr>Finding optimal alpha values to maximize the dual objective function", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Finding optimal alpha values to maximize the dual objective function", "node_type": "subnode"}, {"id": "Recovering w from Alpha", "label": "Recovering w from Alpha", "title": "<b>Recovering w from Alpha</b> (subnode)<hr>Using optimal alphas to find the optimal weight vector w", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Using optimal alphas to find the optimal weight vector w", "node_type": "subnode"}, {"id": "Optimal Intercept b", "label": "Optimal Intercept b", "title": "<b>Optimal Intercept b</b> (subnode)<hr>Calculating the intercept term using the primal problem constraints", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Calculating the intercept term using the primal problem constraints", "node_type": "subnode"}, {"id": "Optimal Parameters Calculation", "label": "Optimal Parameters Calculation", "title": "<b>Optimal Parameters Calculation</b> (subnode)<hr>Calculation of optimal parameters w and b from training data.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Calculation of optimal parameters w and b from training data.", "node_type": "subnode"}, {"id": "Intercept Term Calculation", "label": "Intercept Term Calculation", "title": "<b>Intercept Term Calculation</b> (subnode)<hr>Finding the intercept term b using the primal problem solution.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Finding the intercept term b using the primal problem solution.", "node_type": "subnode"}, {"id": "Prediction Equation", "label": "Prediction Equation", "title": "<b>Prediction Equation</b> (subnode)<hr>Equation for prediction based on inner product and support vectors.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Equation for prediction based on inner product and support vectors.", "node_type": "subnode"}, {"id": "Dual Form Insight", "label": "Dual Form Insight", "title": "<b>Dual Form Insight</b> (subnode)<hr>Insight gained from the dual form of optimization problem.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Insight gained from the dual form of optimization problem.", "node_type": "subnode"}, {"id": "Kernel Application", "label": "Kernel Application", "title": "<b>Kernel Application</b> (subnode)<hr>Application of kernels to classification problems in SVMs.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Application of kernels to classification problems in SVMs.", "node_type": "subnode"}, {"id": "Non-separable Case", "label": "Non-separable Case", "title": "<b>Non-separable Case</b> (subnode)<hr>Handling datasets that are not linearly separable with SVMs.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Handling datasets that are not linearly separable with SVMs.", "node_type": "subnode"}, {"id": "L1 Regularization", "label": "L1 Regularization", "title": "<b>L1 Regularization</b> (subnode)<hr>Penalizes the sum of absolute values of coefficients to handle outliers.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Penalizes the sum of absolute values of coefficients to handle outliers.", "node_type": "subnode"}, {"id": "Dual Formulation of SVM", "label": "Dual Formulation of SVM", "title": "<b>Dual Formulation of SVM</b> (subnode)<hr>Optimization problem reformulated to solve for Lagrange multipliers.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Optimization problem reformulated to solve for Lagrange multipliers.", "node_type": "subnode"}, {"id": "Sequential Minimal Optimization (SMO) Algorithm", "label": "Sequential Minimal Optimization (SMO) Algorithm", "title": "<b>Sequential Minimal Optimization (SMO) Algorithm</b> (subnode)<hr>Efficient algorithm to solve the dual problem of SVM optimization.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Efficient algorithm to solve the dual problem of SVM optimization.", "node_type": "subnode"}, {"id": "Coordinate Ascent Algorithm", "label": "Coordinate Ascent Algorithm", "title": "<b>Coordinate Ascent Algorithm</b> (subnode)<hr>Optimization technique for solving unconstrained problems by iteratively optimizing one variable at a time.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Optimization technique for solving unconstrained problems by iteratively optimizing one variable at a time.", "node_type": "subnode"}, {"id": "Machine Learning Optimization Techniques", "label": "Machine Learning Optimization Techniques", "title": "<b>Machine Learning Optimization Techniques</b> (major)<hr>Techniques for optimizing functions in machine learning problems.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Techniques for optimizing functions in machine learning problems.", "node_type": "major"}, {"id": "Quadratic Function Contours", "label": "Quadratic Function Contours", "title": "<b>Quadratic Function Contours</b> (subnode)<hr>Visual representation of the contours of a quadratic function being optimized.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Visual representation of the contours of a quadratic function being optimized.", "node_type": "subnode"}, {"id": "Dual Optimization Problem", "label": "Dual Optimization Problem", "title": "<b>Dual Optimization Problem</b> (subnode)<hr>The optimization problem in the dual form for SVMs.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "The optimization problem in the dual form for SVMs.", "node_type": "subnode"}, {"id": "Sequential Minimal Optimization (SMO)", "label": "Sequential Minimal Optimization (SMO)", "title": "<b>Sequential Minimal Optimization (SMO)</b> (subnode)<hr>Algorithm for solving the optimization problem in SVM efficiently.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Algorithm for solving the optimization problem in SVM efficiently.", "node_type": "subnode"}, {"id": "Convergence Criteria", "label": "Convergence Criteria", "title": "<b>Convergence Criteria</b> (subnode)<hr>Conditions to determine if SMO has reached a solution.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Conditions to determine if SMO has reached a solution.", "node_type": "subnode"}, {"id": "Efficient Update Mechanism", "label": "Efficient Update Mechanism", "title": "<b>Efficient Update Mechanism</b> (subnode)<hr>Method to update alpha values efficiently in SMO.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Method to update alpha values efficiently in SMO.", "node_type": "subnode"}, {"id": "Constraints Handling", "label": "Constraints Handling", "title": "<b>Constraints Handling</b> (subnode)<hr>Management of constraints during the optimization process.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Management of constraints during the optimization process.", "node_type": "subnode"}, {"id": "Alpha Variables", "label": "Alpha Variables", "title": "<b>Alpha Variables</b> (subnode)<hr>Variables \u03b11 and \u03b22 used to define constraints.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Variables \u03b11 and \u03b22 used to define constraints.", "node_type": "subnode"}, {"id": "Quadratic Function", "label": "Quadratic Function", "title": "<b>Quadratic Function</b> (subnode)<hr>Function representing the objective in terms of quadratic form.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Function representing the objective in terms of quadratic form.", "node_type": "subnode"}, {"id": "Box Constraint", "label": "Box Constraint", "title": "<b>Box Constraint</b> (subnode)<hr>Constraints defining permissible values for \u03b12 within a specified range [L, H].", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Constraints defining permissible values for \u03b12 within a specified range [L, H].", "node_type": "subnode"}, {"id": "Derivation Example", "label": "Derivation Example", "title": "<b>Derivation Example</b> (subnode)<hr>Example of deriving \u03b11 as a function of \u03b12 and y^(2).", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Example of deriving \u03b11 as a function of \u03b12 and y^(2).", "node_type": "subnode"}, {"id": "Machine Learning Overview", "label": "Machine Learning Overview", "title": "<b>Machine Learning Overview</b> (major)<hr>General introduction to machine learning concepts and algorithms.", "shape": "star", "size": 25, "color": "#FF6347", "description": "General introduction to machine learning concepts and algorithms.", "node_type": "major"}, {"id": "Alpha Value Update", "label": "Alpha Value Update", "title": "<b>Alpha Value Update</b> (subnode)<hr>Process of updating alpha values within SMO constraints.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Process of updating alpha values within SMO constraints.", "node_type": "subnode"}, {"id": "Deep Learning Introduction", "label": "Deep Learning Introduction", "title": "<b>Deep Learning Introduction</b> (major)<hr>Introduction to deep learning concepts and neural networks.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Introduction to deep learning concepts and neural networks.", "node_type": "major"}, {"id": "Supervised Learning with Non-Linear Models", "label": "Supervised Learning with Non-Linear Models", "title": "<b>Supervised Learning with Non-Linear Models</b> (subnode)<hr>Exploration of non-linear models in supervised learning context.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Exploration of non-linear models in supervised learning context.", "node_type": "subnode"}, {"id": "Non-linear Model h_\u03b8(x)", "label": "Non-linear Model h_\u03b8(x)", "title": "<b>Non-linear Model h_\u03b8(x)</b> (subnode)<hr>An abstract non-linear model for regression and classification tasks.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "An abstract non-linear model for regression and classification tasks.", "node_type": "subnode"}, {"id": "Training Examples", "label": "Training Examples", "title": "<b>Training Examples</b> (subnode)<hr>Set of training data used to learn the model parameters.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Set of training data used to learn the model parameters.", "node_type": "subnode"}, {"id": "Regression Problems", "label": "Regression Problems", "title": "<b>Regression Problems</b> (subnode)<hr>Problems where output is a real number, using least square cost function.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Problems where output is a real number, using least square cost function.", "node_type": "subnode"}, {"id": "Mean-Square Cost Function", "label": "Mean-Square Cost Function", "title": "<b>Mean-Square Cost Function</b> (subnode)<hr>Cost function for regression tasks, averaged over all training examples.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Cost function for regression tasks, averaged over all training examples.", "node_type": "subnode"}, {"id": "Average Loss", "label": "Average Loss", "title": "<b>Average Loss</b> (subnode)<hr>Total loss divided by number of examples.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Total loss divided by number of examples.", "node_type": "subnode"}, {"id": "Conditional Probabilistic Models", "label": "Conditional Probabilistic Models", "title": "<b>Conditional Probabilistic Models</b> (subnode)<hr>Models where output distribution depends on input features.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Models where output distribution depends on input features.", "node_type": "subnode"}, {"id": "Optimizers", "label": "Optimizers", "title": "<b>Optimizers</b> (major)<hr>Algorithms for minimizing loss functions.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Algorithms for minimizing loss functions.", "node_type": "major"}, {"id": "Gradient Descent (GD)", "label": "Gradient Descent (GD)", "title": "<b>Gradient Descent (GD)</b> (subnode)<hr>Iterative optimization algorithm using gradients of the function.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Iterative optimization algorithm using gradients of the function.", "node_type": "subnode"}, {"id": "Stochastic Gradient Descent (SGD)", "label": "Stochastic Gradient Descent (SGD)", "title": "<b>Stochastic Gradient Descent (SGD)</b> (subnode)<hr>Variant of GD that uses a single example for each update.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Variant of GD that uses a single example for each update.", "node_type": "subnode"}, {"id": "Learning Rate", "label": "Learning Rate", "title": "<b>Learning Rate</b> (subnode)<hr>Hyperparameter controlling the size of steps in gradient descent.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Hyperparameter controlling the size of steps in gradient descent.", "node_type": "subnode"}, {"id": "Negative Likelihood Loss Function", "label": "Negative Likelihood Loss Function", "title": "<b>Negative Likelihood Loss Function</b> (subnode)<hr>Loss function derived from negative log-likelihood for binary classification.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Loss function derived from negative log-likelihood for binary classification.", "node_type": "subnode"}, {"id": "Total Loss Function", "label": "Total Loss Function", "title": "<b>Total Loss Function</b> (subnode)<hr>Average of loss functions over individual training examples.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Average of loss functions over individual training examples.", "node_type": "subnode"}, {"id": "Logits in Multi-class", "label": "Logits in Multi-class", "title": "<b>Logits in Multi-class</b> (subnode)<hr>Output of the model before applying softmax function, representing predictions for each class.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Output of the model before applying softmax function, representing predictions for each class.", "node_type": "subnode"}, {"id": "Negative Log-likelihood Loss Function (Multi-class)", "label": "Negative Log-likelihood Loss Function (Multi-class)", "title": "<b>Negative Log-likelihood Loss Function (Multi-class)</b> (subnode)<hr>Loss function derived from negative log-likelihood in multi-class classification.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Loss function derived from negative log-likelihood in multi-class classification.", "node_type": "subnode"}, {"id": "Single Neuron Network", "label": "Single Neuron Network", "title": "<b>Single Neuron Network</b> (subnode)<hr>A basic network with a single neuron for simple predictions.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "A basic network with a single neuron for simple predictions.", "node_type": "subnode"}, {"id": "Housing Price Prediction", "label": "Housing Price Prediction", "title": "<b>Housing Price Prediction</b> (subnode)<hr>Predicting housing prices based on derived features such as family size, walkability, and school quality.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Predicting housing prices based on derived features such as family size, walkability, and school quality.", "node_type": "subnode"}, {"id": "ReLU Function", "label": "ReLU Function", "title": "<b>ReLU Function</b> (subnode)<hr>Element-wise non-linear transformation using the Rectified Linear Unit (ReLU) function.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Element-wise non-linear transformation using the Rectified Linear Unit (ReLU) function.", "node_type": "subnode"}, {"id": "Activation Functions", "label": "Activation Functions", "title": "<b>Activation Functions</b> (subnode)<hr>Functions that introduce non-linearity in the network, such as ReLU.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Functions that introduce non-linearity in the network, such as ReLU.", "node_type": "subnode"}, {"id": "Single Neuron Model", "label": "Single Neuron Model", "title": "<b>Single Neuron Model</b> (subnode)<hr>Model with a single neuron including weight vector, bias term, and activation function.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Model with a single neuron including weight vector, bias term, and activation function.", "node_type": "subnode"}, {"id": "Stacking Neurons", "label": "Stacking Neurons", "title": "<b>Stacking Neurons</b> (subnode)<hr>Process of combining multiple neurons to form complex neural networks.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Process of combining multiple neurons to form complex neural networks.", "node_type": "subnode"}, {"id": "Housing Prediction Example", "label": "Housing Prediction Example", "title": "<b>Housing Prediction Example</b> (subnode)<hr>Illustration using housing price prediction with multiple features and layers.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Illustration using housing price prediction with multiple features and layers.", "node_type": "subnode"}, {"id": "Mini-batch SGD", "label": "Mini-batch SGD", "title": "<b>Mini-batch SGD</b> (subnode)<hr>Variant of SGD that uses small batches of data for updates.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Variant of SGD that uses small batches of data for updates.", "node_type": "subnode"}, {"id": "Hyperparameters", "label": "Hyperparameters", "title": "<b>Hyperparameters</b> (subnode)<hr>Learning rate and number of iterations in the algorithm.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Learning rate and number of iterations in the algorithm.", "node_type": "subnode"}, {"id": "Mini-batch Hyperparameters", "label": "Mini-batch Hyperparameters", "title": "<b>Mini-batch Hyperparameters</b> (subnode)<hr>Includes learning rate, batch size, and number of iterations.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Includes learning rate, batch size, and number of iterations.", "node_type": "subnode"}, {"id": "Parameter Initialization", "label": "Parameter Initialization", "title": "<b>Parameter Initialization</b> (subnode)<hr>Random initialization of parameters before training starts.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Random initialization of parameters before training starts.", "node_type": "subnode"}, {"id": "Mini-batch Gradient Calculation", "label": "Mini-batch Gradient Calculation", "title": "<b>Mini-batch Gradient Calculation</b> (subnode)<hr>Gradient calculation using multiple examples simultaneously.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Gradient calculation using multiple examples simultaneously.", "node_type": "subnode"}, {"id": "Deep Learning Model Training Steps", "label": "Deep Learning Model Training Steps", "title": "<b>Deep Learning Model Training Steps</b> (subnode)<hr>Steps to train a deep learning model including parametrization and backpropagation.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Steps to train a deep learning model including parametrization and backpropagation.", "node_type": "subnode"}, {"id": "Neural Networks Overview", "label": "Neural Networks Overview", "title": "<b>Neural Networks Overview</b> (major)<hr>Introduction to neural networks as non-linear models for regression and classification problems.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Introduction to neural networks as non-linear models for regression and classification problems.", "node_type": "major"}, {"id": "Parameters (\u03b8)", "label": "Parameters (\u03b8)", "title": "<b>Parameters (\u03b8)</b> (subnode)<hr>Set of parameters that define the model's behavior and are learned during training.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Set of parameters that define the model's behavior and are learned during training.", "node_type": "subnode"}, {"id": "Biological Inspiration", "label": "Biological Inspiration", "title": "<b>Biological Inspiration</b> (subnode)<hr>Comparison between artificial neural networks and biological neural systems.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Comparison between artificial neural networks and biological neural systems.", "node_type": "subnode"}, {"id": "Two-Layer Neural Network", "label": "Two-Layer Neural Network", "title": "<b>Two-Layer Neural Network</b> (subnode)<hr>A simple model with two layers for predicting housing prices using ReLU activation function.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "A simple model with two layers for predicting housing prices using ReLU activation function.", "node_type": "subnode"}, {"id": "Derived Features", "label": "Derived Features", "title": "<b>Derived Features</b> (subnode)<hr>Features like family size, walkable neighborhood, and school quality that influence house price.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Features like family size, walkable neighborhood, and school quality that influence house price.", "node_type": "subnode"}, {"id": "Family Size", "label": "Family Size", "title": "<b>Family Size</b> (subnode)<hr>Size of the household based on house dimensions and number of bedrooms.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Size of the household based on house dimensions and number of bedrooms.", "node_type": "subnode"}, {"id": "Walkability", "label": "Walkability", "title": "<b>Walkability</b> (subnode)<hr>Measure of how easily one can walk to amenities like grocery stores in a neighborhood.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Measure of how easily one can walk to amenities like grocery stores in a neighborhood.", "node_type": "subnode"}, {"id": "School Quality", "label": "School Quality", "title": "<b>School Quality</b> (subnode)<hr>Quality of local elementary schools based on neighborhood wealth and zip code.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Quality of local elementary schools based on neighborhood wealth and zip code.", "node_type": "subnode"}, {"id": "Neural Network Inputs", "label": "Neural Network Inputs", "title": "<b>Neural Network Inputs</b> (subnode)<hr>Input features for a neural network, such as house dimensions and number of bedrooms.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Input features for a neural network, such as house dimensions and number of bedrooms.", "node_type": "subnode"}, {"id": "Hidden Units", "label": "Hidden Units", "title": "<b>Hidden Units</b> (subnode)<hr>Intermediate variables (hidden units) in the neural network that process input features.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Intermediate variables (hidden units) in the neural network that process input features.", "node_type": "subnode"}, {"id": "ReLU Activation Function", "label": "ReLU Activation Function", "title": "<b>ReLU Activation Function</b> (subnode)<hr>Rectified Linear Unit activation function used for hidden layers to introduce non-linearity.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Rectified Linear Unit activation function used for hidden layers to introduce non-linearity.", "node_type": "subnode"}, {"id": "Output Layer", "label": "Output Layer", "title": "<b>Output Layer</b> (subnode)<hr>Final layer of the neural network that produces the output based on processed input features.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Final layer of the neural network that produces the output based on processed input features.", "node_type": "subnode"}, {"id": "Vectorization in Neural Networks", "label": "Vectorization in Neural Networks", "title": "<b>Vectorization in Neural Networks</b> (major)<hr>Process of converting loops into matrix operations for efficiency.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Process of converting loops into matrix operations for efficiency.", "node_type": "major"}, {"id": "Matrix Algebra", "label": "Matrix Algebra", "title": "<b>Matrix Algebra</b> (subnode)<hr>Use of matrices and vectors to represent neural network computations.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Use of matrices and vectors to represent neural network computations.", "node_type": "subnode"}, {"id": "BLAS Packages", "label": "BLAS Packages", "title": "<b>BLAS Packages</b> (subnode)<hr>Highly optimized numerical linear algebra libraries for fast computation.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Highly optimized numerical linear algebra libraries for fast computation.", "node_type": "subnode"}, {"id": "Two-Layer Fully-Connected Network", "label": "Two-Layer Fully-Connected Network", "title": "<b>Two-Layer Fully-Connected Network</b> (subnode)<hr>Example of a neural network structure used to illustrate vectorization concepts.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Example of a neural network structure used to illustrate vectorization concepts.", "node_type": "subnode"}, {"id": "Weight Matrix W^[1]", "label": "Weight Matrix W^[1]", "title": "<b>Weight Matrix W^[1]</b> (subnode)<hr>Matrix representation of weights connecting input layer to hidden layer.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Matrix representation of weights connecting input layer to hidden layer.", "node_type": "subnode"}, {"id": "Fully-Connected Neural Networks", "label": "Fully-Connected Neural Networks", "title": "<b>Fully-Connected Neural Networks</b> (subnode)<hr>A type of neural network where each neuron is connected to every neuron in the previous layer.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "A type of neural network where each neuron is connected to every neuron in the previous layer.", "node_type": "subnode"}, {"id": "Intermediate Variables (a_i)", "label": "Intermediate Variables (a_i)", "title": "<b>Intermediate Variables (a_i)</b> (subnode)<hr>Variables that depend on all inputs and are used for computation within layers.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Variables that depend on all inputs and are used for computation within layers.", "node_type": "subnode"}, {"id": "Vectorization", "label": "Vectorization", "title": "<b>Vectorization</b> (subnode)<hr>Process of using matrix and vector notations to simplify expressions and improve computational efficiency.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Process of using matrix and vector notations to simplify expressions and improve computational efficiency.", "node_type": "subnode"}, {"id": "Weight Matrices", "label": "Weight Matrices", "title": "<b>Weight Matrices</b> (subnode)<hr>Explanation of weight matrices in the context of layers W^[1] and W^[2].", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Explanation of weight matrices in the context of layers W^[1] and W^[2].", "node_type": "subnode"}, {"id": "Bias Vectors", "label": "Bias Vectors", "title": "<b>Bias Vectors</b> (subnode)<hr>Description of bias vectors b^[1] and b^[2] used in each layer.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Description of bias vectors b^[1] and b^[2] used in each layer.", "node_type": "subnode"}, {"id": "Hidden Layer", "label": "Hidden Layer", "title": "<b>Hidden Layer</b> (subnode)<hr>The activation a as the hidden layer output from ReLU function.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "The activation a as the hidden layer output from ReLU function.", "node_type": "subnode"}, {"id": "Multi-layer Networks", "label": "Multi-layer Networks", "title": "<b>Multi-layer Networks</b> (subnode)<hr>Introduction to multi-layer fully-connected neural networks with more than two layers.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Introduction to multi-layer fully-connected neural networks with more than two layers.", "node_type": "subnode"}, {"id": "Multi-layer Fully-Connected Neural Networks", "label": "Multi-layer Fully-Connected Neural Networks", "title": "<b>Multi-layer Fully-Connected Neural Networks</b> (major)<hr>Stacking layers to form deeper neural networks with ReLU activation functions.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Stacking layers to form deeper neural networks with ReLU activation functions.", "node_type": "major"}, {"id": "Weight Matrices and Biases", "label": "Weight Matrices and Biases", "title": "<b>Weight Matrices and Biases</b> (subnode)<hr>Dimensions of weight matrices and biases for compatibility in multi-layer networks.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Dimensions of weight matrices and biases for compatibility in multi-layer networks.", "node_type": "subnode"}, {"id": "Total Number of Neurons", "label": "Total Number of Neurons", "title": "<b>Total Number of Neurons</b> (subnode)<hr>Summation of neurons across all layers in the network.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Summation of neurons across all layers in the network.", "node_type": "subnode"}, {"id": "Total Number of Parameters", "label": "Total Number of Parameters", "title": "<b>Total Number of Parameters</b> (subnode)<hr>Calculation of total parameters including weights and biases.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Calculation of total parameters including weights and biases.", "node_type": "subnode"}, {"id": "Notational Consistency", "label": "Notational Consistency", "title": "<b>Notational Consistency</b> (subnode)<hr>Using consistent notation for input and output layers.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Using consistent notation for input and output layers.", "node_type": "subnode"}, {"id": "Other Activation Functions", "label": "Other Activation Functions", "title": "<b>Other Activation Functions</b> (major)<hr>Alternative non-linear functions to ReLU in neural networks.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Alternative non-linear functions to ReLU in neural networks.", "node_type": "major"}, {"id": "Feature Engineering", "label": "Feature Engineering", "title": "<b>Feature Engineering</b> (subnode)<hr>Process of selecting and transforming raw data into features that improve model performance.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Process of selecting and transforming raw data into features that improve model performance.", "node_type": "subnode"}, {"id": "Neural Networks Parameters", "label": "Neural Networks Parameters", "title": "<b>Neural Networks Parameters</b> (subnode)<hr>Parameters in neural networks, including weights and biases.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Parameters in neural networks, including weights and biases.", "node_type": "subnode"}, {"id": "Learned Features", "label": "Learned Features", "title": "<b>Learned Features</b> (subnode)<hr>Features automatically discovered by deep learning models without explicit feature engineering.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Features automatically discovered by deep learning models without explicit feature engineering.", "node_type": "subnode"}, {"id": "Sigmoid Function", "label": "Sigmoid Function", "title": "<b>Sigmoid Function</b> (subnode)<hr>Maps real numbers to (0, 1) range; less commonly used due to vanishing gradient problem.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Maps real numbers to (0, 1) range; less commonly used due to vanishing gradient problem.", "node_type": "subnode"}, {"id": "Tanh Function", "label": "Tanh Function", "title": "<b>Tanh Function</b> (subnode)<hr>Similar to sigmoid but maps to (-1, 1); also suffers from vanishing gradients.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Similar to sigmoid but maps to (-1, 1); also suffers from vanishing gradients.", "node_type": "subnode"}, {"id": "Leaky ReLU", "label": "Leaky ReLU", "title": "<b>Leaky ReLU</b> (subnode)<hr>Variant of ReLU with a small gradient for negative inputs.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Variant of ReLU with a small gradient for negative inputs.", "node_type": "subnode"}, {"id": "GELU Function", "label": "GELU Function", "title": "<b>GELU Function</b> (subnode)<hr>Gaussian Error Linear Unit; used in advanced NLP models like BERT and GPT.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Gaussian Error Linear Unit; used in advanced NLP models like BERT and GPT.", "node_type": "subnode"}, {"id": "Softplus Function", "label": "Softplus Function", "title": "<b>Softplus Function</b> (subnode)<hr>Smoothed ReLU with a proper second-order derivative but less practical use.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Smoothed ReLU with a proper second-order derivative but less practical use.", "node_type": "subnode"}, {"id": "Identity Function", "label": "Identity Function", "title": "<b>Identity Function</b> (subnode)<hr>Function that outputs the input directly; not used due to lack of non-linearity.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Function that outputs the input directly; not used due to lack of non-linearity.", "node_type": "subnode"}, {"id": "Deep Learning Representations", "label": "Deep Learning Representations", "title": "<b>Deep Learning Representations</b> (major)<hr>Discusses how neural networks automatically discover useful features for prediction.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Discusses how neural networks automatically discover useful features for prediction.", "node_type": "major"}, {"id": "House Price Prediction Example", "label": "House Price Prediction Example", "title": "<b>House Price Prediction Example</b> (subnode)<hr>Illustrates the use of fully-connected neural networks in predicting house prices without specifying intermediate quantities.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Illustrates the use of fully-connected neural networks in predicting house prices without specifying intermediate quantities.", "node_type": "subnode"}, {"id": "Feature Maps and Representation Transferability", "label": "Feature Maps and Representation Transferability", "title": "<b>Feature Maps and Representation Transferability</b> (subnode)<hr>Explains how feature maps from one dataset can be useful for other datasets, indicating essential data information.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Explains how feature maps from one dataset can be useful for other datasets, indicating essential data information.", "node_type": "subnode"}, {"id": "Complex Features in Neural Networks", "label": "Complex Features in Neural Networks", "title": "<b>Complex Features in Neural Networks</b> (subnode)<hr>Discusses the difficulty of human interpretation of complex features discovered by neural networks.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Discusses the difficulty of human interpretation of complex features discovered by neural networks.", "node_type": "subnode"}, {"id": "Matrix Multiplication as a Building Block", "label": "Matrix Multiplication as a Building Block", "title": "<b>Matrix Multiplication as a Building Block</b> (subnode)<hr>Describes matrix multiplication operation with parameters W and b, operating on an input z.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Describes matrix multiplication operation with parameters W and b, operating on an input z.", "node_type": "subnode"}, {"id": "MLP Composition of Modules", "label": "MLP Composition of Modules", "title": "<b>MLP Composition of Modules</b> (subnode)<hr>Explains how MLP can be written as a composition of multiple matrix multiplication modules and nonlinear activation modules.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Explains how MLP can be written as a composition of multiple matrix multiplication modules and nonlinear activation modules.", "node_type": "subnode"}, {"id": "Layer Normalization", "label": "Layer Normalization", "title": "<b>Layer Normalization</b> (major)<hr>Normalization technique that normalizes the inputs in each training example independently.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Normalization technique that normalizes the inputs in each training example independently.", "node_type": "major"}, {"id": "LN-S(z)", "label": "LN-S(z)", "title": "<b>LN-S(z)</b> (subnode)<hr>Standardized output of layer normalization before affine transformation.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Standardized output of layer normalization before affine transformation.", "node_type": "subnode"}, {"id": "Affine Transformation", "label": "Affine Transformation", "title": "<b>Affine Transformation</b> (subnode)<hr>Transformation that scales and shifts the standardized output to desired mean and standard deviation.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Transformation that scales and shifts the standardized output to desired mean and standard deviation.", "node_type": "subnode"}, {"id": "Scaling-Invariant Property", "label": "Scaling-Invariant Property", "title": "<b>Scaling-Invariant Property</b> (subnode)<hr>Property ensuring model invariance under scaling of parameters in subsequent layers.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Property ensuring model invariance under scaling of parameters in subsequent layers.", "node_type": "subnode"}, {"id": "MLP (Multi-Layer Perceptron)", "label": "MLP (Multi-Layer Perceptron)", "title": "<b>MLP (Multi-Layer Perceptron)</b> (subnode)<hr>A neural network model composed of multiple layers with matrix multiplication and activation functions.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "A neural network model composed of multiple layers with matrix multiplication and activation functions.", "node_type": "subnode"}, {"id": "Matrix Multiplication Module", "label": "Matrix Multiplication Module", "title": "<b>Matrix Multiplication Module</b> (subnode)<hr>Basic building block in MLP, involves linear transformation using weights and biases.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Basic building block in MLP, involves linear transformation using weights and biases.", "node_type": "subnode"}, {"id": "Nonlinear Activation Module", "label": "Nonlinear Activation Module", "title": "<b>Nonlinear Activation Module</b> (subnode)<hr>Applies a nonlinear function to the output of matrix multiplication modules.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Applies a nonlinear function to the output of matrix multiplication modules.", "node_type": "subnode"}, {"id": "ResNet (Residual Network)", "label": "ResNet (Residual Network)", "title": "<b>ResNet (Residual Network)</b> (subnode)<hr>Neural network architecture that uses residual blocks for improved training of deep networks.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Neural network architecture that uses residual blocks for improved training of deep networks.", "node_type": "subnode"}, {"id": "Residual Block", "label": "Residual Block", "title": "<b>Residual Block</b> (subnode)<hr>Building block in ResNet, adds input directly to output after nonlinear transformations.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Building block in ResNet, adds input directly to output after nonlinear transformations.", "node_type": "subnode"}, {"id": "Simplified ResNet Architecture", "label": "Simplified ResNet Architecture", "title": "<b>Simplified ResNet Architecture</b> (subnode)<hr>A composition of residual blocks followed by a matrix multiplication layer.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "A composition of residual blocks followed by a matrix multiplication layer.", "node_type": "subnode"}, {"id": "Machine Learning Architectures", "label": "Machine Learning Architectures", "title": "<b>Machine Learning Architectures</b> (major)<hr>Overview of different architectures in machine learning.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Overview of different architectures in machine learning.", "node_type": "major"}, {"id": "ResNet Architecture", "label": "ResNet Architecture", "title": "<b>ResNet Architecture</b> (subnode)<hr>Deep residual network architecture using convolution layers and batch normalization.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Deep residual network architecture using convolution layers and batch normalization.", "node_type": "subnode"}, {"id": "Convolutional Layers", "label": "Convolutional Layers", "title": "<b>Convolutional Layers</b> (subnode)<hr>Layer type commonly used in deep learning for image and signal processing.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Layer type commonly used in deep learning for image and signal processing.", "node_type": "subnode"}, {"id": "Batch Normalization Variants", "label": "Batch Normalization Variants", "title": "<b>Batch Normalization Variants</b> (subnode)<hr>Different types of batch normalization techniques used in neural networks.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Different types of batch normalization techniques used in neural networks.", "node_type": "subnode"}, {"id": "Transformer Architecture", "label": "Transformer Architecture", "title": "<b>Transformer Architecture</b> (subnode)<hr>Architecture widely used in modern large language models.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Architecture widely used in modern large language models.", "node_type": "subnode"}, {"id": "Layer Normalization (LN)", "label": "Layer Normalization (LN)", "title": "<b>Layer Normalization (LN)</b> (subnode)<hr>Normalization technique applied to layers in neural networks.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Normalization technique applied to layers in neural networks.", "node_type": "subnode"}, {"id": "LN-S Module", "label": "LN-S Module", "title": "<b>LN-S Module</b> (subnode)<hr>Sub-module of layer normalization that normalizes vector to mean zero and standard deviation one.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Sub-module of layer normalization that normalizes vector to mean zero and standard deviation one.", "node_type": "subnode"}, {"id": "Affine Transformation in LN", "label": "Affine Transformation in LN", "title": "<b>Affine Transformation in LN</b> (subnode)<hr>Transformation using learnable parameters beta and gamma for desired mean and standard deviation.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Transformation using learnable parameters beta and gamma for desired mean and standard deviation.", "node_type": "subnode"}, {"id": "Parameter Sharing", "label": "Parameter Sharing", "title": "<b>Parameter Sharing</b> (subnode)<hr>Mechanism where the same filter is applied across different positions of input data.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Mechanism where the same filter is applied across different positions of input data.", "node_type": "subnode"}, {"id": "Efficiency Comparison", "label": "Efficiency Comparison", "title": "<b>Efficiency Comparison</b> (subnode)<hr>Comparison between convolution and generic matrix multiplication in terms of computational efficiency.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Comparison between convolution and generic matrix multiplication in terms of computational efficiency.", "node_type": "subnode"}, {"id": "Channel Concept", "label": "Channel Concept", "title": "<b>Channel Concept</b> (subnode)<hr>Concept describing multiple input/output dimensions within a layer.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Concept describing multiple input/output dimensions within a layer.", "node_type": "subnode"}, {"id": "Convolutional Neural Networks (CNN)", "label": "Convolutional Neural Networks (CNN)", "title": "<b>Convolutional Neural Networks (CNN)</b> (subnode)<hr>Type of neural network used for image and signal processing.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Type of neural network used for image and signal processing.", "node_type": "subnode"}, {"id": "1-D Convolution Layer", "label": "1-D Convolution Layer", "title": "<b>1-D Convolution Layer</b> (subnode)<hr>Simplified version of 1-dimensional convolution layer used in CNNs.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Simplified version of 1-dimensional convolution layer used in CNNs.", "node_type": "subnode"}, {"id": "Filter Vector", "label": "Filter Vector", "title": "<b>Filter Vector</b> (subnode)<hr>Vector of weights used to extract features from input data.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Vector of weights used to extract features from input data.", "node_type": "subnode"}, {"id": "Bias Scalar", "label": "Bias Scalar", "title": "<b>Bias Scalar</b> (subnode)<hr>Scalar value added to the output of each neuron in the layer.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Scalar value added to the output of each neuron in the layer.", "node_type": "subnode"}, {"id": "Matrix Multiplication with Shared Parameters", "label": "Matrix Multiplication with Shared Parameters", "title": "<b>Matrix Multiplication with Shared Parameters</b> (subnode)<hr>Operation where convolution is represented as a matrix multiplication with shared parameters.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Operation where convolution is represented as a matrix multiplication with shared parameters.", "node_type": "subnode"}, {"id": "Conv1D-S Module", "label": "Conv1D-S Module", "title": "<b>Conv1D-S Module</b> (subnode)<hr>A 1-dimensional convolutional module with specific parameters.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "A 1-dimensional convolutional module with specific parameters.", "node_type": "subnode"}, {"id": "Total Parameters Conv1D", "label": "Total Parameters Conv1D", "title": "<b>Total Parameters Conv1D</b> (subnode)<hr>Calculation of total number of parameters in a Conv1D layer.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Calculation of total number of parameters in a Conv1D layer.", "node_type": "subnode"}, {"id": "2-D Convolution (Conv2D-S)", "label": "2-D Convolution (Conv2D-S)", "title": "<b>2-D Convolution (Conv2D-S)</b> (subnode)<hr>A 2-dimensional convolutional module with input and output matrices.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "A 2-dimensional convolutional module with input and output matrices.", "node_type": "subnode"}, {"id": "Total Parameters Conv2D", "label": "Total Parameters Conv2D", "title": "<b>Total Parameters Conv2D</b> (subnode)<hr>Calculation of total number of parameters in a Conv2D layer.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Calculation of total number of parameters in a Conv2D layer.", "node_type": "subnode"}, {"id": "Scale-Invariant Property", "label": "Scale-Invariant Property", "title": "<b>Scale-Invariant Property</b> (subnode)<hr>Property of modern DL architectures regarding weight scaling.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Property of modern DL architectures regarding weight scaling.", "node_type": "subnode"}, {"id": "Other Normalization Layers", "label": "Other Normalization Layers", "title": "<b>Other Normalization Layers</b> (subnode)<hr>Alternative normalization techniques used in neural networks.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Alternative normalization techniques used in neural networks.", "node_type": "subnode"}, {"id": "Batch Normalization", "label": "Batch Normalization", "title": "<b>Batch Normalization</b> (subnode)<hr>Normalization technique commonly used in computer vision applications.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Normalization technique commonly used in computer vision applications.", "node_type": "subnode"}, {"id": "Group Normalization", "label": "Group Normalization", "title": "<b>Group Normalization</b> (subnode)<hr>Normalization method for groups of channels in neural networks.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Normalization method for groups of channels in neural networks.", "node_type": "subnode"}, {"id": "Convolutional Neural Networks (CNNs)", "label": "Convolutional Neural Networks (CNNs)", "title": "<b>Convolutional Neural Networks (CNNs)</b> (subnode)<hr>Neural network architecture designed for image and sequence data processing.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Neural network architecture designed for image and sequence data processing.", "node_type": "subnode"}, {"id": "Differentiable Circuit", "label": "Differentiable Circuit", "title": "<b>Differentiable Circuit</b> (subnode)<hr>Composition of arithmetic operations and elementary functions in a network.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Composition of arithmetic operations and elementary functions in a network.", "node_type": "subnode"}, {"id": "Gradient Computation", "label": "Gradient Computation", "title": "<b>Gradient Computation</b> (subnode)<hr>Process of calculating gradients with respect to the loss function and network parameters during backpropagation.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Process of calculating gradients with respect to the loss function and network parameters during backpropagation.", "node_type": "subnode"}, {"id": "Machine Learning Fundamentals", "label": "Machine Learning Fundamentals", "title": "<b>Machine Learning Fundamentals</b> (major)<hr>Basic concepts and principles of machine learning.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Basic concepts and principles of machine learning.", "node_type": "major"}, {"id": "Backward Function in Machine Learning", "label": "Backward Function in Machine Learning", "title": "<b>Backward Function in Machine Learning</b> (subnode)<hr>Explanation of the backward function used to compute gradients.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Explanation of the backward function used to compute gradients.", "node_type": "subnode"}, {"id": "Jacobian Matrix", "label": "Jacobian Matrix", "title": "<b>Jacobian Matrix</b> (subnode)<hr>Matrix representation of partial derivatives, not fully detailed here.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Matrix representation of partial derivatives, not fully detailed here.", "node_type": "subnode"}, {"id": "Chain Rule Application", "label": "Chain Rule Application", "title": "<b>Chain Rule Application</b> (subnode)<hr>Use of the chain rule to compute gradients through intermediate variables.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Use of the chain rule to compute gradients through intermediate variables.", "node_type": "subnode"}, {"id": "Partial Derivatives in ML", "label": "Partial Derivatives in ML", "title": "<b>Partial Derivatives in ML</b> (subnode)<hr>Understanding partial derivatives in the context of machine learning functions.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Understanding partial derivatives in the context of machine learning functions.", "node_type": "subnode"}, {"id": "Chain Rule for Auto-Differentiation", "label": "Chain Rule for Auto-Differentiation", "title": "<b>Chain Rule for Auto-Differentiation</b> (subnode)<hr>Application of chain rule to compute gradients efficiently in neural networks.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Application of chain rule to compute gradients efficiently in neural networks.", "node_type": "subnode"}, {"id": "Scalar Functions and Vectors", "label": "Scalar Functions and Vectors", "title": "<b>Scalar Functions and Vectors</b> (subnode)<hr>Focus on derivatives involving scalar functions with respect to vectors or matrices.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Focus on derivatives involving scalar functions with respect to vectors or matrices.", "node_type": "subnode"}, {"id": "Multi-Variate Function Challenges", "label": "Multi-Variate Function Challenges", "title": "<b>Multi-Variate Function Challenges</b> (subnode)<hr>Challenges and computational difficulties of dealing with multi-variate function derivatives.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Challenges and computational difficulties of dealing with multi-variate function derivatives.", "node_type": "subnode"}, {"id": "Chain Rule", "label": "Chain Rule", "title": "<b>Chain Rule</b> (subnode)<hr>Mathematical rule for computing derivatives of composite functions.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Mathematical rule for computing derivatives of composite functions.", "node_type": "subnode"}, {"id": "Loss Function Composition", "label": "Loss Function Composition", "title": "<b>Loss Function Composition</b> (subnode)<hr>Abstract representation of loss functions as compositions of modules.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Abstract representation of loss functions as compositions of modules.", "node_type": "subnode"}, {"id": "Auto-Differentiation", "label": "Auto-Differentiation", "title": "<b>Auto-Differentiation</b> (subnode)<hr>Automatic computation of gradients in deep learning frameworks.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Automatic computation of gradients in deep learning frameworks.", "node_type": "subnode"}, {"id": "Deep Learning Packages", "label": "Deep Learning Packages", "title": "<b>Deep Learning Packages</b> (subnode)<hr>Software libraries for implementing neural networks and machine learning models.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Software libraries for implementing neural networks and machine learning models.", "node_type": "subnode"}, {"id": "Backpropagation Algorithm", "label": "Backpropagation Algorithm", "title": "<b>Backpropagation Algorithm</b> (subnode)<hr>Algorithm for computing gradients in neural networks.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Algorithm for computing gradients in neural networks.", "node_type": "subnode"}, {"id": "Efficiency of Backward Functions", "label": "Efficiency of Backward Functions", "title": "<b>Efficiency of Backward Functions</b> (subnode)<hr>Efficient computation of backward functions for atomic modules.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Efficient computation of backward functions for atomic modules.", "node_type": "subnode"}, {"id": "Backward Functions", "label": "Backward Functions", "title": "<b>Backward Functions</b> (subnode)<hr>Computation of backward functions for modules used in networks.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Computation of backward functions for modules used in networks.", "node_type": "subnode"}, {"id": "Matrix Multiplication Module (MM)", "label": "Matrix Multiplication Module (MM)", "title": "<b>Matrix Multiplication Module (MM)</b> (subnode)<hr>Calculation of backward function using equation 7.62 and 7.63.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Calculation of backward function using equation 7.62 and 7.63.", "node_type": "subnode"}, {"id": "Activations", "label": "Activations", "title": "<b>Activations</b> (subnode)<hr>Computation of backward functions for activation modules.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Computation of backward functions for activation modules.", "node_type": "subnode"}, {"id": "Backward Function Overview", "label": "Backward Function Overview", "title": "<b>Backward Function Overview</b> (major)<hr>Overview of backward functions in machine learning.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Overview of backward functions in machine learning.", "node_type": "major"}, {"id": "Matrix Multiplication Backward Function", "label": "Matrix Multiplication Backward Function", "title": "<b>Matrix Multiplication Backward Function</b> (subnode)<hr>Details on the backward function for matrix multiplication operations.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Details on the backward function for matrix multiplication operations.", "node_type": "subnode"}, {"id": "Vectorized Notation", "label": "Vectorized Notation", "title": "<b>Vectorized Notation</b> (subnode)<hr>Explanation of vectorized notation used in backward functions.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Explanation of vectorized notation used in backward functions.", "node_type": "subnode"}, {"id": "Efficiency Considerations", "label": "Efficiency Considerations", "title": "<b>Efficiency Considerations</b> (subnode)<hr>Discussion on computational efficiency for matrix multiplication operations.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Discussion on computational efficiency for matrix multiplication operations.", "node_type": "subnode"}, {"id": "Activation Functions Backward Function", "label": "Activation Functions Backward Function", "title": "<b>Activation Functions Backward Function</b> (subnode)<hr>Details on the backward function for activation functions in neural networks.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Details on the backward function for activation functions in neural networks.", "node_type": "subnode"}, {"id": "Binary Classification Problem", "label": "Binary Classification Problem", "title": "<b>Binary Classification Problem</b> (subnode)<hr>A specific type of classification problem with two classes", "shape": "dot", "size": 15, "color": "#4682B4", "description": "A specific type of classification problem with two classes", "node_type": "subnode"}, {"id": "MLP Model", "label": "MLP Model", "title": "<b>MLP Model</b> (subnode)<hr>Multi-layer perceptron model used in binary classification", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Multi-layer perceptron model used in binary classification", "node_type": "subnode"}, {"id": "Modules and Parameters", "label": "Modules and Parameters", "title": "<b>Modules and Parameters</b> (subnode)<hr>Description of modules involved in MLP with parameters or fixed operations", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Description of modules involved in MLP with parameters or fixed operations", "node_type": "subnode"}, {"id": "Intermediate Variables", "label": "Intermediate Variables", "title": "<b>Intermediate Variables</b> (subnode)<hr>Variables used during the computation process in a loss function", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Variables used during the computation process in a loss function", "node_type": "subnode"}, {"id": "Forward Pass", "label": "Forward Pass", "title": "<b>Forward Pass</b> (subnode)<hr>Process of computing intermediate variables sequentially", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Process of computing intermediate variables sequentially", "node_type": "subnode"}, {"id": "Backward Pass", "label": "Backward Pass", "title": "<b>Backward Pass</b> (subnode)<hr>Calculation of derivatives in reverse order to compute gradients", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Calculation of derivatives in reverse order to compute gradients", "node_type": "subnode"}, {"id": "Intermediate Values Storage", "label": "Intermediate Values Storage", "title": "<b>Intermediate Values Storage</b> (subnode)<hr>Storing values like $a^{[i]}$ and $z^{[i]}$ after forward pass.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Storing values like $a^{[i]}$ and $z^{[i]}$ after forward pass.", "node_type": "subnode"}, {"id": "Parallelism in Training Examples", "label": "Parallelism in Training Examples", "title": "<b>Parallelism in Training Examples</b> (major)<hr>Using matrix notation to handle multiple training examples simultaneously.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Using matrix notation to handle multiple training examples simultaneously.", "node_type": "major"}, {"id": "Basic Idea", "label": "Basic Idea", "title": "<b>Basic Idea</b> (subnode)<hr>Concept of evaluating forward and backward passes for multiple examples using matrices.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Concept of evaluating forward and backward passes for multiple examples using matrices.", "node_type": "subnode"}, {"id": "Backward Function for Loss Functions", "label": "Backward Function for Loss Functions", "title": "<b>Backward Function for Loss Functions</b> (subnode)<hr>Explains the backward function computation for different loss functions.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Explains the backward function computation for different loss functions.", "node_type": "subnode"}, {"id": "Squared Loss (MSE)", "label": "Squared Loss (MSE)", "title": "<b>Squared Loss (MSE)</b> (subnode)<hr>Details the backward function for squared loss or mean squared error.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Details the backward function for squared loss or mean squared error.", "node_type": "subnode"}, {"id": "Logistic Loss", "label": "Logistic Loss", "title": "<b>Logistic Loss</b> (subnode)<hr>Explains the backward pass computation for logistic loss.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Explains the backward pass computation for logistic loss.", "node_type": "subnode"}, {"id": "Machine Learning Loss Functions", "label": "Machine Learning Loss Functions", "title": "<b>Machine Learning Loss Functions</b> (major)<hr>Overview of loss functions used in machine learning models.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Overview of loss functions used in machine learning models.", "node_type": "major"}, {"id": "Cross-Entropy Loss Function", "label": "Cross-Entropy Loss Function", "title": "<b>Cross-Entropy Loss Function</b> (subnode)<hr>Loss function used in classification problems to measure the dissimilarity between predicted and actual probability distributions.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Loss function used in classification problems to measure the dissimilarity between predicted and actual probability distributions.", "node_type": "subnode"}, {"id": "Forward Pass in MLP", "label": "Forward Pass in MLP", "title": "<b>Forward Pass in MLP</b> (subnode)<hr>Sequence of operations that transforms input data into predictions using an MLP model.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Sequence of operations that transforms input data into predictions using an MLP model.", "node_type": "subnode"}, {"id": "Matrix Notation in Machine Learning", "label": "Matrix Notation in Machine Learning", "title": "<b>Matrix Notation in Machine Learning</b> (major)<hr>Overview of using matrix notation for machine learning operations.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Overview of using matrix notation for machine learning operations.", "node_type": "major"}, {"id": "Training Examples Representation", "label": "Training Examples Representation", "title": "<b>Training Examples Representation</b> (subnode)<hr>Representation of multiple training examples in matrix form.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Representation of multiple training examples in matrix form.", "node_type": "subnode"}, {"id": "First-Layer Activations", "label": "First-Layer Activations", "title": "<b>First-Layer Activations</b> (subnode)<hr>Calculation of first-layer activations for each example using matrix notation.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Calculation of first-layer activations for each example using matrix notation.", "node_type": "subnode"}, {"id": "Broadcasting", "label": "Broadcasting", "title": "<b>Broadcasting</b> (subnode)<hr>Technique used for adding a scalar or column vector to each column of a matrix.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Technique used for adding a scalar or column vector to each column of a matrix.", "node_type": "subnode"}, {"id": "Generalization to Multiple Layers", "label": "Generalization to Multiple Layers", "title": "<b>Generalization to Multiple Layers</b> (subnode)<hr>Extension of the matricization approach to multiple layers with implementation subtleties.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Extension of the matricization approach to multiple layers with implementation subtleties.", "node_type": "subnode"}, {"id": "Matricization Approach", "label": "Matricization Approach", "title": "<b>Matricization Approach</b> (subnode)<hr>Generalizing the approach to multiple layers with implementation subtleties.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Generalizing the approach to multiple layers with implementation subtleties.", "node_type": "subnode"}, {"id": "Implementation Subtlety", "label": "Implementation Subtlety", "title": "<b>Implementation Subtlety</b> (subnode)<hr>Handling data points as rows in deep learning packages vs columns in papers.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Handling data points as rows in deep learning packages vs columns in papers.", "node_type": "subnode"}, {"id": "Data Matrix Representation", "label": "Data Matrix Representation", "title": "<b>Data Matrix Representation</b> (subnode)<hr>Conversion between row and column vector representations for consistency.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Conversion between row and column vector representations for consistency.", "node_type": "subnode"}, {"id": "Generalization and Regularization", "label": "Generalization and Regularization", "title": "<b>Generalization and Regularization</b> (major)<hr>Analyzing model performance on unseen test data.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Analyzing model performance on unseen test data.", "node_type": "major"}, {"id": "Training Loss Function", "label": "Training Loss Function", "title": "<b>Training Loss Function</b> (subnode)<hr>Function used to fit the training dataset for supervised learning problems.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Function used to fit the training dataset for supervised learning problems.", "node_type": "subnode"}, {"id": "Training Loss", "label": "Training Loss", "title": "<b>Training Loss</b> (subnode)<hr>The loss calculated on the training dataset, also known as empirical loss or risk.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "The loss calculated on the training dataset, also known as empirical loss or risk.", "node_type": "subnode"}, {"id": "Test Error", "label": "Test Error", "title": "<b>Test Error</b> (subnode)<hr>Evaluation metric for a model's performance on unseen data, representing true generalization ability.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Evaluation metric for a model's performance on unseen data, representing true generalization ability.", "node_type": "subnode"}, {"id": "Mean Squared Error (MSE)", "label": "Mean Squared Error (MSE)", "title": "<b>Mean Squared Error (MSE)</b> (subnode)<hr>A specific loss function that measures the average squared difference between predicted and actual values.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "A specific loss function that measures the average squared difference between predicted and actual values.", "node_type": "subnode"}, {"id": "Empirical Distribution", "label": "Empirical Distribution", "title": "<b>Empirical Distribution</b> (subnode)<hr>The distribution of training data, used to approximate population distribution for empirical risk minimization.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "The distribution of training data, used to approximate population distribution for empirical risk minimization.", "node_type": "subnode"}, {"id": "Population Distribution", "label": "Population Distribution", "title": "<b>Population Distribution</b> (subnode)<hr>True underlying distribution from which test examples are drawn, representing the target generalization goal.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "True underlying distribution from which test examples are drawn, representing the target generalization goal.", "node_type": "subnode"}, {"id": "Training vs Test Distributions", "label": "Training vs Test Distributions", "title": "<b>Training vs Test Distributions</b> (subnode)<hr>Difference between training and test distributions and its impact on model performance.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Difference between training and test distributions and its impact on model performance.", "node_type": "subnode"}, {"id": "Generalization Gap", "label": "Generalization Gap", "title": "<b>Generalization Gap</b> (subnode)<hr>Difference between training error and test error indicating model's ability to generalize.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Difference between training error and test error indicating model's ability to generalize.", "node_type": "subnode"}, {"id": "Bias-Variance Tradeoff", "label": "Bias-Variance Tradeoff", "title": "<b>Bias-Variance Tradeoff</b> (major)<hr>Decomposition of test error into bias and variance components for understanding overfitting and underfitting.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Decomposition of test error into bias and variance components for understanding overfitting and underfitting.", "node_type": "major"}, {"id": "Double Descent Phenomenon", "label": "Double Descent Phenomenon", "title": "<b>Double Descent Phenomenon</b> (subnode)<hr>Phenomenon where test errors decrease, increase, then decrease again with increasing model parameters or data samples.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Phenomenon where test errors decrease, increase, then decrease again with increasing model parameters or data samples.", "node_type": "subnode"}, {"id": "Training and Test Datasets", "label": "Training and Test Datasets", "title": "<b>Training and Test Datasets</b> (subnode)<hr>Illustration of datasets used to evaluate model performance.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Illustration of datasets used to evaluate model performance.", "node_type": "subnode"}, {"id": "Linear Regression Models", "label": "Linear Regression Models", "title": "<b>Linear Regression Models</b> (subnode)<hr>Examples of linear models and their limitations in capturing non-linear relationships.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Examples of linear models and their limitations in capturing non-linear relationships.", "node_type": "subnode"}, {"id": "Quadratic Function Example", "label": "Quadratic Function Example", "title": "<b>Quadratic Function Example</b> (subnode)<hr>Example using a quadratic function to demonstrate bias-variance tradeoff concepts.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Example using a quadratic function to demonstrate bias-variance tradeoff concepts.", "node_type": "subnode"}, {"id": "Polynomial Fitting", "label": "Polynomial Fitting", "title": "<b>Polynomial Fitting</b> (subnode)<hr>Fitting polynomials to data sets and the issues that arise.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Fitting polynomials to data sets and the issues that arise.", "node_type": "subnode"}, {"id": "Variance", "label": "Variance", "title": "<b>Variance</b> (subnode)<hr>Measure of variability in models trained on different datasets from the same distribution.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Measure of variability in models trained on different datasets from the same distribution.", "node_type": "subnode"}, {"id": "Linear Model Limitations", "label": "Linear Model Limitations", "title": "<b>Linear Model Limitations</b> (subnode)<hr>Explains the limitations of linear models in capturing data structure.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Explains the limitations of linear models in capturing data structure.", "node_type": "subnode"}, {"id": "Bias Definition", "label": "Bias Definition", "title": "<b>Bias Definition</b> (subnode)<hr>Defines bias as test error with infinite training data.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Defines bias as test error with infinite training data.", "node_type": "subnode"}, {"id": "5th Degree Polynomial Models", "label": "5th Degree Polynomial Models", "title": "<b>5th Degree Polynomial Models</b> (subnode)<hr>Discusses the behavior of 5th degree polynomial models with training data.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Discusses the behavior of 5th degree polynomial models with training data.", "node_type": "subnode"}, {"id": "Generalization Failure", "label": "Generalization Failure", "title": "<b>Generalization Failure</b> (subnode)<hr>Describes the failure of 5th degree polynomials to generalize well on test data.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Describes the failure of 5th degree polynomials to generalize well on test data.", "node_type": "subnode"}, {"id": "Model Complexity", "label": "Model Complexity", "title": "<b>Model Complexity</b> (subnode)<hr>Measure of the simplicity or complexity of a model, often related to parameters.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Measure of the simplicity or complexity of a model, often related to parameters.", "node_type": "subnode"}, {"id": "Test Error Decomposition", "label": "Test Error Decomposition", "title": "<b>Test Error Decomposition</b> (subnode)<hr>Breakdown of test error into bias and variance components.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Breakdown of test error into bias and variance components.", "node_type": "subnode"}, {"id": "Bias Term", "label": "Bias Term", "title": "<b>Bias Term</b> (subnode)<hr>Error due to overly simplistic models unable to capture true relationships.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Error due to overly simplistic models unable to capture true relationships.", "node_type": "subnode"}, {"id": "Variance Term", "label": "Variance Term", "title": "<b>Variance Term</b> (subnode)<hr>Error due to model's sensitivity to training data variations.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Error due to model's sensitivity to training data variations.", "node_type": "subnode"}, {"id": "Model-wise Double Descent", "label": "Model-wise Double Descent", "title": "<b>Model-wise Double Descent</b> (subnode)<hr>Peak in test error related to model capacity and sample size relationship.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Peak in test error related to model capacity and sample size relationship.", "node_type": "subnode"}, {"id": "Model Evaluation", "label": "Model Evaluation", "title": "<b>Model Evaluation</b> (subnode)<hr>Assessment of model performance using metrics like MSE.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Assessment of model performance using metrics like MSE.", "node_type": "subnode"}, {"id": "Average Model (h_avg)", "label": "Average Model (h_avg)", "title": "<b>Average Model (h_avg)</b> (subnode)<hr>Hypothetical model representing the average of predictions from an infinite number of datasets.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Hypothetical model representing the average of predictions from an infinite number of datasets.", "node_type": "subnode"}, {"id": "Unavoidable Noise (\u03c3^2)", "label": "Unavoidable Noise (\u03c3^2)", "title": "<b>Unavoidable Noise (\u03c3^2)</b> (subnode)<hr>Inherent noise in data that cannot be reduced by any model improvement.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Inherent noise in data that cannot be reduced by any model improvement.", "node_type": "subnode"}, {"id": "Training Dataset", "label": "Training Dataset", "title": "<b>Training Dataset</b> (subnode)<hr>Set of data used to train a machine learning model.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Set of data used to train a machine learning model.", "node_type": "subnode"}, {"id": "Test Example", "label": "Test Example", "title": "<b>Test Example</b> (subnode)<hr>Single instance used for evaluating the performance of a trained model.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Single instance used for evaluating the performance of a trained model.", "node_type": "subnode"}, {"id": "Expected Test Error", "label": "Expected Test Error", "title": "<b>Expected Test Error</b> (subnode)<hr>Average error expected on unseen data after training with random datasets.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Average error expected on unseen data after training with random datasets.", "node_type": "subnode"}, {"id": "Claim 8.1.1", "label": "Claim 8.1.1", "title": "<b>Claim 8.1.1</b> (subnode)<hr>Mathematical tool for decomposing MSE into bias and variance terms.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Mathematical tool for decomposing MSE into bias and variance terms.", "node_type": "subnode"}, {"id": "Model Complexity and Test Errors", "label": "Model Complexity and Test Errors", "title": "<b>Model Complexity and Test Errors</b> (subnode)<hr>Relationship between model complexity and test error performance.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Relationship between model complexity and test error performance.", "node_type": "subnode"}, {"id": "Overparameterized Models", "label": "Overparameterized Models", "title": "<b>Overparameterized Models</b> (subnode)<hr>Models with more parameters than necessary for the training dataset.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Models with more parameters than necessary for the training dataset.", "node_type": "subnode"}, {"id": "Sample-wise Double Descent", "label": "Sample-wise Double Descent", "title": "<b>Sample-wise Double Descent</b> (subnode)<hr>Test error pattern observed as the number of training samples increases.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Test error pattern observed as the number of training samples increases.", "node_type": "subnode"}, {"id": "Historical Context and Recent Discoveries", "label": "Historical Context and Recent Discoveries", "title": "<b>Historical Context and Recent Discoveries</b> (subnode)<hr>Overview of historical context and recent advancements in understanding double descent phenomena.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Overview of historical context and recent advancements in understanding double descent phenomena.", "node_type": "subnode"}, {"id": "Optimal Algorithms", "label": "Optimal Algorithms", "title": "<b>Optimal Algorithms</b> (subnode)<hr>Algorithms that can achieve lower test errors when samples are limited.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Algorithms that can achieve lower test errors when samples are limited.", "node_type": "subnode"}, {"id": "Regularization Tuning", "label": "Regularization Tuning", "title": "<b>Regularization Tuning</b> (subnode)<hr>Optimal regularization helps mitigate double descent issues.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Optimal regularization helps mitigate double descent issues.", "node_type": "subnode"}, {"id": "Implicit Regularization", "label": "Implicit Regularization", "title": "<b>Implicit Regularization</b> (subnode)<hr>Optimizer effects like gradient descent provide implicit regularization in overparameterized models.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Optimizer effects like gradient descent provide implicit regularization in overparameterized models.", "node_type": "subnode"}, {"id": "Learning Guarantees", "label": "Learning Guarantees", "title": "<b>Learning Guarantees</b> (subnode)<hr>Conditions under which learning algorithms work well.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Conditions under which learning algorithms work well.", "node_type": "subnode"}, {"id": "Union Bound Lemma", "label": "Union Bound Lemma", "title": "<b>Union Bound Lemma</b> (subnode)<hr>Probability bound for union of events.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Probability bound for union of events.", "node_type": "subnode"}, {"id": "Hoeffding Inequality (Chernoff Bound)", "label": "Hoeffding Inequality (Chernoff Bound)", "title": "<b>Hoeffding Inequality (Chernoff Bound)</b> (subnode)<hr>Bound on deviation from true value in Bernoulli distribution.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Bound on deviation from true value in Bernoulli distribution.", "node_type": "subnode"}, {"id": "Sample Complexity Bounds", "label": "Sample Complexity Bounds", "title": "<b>Sample Complexity Bounds</b> (subnode)<hr>Theoretical bounds on the number of samples needed for learning algorithms to generalize well.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Theoretical bounds on the number of samples needed for learning algorithms to generalize well.", "node_type": "subnode"}, {"id": "Model Selection Methods", "label": "Model Selection Methods", "title": "<b>Model Selection Methods</b> (subnode)<hr>Techniques for choosing the right level of model complexity based on training data.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Techniques for choosing the right level of model complexity based on training data.", "node_type": "subnode"}, {"id": "Generalization Error", "label": "Generalization Error", "title": "<b>Generalization Error</b> (subnode)<hr>Error made by a predictive model when making predictions on data not seen during training.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Error made by a predictive model when making predictions on data not seen during training.", "node_type": "subnode"}, {"id": "Learning Theory", "label": "Learning Theory", "title": "<b>Learning Theory</b> (subnode)<hr>Theoretical foundations of machine learning, including generalization bounds and sample complexity.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Theoretical foundations of machine learning, including generalization bounds and sample complexity.", "node_type": "subnode"}, {"id": "Gradient Descent Optimizer", "label": "Gradient Descent Optimizer", "title": "<b>Gradient Descent Optimizer</b> (subnode)<hr>Optimizer used to find the minimum norm solution for linear models.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Optimizer used to find the minimum norm solution for linear models.", "node_type": "subnode"}, {"id": "Minimum Norm Solution", "label": "Minimum Norm Solution", "title": "<b>Minimum Norm Solution</b> (subnode)<hr>Solution found by gradient descent with zero initialization in overparameterized regime.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Solution found by gradient descent with zero initialization in overparameterized regime.", "node_type": "subnode"}, {"id": "Model Complexity Measures", "label": "Model Complexity Measures", "title": "<b>Model Complexity Measures</b> (subnode)<hr>Different measures of model complexity such as number of parameters or norm.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Different measures of model complexity such as number of parameters or norm.", "node_type": "subnode"}, {"id": "Number of Parameters", "label": "Number of Parameters", "title": "<b>Number of Parameters</b> (subnode)<hr>Common measure of model complexity, often leading to double descent phenomenon.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Common measure of model complexity, often leading to double descent phenomenon.", "node_type": "subnode"}, {"id": "Norm of Learned Model", "label": "Norm of Learned Model", "title": "<b>Norm of Learned Model</b> (subnode)<hr>Alternative measure that can mitigate the occurrence of double descent.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Alternative measure that can mitigate the occurrence of double descent.", "node_type": "subnode"}, {"id": "Regularization Strength", "label": "Regularization Strength", "title": "<b>Regularization Strength</b> (subnode)<hr>Impact of regularization on mitigating double descent phenomenon.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Impact of regularization on mitigating double descent phenomenon.", "node_type": "subnode"}, {"id": "Hypothesis Function", "label": "Hypothesis Function", "title": "<b>Hypothesis Function</b> (subnode)<hr>Function that maps input data to predictions.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Function that maps input data to predictions.", "node_type": "subnode"}, {"id": "Training Error", "label": "Training Error", "title": "<b>Training Error</b> (subnode)<hr>Error measured on the dataset used to train a model.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Error measured on the dataset used to train a model.", "node_type": "subnode"}, {"id": "Empirical Risk Minimization (ERM)", "label": "Empirical Risk Minimization (ERM)", "title": "<b>Empirical Risk Minimization (ERM)</b> (subnode)<hr>Learning algorithm that minimizes the empirical risk over the training set.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Learning algorithm that minimizes the empirical risk over the training set.", "node_type": "subnode"}, {"id": "PAC Assumptions", "label": "PAC Assumptions", "title": "<b>PAC Assumptions</b> (subnode)<hr>Framework for learning theory assumptions, including same-distribution assumption and i.i.d. samples.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Framework for learning theory assumptions, including same-distribution assumption and i.i.d. samples.", "node_type": "subnode"}, {"id": "Hypothesis Class", "label": "Hypothesis Class", "title": "<b>Hypothesis Class</b> (subnode)<hr>Set of all possible hypotheses a learning algorithm can choose from.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Set of all possible hypotheses a learning algorithm can choose from.", "node_type": "subnode"}, {"id": "Finite Hypothesis Classes", "label": "Finite Hypothesis Classes", "title": "<b>Finite Hypothesis Classes</b> (subnode)<hr>Learning problems with a finite set of hypotheses.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Learning problems with a finite set of hypotheses.", "node_type": "subnode"}, {"id": "Uniform Convergence", "label": "Uniform Convergence", "title": "<b>Uniform Convergence</b> (major)<hr>Property ensuring that the difference between empirical and true errors is small with high probability.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Property ensuring that the difference between empirical and true errors is small with high probability.", "node_type": "major"}, {"id": "Training Error vs Generalization Error", "label": "Training Error vs Generalization Error", "title": "<b>Training Error vs Generalization Error</b> (subnode)<hr>Discusses how training error relates to generalization error across different hypotheses.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Discusses how training error relates to generalization error across different hypotheses.", "node_type": "subnode"}, {"id": "Union Bound Application", "label": "Union Bound Application", "title": "<b>Union Bound Application</b> (subnode)<hr>Uses the union bound to extend a probability result from one hypothesis to all in a class.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Uses the union bound to extend a probability result from one hypothesis to all in a class.", "node_type": "subnode"}, {"id": "Probability of Error", "label": "Probability of Error", "title": "<b>Probability of Error</b> (subnode)<hr>Analyzes how error probabilities relate to training and generalization errors.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Analyzes how error probabilities relate to training and generalization errors.", "node_type": "subnode"}, {"id": "Sample Size Determination", "label": "Sample Size Determination", "title": "<b>Sample Size Determination</b> (subnode)<hr>Determines the sample size needed for a given probability of error bound.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Determines the sample size needed for a given probability of error bound.", "node_type": "subnode"}, {"id": "Empirical Risk Minimization", "label": "Empirical Risk Minimization", "title": "<b>Empirical Risk Minimization</b> (subnode)<hr>Process of selecting a hypothesis with the smallest training error.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Process of selecting a hypothesis with the smallest training error.", "node_type": "subnode"}, {"id": "Generalization Error Guarantees", "label": "Generalization Error Guarantees", "title": "<b>Generalization Error Guarantees</b> (subnode)<hr>Strategies for ensuring that training error closely approximates generalization error.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Strategies for ensuring that training error closely approximates generalization error.", "node_type": "subnode"}, {"id": "Bernoulli Random Variable Z", "label": "Bernoulli Random Variable Z", "title": "<b>Bernoulli Random Variable Z</b> (subnode)<hr>Random variable indicating whether a hypothesis misclassifies an example drawn from the distribution.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Random variable indicating whether a hypothesis misclassifies an example drawn from the distribution.", "node_type": "subnode"}, {"id": "Training Set Sampling", "label": "Training Set Sampling", "title": "<b>Training Set Sampling</b> (subnode)<hr>Process of drawing examples independently and identically distributed (iid) from \u0394.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Process of drawing examples independently and identically distributed (iid) from \u0394.", "node_type": "subnode"}, {"id": "Hoeffding Inequality", "label": "Hoeffding Inequality", "title": "<b>Hoeffding Inequality</b> (subnode)<hr>Inequality used to bound the probability that training error deviates significantly from generalization error.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Inequality used to bound the probability that training error deviates significantly from generalization error.", "node_type": "subnode"}, {"id": "Sample Complexity", "label": "Sample Complexity", "title": "<b>Sample Complexity</b> (subnode)<hr>Number of training examples needed for a certain level of performance guarantee.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Number of training examples needed for a certain level of performance guarantee.", "node_type": "subnode"}, {"id": "Hypotheses Space (H)", "label": "Hypotheses Space (H)", "title": "<b>Hypotheses Space (H)</b> (subnode)<hr>Set of all possible hypotheses or models considered in a learning problem.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Set of all possible hypotheses or models considered in a learning problem.", "node_type": "subnode"}, {"id": "Optimal Hypothesis (h*)", "label": "Optimal Hypothesis (h*)", "title": "<b>Optimal Hypothesis (h*)</b> (subnode)<hr>Hypothesis with the lowest true error over hypothesis space H.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Hypothesis with the lowest true error over hypothesis space H.", "node_type": "subnode"}, {"id": "Hypothesis Class Switching", "label": "Hypothesis Class Switching", "title": "<b>Hypothesis Class Switching</b> (major)<hr>Discussion on switching to a larger hypothesis class and its effects.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Discussion on switching to a larger hypothesis class and its effects.", "node_type": "major"}, {"id": "Bias Decrease", "label": "Bias Decrease", "title": "<b>Bias Decrease</b> (subnode)<hr>Explains how bias decreases when moving to a larger hypothesis class.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Explains how bias decreases when moving to a larger hypothesis class.", "node_type": "subnode"}, {"id": "Variance Increase", "label": "Variance Increase", "title": "<b>Variance Increase</b> (subnode)<hr>Describes the increase in variance with a larger hypothesis class.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Describes the increase in variance with a larger hypothesis class.", "node_type": "subnode"}, {"id": "Sample Complexity Bound", "label": "Sample Complexity Bound", "title": "<b>Sample Complexity Bound</b> (major)<hr>Derivation of sample complexity bound for finite hypothesis classes.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Derivation of sample complexity bound for finite hypothesis classes.", "node_type": "major"}, {"id": "Corollary Proof", "label": "Corollary Proof", "title": "<b>Corollary Proof</b> (subnode)<hr>Proof involving fixed \u03b4 and \u03b3 to derive n's value.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Proof involving fixed \u03b4 and \u03b3 to derive n's value.", "node_type": "subnode"}, {"id": "Infinite Hypothesis Classes", "label": "Infinite Hypothesis Classes", "title": "<b>Infinite Hypothesis Classes</b> (major)<hr>Introduction to dealing with hypothesis classes parameterized by real numbers.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Introduction to dealing with hypothesis classes parameterized by real numbers.", "node_type": "major"}, {"id": "Bit Representation", "label": "Bit Representation", "title": "<b>Bit Representation</b> (subnode)<hr>Discussion on the finite representation of real numbers in computers using bits.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Discussion on the finite representation of real numbers in computers using bits.", "node_type": "subnode"}, {"id": "Floating Point Representation", "label": "Floating Point Representation", "title": "<b>Floating Point Representation</b> (subnode)<hr>Use of 64-bit floating point numbers in parameter representation.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Use of 64-bit floating point numbers in parameter representation.", "node_type": "subnode"}, {"id": "Hypothesis Class Size", "label": "Hypothesis Class Size", "title": "<b>Hypothesis Class Size</b> (subnode)<hr>Size of the hypothesis class based on model parameters.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Size of the hypothesis class based on model parameters.", "node_type": "subnode"}, {"id": "Non-ERM Algorithms", "label": "Non-ERM Algorithms", "title": "<b>Non-ERM Algorithms</b> (subnode)<hr>Learning algorithms that do not rely solely on empirical risk minimization.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Learning algorithms that do not rely solely on empirical risk minimization.", "node_type": "subnode"}, {"id": "Hypothesis Space", "label": "Hypothesis Space", "title": "<b>Hypothesis Space</b> (subnode)<hr>The set of all possible hypotheses that a learner can choose from.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "The set of all possible hypotheses that a learner can choose from.", "node_type": "subnode"}, {"id": "Parameterization of Hypotheses", "label": "Parameterization of Hypotheses", "title": "<b>Parameterization of Hypotheses</b> (subnode)<hr>Different ways to parameterize the same hypothesis space.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Different ways to parameterize the same hypothesis space.", "node_type": "subnode"}, {"id": "VC Dimension", "label": "VC Dimension", "title": "<b>VC Dimension</b> (subnode)<hr>Measure of the capacity of a statistical model, defined as the cardinality of the largest set of points that the model can shatter.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Measure of the capacity of a statistical model, defined as the cardinality of the largest set of points that the model can shatter.", "node_type": "subnode"}, {"id": "Shattering Sets", "label": "Shattering Sets", "title": "<b>Shattering Sets</b> (subnode)<hr>A hypothesis class can shatter a set if it can realize any labeling on the set.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "A hypothesis class can shatter a set if it can realize any labeling on the set.", "node_type": "subnode"}, {"id": "Vapnik's Theorem", "label": "Vapnik's Theorem", "title": "<b>Vapnik's Theorem</b> (major)<hr>Theorem linking VC dimension to generalization error bounds", "shape": "star", "size": 25, "color": "#FF6347", "description": "Theorem linking VC dimension to generalization error bounds", "node_type": "major"}, {"id": "Corollary on Training Examples", "label": "Corollary on Training Examples", "title": "<b>Corollary on Training Examples</b> (major)<hr>Number of examples needed for learning is linear in VC dimension", "shape": "star", "size": 25, "color": "#FF6347", "description": "Number of examples needed for learning is linear in VC dimension", "node_type": "major"}, {"id": "Regularization in Deep Learning", "label": "Regularization in Deep Learning", "title": "<b>Regularization in Deep Learning</b> (major)<hr>Overview of regularization techniques and their impact on deep learning models.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Overview of regularization techniques and their impact on deep learning models.", "node_type": "major"}, {"id": "Explicit Regularization Techniques", "label": "Explicit Regularization Techniques", "title": "<b>Explicit Regularization Techniques</b> (subnode)<hr>Techniques such as weight decay, dropout, data augmentation, spectral norm regularization, and Lipschitzness regularization.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Techniques such as weight decay, dropout, data augmentation, spectral norm regularization, and Lipschitzness regularization.", "node_type": "subnode"}, {"id": "Implicit Regularization Effect", "label": "Implicit Regularization Effect", "title": "<b>Implicit Regularization Effect</b> (subnode)<hr>The impact of optimizers on model parameters beyond explicit regularization.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "The impact of optimizers on model parameters beyond explicit regularization.", "node_type": "subnode"}, {"id": "Regularization in Machine Learning", "label": "Regularization in Machine Learning", "title": "<b>Regularization in Machine Learning</b> (major)<hr>Techniques to prevent overfitting by adding a penalty for complexity.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Techniques to prevent overfitting by adding a penalty for complexity.", "node_type": "major"}, {"id": "Sparsity Regularization", "label": "Sparsity Regularization", "title": "<b>Sparsity Regularization</b> (subnode)<hr>Imposing sparsity on model parameters to reduce complexity and improve generalization.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Imposing sparsity on model parameters to reduce complexity and improve generalization.", "node_type": "subnode"}, {"id": "L1 Norm (LASSO)", "label": "L1 Norm (LASSO)", "title": "<b>L1 Norm (LASSO)</b> (subnode)<hr>A common relaxation for \u03b8\u20960, promoting sparsity in linear models.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "A common relaxation for \u03b8\u20960, promoting sparsity in linear models.", "node_type": "subnode"}, {"id": "L2 Norm Regularization", "label": "L2 Norm Regularization", "title": "<b>L2 Norm Regularization</b> (subnode)<hr>Penalizes the squared magnitude of coefficients to prevent overfitting.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Penalizes the squared magnitude of coefficients to prevent overfitting.", "node_type": "subnode"}, {"id": "Deep Learning Regularization Techniques", "label": "Deep Learning Regularization Techniques", "title": "<b>Deep Learning Regularization Techniques</b> (subnode)<hr>Various methods to regularize neural networks and improve generalization.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Various methods to regularize neural networks and improve generalization.", "node_type": "subnode"}, {"id": "Optimizers and Generalization", "label": "Optimizers and Generalization", "title": "<b>Optimizers and Generalization</b> (major)<hr>Discussion on how optimizers affect model generalization.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Discussion on how optimizers affect model generalization.", "node_type": "major"}, {"id": "Global Minima Variability", "label": "Global Minima Variability", "title": "<b>Global Minima Variability</b> (subnode)<hr>Different global minima can lead to different generalization performance.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Different global minima can lead to different generalization performance.", "node_type": "subnode"}, {"id": "Model Selection via Cross Validation", "label": "Model Selection via Cross Validation", "title": "<b>Model Selection via Cross Validation</b> (major)<hr>Process of selecting models using cross validation techniques.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Process of selecting models using cross validation techniques.", "node_type": "major"}, {"id": "Cross Validation Techniques", "label": "Cross Validation Techniques", "title": "<b>Cross Validation Techniques</b> (subnode)<hr>Techniques used for model selection through cross validation.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Techniques used for model selection through cross validation.", "node_type": "subnode"}, {"id": "Regularized Loss", "label": "Regularized Loss", "title": "<b>Regularized Loss</b> (subnode)<hr>Combination of training loss and regularizer term, used in model evaluation.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Combination of training loss and regularizer term, used in model evaluation.", "node_type": "subnode"}, {"id": "Regularizer", "label": "Regularizer", "title": "<b>Regularizer</b> (subnode)<hr>Function that measures model complexity, often \u03b8 norm.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Function that measures model complexity, often \u03b8 norm.", "node_type": "subnode"}, {"id": "\u03bb (Lambda)", "label": "\u03bb (Lambda)", "title": "<b>\u03bb (Lambda)</b> (subnode)<hr>Parameter controlling the trade-off between loss and regularizer.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Parameter controlling the trade-off between loss and regularizer.", "node_type": "subnode"}, {"id": "\u039b_2 Regularization", "label": "\u039b_2 Regularization", "title": "<b>\u039b_2 Regularization</b> (subnode)<hr>Encourages small \u03b8 norm, also known as weight decay.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Encourages small \u03b8 norm, also known as weight decay.", "node_type": "subnode"}, {"id": "Weight Decay", "label": "Weight Decay", "title": "<b>Weight Decay</b> (subnode)<hr>Effect of gradient descent on regularized loss in deep learning.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Effect of gradient descent on regularized loss in deep learning.", "node_type": "subnode"}, {"id": "Inductive Bias", "label": "Inductive Bias", "title": "<b>Inductive Bias</b> (subnode)<hr>Structures or biases imposed by regularization to guide model learning.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Structures or biases imposed by regularization to guide model learning.", "node_type": "subnode"}, {"id": "Chapter 9 Regularization and Model Selection", "label": "Chapter 9 Regularization and Model Selection", "title": "<b>Chapter 9 Regularization and Model Selection</b> (major)<hr>Focuses on techniques for controlling model complexity.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Focuses on techniques for controlling model complexity.", "node_type": "major"}, {"id": "Training Loss/Cost Function", "label": "Training Loss/Cost Function", "title": "<b>Training Loss/Cost Function</b> (subnode)<hr>Function used to evaluate the performance of a model during training.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Function used to evaluate the performance of a model during training.", "node_type": "subnode"}, {"id": "Regularizer Term", "label": "Regularizer Term", "title": "<b>Regularizer Term</b> (subnode)<hr>Additional term added to the loss function to control complexity and prevent overfitting.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Additional term added to the loss function to control complexity and prevent overfitting.", "node_type": "subnode"}, {"id": "Regularization Parameter (\u03bb)", "label": "Regularization Parameter (\u03bb)", "title": "<b>Regularization Parameter (\u03bb)</b> (subnode)<hr>Hyperparameter controlling the influence of the regularizer on the overall loss function.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Hyperparameter controlling the influence of the regularizer on the overall loss function.", "node_type": "subnode"}, {"id": "Model Selection", "label": "Model Selection", "title": "<b>Model Selection</b> (major)<hr>Process of choosing the best model based on validation techniques.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Process of choosing the best model based on validation techniques.", "node_type": "major"}, {"id": "Cross Validation", "label": "Cross Validation", "title": "<b>Cross Validation</b> (subnode)<hr>Technique to evaluate models and select the one with the best performance.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Technique to evaluate models and select the one with the best performance.", "node_type": "subnode"}, {"id": "Polynomial Regression Models", "label": "Polynomial Regression Models", "title": "<b>Polynomial Regression Models</b> (subnode)<hr>Models with varying degrees of polynomial terms for regression analysis.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Models with varying degrees of polynomial terms for regression analysis.", "node_type": "subnode"}, {"id": "Regularization Parameters", "label": "Regularization Parameters", "title": "<b>Regularization Parameters</b> (subnode)<hr>Parameters like C in SVM that control model complexity and prevent overfitting.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Parameters like C in SVM that control model complexity and prevent overfitting.", "node_type": "subnode"}, {"id": "Machine Learning Techniques", "label": "Machine Learning Techniques", "title": "<b>Machine Learning Techniques</b> (major)<hr>Various techniques used in machine learning for data analysis and pattern recognition.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Various techniques used in machine learning for data analysis and pattern recognition.", "node_type": "major"}, {"id": "Validation Set Size", "label": "Validation Set Size", "title": "<b>Validation Set Size</b> (subnode)<hr>Determining an appropriate size for the validation set in machine learning.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Determining an appropriate size for the validation set in machine learning.", "node_type": "subnode"}, {"id": "Hold Out Cross Validation", "label": "Hold Out Cross Validation", "title": "<b>Hold Out Cross Validation</b> (subnode)<hr>A method where a portion of data is held out as a validation set.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "A method where a portion of data is held out as a validation set.", "node_type": "subnode"}, {"id": "k-fold Cross Validation", "label": "k-fold Cross Validation", "title": "<b>k-fold Cross Validation</b> (subnode)<hr>Divides the dataset into k parts, trains on k-1 and tests on 1 part repeatedly.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Divides the dataset into k parts, trains on k-1 and tests on 1 part repeatedly.", "node_type": "subnode"}, {"id": "Retraining on Full Dataset", "label": "Retraining on Full Dataset", "title": "<b>Retraining on Full Dataset</b> (subnode)<hr>Optionally retrain selected model on entire training set after validation.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Optionally retrain selected model on entire training set after validation.", "node_type": "subnode"}, {"id": "Leave-One-Out Cross Validation", "label": "Leave-One-Out Cross Validation", "title": "<b>Leave-One-Out Cross Validation</b> (subnode)<hr>Uses each data point as a test set once while training on all others.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Uses each data point as a test set once while training on all others.", "node_type": "subnode"}, {"id": "Data Scarcity", "label": "Data Scarcity", "title": "<b>Data Scarcity</b> (subnode)<hr>Situation where the amount of available data is limited, affecting model evaluation methods.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Situation where the amount of available data is limited, affecting model evaluation methods.", "node_type": "subnode"}, {"id": "Leave-One-Out CV", "label": "Leave-One-Out CV", "title": "<b>Leave-One-Out CV</b> (subnode)<hr>Method where one training example is held out at a time.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Method where one training example is held out at a time.", "node_type": "subnode"}, {"id": "Bayesian Statistics", "label": "Bayesian Statistics", "title": "<b>Bayesian Statistics</b> (major)<hr>Approach to parameter estimation that treats parameters as random variables.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Approach to parameter estimation that treats parameters as random variables.", "node_type": "major"}, {"id": "MLE", "label": "MLE", "title": "<b>MLE</b> (subnode)<hr>Estimation method where parameters are viewed as constant but unknown values.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Estimation method where parameters are viewed as constant but unknown values.", "node_type": "subnode"}, {"id": "Prior Distribution", "label": "Prior Distribution", "title": "<b>Prior Distribution</b> (subnode)<hr>Distribution expressing prior beliefs about the parameters before seeing data.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Distribution expressing prior beliefs about the parameters before seeing data.", "node_type": "subnode"}, {"id": "Hold-out Cross Validation", "label": "Hold-out Cross Validation", "title": "<b>Hold-out Cross Validation</b> (subnode)<hr>Technique splitting data into training and validation sets for better error estimation.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Technique splitting data into training and validation sets for better error estimation.", "node_type": "subnode"}, {"id": "Training Set S", "label": "Training Set S", "title": "<b>Training Set S</b> (subnode)<hr>Dataset used to train models in machine learning tasks.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Dataset used to train models in machine learning tasks.", "node_type": "subnode"}, {"id": "Hypotheses Training", "label": "Hypotheses Training", "title": "<b>Hypotheses Training</b> (subnode)<hr>Process of training each model on the full dataset S.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Process of training each model on the full dataset S.", "node_type": "subnode"}, {"id": "Training Error Selection", "label": "Training Error Selection", "title": "<b>Training Error Selection</b> (subnode)<hr>Selection based on minimum training error, often leading to overfitting.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Selection based on minimum training error, often leading to overfitting.", "node_type": "subnode"}, {"id": "Validation Set S_cv", "label": "Validation Set S_cv", "title": "<b>Validation Set S_cv</b> (subnode)<hr>Subset of data used for validating model performance after training.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Subset of data used for validating model performance after training.", "node_type": "subnode"}, {"id": "Model Selection Based on Validation Error", "label": "Model Selection Based on Validation Error", "title": "<b>Model Selection Based on Validation Error</b> (subnode)<hr>Choosing the best hypothesis based on its error on the validation set.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Choosing the best hypothesis based on its error on the validation set.", "node_type": "subnode"}, {"id": "Bayesian Machine Learning", "label": "Bayesian Machine Learning", "title": "<b>Bayesian Machine Learning</b> (major)<hr>Predictions made using posterior distribution on parameters.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Predictions made using posterior distribution on parameters.", "node_type": "major"}, {"id": "Predictive Distribution", "label": "Predictive Distribution", "title": "<b>Predictive Distribution</b> (subnode)<hr>Probability distribution of predictions for new examples based on posterior distribution.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Probability distribution of predictions for new examples based on posterior distribution.", "node_type": "subnode"}, {"id": "Fully Bayesian Prediction", "label": "Fully Bayesian Prediction", "title": "<b>Fully Bayesian Prediction</b> (subnode)<hr>Prediction method that averages over the posterior distribution of parameters.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Prediction method that averages over the posterior distribution of parameters.", "node_type": "subnode"}, {"id": "Computational Challenges", "label": "Computational Challenges", "title": "<b>Computational Challenges</b> (subnode)<hr>Difficulties in computing high-dimensional integrals for posterior distributions.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Difficulties in computing high-dimensional integrals for posterior distributions.", "node_type": "subnode"}, {"id": "k-means Algorithm", "label": "k-means Algorithm", "title": "<b>k-means Algorithm</b> (major)<hr>Clustering algorithm that partitions data into k clusters.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Clustering algorithm that partitions data into k clusters.", "node_type": "major"}, {"id": "Initialization", "label": "Initialization", "title": "<b>Initialization</b> (subnode)<hr>Randomly selecting initial cluster centroids.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Randomly selecting initial cluster centroids.", "node_type": "subnode"}, {"id": "Convergence", "label": "Convergence", "title": "<b>Convergence</b> (subnode)<hr>Guaranteed to converge in a certain sense.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Guaranteed to converge in a certain sense.", "node_type": "subnode"}, {"id": "Distortion Function", "label": "Distortion Function", "title": "<b>Distortion Function</b> (subnode)<hr>Measures sum of squared distances between examples and cluster centroids.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Measures sum of squared distances between examples and cluster centroids.", "node_type": "subnode"}, {"id": "Coordinate Descent on J", "label": "Coordinate Descent on J", "title": "<b>Coordinate Descent on J</b> (subnode)<hr>Minimizing distortion function iteratively with respect to c and mu.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Minimizing distortion function iteratively with respect to c and mu.", "node_type": "subnode"}, {"id": "Distortion Function J", "label": "Distortion Function J", "title": "<b>Distortion Function J</b> (subnode)<hr>Function measuring the quality of clustering in k-means.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Function measuring the quality of clustering in k-means.", "node_type": "subnode"}, {"id": "Convergence Properties", "label": "Convergence Properties", "title": "<b>Convergence Properties</b> (subnode)<hr>Properties related to convergence and local optima in k-means.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Properties related to convergence and local optima in k-means.", "node_type": "subnode"}, {"id": "EM Algorithms", "label": "EM Algorithms", "title": "<b>EM Algorithms</b> (major)<hr>Expectation-Maximization algorithm used in probabilistic modeling.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Expectation-Maximization algorithm used in probabilistic modeling.", "node_type": "major"}, {"id": "EM for Mixture of Gaussians", "label": "EM for Mixture of Gaussians", "title": "<b>EM for Mixture of Gaussians</b> (subnode)<hr>Application of EM to model data with Gaussian distributions.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Application of EM to model data with Gaussian distributions.", "node_type": "subnode"}, {"id": "Posterior Approximation", "label": "Posterior Approximation", "title": "<b>Posterior Approximation</b> (subnode)<hr>Techniques for approximating the posterior distribution when exact computation is infeasible.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Techniques for approximating the posterior distribution when exact computation is infeasible.", "node_type": "subnode"}, {"id": "MAP Estimation", "label": "MAP Estimation", "title": "<b>MAP Estimation</b> (subnode)<hr>Estimate parameters by maximizing the posterior probability, incorporating prior knowledge.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Estimate parameters by maximizing the posterior probability, incorporating prior knowledge.", "node_type": "subnode"}, {"id": "MLE vs MAP", "label": "MLE vs MAP", "title": "<b>MLE vs MAP</b> (subnode)<hr>Comparison between maximum likelihood and maximum a posteriori estimation methods.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Comparison between maximum likelihood and maximum a posteriori estimation methods.", "node_type": "subnode"}, {"id": "Prior Selection", "label": "Prior Selection", "title": "<b>Prior Selection</b> (subnode)<hr>Choosing appropriate prior distributions for Bayesian models, e.g., Gaussian distribution.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Choosing appropriate prior distributions for Bayesian models, e.g., Gaussian distribution.", "node_type": "subnode"}, {"id": "Unsupervised Learning", "label": "Unsupervised Learning", "title": "<b>Unsupervised Learning</b> (major)<hr>Learning from data without labeled responses; finding hidden structure in unlabeled data sets.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Learning from data without labeled responses; finding hidden structure in unlabeled data sets.", "node_type": "major"}, {"id": "Clustering", "label": "Clustering", "title": "<b>Clustering</b> (subnode)<hr>Techniques for grouping a set of objects into clusters based on their similarity.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Techniques for grouping a set of objects into clusters based on their similarity.", "node_type": "subnode"}, {"id": "K-Means Algorithm", "label": "K-Means Algorithm", "title": "<b>K-Means Algorithm</b> (subnode)<hr>A popular unsupervised learning algorithm that partitions data points into k clusters.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "A popular unsupervised learning algorithm that partitions data points into k clusters.", "node_type": "subnode"}, {"id": "Mixture of Gaussians Model", "label": "Mixture of Gaussians Model", "title": "<b>Mixture of Gaussians Model</b> (subnode)<hr>Model using multiple Gaussian distributions with latent variables", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Model using multiple Gaussian distributions with latent variables", "node_type": "subnode"}, {"id": "Latent Variables", "label": "Latent Variables", "title": "<b>Latent Variables</b> (subnode)<hr>Hidden random variables that influence the observed data", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Hidden random variables that influence the observed data", "node_type": "subnode"}, {"id": "Joint Distribution", "label": "Joint Distribution", "title": "<b>Joint Distribution</b> (subnode)<hr>Distribution modeling both latent and observable variables", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Distribution modeling both latent and observable variables", "node_type": "subnode"}, {"id": "Likelihood Estimation", "label": "Likelihood Estimation", "title": "<b>Likelihood Estimation</b> (subnode)<hr>Estimating parameters by maximizing likelihood of observed data", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Estimating parameters by maximizing likelihood of observed data", "node_type": "subnode"}, {"id": "EM Algorithm", "label": "EM Algorithm", "title": "<b>EM Algorithm</b> (subnode)<hr>Iterative method for finding maximum likelihood or maximum a posteriori estimates in statistical models with latent variables.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Iterative method for finding maximum likelihood or maximum a posteriori estimates in statistical models with latent variables.", "node_type": "subnode"}, {"id": "E-step", "label": "E-step", "title": "<b>E-step</b> (subnode)<hr>Estimation step where posterior distribution of hidden variables is computed given observed data and current parameters.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Estimation step where posterior distribution of hidden variables is computed given observed data and current parameters.", "node_type": "subnode"}, {"id": "M-step", "label": "M-step", "title": "<b>M-step</b> (subnode)<hr>Maximization step where model parameters are updated to maximize the expected log-likelihood found in E-step.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Maximization step where model parameters are updated to maximize the expected log-likelihood found in E-step.", "node_type": "subnode"}, {"id": "Gaussian Mixture Model", "label": "Gaussian Mixture Model", "title": "<b>Gaussian Mixture Model</b> (subnode)<hr>Model used for clustering data into multiple Gaussian distributions with different means and covariances.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Model used for clustering data into multiple Gaussian distributions with different means and covariances.", "node_type": "subnode"}, {"id": "Soft Assignments", "label": "Soft Assignments", "title": "<b>Soft Assignments</b> (subnode)<hr>Assigns probabilities to each cluster instead of hard assignments, allowing for probabilistic membership in clusters.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Assigns probabilities to each cluster instead of hard assignments, allowing for probabilistic membership in clusters.", "node_type": "subnode"}, {"id": "K-means Clustering", "label": "K-means Clustering", "title": "<b>K-means Clustering</b> (subnode)<hr>Clustering algorithm that assigns data points to the nearest centroid; contrasted with EM's soft assignments.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Clustering algorithm that assigns data points to the nearest centroid; contrasted with EM's soft assignments.", "node_type": "subnode"}, {"id": "Expectation-Maximization Algorithm", "label": "Expectation-Maximization Algorithm", "title": "<b>Expectation-Maximization Algorithm</b> (subnode)<hr>Algorithm used for finding maximum likelihood or maximum a posteriori estimates of parameters in statistical models where the model depends on unobserved latent variables.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Algorithm used for finding maximum likelihood or maximum a posteriori estimates of parameters in statistical models where the model depends on unobserved latent variables.", "node_type": "subnode"}, {"id": "Convergence Guarantees", "label": "Convergence Guarantees", "title": "<b>Convergence Guarantees</b> (subnode)<hr>Conditions and proofs ensuring the algorithm's convergence to optimal solutions.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Conditions and proofs ensuring the algorithm's convergence to optimal solutions.", "node_type": "subnode"}, {"id": "Jensen's Inequality", "label": "Jensen's Inequality", "title": "<b>Jensen's Inequality</b> (major)<hr>Mathematical result used to prove the monotonic increase of log-likelihood in each iteration of EM.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Mathematical result used to prove the monotonic increase of log-likelihood in each iteration of EM.", "node_type": "major"}, {"id": "Strict Convexity", "label": "Strict Convexity", "title": "<b>Strict Convexity</b> (subnode)<hr>Condition for a convex function to be strictly convex, ensuring unique minimum points.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Condition for a convex function to be strictly convex, ensuring unique minimum points.", "node_type": "subnode"}, {"id": "Theorem Statement", "label": "Theorem Statement", "title": "<b>Theorem Statement</b> (subnode)<hr>Formal statement of Jensen's inequality involving expectations and convex functions.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Formal statement of Jensen's inequality involving expectations and convex functions.", "node_type": "subnode"}, {"id": "Concave Functions", "label": "Concave Functions", "title": "<b>Concave Functions</b> (subnode)<hr>Functions where E[f(X)] <= f(E[X]) if f is concave", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Functions where E[f(X)] <= f(E[X]) if f is concave", "node_type": "subnode"}, {"id": "Latent Variable Models", "label": "Latent Variable Models", "title": "<b>Latent Variable Models</b> (subnode)<hr>Models with unobserved variables that influence observed data", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Models with unobserved variables that influence observed data", "node_type": "subnode"}, {"id": "Log-Likelihood Maximization", "label": "Log-Likelihood Maximization", "title": "<b>Log-Likelihood Maximization</b> (subnode)<hr>Process of finding parameter values that maximize the probability of observed data under a statistical model.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Process of finding parameter values that maximize the probability of observed data under a statistical model.", "node_type": "subnode"}, {"id": "Machine_Learning_Concepts", "label": "Machine_Learning_Concepts", "title": "<b>Machine_Learning_Concepts</b> (major)<hr>Overview of machine learning concepts and algorithms.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Overview of machine learning concepts and algorithms.", "node_type": "major"}, {"id": "EM_Algorithm", "label": "EM_Algorithm", "title": "<b>EM_Algorithm</b> (subnode)<hr>Efficient method for maximum likelihood estimation in probabilistic models.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Efficient method for maximum likelihood estimation in probabilistic models.", "node_type": "subnode"}, {"id": "Likelihood_Estimation", "label": "Likelihood_Estimation", "title": "<b>Likelihood_Estimation</b> (subnode)<hr>Process of estimating parameters to maximize the likelihood function.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Process of estimating parameters to maximize the likelihood function.", "node_type": "subnode"}, {"id": "Non_Convex_Optimization", "label": "Non_Convex_Optimization", "title": "<b>Non_Convex_Optimization</b> (subnode)<hr>Challenges in optimizing non-convex functions for parameter estimation.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Challenges in optimizing non-convex functions for parameter estimation.", "node_type": "subnode"}, {"id": "E_Step", "label": "E_Step", "title": "<b>E_Step</b> (subnode)<hr>Expectation step where a lower bound on the likelihood is constructed.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Expectation step where a lower bound on the likelihood is constructed.", "node_type": "subnode"}, {"id": "M_Step", "label": "M_Step", "title": "<b>M_Step</b> (subnode)<hr>Maximization step where the lower bound is optimized to update parameters.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Maximization step where the lower bound is optimized to update parameters.", "node_type": "subnode"}, {"id": "Latent_Variables", "label": "Latent_Variables", "title": "<b>Latent_Variables</b> (subnode)<hr>Random variables that are not directly observed but influence the model.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Random variables that are not directly observed but influence the model.", "node_type": "subnode"}, {"id": "Single_Example_Optimization", "label": "Single_Example_Optimization", "title": "<b>Single_Example_Optimization</b> (subnode)<hr>Simplification of EM algorithm for optimizing likelihood of a single example.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Simplification of EM algorithm for optimizing likelihood of a single example.", "node_type": "subnode"}, {"id": "Evidence Lower Bound (ELBO)", "label": "Evidence Lower Bound (ELBO)", "title": "<b>Evidence Lower Bound (ELBO)</b> (subnode)<hr>Objective function used in variational inference to approximate complex probability distributions.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Objective function used in variational inference to approximate complex probability distributions.", "node_type": "subnode"}, {"id": "Lower Bound Derivation", "label": "Lower Bound Derivation", "title": "<b>Lower Bound Derivation</b> (subnode)<hr>Deriving a lower bound on the log-likelihood using Jensen's inequality.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Deriving a lower bound on the log-likelihood using Jensen's inequality.", "node_type": "subnode"}, {"id": "Optimizing Q Distribution", "label": "Optimizing Q Distribution", "title": "<b>Optimizing Q Distribution</b> (subnode)<hr>Choosing an optimal distribution Q to make the lower bound tight for given parameters.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Choosing an optimal distribution Q to make the lower bound tight for given parameters.", "node_type": "subnode"}, {"id": "Log-Likelihood Optimization", "label": "Log-Likelihood Optimization", "title": "<b>Log-Likelihood Optimization</b> (subnode)<hr>Optimizing the log-likelihood function under the EM framework.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Optimizing the log-likelihood function under the EM framework.", "node_type": "subnode"}, {"id": "Single Example Case", "label": "Single Example Case", "title": "<b>Single Example Case</b> (subnode)<hr>Discussion of optimizing for a single training example.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Discussion of optimizing for a single training example.", "node_type": "subnode"}, {"id": "Multiple Examples Case", "label": "Multiple Examples Case", "title": "<b>Multiple Examples Case</b> (subnode)<hr>Extending the optimization to multiple training examples.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Extending the optimization to multiple training examples.", "node_type": "subnode"}, {"id": "E-step Calculation", "label": "E-step Calculation", "title": "<b>E-step Calculation</b> (subnode)<hr>Calculates the probability of latent variables given observed data and current parameter estimates.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Calculates the probability of latent variables given observed data and current parameter estimates.", "node_type": "subnode"}, {"id": "M-step Maximization", "label": "M-step Maximization", "title": "<b>M-step Maximization</b> (subnode)<hr>Maximizes the expected log-likelihood found in the E step as a function of the parameters.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Maximizes the expected log-likelihood found in the E step as a function of the parameters.", "node_type": "subnode"}, {"id": "Parameter Updates", "label": "Parameter Updates", "title": "<b>Parameter Updates</b> (subnode)<hr>Updates for \u03c6, \u03bc, and \u03a3 based on maximizing expected log-likelihood.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Updates for \u03c6, \u03bc, and \u03a3 based on maximizing expected log-likelihood.", "node_type": "subnode"}, {"id": "\u03b8 Update Rule", "label": "\u03b8 Update Rule", "title": "<b>\u03b8 Update Rule</b> (subnode)<hr>Rule for updating parameters in the M-step to maximize likelihood function.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Rule for updating parameters in the M-step to maximize likelihood function.", "node_type": "subnode"}, {"id": "ELBO Interpretation", "label": "ELBO Interpretation", "title": "<b>ELBO Interpretation</b> (subnode)<hr>Various interpretations of Evidence Lower Bound (ELBO).", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Various interpretations of Evidence Lower Bound (ELBO).", "node_type": "subnode"}, {"id": "Alternative ELBO Formulations", "label": "Alternative ELBO Formulations", "title": "<b>Alternative ELBO Formulations</b> (subnode)<hr>Different mathematical formulations of the ELBO equation.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Different mathematical formulations of the ELBO equation.", "node_type": "subnode"}, {"id": "KL Divergence in ELBO", "label": "KL Divergence in ELBO", "title": "<b>KL Divergence in ELBO</b> (subnode)<hr>Explanation of KL divergence within ELBO context.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Explanation of KL divergence within ELBO context.", "node_type": "subnode"}, {"id": "Mixture of Gaussians", "label": "Mixture of Gaussians", "title": "<b>Mixture of Gaussians</b> (subnode)<hr>Application of EM algorithm to Gaussian mixture models for parameter estimation.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Application of EM algorithm to Gaussian mixture models for parameter estimation.", "node_type": "subnode"}, {"id": "EM Algorithm Steps", "label": "EM Algorithm Steps", "title": "<b>EM Algorithm Steps</b> (subnode)<hr>E-step and M-step processes in the context of Gaussian mixtures.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "E-step and M-step processes in the context of Gaussian mixtures.", "node_type": "subnode"}, {"id": "Convergence Proof", "label": "Convergence Proof", "title": "<b>Convergence Proof</b> (subnode)<hr>Proof showing EM algorithm monotonically increases log-likelihood until convergence.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Proof showing EM algorithm monotonically increases log-likelihood until convergence.", "node_type": "subnode"}, {"id": "ELBO (Evidence Lower Bound)", "label": "ELBO (Evidence Lower Bound)", "title": "<b>ELBO (Evidence Lower Bound)</b> (major)<hr>Objective function used in variational inference and EM algorithm as a lower bound on the log-likelihood.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Objective function used in variational inference and EM algorithm as a lower bound on the log-likelihood.", "node_type": "major"}, {"id": "Expectation-Maximization (EM) Algorithm", "label": "Expectation-Maximization (EM) Algorithm", "title": "<b>Expectation-Maximization (EM) Algorithm</b> (subnode)<hr>Iterative method for finding maximum likelihood or maximum a posteriori estimates of parameters in statistical models, where the model depends on unobserved latent variables.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Iterative method for finding maximum likelihood or maximum a posteriori estimates of parameters in statistical models, where the model depends on unobserved latent variables.", "node_type": "subnode"}, {"id": "M-step Update Rule", "label": "M-step Update Rule", "title": "<b>M-step Update Rule</b> (subnode)<hr>Rule used to update parameters during the maximization step of the EM algorithm.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Rule used to update parameters during the maximization step of the EM algorithm.", "node_type": "subnode"}, {"id": "Lagrangian Method", "label": "Lagrangian Method", "title": "<b>Lagrangian Method</b> (subnode)<hr>Mathematical technique for finding local maxima and minima of a function subject to equality constraints.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Mathematical technique for finding local maxima and minima of a function subject to equality constraints.", "node_type": "subnode"}, {"id": "Variational Inference", "label": "Variational Inference", "title": "<b>Variational Inference</b> (subnode)<hr>Technique used in machine learning to approximate posterior distributions over unobserved variables, especially useful when exact inference is computationally infeasible.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Technique used in machine learning to approximate posterior distributions over unobserved variables, especially useful when exact inference is computationally infeasible.", "node_type": "subnode"}, {"id": "Variational Auto-Encoder (VAE)", "label": "Variational Auto-Encoder (VAE)", "title": "<b>Variational Auto-Encoder (VAE)</b> (subnode)<hr>Type of generative model that uses variational inference to learn a latent variable model for a set of observed data.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Type of generative model that uses variational inference to learn a latent variable model for a set of observed data.", "node_type": "subnode"}, {"id": "ELBO", "label": "ELBO", "title": "<b>ELBO</b> (subnode)<hr>Evidence Lower Bound used to optimize variational inference.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Evidence Lower Bound used to optimize variational inference.", "node_type": "subnode"}, {"id": "Mean Field Assumption", "label": "Mean Field Assumption", "title": "<b>Mean Field Assumption</b> (subnode)<hr>Assumption that latent variables are independent, simplifying the optimization problem.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Assumption that latent variables are independent, simplifying the optimization problem.", "node_type": "subnode"}, {"id": "Discrete Latent Variables", "label": "Discrete Latent Variables", "title": "<b>Discrete Latent Variables</b> (subnode)<hr>Application of mean field assumption to discrete latent variable models.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Application of mean field assumption to discrete latent variable models.", "node_type": "subnode"}, {"id": "Continuous Latent Variables", "label": "Continuous Latent Variables", "title": "<b>Continuous Latent Variables</b> (subnode)<hr>Handling continuous variables requires additional techniques beyond mean field assumptions.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Handling continuous variables requires additional techniques beyond mean field assumptions.", "node_type": "subnode"}, {"id": "Re-parametrization Trick", "label": "Re-parametrization Trick", "title": "<b>Re-parametrization Trick</b> (subnode)<hr>Method to enable backpropagation through stochastic nodes.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Method to enable backpropagation through stochastic nodes.", "node_type": "subnode"}, {"id": "Gaussian Mixture Models", "label": "Gaussian Mixture Models", "title": "<b>Gaussian Mixture Models</b> (subnode)<hr>Model for clustering data into multiple Gaussian distributions.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Model for clustering data into multiple Gaussian distributions.", "node_type": "subnode"}, {"id": "Optimizing Continuous Latent Variables", "label": "Optimizing Continuous Latent Variables", "title": "<b>Optimizing Continuous Latent Variables</b> (major)<hr>Process of optimizing parameters for continuous latent variables in machine learning models.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Process of optimizing parameters for continuous latent variables in machine learning models.", "node_type": "major"}, {"id": "Succinct Representation of Distribution Qi", "label": "Succinct Representation of Distribution Qi", "title": "<b>Succinct Representation of Distribution Qi</b> (subnode)<hr>Using a Gaussian distribution to represent Qi succinctly over an infinite number of points.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Using a Gaussian distribution to represent Qi succinctly over an infinite number of points.", "node_type": "subnode"}, {"id": "Mean and Variance Functions", "label": "Mean and Variance Functions", "title": "<b>Mean and Variance Functions</b> (subnode)<hr>Functions q(x;phi) and v(x;psi) map from dimension d to k, parameterized by phi and psi.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Functions q(x;phi) and v(x;psi) map from dimension d to k, parameterized by phi and psi.", "node_type": "subnode"}, {"id": "Encoder-Decoder Framework", "label": "Encoder-Decoder Framework", "title": "<b>Encoder-Decoder Framework</b> (subnode)<hr>In variational auto-encoders, q and v are often neural networks acting as encoders, g(z;theta) as decoder.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "In variational auto-encoders, q and v are often neural networks acting as encoders, g(z;theta) as decoder.", "node_type": "subnode"}, {"id": "Efficient Evaluation of ELBO", "label": "Efficient Evaluation of ELBO", "title": "<b>Efficient Evaluation of ELBO</b> (subnode)<hr>Verification process to ensure efficient evaluation of Evidence Lower Bound for fixed Q.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Verification process to ensure efficient evaluation of Evidence Lower Bound for fixed Q.", "node_type": "subnode"}, {"id": "ELBO Optimization", "label": "ELBO Optimization", "title": "<b>ELBO Optimization</b> (subnode)<hr>Techniques for optimizing the Evidence Lower Bound (ELBO).", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Techniques for optimizing the Evidence Lower Bound (ELBO).", "node_type": "subnode"}, {"id": "Gradient Ascent in ELBO", "label": "Gradient Ascent in ELBO", "title": "<b>Gradient Ascent in ELBO</b> (subnode)<hr>Using gradient ascent for optimizing parameters in ELBO.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Using gradient ascent for optimizing parameters in ELBO.", "node_type": "subnode"}, {"id": "Gaussian Distributions", "label": "Gaussian Distributions", "title": "<b>Gaussian Distributions</b> (subnode)<hr>Utilizing Gaussian distributions to efficiently evaluate ELBO values.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Utilizing Gaussian distributions to efficiently evaluate ELBO values.", "node_type": "subnode"}, {"id": "Expectation Maximization (EM)", "label": "Expectation Maximization (EM)", "title": "<b>Expectation Maximization (EM)</b> (subnode)<hr>Technique for finding maximum likelihood or maximum a posteriori estimates of parameters in statistical models where the model depends on unobserved latent variables.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Technique for finding maximum likelihood or maximum a posteriori estimates of parameters in statistical models where the model depends on unobserved latent variables.", "node_type": "subnode"}, {"id": "Reparameterization Trick", "label": "Reparameterization Trick", "title": "<b>Reparameterization Trick</b> (subnode)<hr>Technique to compute gradients through stochastic variables by re-expressing them in a differentiable form.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Technique to compute gradients through stochastic variables by re-expressing them in a differentiable form.", "node_type": "subnode"}, {"id": "Data Normalization", "label": "Data Normalization", "title": "<b>Data Normalization</b> (major)<hr>Process of standardizing data attributes to ensure comparability and zero mean.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Process of standardizing data attributes to ensure comparability and zero mean.", "node_type": "major"}, {"id": "Mean Removal", "label": "Mean Removal", "title": "<b>Mean Removal</b> (subnode)<hr>Subtracting the mean from each feature to center the data around zero.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Subtracting the mean from each feature to center the data around zero.", "node_type": "subnode"}, {"id": "Variance Scaling", "label": "Variance Scaling", "title": "<b>Variance Scaling</b> (subnode)<hr>Dividing by standard deviation to ensure unit variance for comparability.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Dividing by standard deviation to ensure unit variance for comparability.", "node_type": "subnode"}, {"id": "Major Axis of Variation", "label": "Major Axis of Variation", "title": "<b>Major Axis of Variation</b> (major)<hr>Direction in which the data shows maximum variation after normalization.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Direction in which the data shows maximum variation after normalization.", "node_type": "major"}, {"id": "Projection and Variance Maximization", "label": "Projection and Variance Maximization", "title": "<b>Projection and Variance Maximization</b> (subnode)<hr>Finding unit vector u to maximize variance when data is projected onto it.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Finding unit vector u to maximize variance when data is projected onto it.", "node_type": "subnode"}, {"id": "Gradient Estimation", "label": "Gradient Estimation", "title": "<b>Gradient Estimation</b> (subnode)<hr>Process of estimating gradients for optimization in probabilistic models.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Process of estimating gradients for optimization in probabilistic models.", "node_type": "subnode"}, {"id": "Principal Components Analysis (PCA)", "label": "Principal Components Analysis (PCA)", "title": "<b>Principal Components Analysis (PCA)</b> (major)<hr>Dimensionality reduction technique that identifies the subspace where data approximately lies.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Dimensionality reduction technique that identifies the subspace where data approximately lies.", "node_type": "major"}, {"id": "Data Subspace Identification", "label": "Data Subspace Identification", "title": "<b>Data Subspace Identification</b> (subnode)<hr>Process of identifying a lower-dimensional space in which the data can be represented accurately.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Process of identifying a lower-dimensional space in which the data can be represented accurately.", "node_type": "subnode"}, {"id": "Data Redundancy Detection", "label": "Data Redundancy Detection", "title": "<b>Data Redundancy Detection</b> (major)<hr>Identifying and removing redundant data attributes.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Identifying and removing redundant data attributes.", "node_type": "major"}, {"id": "PCA Algorithm Introduction", "label": "PCA Algorithm Introduction", "title": "<b>PCA Algorithm Introduction</b> (subnode)<hr>Introduction to Principal Component Analysis for detecting redundancy.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Introduction to Principal Component Analysis for detecting redundancy.", "node_type": "subnode"}, {"id": "Normalization Process", "label": "Normalization Process", "title": "<b>Normalization Process</b> (subnode)<hr>Preprocessing step before PCA involving mean and variance adjustment.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Preprocessing step before PCA involving mean and variance adjustment.", "node_type": "subnode"}, {"id": "Car Example", "label": "Car Example", "title": "<b>Car Example</b> (subnode)<hr>Example illustrating linear dependency in car attributes.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Example illustrating linear dependency in car attributes.", "node_type": "subnode"}, {"id": "Pilot Survey Example", "label": "Pilot Survey Example", "title": "<b>Pilot Survey Example</b> (subnode)<hr>Survey data example showing correlation between piloting skill and enjoyment.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Survey data example showing correlation between piloting skill and enjoyment.", "node_type": "subnode"}, {"id": "Normalization Formula", "label": "Normalization Formula", "title": "<b>Normalization Formula</b> (subnode)<hr>Formula for normalizing features to have mean 0 and variance 1.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Formula for normalizing features to have mean 0 and variance 1.", "node_type": "subnode"}, {"id": "Principal Component Analysis (PCA)", "label": "Principal Component Analysis (PCA)", "title": "<b>Principal Component Analysis (PCA)</b> (major)<hr>Dimensionality reduction technique that transforms data into principal components.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Dimensionality reduction technique that transforms data into principal components.", "node_type": "major"}, {"id": "Projection of Data Points", "label": "Projection of Data Points", "title": "<b>Projection of Data Points</b> (subnode)<hr>Projecting data points onto a unit vector to reduce dimensionality.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Projecting data points onto a unit vector to reduce dimensionality.", "node_type": "subnode"}, {"id": "Variance Maximization", "label": "Variance Maximization", "title": "<b>Variance Maximization</b> (subnode)<hr>Maximizing the variance of projections for optimal direction selection.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Maximizing the variance of projections for optimal direction selection.", "node_type": "subnode"}, {"id": "Empirical Covariance Matrix", "label": "Empirical Covariance Matrix", "title": "<b>Empirical Covariance Matrix</b> (subnode)<hr>Matrix representing the covariance between data points.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Matrix representing the covariance between data points.", "node_type": "subnode"}, {"id": "Principal Eigenvector", "label": "Principal Eigenvector", "title": "<b>Principal Eigenvector</b> (subnode)<hr>Eigenvector corresponding to the largest eigenvalue of the covariance matrix.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Eigenvector corresponding to the largest eigenvalue of the covariance matrix.", "node_type": "subnode"}, {"id": "k-Dimensional Subspace", "label": "k-Dimensional Subspace", "title": "<b>k-Dimensional Subspace</b> (subnode)<hr>Projection of data into a lower-dimensional space using top k eigenvectors.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Projection of data into a lower-dimensional space using top k eigenvectors.", "node_type": "subnode"}, {"id": "PCA", "label": "PCA", "title": "<b>PCA</b> (major)<hr>Principal Component Analysis for dimensionality reduction", "shape": "star", "size": 25, "color": "#FF6347", "description": "Principal Component Analysis for dimensionality reduction", "node_type": "major"}, {"id": "Eigenvectors of Sigma", "label": "Eigenvectors of Sigma", "title": "<b>Eigenvectors of Sigma</b> (subnode)<hr>Top k eigenvectors used to form a new orthogonal basis", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Top k eigenvectors used to form a new orthogonal basis", "node_type": "subnode"}, {"id": "Dimensionality Reduction", "label": "Dimensionality Reduction", "title": "<b>Dimensionality Reduction</b> (subnode)<hr>Reduces data from d dimensions to k dimensions", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Reduces data from d dimensions to k dimensions", "node_type": "subnode"}, {"id": "Principal Components", "label": "Principal Components", "title": "<b>Principal Components</b> (subnode)<hr>First k eigenvectors of Sigma, representing the new basis", "shape": "dot", "size": 15, "color": "#4682B4", "description": "First k eigenvectors of Sigma, representing the new basis", "node_type": "subnode"}, {"id": "Approximation Error Minimization", "label": "Approximation Error Minimization", "title": "<b>Approximation Error Minimization</b> (subnode)<hr>Derivation based on minimizing error from projection onto k-dimensional subspace", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Derivation based on minimizing error from projection onto k-dimensional subspace", "node_type": "subnode"}, {"id": "Applications", "label": "Applications", "title": "<b>Applications</b> (subnode)<hr>Various uses including data compression and visualization", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Various uses including data compression and visualization", "node_type": "subnode"}, {"id": "Independent Component Analysis (ICA)", "label": "Independent Component Analysis (ICA)", "title": "<b>Independent Component Analysis (ICA)</b> (major)<hr>Technique for separating mixed signals into independent components.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Technique for separating mixed signals into independent components.", "node_type": "major"}, {"id": "Cocktail Party Problem", "label": "Cocktail Party Problem", "title": "<b>Cocktail Party Problem</b> (subnode)<hr>Motivating example involving separating mixed audio signals into individual sources.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Motivating example involving separating mixed audio signals into individual sources.", "node_type": "subnode"}, {"id": "Mixing Matrix (A)", "label": "Mixing Matrix (A)", "title": "<b>Mixing Matrix (A)</b> (subnode)<hr>Matrix representing the mixing process of original sources into observed data.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Matrix representing the mixing process of original sources into observed data.", "node_type": "subnode"}, {"id": "Unmixing Matrix (W)", "label": "Unmixing Matrix (W)", "title": "<b>Unmixing Matrix (W)</b> (subnode)<hr>Inverse matrix used to recover original sources from mixed signals.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Inverse matrix used to recover original sources from mixed signals.", "node_type": "subnode"}, {"id": "ICA Ambiguities", "label": "ICA Ambiguities", "title": "<b>ICA Ambiguities</b> (major)<hr>Discussion on the limitations and uncertainties in recovering the unmixing matrix W.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Discussion on the limitations and uncertainties in recovering the unmixing matrix W.", "node_type": "major"}, {"id": "Data Visualization", "label": "Data Visualization", "title": "<b>Data Visualization</b> (subnode)<hr>Plotting transformed data to identify clusters and similarities.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Plotting transformed data to identify clusters and similarities.", "node_type": "subnode"}, {"id": "Dimension Reduction", "label": "Dimension Reduction", "title": "<b>Dimension Reduction</b> (subnode)<hr>Reducing dataset dimensions for computational efficiency and overfitting prevention.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Reducing dataset dimensions for computational efficiency and overfitting prevention.", "node_type": "subnode"}, {"id": "Noise Reduction", "label": "Noise Reduction", "title": "<b>Noise Reduction</b> (subnode)<hr>Estimating intrinsic features from noisy data to improve signal clarity.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Estimating intrinsic features from noisy data to improve signal clarity.", "node_type": "subnode"}, {"id": "Eigenfaces Method", "label": "Eigenfaces Method", "title": "<b>Eigenfaces Method</b> (subnode)<hr>Applying PCA to face images for dimension reduction and noise removal.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Applying PCA to face images for dimension reduction and noise removal.", "node_type": "subnode"}, {"id": "Independent Components Analysis (ICA)", "label": "Independent Components Analysis (ICA)", "title": "<b>Independent Components Analysis (ICA)</b> (major)<hr>Technique for finding independent components in data, differing from PCA in its objectives.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Technique for finding independent components in data, differing from PCA in its objectives.", "node_type": "major"}, {"id": "Permutation Matrix", "label": "Permutation Matrix", "title": "<b>Permutation Matrix</b> (subnode)<hr>A matrix used to permute vectors", "shape": "dot", "size": 15, "color": "#4682B4", "description": "A matrix used to permute vectors", "node_type": "subnode"}, {"id": "Scaling Ambiguity", "label": "Scaling Ambiguity", "title": "<b>Scaling Ambiguity</b> (subnode)<hr>Ambiguity in scaling factors of sources and mixing matrix", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Ambiguity in scaling factors of sources and mixing matrix", "node_type": "subnode"}, {"id": "Sign Change Ambiguity", "label": "Sign Change Ambiguity", "title": "<b>Sign Change Ambiguity</b> (subnode)<hr>Ambiguity due to sign changes in source signals", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Ambiguity due to sign changes in source signals", "node_type": "subnode"}, {"id": "Density Transformation", "label": "Density Transformation", "title": "<b>Density Transformation</b> (major)<hr>Transformation of density functions under linear transformations.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Transformation of density functions under linear transformations.", "node_type": "major"}, {"id": "1D Example", "label": "1D Example", "title": "<b>1D Example</b> (subnode)<hr>Illustration with a 1-dimensional example.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Illustration with a 1-dimensional example.", "node_type": "subnode"}, {"id": "General Case", "label": "General Case", "title": "<b>General Case</b> (subnode)<hr>Extension to vector-valued distributions and general matrices.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Extension to vector-valued distributions and general matrices.", "node_type": "subnode"}, {"id": "ICA Algorithm", "label": "ICA Algorithm", "title": "<b>ICA Algorithm</b> (major)<hr>Derivation of an Independent Component Analysis algorithm based on maximum likelihood estimation.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Derivation of an Independent Component Analysis algorithm based on maximum likelihood estimation.", "node_type": "major"}, {"id": "Scaling Factor", "label": "Scaling Factor", "title": "<b>Scaling Factor</b> (subnode)<hr>Impact of scaling a speaker's speech signal by a positive factor.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Impact of scaling a speaker's speech signal by a positive factor.", "node_type": "subnode"}, {"id": "Sign Changes Irrelevance", "label": "Sign Changes Irrelevance", "title": "<b>Sign Changes Irrelevance</b> (subnode)<hr>Explanation that sign changes in the signal do not affect the outcome.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Explanation that sign changes in the signal do not affect the outcome.", "node_type": "subnode"}, {"id": "Non-Gaussian Sources", "label": "Non-Gaussian Sources", "title": "<b>Non-Gaussian Sources</b> (subnode)<hr>Sources are non-Gaussian, which resolves ambiguities.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Sources are non-Gaussian, which resolves ambiguities.", "node_type": "subnode"}, {"id": "Gaussian Data Example", "label": "Gaussian Data Example", "title": "<b>Gaussian Data Example</b> (subnode)<hr>Example with Gaussian data showing rotational symmetry and ambiguity.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Example with Gaussian data showing rotational symmetry and ambiguity.", "node_type": "subnode"}, {"id": "Mixing Matrix A", "label": "Mixing Matrix A", "title": "<b>Mixing Matrix A</b> (subnode)<hr>Introduction of mixing matrix A in the context of Gaussian data.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Introduction of mixing matrix A in the context of Gaussian data.", "node_type": "subnode"}, {"id": "Rotation Matrix R", "label": "Rotation Matrix R", "title": "<b>Rotation Matrix R</b> (subnode)<hr>Explanation of rotation matrix R and its properties.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Explanation of rotation matrix R and its properties.", "node_type": "subnode"}, {"id": "Mixed Data x'", "label": "Mixed Data x'", "title": "<b>Mixed Data x'</b> (subnode)<hr>Observation of mixed data under different mixing matrices A'.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Observation of mixed data under different mixing matrices A'.", "node_type": "subnode"}, {"id": "Mixing Matrix", "label": "Mixing Matrix", "title": "<b>Mixing Matrix</b> (subnode)<hr>Matrix representing the mixing process in ICA.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Matrix representing the mixing process in ICA.", "node_type": "subnode"}, {"id": "Gaussian Data Limitation", "label": "Gaussian Data Limitation", "title": "<b>Gaussian Data Limitation</b> (subnode)<hr>Limitations of ICA when data is Gaussian distributed.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Limitations of ICA when data is Gaussian distributed.", "node_type": "subnode"}, {"id": "Rotationally Symmetric Distributions", "label": "Rotationally Symmetric Distributions", "title": "<b>Rotationally Symmetric Distributions</b> (subnode)<hr>Property of multivariate standard normal distribution affecting ICA.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Property of multivariate standard normal distribution affecting ICA.", "node_type": "subnode"}, {"id": "Densities and Linear Transformations", "label": "Densities and Linear Transformations", "title": "<b>Densities and Linear Transformations</b> (major)<hr>Effect of linear transformations on probability densities in machine learning.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Effect of linear transformations on probability densities in machine learning.", "node_type": "major"}, {"id": "Linear Transformation Impact", "label": "Linear Transformation Impact", "title": "<b>Linear Transformation Impact</b> (subnode)<hr>Impact of linear transformations on the density function of random variables.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Impact of linear transformations on the density function of random variables.", "node_type": "subnode"}, {"id": "ICA Overview", "label": "ICA Overview", "title": "<b>ICA Overview</b> (major)<hr>Introduction to Independent Component Analysis concepts and principles.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Introduction to Independent Component Analysis concepts and principles.", "node_type": "major"}, {"id": "Joint Distribution of Sources", "label": "Joint Distribution of Sources", "title": "<b>Joint Distribution of Sources</b> (subnode)<hr>Modeling the joint distribution as a product of marginals for independent sources.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Modeling the joint distribution as a product of marginals for independent sources.", "node_type": "subnode"}, {"id": "Density on x=As=W^-1s", "label": "Density on x=As=W^-1s", "title": "<b>Density on x=As=W^-1s</b> (subnode)<hr>Deriving density function based on transformation from source to mixed signals.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Deriving density function based on transformation from source to mixed signals.", "node_type": "subnode"}, {"id": "Cumulative Distribution Function (CDF)", "label": "Cumulative Distribution Function (CDF)", "title": "<b>Cumulative Distribution Function (CDF)</b> (subnode)<hr>Definition and properties of CDF for real-valued random variables.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Definition and properties of CDF for real-valued random variables.", "node_type": "subnode"}, {"id": "Sigmoid Function as Default Density", "label": "Sigmoid Function as Default Density", "title": "<b>Sigmoid Function as Default Density</b> (subnode)<hr>Using sigmoid function to define density for sources in ICA due to its desirable properties.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Using sigmoid function to define density for sources in ICA due to its desirable properties.", "node_type": "subnode"}, {"id": "Data Preprocessing Assumptions", "label": "Data Preprocessing Assumptions", "title": "<b>Data Preprocessing Assumptions</b> (subnode)<hr>Assumption that data has zero mean or can be expected to have zero mean.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Assumption that data has zero mean or can be expected to have zero mean.", "node_type": "subnode"}, {"id": "Logistic Function Properties", "label": "Logistic Function Properties", "title": "<b>Logistic Function Properties</b> (subnode)<hr>Properties of the logistic function and its derivative.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Properties of the logistic function and its derivative.", "node_type": "subnode"}, {"id": "Log Likelihood Function", "label": "Log Likelihood Function", "title": "<b>Log Likelihood Function</b> (subnode)<hr>Function used to evaluate the performance of a model based on training data.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Function used to evaluate the performance of a model based on training data.", "node_type": "subnode"}, {"id": "Gradient Ascent Rule", "label": "Gradient Ascent Rule", "title": "<b>Gradient Ascent Rule</b> (subnode)<hr>Rule for updating parameters during training using gradient ascent method.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Rule for updating parameters during training using gradient ascent method.", "node_type": "subnode"}, {"id": "Training Example Independence Assumption", "label": "Training Example Independence Assumption", "title": "<b>Training Example Independence Assumption</b> (subnode)<hr>Assumption that training examples are independent of each other, and its implications on model performance.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Assumption that training examples are independent of each other, and its implications on model performance.", "node_type": "subnode"}, {"id": "Stochastic Gradient Ascent", "label": "Stochastic Gradient Ascent", "title": "<b>Stochastic Gradient Ascent</b> (subnode)<hr>Optimization technique used for minimizing loss functions in machine learning models.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Optimization technique used for minimizing loss functions in machine learning models.", "node_type": "subnode"}, {"id": "Self-supervised Learning", "label": "Self-supervised Learning", "title": "<b>Self-supervised Learning</b> (major)<hr>Learning paradigm where the model learns from unlabeled data using self-generated supervisory signals.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Learning paradigm where the model learns from unlabeled data using self-generated supervisory signals.", "node_type": "major"}, {"id": "Foundation Models", "label": "Foundation Models", "title": "<b>Foundation Models</b> (subnode)<hr>Large-scale models pre-trained on broad datasets and adaptable to various downstream tasks.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Large-scale models pre-trained on broad datasets and adaptable to various downstream tasks.", "node_type": "subnode"}, {"id": "Pretraining Phase", "label": "Pretraining Phase", "title": "<b>Pretraining Phase</b> (subnode)<hr>Training a model on an unlabeled dataset to learn general representations.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Training a model on an unlabeled dataset to learn general representations.", "node_type": "subnode"}, {"id": "Adaptation Phase", "label": "Adaptation Phase", "title": "<b>Adaptation Phase</b> (subnode)<hr>Phase where the pre-trained model is fine-tuned for specific tasks with limited labeled data.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Phase where the pre-trained model is fine-tuned for specific tasks with limited labeled data.", "node_type": "subnode"}, {"id": "Transfer Learning", "label": "Transfer Learning", "title": "<b>Transfer Learning</b> (subnode)<hr>Using a pre-trained model as the starting point for fine-tuning on a new task.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Using a pre-trained model as the starting point for fine-tuning on a new task.", "node_type": "subnode"}, {"id": "Unlabeled Dataset", "label": "Unlabeled Dataset", "title": "<b>Unlabeled Dataset</b> (subnode)<hr>Dataset used during pretraining phase, typically large and unlabeled.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Dataset used during pretraining phase, typically large and unlabeled.", "node_type": "subnode"}, {"id": "Labeled Task Dataset", "label": "Labeled Task Dataset", "title": "<b>Labeled Task Dataset</b> (subnode)<hr>Dataset with labeled data for fine-tuning the model on specific tasks.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Dataset with labeled data for fine-tuning the model on specific tasks.", "node_type": "subnode"}, {"id": "Pretrained Model", "label": "Pretrained Model", "title": "<b>Pretrained Model</b> (subnode)<hr>Model trained in pretraining phase, used as a starting point for adaptation.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Model trained in pretraining phase, used as a starting point for adaptation.", "node_type": "subnode"}, {"id": "Self-Supervised Loss", "label": "Self-Supervised Loss", "title": "<b>Self-Supervised Loss</b> (subnode)<hr>Loss function that uses the data itself to provide supervision during pretraining.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Loss function that uses the data itself to provide supervision during pretraining.", "node_type": "subnode"}, {"id": "Machine Learning Adaptation Methods", "label": "Machine Learning Adaptation Methods", "title": "<b>Machine Learning Adaptation Methods</b> (major)<hr>Overview of methods for adapting machine learning models to new tasks.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Overview of methods for adapting machine learning models to new tasks.", "node_type": "major"}, {"id": "Labeled Dataset", "label": "Labeled Dataset", "title": "<b>Labeled Dataset</b> (subnode)<hr>Dataset used in downstream tasks with labeled examples.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Dataset used in downstream tasks with labeled examples.", "node_type": "subnode"}, {"id": "Zero-Shot Learning", "label": "Zero-Shot Learning", "title": "<b>Zero-Shot Learning</b> (subnode)<hr>Scenario where no labeled data is available for the task.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Scenario where no labeled data is available for the task.", "node_type": "subnode"}, {"id": "Few-Shhot Learning", "label": "Few-Shhot Learning", "title": "<b>Few-Shhot Learning</b> (subnode)<hr>Situation with a small number of labeled examples (1-50).", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Situation with a small number of labeled examples (1-50).", "node_type": "subnode"}, {"id": "Adaptation Algorithm", "label": "Adaptation Algorithm", "title": "<b>Adaptation Algorithm</b> (subnode)<hr>Algorithm that takes in a downstream dataset and pretrained model to output an adapted model.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Algorithm that takes in a downstream dataset and pretrained model to output an adapted model.", "node_type": "subnode"}, {"id": "Linear Probe Approach", "label": "Linear Probe Approach", "title": "<b>Linear Probe Approach</b> (subnode)<hr>Uses a linear head on top of the representation for prediction without modifying the pretrained model.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Uses a linear head on top of the representation for prediction without modifying the pretrained model.", "node_type": "subnode"}, {"id": "Finetuning Algorithm", "label": "Finetuning Algorithm", "title": "<b>Finetuning Algorithm</b> (subnode)<hr>Further finetunes the pretrained model along with the downstream prediction model.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Further finetunes the pretrained model along with the downstream prediction model.", "node_type": "subnode"}, {"id": "Language Problem Methods", "label": "Language Problem Methods", "title": "<b>Language Problem Methods</b> (subnode)<hr>Specific methods for language problems introduced in 14.3.2.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Specific methods for language problems introduced in 14.3.2.", "node_type": "subnode"}, {"id": "Self-Supervised Learning", "label": "Self-Supervised Learning", "title": "<b>Self-Supervised Learning</b> (major)<hr>Training models using only unlabeled data.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Training models using only unlabeled data.", "node_type": "major"}, {"id": "Representation Function", "label": "Representation Function", "title": "<b>Representation Function</b> (subnode)<hr>Maps semantically similar images to similar representations.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Maps semantically similar images to similar representations.", "node_type": "subnode"}, {"id": "Supervised Contrastive Algorithms", "label": "Supervised Contrastive Algorithms", "title": "<b>Supervised Contrastive Algorithms</b> (subnode)<hr>Works well with labeled pretraining datasets.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Works well with labeled pretraining datasets.", "node_type": "subnode"}, {"id": "Data Augmentation", "label": "Data Augmentation", "title": "<b>Data Augmentation</b> (subnode)<hr>Generates pairs of augmented images from the same original image.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Generates pairs of augmented images from the same original image.", "node_type": "subnode"}, {"id": "Positive Pair", "label": "Positive Pair", "title": "<b>Positive Pair</b> (subnode)<hr>Pair of samples that are semantically similar in the dataset.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Pair of samples that are semantically similar in the dataset.", "node_type": "subnode"}, {"id": "Negative Pair", "label": "Negative Pair", "title": "<b>Negative Pair</b> (subnode)<hr>Randomly selected augmented images from different original images, not necessarily semantically related.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Randomly selected augmented images from different original images, not necessarily semantically related.", "node_type": "subnode"}, {"id": "Finetuning Pretrained Models", "label": "Finetuning Pretrained Models", "title": "<b>Finetuning Pretrained Models</b> (subnode)<hr>Process of adjusting pretrained model parameters for specific tasks.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Process of adjusting pretrained model parameters for specific tasks.", "node_type": "subnode"}, {"id": "Prediction Model Structure", "label": "Prediction Model Structure", "title": "<b>Prediction Model Structure</b> (subnode)<hr>Structure involving both fixed and trainable parts of the model.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Structure involving both fixed and trainable parts of the model.", "node_type": "subnode"}, {"id": "Optimization Goal", "label": "Optimization Goal", "title": "<b>Optimization Goal</b> (subnode)<hr>Objective function to minimize for fitting downstream data.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Objective function to minimize for fitting downstream data.", "node_type": "subnode"}, {"id": "Pretraining Methods in CV", "label": "Pretraining Methods in CV", "title": "<b>Pretraining Methods in CV</b> (major)<hr>Techniques used to pretrain models specifically for computer vision tasks.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Techniques used to pretrain models specifically for computer vision tasks.", "node_type": "major"}, {"id": "Supervised Pretraining", "label": "Supervised Pretraining", "title": "<b>Supervised Pretraining</b> (subnode)<hr>Training with labeled data to initialize model parameters.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Training with labeled data to initialize model parameters.", "node_type": "subnode"}, {"id": "Contrastive Learning", "label": "Contrastive Learning", "title": "<b>Contrastive Learning</b> (subnode)<hr>Self-supervised learning using unlabeled data to find similar image representations.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Self-supervised learning using unlabeled data to find similar image representations.", "node_type": "subnode"}, {"id": "Loss Function Analysis", "label": "Loss Function Analysis", "title": "<b>Loss Function Analysis</b> (subnode)<hr>Analysis of loss functions and their impact on model behavior.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Analysis of loss functions and their impact on model behavior.", "node_type": "subnode"}, {"id": "Pretrained Large Language Models", "label": "Pretrained Large Language Models", "title": "<b>Pretrained Large Language Models</b> (major)<hr>Overview of pretraining models in natural language processing.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Overview of pretraining models in natural language processing.", "node_type": "major"}, {"id": "Language Model Probability Distribution", "label": "Language Model Probability Distribution", "title": "<b>Language Model Probability Distribution</b> (subnode)<hr>Explanation of the probability distribution used in language modeling.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Explanation of the probability distribution used in language modeling.", "node_type": "subnode"}, {"id": "SIMCLR Algorithm", "label": "SIMCLR Algorithm", "title": "<b>SIMCLR Algorithm</b> (subnode)<hr>Specific algorithm based on contrastive learning principle introduced in 2020.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Specific algorithm based on contrastive learning principle introduced in 2020.", "node_type": "subnode"}, {"id": "Augmentation Techniques", "label": "Augmentation Techniques", "title": "<b>Augmentation Techniques</b> (subnode)<hr>Methods for creating variations of input data to improve model robustness.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Methods for creating variations of input data to improve model robustness.", "node_type": "subnode"}, {"id": "Conditional Probability Modeling", "label": "Conditional Probability Modeling", "title": "<b>Conditional Probability Modeling</b> (subnode)<hr>Modeling the probability of an event given that another event has occurred.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Modeling the probability of an event given that another event has occurred.", "node_type": "subnode"}, {"id": "Parameterized Model", "label": "Parameterized Model", "title": "<b>Parameterized Model</b> (subnode)<hr>A model where parameters are used to adjust predictions based on input data.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "A model where parameters are used to adjust predictions based on input data.", "node_type": "subnode"}, {"id": "Embeddings and Representations", "label": "Embeddings and Representations", "title": "<b>Embeddings and Representations</b> (subnode)<hr>Numerical representations of categorical variables, such as words.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Numerical representations of categorical variables, such as words.", "node_type": "subnode"}, {"id": "Transformer Model", "label": "Transformer Model", "title": "<b>Transformer Model</b> (subnode)<hr>A model architecture for handling sequence data with self-attention mechanisms.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "A model architecture for handling sequence data with self-attention mechanisms.", "node_type": "subnode"}, {"id": "Input-Output Interface", "label": "Input-Output Interface", "title": "<b>Input-Output Interface</b> (subnode)<hr>The way input sequences are transformed into output sequences using embeddings and a blackbox function.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "The way input sequences are transformed into output sequences using embeddings and a blackbox function.", "node_type": "subnode"}, {"id": "Training Process", "label": "Training Process", "title": "<b>Training Process</b> (subnode)<hr>Involves minimizing the negative log-likelihood of data under a probabilistic model.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Involves minimizing the negative log-likelihood of data under a probabilistic model.", "node_type": "subnode"}, {"id": "Autoregressive Text Decoding", "label": "Autoregressive Text Decoding", "title": "<b>Autoregressive Text Decoding</b> (major)<hr>Process of generating text sequentially using a trained Transformer model.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Process of generating text sequentially using a trained Transformer model.", "node_type": "major"}, {"id": "Machine Learning Adaptation Techniques", "label": "Machine Learning Adaptation Techniques", "title": "<b>Machine Learning Adaptation Techniques</b> (major)<hr>Techniques for adapting machine learning models to new tasks or domains.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Techniques for adapting machine learning models to new tasks or domains.", "node_type": "major"}, {"id": "Zero-shot Learning", "label": "Zero-shot Learning", "title": "<b>Zero-shot Learning</b> (subnode)<hr>Adapting a model to perform tasks without any input-output pairs from the task.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Adapting a model to perform tasks without any input-output pairs from the task.", "node_type": "subnode"}, {"id": "In-context Learning", "label": "In-context Learning", "title": "<b>In-context Learning</b> (subnode)<hr>Learning from a small set of examples provided in the context.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Learning from a small set of examples provided in the context.", "node_type": "subnode"}, {"id": "Language Model Utilization", "label": "Language Model Utilization", "title": "<b>Language Model Utilization</b> (subnode)<hr>Methods to decode answers from language models in zero-shot settings.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Methods to decode answers from language models in zero-shot settings.", "node_type": "subnode"}, {"id": "Prompt Construction", "label": "Prompt Construction", "title": "<b>Prompt Construction</b> (subnode)<hr>Creating prompts by concatenating labeled examples and test data for model generation.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Creating prompts by concatenating labeled examples and test data for model generation.", "node_type": "subnode"}, {"id": "Reinforcement Learning", "label": "Reinforcement Learning", "title": "<b>Reinforcement Learning</b> (major)<hr>Field of machine learning concerned with how software agents ought to take actions in an environment to maximize some notion of cumulative reward.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Field of machine learning concerned with how software agents ought to take actions in an environment to maximize some notion of cumulative reward.", "node_type": "major"}, {"id": "Sequential Decision Making", "label": "Sequential Decision Making", "title": "<b>Sequential Decision Making</b> (subnode)<hr>Decision making in sequences without explicit supervision.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Decision making in sequences without explicit supervision.", "node_type": "subnode"}, {"id": "Reward Function", "label": "Reward Function", "title": "<b>Reward Function</b> (subnode)<hr>Function that assigns a scalar value to each possible input or state-action pair.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Function that assigns a scalar value to each possible input or state-action pair.", "node_type": "subnode"}, {"id": "Language Models", "label": "Language Models", "title": "<b>Language Models</b> (subnode)<hr>Models that generate text based on learned patterns from large datasets.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Models that generate text based on learned patterns from large datasets.", "node_type": "subnode"}, {"id": "Conditional Probability in Language Models", "label": "Conditional Probability in Language Models", "title": "<b>Conditional Probability in Language Models</b> (subnode)<hr>Probability of generating the next token given previous tokens.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Probability of generating the next token given previous tokens.", "node_type": "subnode"}, {"id": "Temperature Parameter", "label": "Temperature Parameter", "title": "<b>Temperature Parameter</b> (subnode)<hr>Parameter to adjust the randomness or determinism of generated text.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Parameter to adjust the randomness or determinism of generated text.", "node_type": "subnode"}, {"id": "Adaptation Methods", "label": "Adaptation Methods", "title": "<b>Adaptation Methods</b> (subnode)<hr>Techniques for adapting pretrained models to new tasks without additional training data.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Techniques for adapting pretrained models to new tasks without additional training data.", "node_type": "subnode"}, {"id": "Finetuning", "label": "Finetuning", "title": "<b>Finetuning</b> (subnode)<hr>Adjusting model parameters based on specific task data.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Adjusting model parameters based on specific task data.", "node_type": "subnode"}, {"id": "Policy Execution", "label": "Policy Execution", "title": "<b>Policy Execution</b> (major)<hr>Process of selecting actions based on a policy in given states.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Process of selecting actions based on a policy in given states.", "node_type": "major"}, {"id": "Value Function", "label": "Value Function", "title": "<b>Value Function</b> (subnode)<hr>Function that calculates the expected sum of discounted rewards for a given state under a policy.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Function that calculates the expected sum of discounted rewards for a given state under a policy.", "node_type": "subnode"}, {"id": "Bellman Equations", "label": "Bellman Equations", "title": "<b>Bellman Equations</b> (subnode)<hr>Set of equations used to solve for the value function in an MDP.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Set of equations used to solve for the value function in an MDP.", "node_type": "subnode"}, {"id": "Immediate Reward", "label": "Immediate Reward", "title": "<b>Immediate Reward</b> (subnode)<hr>Reward received immediately upon entering a state.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Reward received immediately upon entering a state.", "node_type": "subnode"}, {"id": "Optimal Value Function", "label": "Optimal Value Function", "title": "<b>Optimal Value Function</b> (subnode)<hr>Finding the optimal value function in a finite-horizon setting.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Finding the optimal value function in a finite-horizon setting.", "node_type": "subnode"}, {"id": "Bellman's Equation", "label": "Bellman's Equation", "title": "<b>Bellman's Equation</b> (subnode)<hr>Use of Bellman's equation for solving dynamic programming problems.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Use of Bellman's equation for solving dynamic programming problems.", "node_type": "subnode"}, {"id": "Policy", "label": "Policy", "title": "<b>Policy</b> (major)<hr>Strategy that defines an action for each state in a given environment.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Strategy that defines an action for each state in a given environment.", "node_type": "major"}, {"id": "Optimal Policy", "label": "Optimal Policy", "title": "<b>Optimal Policy</b> (subnode)<hr>Strategy that maximizes the expected cumulative reward over time, which can be stationary or non-stationary depending on the MDP type.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Strategy that maximizes the expected cumulative reward over time, which can be stationary or non-stationary depending on the MDP type.", "node_type": "subnode"}, {"id": "Markov Decision Processes (MDP)", "label": "Markov Decision Processes (MDP)", "title": "<b>Markov Decision Processes (MDP)</b> (subnode)<hr>Models decision-making scenarios where outcomes are partly random and partly under the control of a decision maker.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Models decision-making scenarios where outcomes are partly random and partly under the control of a decision maker.", "node_type": "subnode"}, {"id": "States", "label": "States", "title": "<b>States</b> (subnode)<hr>Set of all possible conditions or configurations in an environment.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Set of all possible conditions or configurations in an environment.", "node_type": "subnode"}, {"id": "Actions", "label": "Actions", "title": "<b>Actions</b> (subnode)<hr>Set of all possible actions that can be taken from a given state.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Set of all possible actions that can be taken from a given state.", "node_type": "subnode"}, {"id": "State Transition Probabilities", "label": "State Transition Probabilities", "title": "<b>State Transition Probabilities</b> (subnode)<hr>Knowledge of state transition probabilities in the context of solving MDPs.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Knowledge of state transition probabilities in the context of solving MDPs.", "node_type": "subnode"}, {"id": "Discount Factor", "label": "Discount Factor", "title": "<b>Discount Factor</b> (subnode)<hr>Parameter that determines the present value of future rewards in reinforcement learning.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Parameter that determines the present value of future rewards in reinforcement learning.", "node_type": "subnode"}, {"id": "Reinforcement Learning Overview", "label": "Reinforcement Learning Overview", "title": "<b>Reinforcement Learning Overview</b> (major)<hr>Introduction to reinforcement learning concepts and processes.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Introduction to reinforcement learning concepts and processes.", "node_type": "major"}, {"id": "Markov Decision Process (MDP)", "label": "Markov Decision Process (MDP)", "title": "<b>Markov Decision Process (MDP)</b> (subnode)<hr>A framework for modeling decision making in situations where outcomes are partly random and partly under the control of a decision maker.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "A framework for modeling decision making in situations where outcomes are partly random and partly under the control of a decision maker.", "node_type": "subnode"}, {"id": "State Transition", "label": "State Transition", "title": "<b>State Transition</b> (subnode)<hr>The process by which states change based on actions taken.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "The process by which states change based on actions taken.", "node_type": "subnode"}, {"id": "Total Payoff Calculation", "label": "Total Payoff Calculation", "title": "<b>Total Payoff Calculation</b> (subnode)<hr>Calculation of total rewards over time, considering discount factors.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Calculation of total rewards over time, considering discount factors.", "node_type": "subnode"}, {"id": "Discount Factor (\u03b3)", "label": "Discount Factor (\u03b3)", "title": "<b>Discount Factor (\u03b3)</b> (subnode)<hr>A factor used to discount future rewards based on their temporal distance from the present.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "A factor used to discount future rewards based on their temporal distance from the present.", "node_type": "subnode"}, {"id": "Policy Definition", "label": "Policy Definition", "title": "<b>Policy Definition</b> (subnode)<hr>Definition of a policy as a function mapping states to actions.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Definition of a policy as a function mapping states to actions.", "node_type": "subnode"}, {"id": "Markov Decision Processes (MDPs)", "label": "Markov Decision Processes (MDPs)", "title": "<b>Markov Decision Processes (MDPs)</b> (subnode)<hr>Models for decision-making problems under uncertainty.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Models for decision-making problems under uncertainty.", "node_type": "subnode"}, {"id": "Value Iteration", "label": "Value Iteration", "title": "<b>Value Iteration</b> (subnode)<hr>Algorithm for finding optimal policies in reinforcement learning by iteratively updating value functions.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Algorithm for finding optimal policies in reinforcement learning by iteratively updating value functions.", "node_type": "subnode"}, {"id": "Policy Iteration", "label": "Policy Iteration", "title": "<b>Policy Iteration</b> (subnode)<hr>Alternative algorithm for finding optimal policies in MDPs through successive policy evaluations and improvements.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Alternative algorithm for finding optimal policies in MDPs through successive policy evaluations and improvements.", "node_type": "subnode"}, {"id": "Learning Model for MDPs", "label": "Learning Model for MDPs", "title": "<b>Learning Model for MDPs</b> (subnode)<hr>Estimating transition probabilities and rewards from data in MDP problems.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Estimating transition probabilities and rewards from data in MDP problems.", "node_type": "subnode"}, {"id": "Inverted Pendulum Problem", "label": "Inverted Pendulum Problem", "title": "<b>Inverted Pendulum Problem</b> (subnode)<hr>Example problem used to illustrate learning models in MDPs.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Example problem used to illustrate learning models in MDPs.", "node_type": "subnode"}, {"id": "State Transition Probabilities Estimation", "label": "State Transition Probabilities Estimation", "title": "<b>State Transition Probabilities Estimation</b> (subnode)<hr>Method of estimating transition probabilities from observed state-action pairs and outcomes.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Method of estimating transition probabilities from observed state-action pairs and outcomes.", "node_type": "subnode"}, {"id": "Optimal Policy in MDPs", "label": "Optimal Policy in MDPs", "title": "<b>Optimal Policy in MDPs</b> (subnode)<hr>Discussion on the existence of a single optimal policy for all states in an MDP.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Discussion on the existence of a single optimal policy for all states in an MDP.", "node_type": "subnode"}, {"id": "Value Iteration and Policy Iteration", "label": "Value Iteration and Policy Iteration", "title": "<b>Value Iteration and Policy Iteration</b> (subnode)<hr>Efficient algorithms for solving finite-state Markov Decision Processes (MDPs).", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Efficient algorithms for solving finite-state Markov Decision Processes (MDPs).", "node_type": "subnode"}, {"id": "Finite-State MDPs", "label": "Finite-State MDPs", "title": "<b>Finite-State MDPs</b> (subnode)<hr>Consideration of MDPs with a finite number of states and actions.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Consideration of MDPs with a finite number of states and actions.", "node_type": "subnode"}, {"id": "Value Iteration Algorithm", "label": "Value Iteration Algorithm", "title": "<b>Value Iteration Algorithm</b> (subnode)<hr>Algorithm for computing the optimal value function using iterative methods.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Algorithm for computing the optimal value function using iterative methods.", "node_type": "subnode"}, {"id": "Synchronous Updates", "label": "Synchronous Updates", "title": "<b>Synchronous Updates</b> (subnode)<hr>Method of updating all state values simultaneously before applying them.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Method of updating all state values simultaneously before applying them.", "node_type": "subnode"}, {"id": "Asynchronous Updates", "label": "Asynchronous Updates", "title": "<b>Asynchronous Updates</b> (subnode)<hr>Technique for updating state values one at a time in sequence.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Technique for updating state values one at a time in sequence.", "node_type": "subnode"}, {"id": "Convergence of Value Functions", "label": "Convergence of Value Functions", "title": "<b>Convergence of Value Functions</b> (subnode)<hr>Process by which value functions approach the optimal values in iterative algorithms.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Process by which value functions approach the optimal values in iterative algorithms.", "node_type": "subnode"}, {"id": "Optimal Policy Determination", "label": "Optimal Policy Determination", "title": "<b>Optimal Policy Determination</b> (subnode)<hr>Finding the best policy given an MDP, often through value or policy iteration.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Finding the best policy given an MDP, often through value or policy iteration.", "node_type": "subnode"}, {"id": "Bellman's Equations", "label": "Bellman's Equations", "title": "<b>Bellman's Equations</b> (subnode)<hr>Set of equations used to determine optimal policies in decision-making processes.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Set of equations used to determine optimal policies in decision-making processes.", "node_type": "subnode"}, {"id": "Greedy Policy with Respect to V", "label": "Greedy Policy with Respect to V", "title": "<b>Greedy Policy with Respect to V</b> (subnode)<hr>Policy derived from the current value function that maximizes expected rewards.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Policy derived from the current value function that maximizes expected rewards.", "node_type": "subnode"}, {"id": "Comparison Between Algorithms", "label": "Comparison Between Algorithms", "title": "<b>Comparison Between Algorithms</b> (subnode)<hr>Discussion on pros and cons of different algorithms for solving MDPs.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Discussion on pros and cons of different algorithms for solving MDPs.", "node_type": "subnode"}, {"id": "Estimating State Transitions", "label": "Estimating State Transitions", "title": "<b>Estimating State Transitions</b> (subnode)<hr>Methods for estimating transition probabilities based on observed data.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Methods for estimating transition probabilities based on observed data.", "node_type": "subnode"}, {"id": "Expected Immediate Reward", "label": "Expected Immediate Reward", "title": "<b>Expected Immediate Reward</b> (subnode)<hr>The expected reward received when an agent is in a particular state.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "The expected reward received when an agent is in a particular state.", "node_type": "subnode"}, {"id": "Learning in MDPs with Unknown Transitions", "label": "Learning in MDPs with Unknown Transitions", "title": "<b>Learning in MDPs with Unknown Transitions</b> (subnode)<hr>Approach for learning policies when state transition probabilities are not known.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Approach for learning policies when state transition probabilities are not known.", "node_type": "subnode"}, {"id": "Continuous State Space", "label": "Continuous State Space", "title": "<b>Continuous State Space</b> (major)<hr>Extension of MDPs to handle continuous states rather than discrete ones.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Extension of MDPs to handle continuous states rather than discrete ones.", "node_type": "major"}, {"id": "Continuous State MDPs", "label": "Continuous State MDPs", "title": "<b>Continuous State MDPs</b> (subnode)<hr>MDPs with infinite state spaces, such as car or helicopter states.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "MDPs with infinite state spaces, such as car or helicopter states.", "node_type": "subnode"}, {"id": "Finite State MDPs", "label": "Finite State MDPs", "title": "<b>Finite State MDPs</b> (subnode)<hr>MDPs with a finite number of states, contrasting with continuous ones.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "MDPs with a finite number of states, contrasting with continuous ones.", "node_type": "subnode"}, {"id": "Model Creation Methods", "label": "Model Creation Methods", "title": "<b>Model Creation Methods</b> (major)<hr>Different methods to obtain a model for state transitions in machine learning.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Different methods to obtain a model for state transitions in machine learning.", "node_type": "major"}, {"id": "Physics Simulation", "label": "Physics Simulation", "title": "<b>Physics Simulation</b> (subnode)<hr>Using physical laws or software packages to simulate system behavior.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Using physical laws or software packages to simulate system behavior.", "node_type": "subnode"}, {"id": "Open Dynamics Engine", "label": "Open Dynamics Engine", "title": "<b>Open Dynamics Engine</b> (subnode)<hr>Free/open-source physics simulator for simulating mechanical systems.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Free/open-source physics simulator for simulating mechanical systems.", "node_type": "subnode"}, {"id": "Learning from Data", "label": "Learning from Data", "title": "<b>Learning from Data</b> (subnode)<hr>Inferring state transition probabilities from collected data in an MDP.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Inferring state transition probabilities from collected data in an MDP.", "node_type": "subnode"}, {"id": "Discretization in MDPs", "label": "Discretization in MDPs", "title": "<b>Discretization in MDPs</b> (major)<hr>Process of converting continuous state space into discrete states for easier computation.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Process of converting continuous state space into discrete states for easier computation.", "node_type": "major"}, {"id": "Supervised Learning Problem", "label": "Supervised Learning Problem", "title": "<b>Supervised Learning Problem</b> (subnode)<hr>Example of fitting a function using linear regression and piecewise constant representation.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Example of fitting a function using linear regression and piecewise constant representation.", "node_type": "subnode"}, {"id": "Piecewise Constant Representation", "label": "Piecewise Constant Representation", "title": "<b>Piecewise Constant Representation</b> (subnode)<hr>Representation that assumes value is constant within each discretized interval.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Representation that assumes value is constant within each discretized interval.", "node_type": "subnode"}, {"id": "Curse of Dimensionality", "label": "Curse of Dimensionality", "title": "<b>Curse of Dimensionality</b> (subnode)<hr>Problem where the volume of the state space increases exponentially with dimensionality, making it hard to represent accurately.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Problem where the volume of the state space increases exponentially with dimensionality, making it hard to represent accurately.", "node_type": "subnode"}, {"id": "State Representation", "label": "State Representation", "title": "<b>State Representation</b> (subnode)<hr>Methods for representing states in machine learning problems.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Methods for representing states in machine learning problems.", "node_type": "subnode"}, {"id": "Value Function Approximation", "label": "Value Function Approximation", "title": "<b>Value Function Approximation</b> (major)<hr>Approximating the value function using supervised learning methods such as linear regression.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Approximating the value function using supervised learning methods such as linear regression.", "node_type": "major"}, {"id": "Model or Simulator", "label": "Model or Simulator", "title": "<b>Model or Simulator</b> (subnode)<hr>Black-box model providing next-state transitions based on current state and action.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Black-box model providing next-state transitions based on current state and action.", "node_type": "subnode"}, {"id": "Linear Model Prediction", "label": "Linear Model Prediction", "title": "<b>Linear Model Prediction</b> (subnode)<hr>Predicting the next state using a linear model based on current state and action.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Predicting the next state using a linear model based on current state and action.", "node_type": "subnode"}, {"id": "Learning Algorithm", "label": "Learning Algorithm", "title": "<b>Learning Algorithm</b> (subnode)<hr>Algorithm used to estimate parameters A and B in the linear prediction model.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Algorithm used to estimate parameters A and B in the linear prediction model.", "node_type": "subnode"}, {"id": "Deterministic vs Stochastic Models", "label": "Deterministic vs Stochastic Models", "title": "<b>Deterministic vs Stochastic Models</b> (subnode)<hr>Comparison between deterministic and stochastic models for predicting next states.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Comparison between deterministic and stochastic models for predicting next states.", "node_type": "subnode"}, {"id": "Non-linear Functions", "label": "Non-linear Functions", "title": "<b>Non-linear Functions</b> (subnode)<hr>Use of non-linear functions in state transition prediction models.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Use of non-linear functions in state transition prediction models.", "node_type": "subnode"}, {"id": "Fitted Value Iteration", "label": "Fitted Value Iteration", "title": "<b>Fitted Value Iteration</b> (subnode)<hr>Algorithm for approximating the value function through iterative sampling and regression.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Algorithm for approximating the value function through iterative sampling and regression.", "node_type": "subnode"}, {"id": "Regression Algorithms", "label": "Regression Algorithms", "title": "<b>Regression Algorithms</b> (subnode)<hr>Techniques to predict continuous outcomes from input data.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Techniques to predict continuous outcomes from input data.", "node_type": "subnode"}, {"id": "Non-linear Feature Mappings", "label": "Non-linear Feature Mappings", "title": "<b>Non-linear Feature Mappings</b> (subnode)<hr>Feature mappings that transform states and actions into non-linear features.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Feature mappings that transform states and actions into non-linear features.", "node_type": "subnode"}, {"id": "Discrete Action Space", "label": "Discrete Action Space", "title": "<b>Discrete Action Space</b> (subnode)<hr>Assumption of a small, discrete set of actions available at each state.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Assumption of a small, discrete set of actions available at each state.", "node_type": "subnode"}, {"id": "Supervised Learning Algorithm", "label": "Supervised Learning Algorithm", "title": "<b>Supervised Learning Algorithm</b> (subnode)<hr>Use of linear or non-linear regression to approximate the value function based on state features.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Use of linear or non-linear regression to approximate the value function based on state features.", "node_type": "subnode"}, {"id": "State Sampling", "label": "State Sampling", "title": "<b>State Sampling</b> (subnode)<hr>Random sampling of states for value function approximation.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Random sampling of states for value function approximation.", "node_type": "subnode"}, {"id": "Action Evaluation", "label": "Action Evaluation", "title": "<b>Action Evaluation</b> (subnode)<hr>Evaluation of actions based on expected future rewards and state transitions.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Evaluation of actions based on expected future rewards and state transitions.", "node_type": "subnode"}, {"id": "Expectation Approximation", "label": "Expectation Approximation", "title": "<b>Expectation Approximation</b> (subnode)<hr>Techniques for estimating expectations in reinforcement learning algorithms, such as sampling and deterministic noise removal.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Techniques for estimating expectations in reinforcement learning algorithms, such as sampling and deterministic noise removal.", "node_type": "subnode"}, {"id": "Deterministic Simulators", "label": "Deterministic Simulators", "title": "<b>Deterministic Simulators</b> (subnode)<hr>Simulations where the next state is determined solely by current state and action without random noise.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Simulations where the next state is determined solely by current state and action without random noise.", "node_type": "subnode"}, {"id": "Gaussian Noise Model", "label": "Gaussian Noise Model", "title": "<b>Gaussian Noise Model</b> (subnode)<hr>Modeling simulator transitions with a deterministic function plus Gaussian noise for approximation purposes.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Modeling simulator transitions with a deterministic function plus Gaussian noise for approximation purposes.", "node_type": "subnode"}, {"id": "Bellman Updates", "label": "Bellman Updates", "title": "<b>Bellman Updates</b> (subnode)<hr>Iterative process of updating the value function based on Bellman's equation to converge towards an optimal policy.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Iterative process of updating the value function based on Bellman's equation to converge towards an optimal policy.", "node_type": "subnode"}, {"id": "VE Procedure", "label": "VE Procedure", "title": "<b>VE Procedure</b> (subnode)<hr>Procedure used to evaluate value function under a given policy.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Procedure used to evaluate value function under a given policy.", "node_type": "subnode"}, {"id": "k Parameter", "label": "k Parameter", "title": "<b>k Parameter</b> (subnode)<hr>Hyperparameter controlling the number of iterations in VE procedure.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Hyperparameter controlling the number of iterations in VE procedure.", "node_type": "subnode"}, {"id": "Initialization Option 1", "label": "Initialization Option 1", "title": "<b>Initialization Option 1</b> (subnode)<hr>Initialize value function to zero for all states.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Initialize value function to zero for all states.", "node_type": "subnode"}, {"id": "Initialization Option 2", "label": "Initialization Option 2", "title": "<b>Initialization Option 2</b> (subnode)<hr>Initialize value function based on previous iterations' values.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Initialize value function based on previous iterations' values.", "node_type": "subnode"}, {"id": "Update Rule (15.12)", "label": "Update Rule (15.12)", "title": "<b>Update Rule (15.12)</b> (subnode)<hr>Rule for updating state values using Bellman equation.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Rule for updating state values using Bellman equation.", "node_type": "subnode"}, {"id": "Policy Update Rule (15.13)", "label": "Policy Update Rule (15.13)", "title": "<b>Policy Update Rule (15.13)</b> (subnode)<hr>Rule for updating policy based on value function estimates.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Rule for updating policy based on value function estimates.", "node_type": "subnode"}, {"id": "Chapter 15 Overview", "label": "Chapter 15 Overview", "title": "<b>Chapter 15 Overview</b> (major)<hr>Overview of MDPs and value/policy iteration methods.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Overview of MDPs and value/policy iteration methods.", "node_type": "major"}, {"id": "Optimal Bellman Equation", "label": "Optimal Bellman Equation", "title": "<b>Optimal Bellman Equation</b> (subnode)<hr>Equation defining the optimal value function for an optimal policy.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Equation defining the optimal value function for an optimal policy.", "node_type": "subnode"}, {"id": "Policy Iteration Speedup", "label": "Policy Iteration Speedup", "title": "<b>Policy Iteration Speedup</b> (subnode)<hr>Discussion on how policy iteration can be faster than repeated single-step updates.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Discussion on how policy iteration can be faster than repeated single-step updates.", "node_type": "subnode"}, {"id": "Value Iteration Preference", "label": "Value Iteration Preference", "title": "<b>Value Iteration Preference</b> (subnode)<hr>When value iteration is preferred over policy iteration due to computational constraints.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "When value iteration is preferred over policy iteration due to computational constraints.", "node_type": "subnode"}, {"id": "Chapter 16 Introduction", "label": "Chapter 16 Introduction", "title": "<b>Chapter 16 Introduction</b> (major)<hr>Introduction to LQR, DDP and LQG concepts in MDPs.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Introduction to LQR, DDP and LQG concepts in MDPs.", "node_type": "major"}, {"id": "Optimal Value Function Recovery", "label": "Optimal Value Function Recovery", "title": "<b>Optimal Value Function Recovery</b> (subnode)<hr>Recovering the optimal policy from the optimal value function.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Recovering the optimal policy from the optimal value function.", "node_type": "subnode"}, {"id": "Finite Horizon MDPs", "label": "Finite Horizon MDPs", "title": "<b>Finite Horizon MDPs</b> (subnode)<hr>Models considering a finite sequence of decisions without the need for a discount factor.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Models considering a finite sequence of decisions without the need for a discount factor.", "node_type": "subnode"}, {"id": "Time-Dependent Policies", "label": "Time-Dependent Policies", "title": "<b>Time-Dependent Policies</b> (subnode)<hr>Policies that change over time in response to the environment.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Policies that change over time in response to the environment.", "node_type": "subnode"}, {"id": "Non-Stationary Optimal Policy", "label": "Non-Stationary Optimal Policy", "title": "<b>Non-Stationary Optimal Policy</b> (subnode)<hr>Optimal policies vary based on remaining steps and current state.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Optimal policies vary based on remaining steps and current state.", "node_type": "subnode"}, {"id": "Dynamic Environment Models", "label": "Dynamic Environment Models", "title": "<b>Dynamic Environment Models</b> (subnode)<hr>Models that account for changing dynamics over time.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Models that account for changing dynamics over time.", "node_type": "subnode"}, {"id": "Expectation Calculation", "label": "Expectation Calculation", "title": "<b>Expectation Calculation</b> (subnode)<hr>Calculating expectations under certain conditions that simplify the expression for policy gradients.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Calculating expectations under certain conditions that simplify the expression for policy gradients.", "node_type": "subnode"}, {"id": "Rewards Dependency on States and Actions", "label": "Rewards Dependency on States and Actions", "title": "<b>Rewards Dependency on States and Actions</b> (subnode)<hr>Description of how rewards depend on both states and actions in an MDP.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Description of how rewards depend on both states and actions in an MDP.", "node_type": "subnode"}, {"id": "Infinite Horizon MDPs", "label": "Infinite Horizon MDPs", "title": "<b>Infinite Horizon MDPs</b> (subnode)<hr>Models considering an infinite sequence of decisions with a discount factor for future rewards.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Models considering an infinite sequence of decisions with a discount factor for future rewards.", "node_type": "subnode"}, {"id": "Discount Factor \u03b3", "label": "Discount Factor \u03b3", "title": "<b>Discount Factor \u03b3</b> (subnode)<hr>Parameter used to ensure convergence in infinite horizon models by discounting future rewards.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Parameter used to ensure convergence in infinite horizon models by discounting future rewards.", "node_type": "subnode"}, {"id": "Value Function in RL", "label": "Value Function in RL", "title": "<b>Value Function in RL</b> (subnode)<hr>Definition and computation of value functions in reinforcement learning.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Definition and computation of value functions in reinforcement learning.", "node_type": "subnode"}, {"id": "Dynamic Programming", "label": "Dynamic Programming", "title": "<b>Dynamic Programming</b> (subnode)<hr>Application of dynamic programming to reinforcement learning problems.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Application of dynamic programming to reinforcement learning problems.", "node_type": "subnode"}, {"id": "Bellman Update", "label": "Bellman Update", "title": "<b>Bellman Update</b> (subnode)<hr>Operator used to update value functions in reinforcement learning.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Operator used to update value functions in reinforcement learning.", "node_type": "subnode"}, {"id": "Geometric Convergence", "label": "Geometric Convergence", "title": "<b>Geometric Convergence</b> (subnode)<hr>Rate at which the approximation error decreases with each iteration.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Rate at which the approximation error decreases with each iteration.", "node_type": "subnode"}, {"id": "Continuous Setting", "label": "Continuous Setting", "title": "<b>Continuous Setting</b> (subnode)<hr>Model assumptions for LQR including state and action spaces as real vectors.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Model assumptions for LQR including state and action spaces as real vectors.", "node_type": "subnode"}, {"id": "Linear Transitions", "label": "Linear Transitions", "title": "<b>Linear Transitions</b> (subnode)<hr>Assumption of linear dynamics in the system with Gaussian noise.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Assumption of linear dynamics in the system with Gaussian noise.", "node_type": "subnode"}, {"id": "Quadratic Rewards", "label": "Quadratic Rewards", "title": "<b>Quadratic Rewards</b> (subnode)<hr>Rewards defined as quadratic functions of state and action vectors.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Rewards defined as quadratic functions of state and action vectors.", "node_type": "subnode"}, {"id": "Quadratic Assumption", "label": "Quadratic Assumption", "title": "<b>Quadratic Assumption</b> (subnode)<hr>Assumes that the value functions are quadratic for simplification.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Assumes that the value functions are quadratic for simplification.", "node_type": "subnode"}, {"id": "Dynamics of Model", "label": "Dynamics of Model", "title": "<b>Dynamics of Model</b> (subnode)<hr>Incorporates model dynamics into the optimal value function calculation.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Incorporates model dynamics into the optimal value function calculation.", "node_type": "subnode"}, {"id": "Linear Optimal Action", "label": "Linear Optimal Action", "title": "<b>Linear Optimal Action</b> (subnode)<hr>Derives the formula for the optimal action which is a linear function of the state.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Derives the formula for the optimal action which is a linear function of the state.", "node_type": "subnode"}, {"id": "LQR Model Assumptions", "label": "LQR Model Assumptions", "title": "<b>LQR Model Assumptions</b> (subnode)<hr>Assumptions made in the Linear Quadratic Regulator model.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Assumptions made in the Linear Quadratic Regulator model.", "node_type": "subnode"}, {"id": "LQR Algorithm Steps", "label": "LQR Algorithm Steps", "title": "<b>LQR Algorithm Steps</b> (subnode)<hr>Steps involved in implementing the LQR algorithm.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Steps involved in implementing the LQR algorithm.", "node_type": "subnode"}, {"id": "Step 1: Estimate Matrices", "label": "Step 1: Estimate Matrices", "title": "<b>Step 1: Estimate Matrices</b> (subnode)<hr>Estimating matrices A, B, and Sigma using linear regression.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Estimating matrices A, B, and Sigma using linear regression.", "node_type": "subnode"}, {"id": "Step 2: Derive Optimal Policy", "label": "Step 2: Derive Optimal Policy", "title": "<b>Step 2: Derive Optimal Policy</b> (subnode)<hr>Deriving the optimal policy given known model parameters.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Deriving the optimal policy given known model parameters.", "node_type": "subnode"}, {"id": "Dynamic Programming Application", "label": "Dynamic Programming Application", "title": "<b>Dynamic Programming Application</b> (subnode)<hr>Application of dynamic programming to compute V_t* in LQR context.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Application of dynamic programming to compute V_t* in LQR context.", "node_type": "subnode"}, {"id": "Linear Quadratic Regulator (LQR)", "label": "Linear Quadratic Regulator (LQR)", "title": "<b>Linear Quadratic Regulator (LQR)</b> (subnode)<hr>A method to find optimal control policies in linear systems with quadratic cost functions.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "A method to find optimal control policies in linear systems with quadratic cost functions.", "node_type": "subnode"}, {"id": "Discrete Ricatti Equations", "label": "Discrete Ricatti Equations", "title": "<b>Discrete Ricatti Equations</b> (subnode)<hr>Set of equations used to solve the LQR problem iteratively.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Set of equations used to solve the LQR problem iteratively.", "node_type": "subnode"}, {"id": "Inverted Pendulum Example", "label": "Inverted Pendulum Example", "title": "<b>Inverted Pendulum Example</b> (subnode)<hr>Example system demonstrating the application of linearization techniques.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Example system demonstrating the application of linearization techniques.", "node_type": "subnode"}, {"id": "Linearization of Dynamics", "label": "Linearization of Dynamics", "title": "<b>Linearization of Dynamics</b> (subnode)<hr>Process of approximating nonlinear dynamics with a linear model for easier analysis.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Process of approximating nonlinear dynamics with a linear model for easier analysis.", "node_type": "subnode"}, {"id": "Taylor Expansion", "label": "Taylor Expansion", "title": "<b>Taylor Expansion</b> (subnode)<hr>Mathematical technique used to approximate functions using polynomials, crucial in linearizing systems.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Mathematical technique used to approximate functions using polynomials, crucial in linearizing systems.", "node_type": "subnode"}, {"id": "Optimization in RL", "label": "Optimization in RL", "title": "<b>Optimization in RL</b> (subnode)<hr>Techniques for optimizing policies in reinforcement learning.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Techniques for optimizing policies in reinforcement learning.", "node_type": "subnode"}, {"id": "LQG Framework", "label": "LQG Framework", "title": "<b>LQG Framework</b> (major)<hr>Extension of LQR to handle stochastic systems and partial observability.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Extension of LQR to handle stochastic systems and partial observability.", "node_type": "major"}, {"id": "Partial Observability", "label": "Partial Observability", "title": "<b>Partial Observability</b> (subnode)<hr>Situation where the full state is not observable, requiring models like LQG.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Situation where the full state is not observable, requiring models like LQG.", "node_type": "subnode"}, {"id": "Nominal Trajectory Generation", "label": "Nominal Trajectory Generation", "title": "<b>Nominal Trajectory Generation</b> (subnode)<hr>Creating an initial approximate path using a naive controller.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Creating an initial approximate path using a naive controller.", "node_type": "subnode"}, {"id": "Rewriting Dynamics", "label": "Rewriting Dynamics", "title": "<b>Rewriting Dynamics</b> (subnode)<hr>Expressing the state transition using matrices A and B for non-stationary settings.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Expressing the state transition using matrices A and B for non-stationary settings.", "node_type": "subnode"}, {"id": "Reward Function Approximation", "label": "Reward Function Approximation", "title": "<b>Reward Function Approximation</b> (subnode)<hr>Using Taylor expansion to approximate rewards around nominal trajectory points.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Using Taylor expansion to approximate rewards around nominal trajectory points.", "node_type": "subnode"}, {"id": "Partially Observable MDPs (POMDP)", "label": "Partially Observable MDPs (POMDP)", "title": "<b>Partially Observable MDPs (POMDP)</b> (subnode)<hr>MDPs with an additional observation layer to handle partial observability.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "MDPs with an additional observation layer to handle partial observability.", "node_type": "subnode"}, {"id": "Observation Layer", "label": "Observation Layer", "title": "<b>Observation Layer</b> (subnode)<hr>Introduces new variable o_t representing observations given the current state s_t.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Introduces new variable o_t representing observations given the current state s_t.", "node_type": "subnode"}, {"id": "Belief State", "label": "Belief State", "title": "<b>Belief State</b> (subnode)<hr>Maintains a distribution over states based on past observations to inform policy decisions.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Maintains a distribution over states based on past observations to inform policy decisions.", "node_type": "subnode"}, {"id": "LQR Extension", "label": "LQR Extension", "title": "<b>LQR Extension</b> (subnode)<hr>Extension of Linear Quadratic Regulator to handle partial observability scenarios.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Extension of Linear Quadratic Regulator to handle partial observability scenarios.", "node_type": "subnode"}, {"id": "Kalman Filter", "label": "Kalman Filter", "title": "<b>Kalman Filter</b> (subnode)<hr>Algorithm used for efficient computation and updating of belief states over time.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Algorithm used for efficient computation and updating of belief states over time.", "node_type": "subnode"}, {"id": "LQR Updates", "label": "LQR Updates", "title": "<b>LQR Updates</b> (subnode)<hr>Backward pass computations for optimal control policies.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Backward pass computations for optimal control policies.", "node_type": "subnode"}, {"id": "Randomized Policy", "label": "Randomized Policy", "title": "<b>Randomized Policy</b> (subnode)<hr>Learning policy that outputs actions probabilistically based on state input.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Learning policy that outputs actions probabilistically based on state input.", "node_type": "subnode"}, {"id": "Expected Total Payoff", "label": "Expected Total Payoff", "title": "<b>Expected Total Payoff</b> (subnode)<hr>Objective function for optimizing policy parameters over trajectories.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Objective function for optimizing policy parameters over trajectories.", "node_type": "subnode"}, {"id": "Predict Step", "label": "Predict Step", "title": "<b>Predict Step</b> (subnode)<hr>Estimates the next state based on current state distribution.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Estimates the next state based on current state distribution.", "node_type": "subnode"}, {"id": "Update Step", "label": "Update Step", "title": "<b>Update Step</b> (subnode)<hr>Refine state estimate with new observation at each time step.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Refine state estimate with new observation at each time step.", "node_type": "subnode"}, {"id": "Belief States Update", "label": "Belief States Update", "title": "<b>Belief States Update</b> (subnode)<hr>Updates belief states through predict and update steps iteratively.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Updates belief states through predict and update steps iteratively.", "node_type": "subnode"}, {"id": "Kalman Gain", "label": "Kalman Gain", "title": "<b>Kalman Gain</b> (subnode)<hr>Matrix used in update step to refine state estimate based on new observation.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Matrix used in update step to refine state estimate based on new observation.", "node_type": "subnode"}, {"id": "Step 1", "label": "Step 1", "title": "<b>Step 1</b> (subnode)<hr>Initial step to set up the system dynamics and noise model.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Initial step to set up the system dynamics and noise model.", "node_type": "subnode"}, {"id": "Step 2", "label": "Step 2", "title": "<b>Step 2</b> (subnode)<hr>Use mean of distribution as state approximation.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Use mean of distribution as state approximation.", "node_type": "subnode"}, {"id": "Step 3", "label": "Step 3", "title": "<b>Step 3</b> (subnode)<hr>Set action based on approximated state and LQR algorithm.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Set action based on approximated state and LQR algorithm.", "node_type": "subnode"}, {"id": "System Dynamics", "label": "System Dynamics", "title": "<b>System Dynamics</b> (major)<hr>Model describing how the system evolves over time and is affected by noise.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Model describing how the system evolves over time and is affected by noise.", "node_type": "major"}, {"id": "LQR Algorithm", "label": "LQR Algorithm", "title": "<b>LQR Algorithm</b> (major)<hr>Linear Quadratic Regulator algorithm used to determine optimal control actions.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Linear Quadratic Regulator algorithm used to determine optimal control actions.", "node_type": "major"}, {"id": "Policy Gradients", "label": "Policy Gradients", "title": "<b>Policy Gradients</b> (subnode)<hr>Techniques for optimizing policies in reinforcement learning environments.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Techniques for optimizing policies in reinforcement learning environments.", "node_type": "subnode"}, {"id": "Expectation Estimation", "label": "Expectation Estimation", "title": "<b>Expectation Estimation</b> (subnode)<hr>Estimating the expected value of a function under a policy distribution.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Estimating the expected value of a function under a policy distribution.", "node_type": "subnode"}, {"id": "Sample-Based Estimation", "label": "Sample-Based Estimation", "title": "<b>Sample-Based Estimation</b> (subnode)<hr>Using samples to estimate the gradient of expected values.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Using samples to estimate the gradient of expected values.", "node_type": "subnode"}, {"id": "Log Probability Calculation", "label": "Log Probability Calculation", "title": "<b>Log Probability Calculation</b> (subnode)<hr>Calculating log probabilities for policy gradients.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Calculating log probabilities for policy gradients.", "node_type": "subnode"}, {"id": "Reward Function Estimation", "label": "Reward Function Estimation", "title": "<b>Reward Function Estimation</b> (subnode)<hr>Estimating gradients without knowing the exact form of the reward function.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Estimating gradients without knowing the exact form of the reward function.", "node_type": "subnode"}, {"id": "Expectation Maximization", "label": "Expectation Maximization", "title": "<b>Expectation Maximization</b> (subnode)<hr>Using expectations of rewards over policy distributions to estimate gradients.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Using expectations of rewards over policy distributions to estimate gradients.", "node_type": "subnode"}, {"id": "Reparametrization Technique", "label": "Reparametrization Technique", "title": "<b>Reparametrization Technique</b> (subnode)<hr>Technique used in VAEs for gradient estimation, not applicable here.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Technique used in VAEs for gradient estimation, not applicable here.", "node_type": "subnode"}, {"id": "REINFORCE Algorithm", "label": "REINFORCE Algorithm", "title": "<b>REINFORCE Algorithm</b> (subnode)<hr>Algorithm for estimating gradients of policy performance without knowing the reward function.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Algorithm for estimating gradients of policy performance without knowing the reward function.", "node_type": "subnode"}, {"id": "Policy Gradient Theorem", "label": "Policy Gradient Theorem", "title": "<b>Policy Gradient Theorem</b> (major)<hr>Theorem that connects policy gradients to expected payoff.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Theorem that connects policy gradients to expected payoff.", "node_type": "major"}, {"id": "Log Probability Derivative", "label": "Log Probability Derivative", "title": "<b>Log Probability Derivative</b> (subnode)<hr>Derivative of log probability with respect to parameters \u03b8.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Derivative of log probability with respect to parameters \u03b8.", "node_type": "subnode"}, {"id": "Vanilla REINFORCE Algorithm", "label": "Vanilla REINFORCE Algorithm", "title": "<b>Vanilla REINFORCE Algorithm</b> (subnode)<hr>Algorithm that updates policy parameters using estimated gradients.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Algorithm that updates policy parameters using estimated gradients.", "node_type": "subnode"}, {"id": "Trajectory Probability Change", "label": "Trajectory Probability Change", "title": "<b>Trajectory Probability Change</b> (subnode)<hr>Change in trajectory probability due to parameter changes.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Change in trajectory probability due to parameter changes.", "node_type": "subnode"}, {"id": "Empirical Trajectories Estimation", "label": "Empirical Trajectories Estimation", "title": "<b>Empirical Trajectories Estimation</b> (subnode)<hr>Estimating gradients using sample trajectories.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Estimating gradients using sample trajectories.", "node_type": "subnode"}, {"id": "Trajectory Probability", "label": "Trajectory Probability", "title": "<b>Trajectory Probability</b> (subnode)<hr>Probability of an agent following a specific sequence of actions and states.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Probability of an agent following a specific sequence of actions and states.", "node_type": "subnode"}, {"id": "Expectation Equations", "label": "Expectation Equations", "title": "<b>Expectation Equations</b> (subnode)<hr>Mathematical expressions for expected values in reinforcement learning scenarios.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Mathematical expressions for expected values in reinforcement learning scenarios.", "node_type": "subnode"}, {"id": "Simplification of Formula (17.8)", "label": "Simplification of Formula (17.8)", "title": "<b>Simplification of Formula (17.8)</b> (subnode)<hr>Derivation and simplification process based on constant reward assumption.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Derivation and simplification process based on constant reward assumption.", "node_type": "subnode"}, {"id": "Policy Gradient Methods", "label": "Policy Gradient Methods", "title": "<b>Policy Gradient Methods</b> (subnode)<hr>Techniques for optimizing the parameters of a policy directly based on the performance measure, such as expected return or discounted sum of rewards.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Techniques for optimizing the parameters of a policy directly based on the performance measure, such as expected return or discounted sum of rewards.", "node_type": "subnode"}, {"id": "Law of Total Expectation", "label": "Law of Total Expectation", "title": "<b>Law of Total Expectation</b> (subnode)<hr>A theorem in probability theory that allows one to compute the expectation of any function of a random variable by conditioning on another random variable.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "A theorem in probability theory that allows one to compute the expectation of any function of a random variable by conditioning on another random variable.", "node_type": "subnode"}, {"id": "Estimator Simplification", "label": "Estimator Simplification", "title": "<b>Estimator Simplification</b> (subnode)<hr>Simplifying an estimator using the law of total expectation, making it easier to understand and compute.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Simplifying an estimator using the law of total expectation, making it easier to understand and compute.", "node_type": "subnode"}, {"id": "Baseline Estimation", "label": "Baseline Estimation", "title": "<b>Baseline Estimation</b> (subnode)<hr>Techniques to reduce variance in policy gradient estimators using baselines.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Techniques to reduce variance in policy gradient estimators using baselines.", "node_type": "subnode"}, {"id": "Trajectory Collection", "label": "Trajectory Collection", "title": "<b>Trajectory Collection</b> (subnode)<hr>Process of collecting data through interaction with an environment to train policies.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Process of collecting data through interaction with an environment to train policies.", "node_type": "subnode"}, {"id": "Gradient Estimator Update", "label": "Gradient Estimator Update", "title": "<b>Gradient Estimator Update</b> (subnode)<hr>Updating policy parameters based on the gradient estimator using baselines.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Updating policy parameters based on the gradient estimator using baselines.", "node_type": "subnode"}, {"id": "Machine_Learning_Theory", "label": "Machine_Learning_Theory", "title": "<b>Machine_Learning_Theory</b> (major)<hr>Theoretical foundations of machine learning including generalization and double descent phenomena.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Theoretical foundations of machine learning including generalization and double descent phenomena.", "node_type": "major"}, {"id": "Double_Descent_Phenomenon", "label": "Double_Descent_Phenomenon", "title": "<b>Double_Descent_Phenomenon</b> (subnode)<hr>Phenomenon where model performance initially improves then worsens before improving again with increased complexity.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Phenomenon where model performance initially improves then worsens before improving again with increased complexity.", "node_type": "subnode"}, {"id": "Statistical_Mechanics_of_Learning", "label": "Statistical_Mechanics_of_Learning", "title": "<b>Statistical_Mechanics_of_Learning</b> (subnode)<hr>Application of statistical mechanics principles to understand learning processes and generalization in neural networks.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Application of statistical mechanics principles to understand learning processes and generalization in neural networks.", "node_type": "subnode"}, {"id": "Machine Learning Literature", "label": "Machine Learning Literature", "title": "<b>Machine Learning Literature</b> (major)<hr>Collection of key papers and reviews in machine learning.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Collection of key papers and reviews in machine learning.", "node_type": "major"}, {"id": "Bias-Variance Trade-off Reconciliation", "label": "Bias-Variance Trade-off Reconciliation", "title": "<b>Bias-Variance Trade-off Reconciliation</b> (subnode)<hr>Paper discussing the reconciliation between modern ML practice and classical bias-variance trade-off.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Paper discussing the reconciliation between modern ML practice and classical bias-variance trade-off.", "node_type": "subnode"}, {"id": "Double Descent for Weak Features", "label": "Double Descent for Weak Features", "title": "<b>Double Descent for Weak Features</b> (subnode)<hr>Study on double descent phenomenon in machine learning models with weak features.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Study on double descent phenomenon in machine learning models with weak features.", "node_type": "subnode"}, {"id": "Variational Inference Review", "label": "Variational Inference Review", "title": "<b>Variational Inference Review</b> (subnode)<hr>Review paper discussing variational inference methods for statisticians.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Review paper discussing variational inference methods for statisticians.", "node_type": "subnode"}, {"id": "Foundation Models Opportunities and Risks", "label": "Foundation Models Opportunities and Risks", "title": "<b>Foundation Models Opportunities and Risks</b> (subnode)<hr>Discussion on the opportunities and risks associated with foundation models in ML.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Discussion on the opportunities and risks associated with foundation models in ML.", "node_type": "subnode"}, {"id": "Few-Shot Learning Capabilities", "label": "Few-Shot Learning Capabilities", "title": "<b>Few-Shot Learning Capabilities</b> (subnode)<hr>Research highlighting the few-shot learning capabilities of language models.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Research highlighting the few-shot learning capabilities of language models.", "node_type": "subnode"}, {"id": "Contrastive Learning Framework", "label": "Contrastive Learning Framework", "title": "<b>Contrastive Learning Framework</b> (subnode)<hr>Framework for contrastive learning to improve visual representation in machine learning.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Framework for contrastive learning to improve visual representation in machine learning.", "node_type": "subnode"}, {"id": "BERT Pre-training Methodology", "label": "BERT Pre-training Methodology", "title": "<b>BERT Pre-training Methodology</b> (subnode)<hr>Introduction of BERT, a deep bidirectional transformer model for language understanding.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Introduction of BERT, a deep bidirectional transformer model for language understanding.", "node_type": "subnode"}, {"id": "Implicit Bias Study", "label": "Implicit Bias Study", "title": "<b>Implicit Bias Study</b> (subnode)<hr>Research on the implicit bias in machine learning models due to noise covariance shape.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Research on the implicit bias in machine learning models due to noise covariance shape.", "node_type": "subnode"}, {"id": "High-Dimensional Statistical Analysis", "label": "High-Dimensional Statistical Analysis", "title": "<b>High-Dimensional Statistical Analysis</b> (subnode)<hr>Discussion on surprising phenomena in high-dimensional statistical analysis and machine learning.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Discussion on surprising phenomena in high-dimensional statistical analysis and machine learning.", "node_type": "subnode"}, {"id": "Machine Learning Papers", "label": "Machine Learning Papers", "title": "<b>Machine Learning Papers</b> (major)<hr>Collection of research papers related to machine learning and statistical learning theory.", "shape": "star", "size": 25, "color": "#FF6347", "description": "Collection of research papers related to machine learning and statistical learning theory.", "node_type": "major"}, {"id": "Implicit Bias in Machine Learning", "label": "Implicit Bias in Machine Learning", "title": "<b>Implicit Bias in Machine Learning</b> (subnode)<hr>Research on the implicit bias introduced by different noise covariances and ridgeless least squares interpolation.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Research on the implicit bias introduced by different noise covariances and ridgeless least squares interpolation.", "node_type": "subnode"}, {"id": "Deep Residual Learning", "label": "Deep Residual Learning", "title": "<b>Deep Residual Learning</b> (subnode)<hr>Introduction to deep residual learning for image recognition, a key technique in deep neural networks.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Introduction to deep residual learning for image recognition, a key technique in deep neural networks.", "node_type": "subnode"}, {"id": "Theoretical Guarantees for Deep Reinforcement Learning", "label": "Theoretical Guarantees for Deep Reinforcement Learning", "title": "<b>Theoretical Guarantees for Deep Reinforcement Learning</b> (subnode)<hr>Research on theoretical frameworks for model-based deep reinforcement learning with guarantees.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Research on theoretical frameworks for model-based deep reinforcement learning with guarantees.", "node_type": "subnode"}, {"id": "Generalization Error Analysis", "label": "Generalization Error Analysis", "title": "<b>Generalization Error Analysis</b> (subnode)<hr>Analysis of generalization error in random features regression and linear regression models.", "shape": "dot", "size": 15, "color": "#4682B4", "description": "Analysis of generalization error in random features regression and linear regression models.", "node_type": "subnode"}];
        const hierarchicalEdgesData = [{"from": "Supervised Learning", "to": "Regression Problem", "arrows": "to", "title": "Subtopic of Supervised Learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Supervised Learning", "to": "Classification Problem", "arrows": "to", "title": "Subtopic of Supervised Learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Supervised Learning", "to": "Hypothesis", "arrows": "to", "title": "Subtopic of Supervised Learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Linear Regression", "to": "Feature Selection", "arrows": "to", "title": "Subtopic of Linear Regression", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Basics", "to": "Gradient Descent", "arrows": "to", "title": "Subtopic of Machine Learning Basics", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Gradient Descent", "to": "LMS Update Rule", "arrows": "to", "title": "Subtopic of Gradient Descent", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "LMS Update Rule", "to": "Widrow-Hoff Learning Rule", "arrows": "to", "title": "Subtopic of LMS Update Rule", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "LMS Update Rule", "to": "Error Term", "arrows": "to", "title": "Subtopic of LMS Update Rule", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Gradient Descent", "to": "Batch Gradient Descent", "arrows": "to", "title": "Subtopic of Gradient Descent", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Matrix Derivatives", "to": "Gradient Calculation", "arrows": "to", "title": "Subtopic of Matrix Derivatives", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Least Squares Revisited", "to": "Design Matrix", "arrows": "to", "title": "Subtopic of Least Squares Revisited", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Least Squares Revisited", "to": "Vector y", "arrows": "to", "title": "Subtopic of Least Squares Revisited", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Linear Regression", "to": "Locally Weighted Linear Regression (LWLR)", "arrows": "to", "title": "Subtopic of Linear Regression", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Locally Weighted Linear Regression (LWLR)", "to": "Weight Calculation", "arrows": "to", "title": "Subtopic of Locally Weighted Linear Regression (LWLR)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Weight Calculation", "to": "Bandwidth Parameter (\u03c4)", "arrows": "to", "title": "Subtopic of Weight Calculation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Learning a model for an MDP", "to": "Continuous state MDPs", "arrows": "to", "title": "Subtopic of Learning a model for an MDP", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Continuous state MDPs", "to": "Discretization", "arrows": "to", "title": "Subtopic of Continuous state MDPs", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Continuous state MDPs", "to": "Value function approximation", "arrows": "to", "title": "Subtopic of Continuous state MDPs", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Learning a model for an MDP", "to": "Connections between Policy and Value Iteration (Optional)", "arrows": "to", "title": "Subtopic of Learning a model for an MDP", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "LQR, DDP and LQG", "to": "Finite-horizon MDPs", "arrows": "to", "title": "Subtopic of LQR, DDP and LQG", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "LQR, DDP and LQG", "to": "Linear Quadratic Regulation (LQR)", "arrows": "to", "title": "Subtopic of LQR, DDP and LQG", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "LQR, DDP and LQG", "to": "From non-linear dynamics to LQR", "arrows": "to", "title": "Subtopic of LQR, DDP and LQG", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "From non-linear dynamics to LQR", "to": "Linearization of dynamics", "arrows": "to", "title": "Subtopic of From non-linear dynamics to LQR", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "From non-linear dynamics to LQR", "to": "Differential Dynamic Programming (DDP)", "arrows": "to", "title": "Subtopic of From non-linear dynamics to LQR", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "LQR, DDP and LQG", "to": "Linear Quadratic Gaussian (LQG)", "arrows": "to", "title": "Subtopic of LQR, DDP and LQG", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Linear Regression", "to": "Underfitting", "arrows": "to", "title": "Subtopic of Linear Regression", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Linear Regression", "to": "Overfitting", "arrows": "to", "title": "Subtopic of Linear Regression", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "Locally Weighted Linear Regression (LWR)", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "Maximum Likelihood Estimation (MLE)", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "Least Squares Regression", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Maximum Likelihood Estimation (MLE)", "to": "Probabilistic Assumptions", "arrows": "to", "title": "Subtopic of Maximum Likelihood Estimation (MLE)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Least Squares Regression", "to": "Cost Function J(\u03b8)", "arrows": "to", "title": "Subtopic of Least Squares Regression", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Basics", "to": "Function Representation", "arrows": "to", "title": "Subtopic of Machine Learning Basics", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Function Representation", "to": "Linear Function Approximation", "arrows": "to", "title": "Subtopic of Function Representation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Linear Function Approximation", "to": "Parameters (Weights)", "arrows": "to", "title": "Subtopic of Linear Function Approximation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Function Representation", "to": "Cost Function", "arrows": "to", "title": "Subtopic of Function Representation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Cost Function", "to": "Ordinary Least Squares", "arrows": "to", "title": "Subtopic of Cost Function", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Function Representation", "to": "LMS Algorithm", "arrows": "to", "title": "Subtopic of Function Representation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Gradient Descent", "to": "Convex Function", "arrows": "to", "title": "Subtopic of Gradient Descent", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Gradient Descent", "to": "Stochastic Gradient Descent", "arrows": "to", "title": "Subtopic of Gradient Descent", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Stochastic Gradient Descent", "to": "Learning Rate Decay", "arrows": "to", "title": "Subtopic of Stochastic Gradient Descent", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Linear Regression", "to": "Least-Squares Cost Function", "arrows": "to", "title": "Subtopic of Linear Regression", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Linear Regression", "to": "Probabilistic Interpretation", "arrows": "to", "title": "Subtopic of Linear Regression", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Least-Squares Cost Function", "to": "Invertibility of X^TX Matrix", "arrows": "to", "title": "Subtopic of Least-Squares Cost Function", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Probabilistic Interpretation", "to": "Gaussian Distribution Assumption", "arrows": "to", "title": "Subtopic of Probabilistic Interpretation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Linear Regression", "to": "Normal Equations", "arrows": "to", "title": "Subtopic of Linear Regression", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Basics", "to": "Probability Distribution", "arrows": "to", "title": "Subtopic of Machine Learning Basics", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Basics", "to": "Design Matrix X", "arrows": "to", "title": "Subtopic of Machine Learning Basics", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Probability Distribution", "to": "Likelihood Function", "arrows": "to", "title": "Subtopic of Probability Distribution", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Likelihood Function", "to": "Independence Assumption", "arrows": "to", "title": "Subtopic of Likelihood Function", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Basics", "to": "Maximum Likelihood Estimation", "arrows": "to", "title": "Subtopic of Machine Learning Basics", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Maximum Likelihood Estimation", "to": "Log Likelihood", "arrows": "to", "title": "Subtopic of Maximum Likelihood Estimation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Gradient Descent", "to": "Learning Rate (\u03b1)", "arrows": "to", "title": "Subtopic of Gradient Descent", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Gradient Descent", "to": "Update Rule", "arrows": "to", "title": "Subtopic of Gradient Descent", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Algorithms", "to": "Locally Weighted Linear Regression", "arrows": "to", "title": "Subtopic of Machine Learning Algorithms", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Algorithms", "to": "Non-Parametric Algorithms", "arrows": "to", "title": "Subtopic of Machine Learning Algorithms", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Algorithms", "to": "Parametric Algorithms", "arrows": "to", "title": "Subtopic of Machine Learning Algorithms", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Classification Problem", "to": "Binary Classification", "arrows": "to", "title": "Subtopic of Classification Problem", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Classification Problem", "to": "Logistic Regression", "arrows": "to", "title": "Subtopic of Classification Problem", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Classification Problem", "to": "Linear Regression Approach", "arrows": "to", "title": "Subtopic of Classification Problem", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Logistic Regression", "to": "Logistic Function", "arrows": "to", "title": "Subtopic of Logistic Regression", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Logistic Function", "to": "Derivative of Sigmoid", "arrows": "to", "title": "Subtopic of Logistic Function", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Models", "to": "Classification Model", "arrows": "to", "title": "Subtopic of Machine Learning Models", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Likelihood Function", "to": "Log-Likelihood", "arrows": "to", "title": "Subtopic of Likelihood Function", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Maximum Likelihood Estimation (MLE)", "to": "Gradient Ascent", "arrows": "to", "title": "Subtopic of Maximum Likelihood Estimation (MLE)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "Logistic Regression Derivation", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "Perceptron Algorithm", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "Multi-class Classification", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Supervised Learning", "to": "Classification and Logistic Regression", "arrows": "to", "title": "Subtopic of Supervised Learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Classification and Logistic Regression", "to": "Perceptron Learning Algorithm", "arrows": "to", "title": "Subtopic of Classification and Logistic Regression", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Classification and Logistic Regression", "to": "Maximizing l(theta)", "arrows": "to", "title": "Subtopic of Classification and Logistic Regression", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Supervised Learning", "to": "Generalized Linear Models", "arrows": "to", "title": "Subtopic of Supervised Learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Generalized Linear Models", "to": "Exponential Family", "arrows": "to", "title": "Subtopic of Generalized Linear Models", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Generalized Linear Models", "to": "Constructing GLMs", "arrows": "to", "title": "Subtopic of Generalized Linear Models", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Constructing GLMs", "to": "Logistic Regression (GLM)", "arrows": "to", "title": "Subtopic of Constructing GLMs", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Supervised Learning", "to": "Generative Learning Algorithms", "arrows": "to", "title": "Subtopic of Supervised Learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Generative Learning Algorithms", "to": "Gaussian Discriminant Analysis", "arrows": "to", "title": "Subtopic of Generative Learning Algorithms", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Gaussian Discriminant Analysis", "to": "Multivariate Normal Distribution", "arrows": "to", "title": "Subtopic of Gaussian Discriminant Analysis", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Gaussian Discriminant Analysis", "to": "GDA Model", "arrows": "to", "title": "Subtopic of Gaussian Discriminant Analysis", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Gaussian Discriminant Analysis", "to": "Discussion: GDA and Logistic Regression", "arrows": "to", "title": "Subtopic of Gaussian Discriminant Analysis", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Generative Learning Algorithms", "to": "Naive Bayes", "arrows": "to", "title": "Subtopic of Generative Learning Algorithms", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Naive Bayes", "to": "Laplace Smoothing", "arrows": "to", "title": "Subtopic of Naive Bayes", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Naive Bayes", "to": "Event Models for Text Classification", "arrows": "to", "title": "Subtopic of Naive Bayes", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Kernel Methods", "to": "Feature Maps", "arrows": "to", "title": "Subtopic of Kernel Methods", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Kernel Methods", "to": "LMS with Features", "arrows": "to", "title": "Subtopic of Kernel Methods", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Kernel Methods", "to": "LMS with Kernel Trick", "arrows": "to", "title": "Subtopic of Kernel Methods", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Kernel Methods", "to": "Properties of Kernels", "arrows": "to", "title": "Subtopic of Kernel Methods", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Support Vector Machines", "to": "Margins: Intuition", "arrows": "to", "title": "Subtopic of Support Vector Machines", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Support Vector Machines", "to": "Notation (Optional)", "arrows": "to", "title": "Subtopic of Support Vector Machines", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Support Vector Machines", "to": "Functional and Geometric Margins", "arrows": "to", "title": "Subtopic of Support Vector Machines", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Support Vector Machines", "to": "Optimal Margin Classifier (Optional)", "arrows": "to", "title": "Subtopic of Support Vector Machines", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Support Vector Machines", "to": "Lagrange Duality (Optional)", "arrows": "to", "title": "Subtopic of Support Vector Machines", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Support Vector Machines", "to": "Dual Formulation (Optional)", "arrows": "to", "title": "Subtopic of Support Vector Machines", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Support Vector Machines", "to": "Regularization and Non-separable Case (Optional)", "arrows": "to", "title": "Subtopic of Support Vector Machines", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Support Vector Machines", "to": "SMO Algorithm (Optional)", "arrows": "to", "title": "Subtopic of Support Vector Machines", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "SMO Algorithm (Optional)", "to": "Coordinate Ascent", "arrows": "to", "title": "Subtopic of SMO Algorithm (Optional)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "SMO Algorithm (Optional)", "to": "SMO Details", "arrows": "to", "title": "Subtopic of SMO Algorithm (Optional)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Deep Learning", "to": "Supervised Learning with Non-linear Models", "arrows": "to", "title": "Subtopic of Deep Learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Deep Learning", "to": "Neural Networks", "arrows": "to", "title": "Subtopic of Deep Learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Deep Learning", "to": "Modules in Modern Neural Networks", "arrows": "to", "title": "Subtopic of Deep Learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Deep Learning", "to": "Backpropagation", "arrows": "to", "title": "Subtopic of Deep Learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Backpropagation", "to": "Preliminaries on Partial Derivatives", "arrows": "to", "title": "Subtopic of Backpropagation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Backpropagation", "to": "General Strategy of Backpropagation", "arrows": "to", "title": "Subtopic of Backpropagation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Backpropagation", "to": "Backward Functions for Basic Modules", "arrows": "to", "title": "Subtopic of Backpropagation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Backpropagation", "to": "Back-propagation for MLPs", "arrows": "to", "title": "Subtopic of Backpropagation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Logistic Regression", "to": "Stochastic Gradient Ascent Rule", "arrows": "to", "title": "Subtopic of Logistic Regression", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Logistic Regression", "to": "Logistic Loss Function", "arrows": "to", "title": "Subtopic of Logistic Regression", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Logistic Regression", "to": "Negative Log-Likelihood", "arrows": "to", "title": "Subtopic of Logistic Regression", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Logistic Loss Function", "to": "Logit", "arrows": "to", "title": "Subtopic of Logistic Loss Function", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Basics", "to": "Loss Functions", "arrows": "to", "title": "Subtopic of Machine Learning Basics", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Loss Functions", "to": "Cross-Entropy Loss", "arrows": "to", "title": "Subtopic of Loss Functions", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Cross-Entropy Loss", "to": "Softmax Function", "arrows": "to", "title": "Subtopic of Cross-Entropy Loss", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning", "to": "Classification", "arrows": "to", "title": "Subtopic of Machine Learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Multi-class Classification", "to": "Multinomial Distribution", "arrows": "to", "title": "Subtopic of Multi-class Classification", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Softmax Function", "to": "Logits", "arrows": "to", "title": "Subtopic of Softmax Function", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Softmax Function", "to": "Probability Vector", "arrows": "to", "title": "Subtopic of Softmax Function", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Newton's Method", "to": "Finding Roots", "arrows": "to", "title": "Subtopic of Newton's Method", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Newton's Method", "to": "Maximizing Functions", "arrows": "to", "title": "Subtopic of Newton's Method", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Newton's Method", "to": "Multidimensional Generalization", "arrows": "to", "title": "Subtopic of Newton's Method", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Multidimensional Generalization", "to": "Hessian Matrix", "arrows": "to", "title": "Subtopic of Multidimensional Generalization", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "Optimization Methods", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Optimization Methods", "to": "Fisher Scoring", "arrows": "to", "title": "Subtopic of Optimization Methods", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Generalized Linear Models (GLMs)", "to": "Exponential Family Distributions", "arrows": "to", "title": "Subtopic of Generalized Linear Models (GLMs)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Basics", "to": "Loss Function", "arrows": "to", "title": "Subtopic of Machine Learning Basics", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Loss Function", "to": "Cross Entropy Loss", "arrows": "to", "title": "Subtopic of Loss Function", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Newton's Method", "to": "Algorithm Application", "arrows": "to", "title": "Subtopic of Newton's Method", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Backpropagation", "to": "Preliminaries on partial derivatives", "arrows": "to", "title": "Subtopic of Backpropagation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Backpropagation", "to": "General strategy of backpropagation", "arrows": "to", "title": "Subtopic of Backpropagation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Backpropagation", "to": "Backward functions for basic modules", "arrows": "to", "title": "Subtopic of Backpropagation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Modern Neural Networks", "to": "Vectorization over training examples", "arrows": "to", "title": "Subtopic of Modern Neural Networks", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Generalization and regularization", "to": "Generalization", "arrows": "to", "title": "Subtopic of Generalization and regularization", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Generalization", "to": "Bias-variance tradeoff", "arrows": "to", "title": "Subtopic of Generalization", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Bias-variance tradeoff", "to": "A mathematical decomposition (for regression)", "arrows": "to", "title": "Subtopic of Bias-variance tradeoff", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Generalization", "to": "The double descent phenomenon", "arrows": "to", "title": "Subtopic of Generalization", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Generalization", "to": "Sample complexity bounds (optional readings)", "arrows": "to", "title": "Subtopic of Generalization", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Regularization and model selection", "to": "Regularization", "arrows": "to", "title": "Subtopic of Regularization and model selection", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Regularization and model selection", "to": "Implicit regularization effect (optional reading)", "arrows": "to", "title": "Subtopic of Regularization and model selection", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Regularization and model selection", "to": "Model selection via cross validation", "arrows": "to", "title": "Subtopic of Regularization and model selection", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Regularization and model selection", "to": "Bayesian statistics and regularization", "arrows": "to", "title": "Subtopic of Regularization and model selection", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Unsupervised learning", "to": "Clustering and the k-means algorithm", "arrows": "to", "title": "Subtopic of Unsupervised learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Unsupervised learning", "to": "EM algorithms", "arrows": "to", "title": "Subtopic of Unsupervised learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "EM algorithms", "to": "EM for mixture of Gaussians", "arrows": "to", "title": "Subtopic of EM algorithms", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "EM algorithms", "to": "Jensen's inequality", "arrows": "to", "title": "Subtopic of EM algorithms", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "EM algorithms", "to": "General EM algorithms", "arrows": "to", "title": "Subtopic of EM algorithms", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "General EM algorithms", "to": "Other interpretation of ELBO", "arrows": "to", "title": "Subtopic of General EM algorithms", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "EM algorithms", "to": "Mixture of Gaussians revisited", "arrows": "to", "title": "Subtopic of EM algorithms", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "EM algorithms", "to": "Variational inference and variational auto-encoder (optional reading)", "arrows": "to", "title": "Subtopic of EM algorithms", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Unsupervised learning", "to": "Principal components analysis", "arrows": "to", "title": "Subtopic of Unsupervised learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Unsupervised learning", "to": "Independent components analysis", "arrows": "to", "title": "Subtopic of Unsupervised learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Independent components analysis", "to": "ICA ambiguities", "arrows": "to", "title": "Subtopic of Independent components analysis", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Independent components analysis", "to": "Densities and linear transformations", "arrows": "to", "title": "Subtopic of Independent components analysis", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Independent components analysis", "to": "ICA algorithm", "arrows": "to", "title": "Subtopic of Independent components analysis", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Self-supervised learning and foundation models", "to": "Pretraining and adaptation", "arrows": "to", "title": "Subtopic of Self-supervised learning and foundation models", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Self-supervised learning and foundation models", "to": "Pretraining methods in computer vision", "arrows": "to", "title": "Subtopic of Self-supervised learning and foundation models", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Self-supervised learning and foundation models", "to": "Pretrained large language models", "arrows": "to", "title": "Subtopic of Self-supervised learning and foundation models", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Pretrained large language models", "to": "Open up the blackbox of Transformers", "arrows": "to", "title": "Subtopic of Pretrained large language models", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Pretrained large language models", "to": "Zero-shot learning and in-context learning", "arrows": "to", "title": "Subtopic of Pretrained large language models", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Reinforcement Learning and Control", "to": "Reinforcement learning", "arrows": "to", "title": "Subtopic of Reinforcement Learning and Control", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Reinforcement learning", "to": "Markov decision processes", "arrows": "to", "title": "Subtopic of Reinforcement learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Reinforcement learning", "to": "Value iteration and policy iteration", "arrows": "to", "title": "Subtopic of Reinforcement learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Models", "to": "Generalized Linear Model (GLM)", "arrows": "to", "title": "Subtopic of Machine Learning Models", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Generalized Linear Model (GLM)", "to": "Poisson Distribution", "arrows": "to", "title": "Subtopic of Generalized Linear Model (GLM)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Generalized Linear Model (GLM)", "to": "Conditional Distribution Assumption", "arrows": "to", "title": "Subtopic of Generalized Linear Model (GLM)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Generalized Linear Model (GLM)", "to": "Prediction Goal", "arrows": "to", "title": "Subtopic of Generalized Linear Model (GLM)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Generalized Linear Model (GLM)", "to": "Linear Relationship Assumption", "arrows": "to", "title": "Subtopic of Generalized Linear Model (GLM)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Models", "to": "Generalized Linear Models (GLM)", "arrows": "to", "title": "Subtopic of Machine Learning Models", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Generalized Linear Models (GLM)", "to": "Bernoulli Distribution", "arrows": "to", "title": "Subtopic of Generalized Linear Models (GLM)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Exponential Family Distributions", "to": "Gaussian Distribution", "arrows": "to", "title": "Subtopic of Exponential Family Distributions", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Exponential Family Distributions", "to": "Natural Parameter", "arrows": "to", "title": "Subtopic of Exponential Family Distributions", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Exponential Family Distributions", "to": "Sufficient Statistic", "arrows": "to", "title": "Subtopic of Exponential Family Distributions", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Exponential Family Distributions", "to": "Log Partition Function", "arrows": "to", "title": "Subtopic of Exponential Family Distributions", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Bernoulli Distribution", "to": "Natural Parameter for Bernoulli", "arrows": "to", "title": "Subtopic of Bernoulli Distribution", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Bernoulli Distribution", "to": "Sufficient Statistic for Bernoulli", "arrows": "to", "title": "Subtopic of Bernoulli Distribution", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Bernoulli Distribution", "to": "Log Partition Function for Bernoulli", "arrows": "to", "title": "Subtopic of Bernoulli Distribution", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Generalized Linear Models (GLMs)", "to": "Assumptions/Design Choices", "arrows": "to", "title": "Subtopic of Generalized Linear Models (GLMs)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Ordinary Least Squares", "to": "Response Variable", "arrows": "to", "title": "Subtopic of Ordinary Least Squares", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Ordinary Least Squares", "to": "Conditional Distribution", "arrows": "to", "title": "Subtopic of Ordinary Least Squares", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Bayesian Classification", "to": "Class Priors", "arrows": "to", "title": "Subtopic of Bayesian Classification", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Bayesian Classification", "to": "Conditional Probability", "arrows": "to", "title": "Subtopic of Bayesian Classification", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Bayesian Classification", "to": "Posterior Distribution", "arrows": "to", "title": "Subtopic of Bayesian Classification", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Models", "to": "Conditional Distribution Modeling", "arrows": "to", "title": "Subtopic of Machine Learning Models", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Exponential Family Distributions", "to": "Canonical Response Function", "arrows": "to", "title": "Subtopic of Exponential Family Distributions", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Exponential Family Distributions", "to": "Canonical Link Function", "arrows": "to", "title": "Subtopic of Exponential Family Distributions", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Algorithms", "to": "Discriminative Learning Algorithms", "arrows": "to", "title": "Subtopic of Machine Learning Algorithms", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Generative Learning Algorithms", "to": "Bayes Rule", "arrows": "to", "title": "Subtopic of Generative Learning Algorithms", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Basics", "to": "Probability Distributions", "arrows": "to", "title": "Subtopic of Machine Learning Basics", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Gaussian Distribution", "to": "Mean", "arrows": "to", "title": "Subtopic of Gaussian Distribution", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Gaussian Distribution", "to": "Covariance", "arrows": "to", "title": "Subtopic of Gaussian Distribution", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Gaussian Distribution", "to": "Standard Normal Distribution", "arrows": "to", "title": "Subtopic of Gaussian Distribution", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Gaussian Distribution", "to": "Density Visualization", "arrows": "to", "title": "Subtopic of Gaussian Distribution", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Multivariate Normal Distribution", "to": "Covariance Matrix", "arrows": "to", "title": "Subtopic of Multivariate Normal Distribution", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Multivariate Normal Distribution", "to": "Contour Plots", "arrows": "to", "title": "Subtopic of Multivariate Normal Distribution", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Gaussian Discriminant Analysis (GDA)", "to": "Multivariate Normal Distributions for Classes", "arrows": "to", "title": "Subtopic of Gaussian Discriminant Analysis (GDA)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Models", "to": "Decision Boundaries", "arrows": "to", "title": "Subtopic of Machine Learning Models", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Models", "to": "Model Assumptions", "arrows": "to", "title": "Subtopic of Machine Learning Models", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Gaussian Discriminant Analysis (GDA)", "to": "Model Parameters", "arrows": "to", "title": "Subtopic of Gaussian Discriminant Analysis (GDA)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Model Parameters", "to": "Log-Likelihood Function", "arrows": "to", "title": "Subtopic of Model Parameters", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Gaussian Discriminant Analysis (GDA)", "to": "Decision Boundary", "arrows": "to", "title": "Subtopic of Gaussian Discriminant Analysis (GDA)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Models", "to": "GDA (Generative Discriminative Approach)", "arrows": "to", "title": "Subtopic of Machine Learning Models", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Logistic Regression", "to": "Robustness of Logistic Regression", "arrows": "to", "title": "Subtopic of Logistic Regression", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Models", "to": "Naive Bayes (Discrete Features)", "arrows": "to", "title": "Subtopic of Machine Learning Models", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "Feature Vector Selection", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "Generative Models", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Generative Models", "to": "Naive Bayes Classifier", "arrows": "to", "title": "Subtopic of Generative Models", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Naive Bayes Classifier", "to": "Conditional Independence Assumption", "arrows": "to", "title": "Subtopic of Naive Bayes Classifier", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning", "to": "Text Classification", "arrows": "to", "title": "Subtopic of Machine Learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Text Classification", "to": "Spam Filter", "arrows": "to", "title": "Subtopic of Text Classification", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Spam Filter", "to": "Training Set", "arrows": "to", "title": "Subtopic of Spam Filter", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Spam Filter", "to": "Feature Vector", "arrows": "to", "title": "Subtopic of Spam Filter", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Feature Vector", "to": "Vocabulary", "arrows": "to", "title": "Subtopic of Feature Vector", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Training Set", "to": "Stop Words", "arrows": "to", "title": "Subtopic of Training Set", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Naive Bayes Algorithm", "to": "Bayesian Inference", "arrows": "to", "title": "Subtopic of Naive Bayes Algorithm", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Naive Bayes Algorithm", "to": "Parameter Estimation", "arrows": "to", "title": "Subtopic of Naive Bayes Algorithm", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Naive Bayes Algorithm", "to": "Binary Features", "arrows": "to", "title": "Subtopic of Naive Bayes Algorithm", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Naive Bayes Algorithm", "to": "Multinomial Features", "arrows": "to", "title": "Subtopic of Naive Bayes Algorithm", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Laplace Smoothing", "to": "Spam Classification Example", "arrows": "to", "title": "Subtopic of Laplace Smoothing", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Conferences", "to": "NeurIPS Conference", "arrows": "to", "title": "Subtopic of Machine Learning Conferences", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Parameter Estimation", "to": "Zero Frequency Problem", "arrows": "to", "title": "Subtopic of Parameter Estimation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Probability Estimation", "to": "Multinomial Random Variable", "arrows": "to", "title": "Subtopic of Probability Estimation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Probability Estimation", "to": "Maximum Likelihood Estimates", "arrows": "to", "title": "Subtopic of Probability Estimation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Naive Bayes Classifier", "to": "Bernoulli Event Model", "arrows": "to", "title": "Subtopic of Naive Bayes Classifier", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Generative Learning Algorithms", "to": "Multinomial Event Model", "arrows": "to", "title": "Subtopic of Generative Learning Algorithms", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Multinomial Event Model", "to": "Spam/Non-Spam Classification", "arrows": "to", "title": "Subtopic of Multinomial Event Model", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Multinomial Event Model", "to": "Word Generation Process", "arrows": "to", "title": "Subtopic of Multinomial Event Model", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Multinomial Event Model", "to": "Probability Calculation", "arrows": "to", "title": "Subtopic of Multinomial Event Model", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "Feature Map", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "Linear Function Over Features", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Feature Map", "to": "Attributes", "arrows": "to", "title": "Subtopic of Feature Map", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Feature Map", "to": "Features Variables", "arrows": "to", "title": "Subtopic of Feature Map", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Linear Function Over Features", "to": "Cubic Function Representation", "arrows": "to", "title": "Subtopic of Linear Function Over Features", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "LMS with Features", "to": "Gradient Descent Update Rule", "arrows": "to", "title": "Subtopic of LMS with Features", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Gradient Descent Update Rule", "to": "Feature Mapping", "arrows": "to", "title": "Subtopic of Gradient Descent Update Rule", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Algorithms", "to": "Kernel Trick", "arrows": "to", "title": "Subtopic of Machine Learning Algorithms", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Gradient Descent Update Rule", "to": "Computational Complexity", "arrows": "to", "title": "Subtopic of Gradient Descent Update Rule", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Feature Mapping", "to": "Iterative Update Process", "arrows": "to", "title": "Subtopic of Feature Mapping", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Kernel Trick", "to": "Linear Combination Representation", "arrows": "to", "title": "Subtopic of Kernel Trick", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "Feature Maps and Kernels", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Feature Maps and Kernels", "to": "Kernel Function Definition", "arrows": "to", "title": "Subtopic of Feature Maps and Kernels", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Feature Maps and Kernels", "to": "Inner Product Computation", "arrows": "to", "title": "Subtopic of Feature Maps and Kernels", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Feature Maps and Kernels", "to": "Algorithm Implementation", "arrows": "to", "title": "Subtopic of Feature Maps and Kernels", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Batch Gradient Descent", "to": "Beta Update Equation", "arrows": "to", "title": "Subtopic of Batch Gradient Descent", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Batch Gradient Descent", "to": "Feature Map Phi", "arrows": "to", "title": "Subtopic of Batch Gradient Descent", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Inner Product Computation", "to": "Pre-computation Strategy", "arrows": "to", "title": "Subtopic of Inner Product Computation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Inner Product Computation", "to": "Efficient Inner Product Calculation", "arrows": "to", "title": "Subtopic of Inner Product Computation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "Kernels in Machine Learning", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Kernels in Machine Learning", "to": "Kernel Functions", "arrows": "to", "title": "Subtopic of Kernels in Machine Learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Kernel Functions", "to": "Explicit Definition of Kernels", "arrows": "to", "title": "Subtopic of Kernel Functions", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Kernel Functions", "to": "Characterization of Valid Kernels", "arrows": "to", "title": "Subtopic of Kernel Functions", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Kernel Functions", "to": "Computational Efficiency", "arrows": "to", "title": "Subtopic of Kernel Functions", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Feature Mapping", "to": "Polynomial Kernels", "arrows": "to", "title": "Subtopic of Feature Mapping", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "Kernels as Similarity Metrics", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Kernels as Similarity Metrics", "to": "Gaussian Kernel", "arrows": "to", "title": "Subtopic of Kernels as Similarity Metrics", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "Valid Kernels Conditions", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "Feature Extraction for Strings", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Kernel Methods", "to": "Support Vector Machines (SVM)", "arrows": "to", "title": "Subtopic of Kernel Methods", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Kernel Matrix Properties", "to": "Sufficient Conditions for Valid Kernels", "arrows": "to", "title": "Subtopic of Kernel Matrix Properties", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Sufficient Conditions for Valid Kernels", "to": "Mercer's Theorem", "arrows": "to", "title": "Subtopic of Sufficient Conditions for Valid Kernels", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Sufficient Conditions for Valid Kernels", "to": "Testing Kernel Validity", "arrows": "to", "title": "Subtopic of Sufficient Conditions for Valid Kernels", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Kernel Functions", "to": "Necessary Conditions for Valid Kernels", "arrows": "to", "title": "Subtopic of Kernel Functions", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Necessary Conditions for Valid Kernels", "to": "Symmetry Property", "arrows": "to", "title": "Subtopic of Necessary Conditions for Valid Kernels", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Necessary Conditions for Valid Kernels", "to": "Positive Semi-Definiteness", "arrows": "to", "title": "Subtopic of Necessary Conditions for Valid Kernels", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Necessary Conditions for Valid Kernels", "to": "Kernel Matrix", "arrows": "to", "title": "Subtopic of Necessary Conditions for Valid Kernels", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "Functional Margins", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "Geometric Margins", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "Support Vector Machines (SVMs)", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Support Vector Machines (SVMs)", "to": "Notation for SVMs", "arrows": "to", "title": "Subtopic of Support Vector Machines (SVMs)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Support Vector Machines (SVMs)", "to": "Functional Margin", "arrows": "to", "title": "Subtopic of Support Vector Machines (SVMs)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Support Vector Machines (SVMs)", "to": "Geometric Margin", "arrows": "to", "title": "Subtopic of Support Vector Machines (SVMs)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Functional Margin", "to": "Confidence Measure Limitation", "arrows": "to", "title": "Subtopic of Functional Margin", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Confidence Measure Limitation", "to": "Normalization Condition", "arrows": "to", "title": "Subtopic of Confidence Measure Limitation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Functional Margin", "to": "Function Margin with Training Set", "arrows": "to", "title": "Subtopic of Functional Margin", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Support Vector Machines (SVM)", "to": "Margins", "arrows": "to", "title": "Subtopic of Support Vector Machines (SVM)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Support Vector Machines (SVM)", "to": "Optimal Margin Classifier", "arrows": "to", "title": "Subtopic of Support Vector Machines (SVM)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Support Vector Machines (SVM)", "to": "Lagrange Duality", "arrows": "to", "title": "Subtopic of Support Vector Machines (SVM)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Support Vector Machines (SVM)", "to": "Kernels", "arrows": "to", "title": "Subtopic of Support Vector Machines (SVM)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Support Vector Machines (SVM)", "to": "SMO Algorithm", "arrows": "to", "title": "Subtopic of Support Vector Machines (SVM)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Decision Boundary", "to": "Vector w", "arrows": "to", "title": "Subtopic of Decision Boundary", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Decision Boundary", "to": "Distance to Decision Boundary", "arrows": "to", "title": "Subtopic of Decision Boundary", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Vector w", "to": "Unit Vector w/||w||", "arrows": "to", "title": "Subtopic of Vector w", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "Model Parameters (w, b)", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Model Parameters (w, b)", "to": "Scaling Constraint on w", "arrows": "to", "title": "Subtopic of Model Parameters (w, b)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Optimal Margin Classifier", "to": "Maximizing Geometric Margin", "arrows": "to", "title": "Subtopic of Optimal Margin Classifier", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Optimization Problem", "to": "Non-Convex Constraint", "arrows": "to", "title": "Subtopic of Optimization Problem", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Optimization Problem", "to": "Scaling Constraint", "arrows": "to", "title": "Subtopic of Optimization Problem", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Optimization Problem", "to": "Objective Function Transformation", "arrows": "to", "title": "Subtopic of Optimization Problem", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Constrained Optimization", "to": "Lagrange Multipliers", "arrows": "to", "title": "Subtopic of Constrained Optimization", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Constrained Optimization", "to": "Generalized Lagrangian", "arrows": "to", "title": "Subtopic of Constrained Optimization", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Constrained Optimization", "to": "Primal Problem", "arrows": "to", "title": "Subtopic of Constrained Optimization", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Generalized Lagrangian", "to": "\\(\\theta_{\\cal P}(w)\\)", "arrows": "to", "title": "Subtopic of Generalized Lagrangian", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Optimization Problem", "to": "Convex Quadratic Objective", "arrows": "to", "title": "Subtopic of Optimization Problem", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Optimization Problem", "to": "Linear Constraints", "arrows": "to", "title": "Subtopic of Optimization Problem", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Lagrange Duality", "to": "Dual Formulation", "arrows": "to", "title": "Subtopic of Lagrange Duality", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "Optimization Problems", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Optimization Problems", "to": "KKT Conditions", "arrows": "to", "title": "Subtopic of Optimization Problems", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "KKT Conditions", "to": "Dual Complementarity Condition", "arrows": "to", "title": "Subtopic of KKT Conditions", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Optimization Problems", "to": "Support Vectors", "arrows": "to", "title": "Subtopic of Optimization Problems", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Optimization Problems", "to": "SVM Optimization Problem", "arrows": "to", "title": "Subtopic of Optimization Problems", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Primal Problem", "to": "Objective Function Primal", "arrows": "to", "title": "Subtopic of Primal Problem", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Dual Problem", "to": "Objective Function Dual", "arrows": "to", "title": "Subtopic of Dual Problem", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Primal Problem", "to": "Primal Constraints", "arrows": "to", "title": "Subtopic of Primal Problem", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Dual Problem", "to": "Dual Constraints", "arrows": "to", "title": "Subtopic of Dual Problem", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Primal Problem", "to": "Value Primal Problem", "arrows": "to", "title": "Subtopic of Primal Problem", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Dual Problem", "to": "Value Dual Problem", "arrows": "to", "title": "Subtopic of Dual Problem", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Optimization Problems", "to": "Duality Gap", "arrows": "to", "title": "Subtopic of Optimization Problems", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Optimization Problems", "to": "Convex Functions", "arrows": "to", "title": "Subtopic of Optimization Problems", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Optimization Problems", "to": "Affine Constraints", "arrows": "to", "title": "Subtopic of Optimization Problems", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Optimization Problems", "to": "Feasibility Conditions", "arrows": "to", "title": "Subtopic of Optimization Problems", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Optimization Problems", "to": "Karush-Kuhn-Tucker (KKT) Conditions", "arrows": "to", "title": "Subtopic of Optimization Problems", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Kernel Trick", "to": "Inner Products", "arrows": "to", "title": "Subtopic of Kernel Trick", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Lagrangian Function", "to": "Optimization Constraints", "arrows": "to", "title": "Subtopic of Lagrangian Function", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Support Vectors", "to": "Support Vectors Definition", "arrows": "to", "title": "Subtopic of Support Vectors", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Lagrangian Optimization", "to": "Dual Problem Formulation", "arrows": "to", "title": "Subtopic of Lagrangian Optimization", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Dual Problem Formulation", "to": "Optimal Parameters Alpha", "arrows": "to", "title": "Subtopic of Dual Problem Formulation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Lagrangian Optimization", "to": "Recovering w from Alpha", "arrows": "to", "title": "Subtopic of Lagrangian Optimization", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Lagrangian Optimization", "to": "Optimal Intercept b", "arrows": "to", "title": "Subtopic of Lagrangian Optimization", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Support Vector Machines (SVM)", "to": "Optimal Parameters Calculation", "arrows": "to", "title": "Subtopic of Support Vector Machines (SVM)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Optimal Parameters Calculation", "to": "Intercept Term Calculation", "arrows": "to", "title": "Subtopic of Optimal Parameters Calculation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Optimal Parameters Calculation", "to": "Prediction Equation", "arrows": "to", "title": "Subtopic of Optimal Parameters Calculation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Support Vector Machines (SVM)", "to": "Dual Form Insight", "arrows": "to", "title": "Subtopic of Support Vector Machines (SVM)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Support Vector Machines (SVM)", "to": "Kernel Application", "arrows": "to", "title": "Subtopic of Support Vector Machines (SVM)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Support Vector Machines", "to": "Non-separable Case", "arrows": "to", "title": "Subtopic of Support Vector Machines", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Regularization", "to": "L1 Regularization", "arrows": "to", "title": "Subtopic of Regularization", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Support Vector Machines (SVM)", "to": "Dual Formulation of SVM", "arrows": "to", "title": "Subtopic of Support Vector Machines (SVM)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Support Vector Machines (SVM)", "to": "Sequential Minimal Optimization (SMO) Algorithm", "arrows": "to", "title": "Subtopic of Support Vector Machines (SVM)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Algorithms", "to": "Coordinate Ascent Algorithm", "arrows": "to", "title": "Subtopic of Machine Learning Algorithms", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Coordinate Ascent", "to": "Quadratic Function Contours", "arrows": "to", "title": "Subtopic of Coordinate Ascent", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Support Vector Machines (SVMs)", "to": "Dual Optimization Problem", "arrows": "to", "title": "Subtopic of Support Vector Machines (SVMs)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Algorithms", "to": "Sequential Minimal Optimization (SMO)", "arrows": "to", "title": "Subtopic of Machine Learning Algorithms", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Sequential Minimal Optimization (SMO)", "to": "Convergence Criteria", "arrows": "to", "title": "Subtopic of Sequential Minimal Optimization (SMO)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Sequential Minimal Optimization (SMO)", "to": "Efficient Update Mechanism", "arrows": "to", "title": "Subtopic of Sequential Minimal Optimization (SMO)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Efficient Update Mechanism", "to": "Constraints Handling", "arrows": "to", "title": "Subtopic of Efficient Update Mechanism", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Optimization Constraints", "to": "Alpha Variables", "arrows": "to", "title": "Subtopic of Optimization Constraints", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Optimization Constraints", "to": "Quadratic Function", "arrows": "to", "title": "Subtopic of Optimization Constraints", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Optimization Constraints", "to": "Box Constraint", "arrows": "to", "title": "Subtopic of Optimization Constraints", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Alpha Variables", "to": "Derivation Example", "arrows": "to", "title": "Subtopic of Alpha Variables", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "SMO Algorithm", "to": "Alpha Value Update", "arrows": "to", "title": "Subtopic of SMO Algorithm", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Deep Learning Introduction", "to": "Supervised Learning with Non-Linear Models", "arrows": "to", "title": "Subtopic of Deep Learning Introduction", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Models", "to": "Non-linear Model h_\u03b8(x)", "arrows": "to", "title": "Subtopic of Machine Learning Models", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Models", "to": "Training Examples", "arrows": "to", "title": "Subtopic of Machine Learning Models", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Models", "to": "Regression Problems", "arrows": "to", "title": "Subtopic of Machine Learning Models", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Regression Problems", "to": "Mean-Square Cost Function", "arrows": "to", "title": "Subtopic of Regression Problems", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Loss Function", "to": "Average Loss", "arrows": "to", "title": "Subtopic of Loss Function", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Negative Log-Likelihood", "to": "Conditional Probabilistic Models", "arrows": "to", "title": "Subtopic of Negative Log-Likelihood", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Optimizers", "to": "Gradient Descent (GD)", "arrows": "to", "title": "Subtopic of Optimizers", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Optimizers", "to": "Stochastic Gradient Descent (SGD)", "arrows": "to", "title": "Subtopic of Optimizers", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Gradient Descent (GD)", "to": "Learning Rate", "arrows": "to", "title": "Subtopic of Gradient Descent (GD)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Logistic Regression", "to": "Negative Likelihood Loss Function", "arrows": "to", "title": "Subtopic of Logistic Regression", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Logistic Regression", "to": "Total Loss Function", "arrows": "to", "title": "Subtopic of Logistic Regression", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Multi-class Classification", "to": "Logits in Multi-class", "arrows": "to", "title": "Subtopic of Multi-class Classification", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Multi-class Classification", "to": "Negative Log-likelihood Loss Function (Multi-class)", "arrows": "to", "title": "Subtopic of Multi-class Classification", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Neural Networks", "to": "Single Neuron Network", "arrows": "to", "title": "Subtopic of Neural Networks", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Single Neuron Network", "to": "Housing Price Prediction", "arrows": "to", "title": "Subtopic of Single Neuron Network", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Single Neuron Network", "to": "ReLU Function", "arrows": "to", "title": "Subtopic of Single Neuron Network", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Neural Networks", "to": "Activation Functions", "arrows": "to", "title": "Subtopic of Neural Networks", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Neural Networks", "to": "Single Neuron Model", "arrows": "to", "title": "Subtopic of Neural Networks", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Neural Networks", "to": "Stacking Neurons", "arrows": "to", "title": "Subtopic of Neural Networks", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Stacking Neurons", "to": "Housing Prediction Example", "arrows": "to", "title": "Subtopic of Stacking Neurons", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Algorithms", "to": "Mini-batch SGD", "arrows": "to", "title": "Subtopic of Machine Learning Algorithms", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Stochastic Gradient Descent (SGD)", "to": "Hyperparameters", "arrows": "to", "title": "Subtopic of Stochastic Gradient Descent (SGD)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Mini-batch SGD", "to": "Mini-batch Hyperparameters", "arrows": "to", "title": "Subtopic of Mini-batch SGD", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Algorithms", "to": "Parameter Initialization", "arrows": "to", "title": "Subtopic of Machine Learning Algorithms", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Mini-batch SGD", "to": "Mini-batch Gradient Calculation", "arrows": "to", "title": "Subtopic of Mini-batch SGD", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Algorithms", "to": "Deep Learning Model Training Steps", "arrows": "to", "title": "Subtopic of Machine Learning Algorithms", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Neural Networks", "to": "Parameters (\u03b8)", "arrows": "to", "title": "Subtopic of Neural Networks", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Neural Networks", "to": "Biological Inspiration", "arrows": "to", "title": "Subtopic of Neural Networks", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Neural Networks", "to": "Two-Layer Neural Network", "arrows": "to", "title": "Subtopic of Neural Networks", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Housing Price Prediction", "to": "Derived Features", "arrows": "to", "title": "Subtopic of Housing Price Prediction", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Derived Features", "to": "Family Size", "arrows": "to", "title": "Subtopic of Derived Features", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Derived Features", "to": "Walkability", "arrows": "to", "title": "Subtopic of Derived Features", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Derived Features", "to": "School Quality", "arrows": "to", "title": "Subtopic of Derived Features", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "Neural Network Inputs", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "Hidden Units", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "ReLU Activation Function", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "Output Layer", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Vectorization in Neural Networks", "to": "Matrix Algebra", "arrows": "to", "title": "Subtopic of Vectorization in Neural Networks", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Vectorization in Neural Networks", "to": "BLAS Packages", "arrows": "to", "title": "Subtopic of Vectorization in Neural Networks", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Vectorization in Neural Networks", "to": "Two-Layer Fully-Connected Network", "arrows": "to", "title": "Subtopic of Vectorization in Neural Networks", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Two-Layer Fully-Connected Network", "to": "Weight Matrix W^[1]", "arrows": "to", "title": "Subtopic of Two-Layer Fully-Connected Network", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Models", "to": "Fully-Connected Neural Networks", "arrows": "to", "title": "Subtopic of Machine Learning Models", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Fully-Connected Neural Networks", "to": "Intermediate Variables (a_i)", "arrows": "to", "title": "Subtopic of Fully-Connected Neural Networks", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Models", "to": "Vectorization", "arrows": "to", "title": "Subtopic of Machine Learning Models", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Two-Layer Neural Network", "to": "Weight Matrices", "arrows": "to", "title": "Subtopic of Two-Layer Neural Network", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Two-Layer Neural Network", "to": "Bias Vectors", "arrows": "to", "title": "Subtopic of Two-Layer Neural Network", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Two-Layer Neural Network", "to": "Hidden Layer", "arrows": "to", "title": "Subtopic of Two-Layer Neural Network", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Neural Networks", "to": "Multi-layer Networks", "arrows": "to", "title": "Subtopic of Neural Networks", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Multi-layer Fully-Connected Neural Networks", "to": "Weight Matrices and Biases", "arrows": "to", "title": "Subtopic of Multi-layer Fully-Connected Neural Networks", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Multi-layer Fully-Connected Neural Networks", "to": "Total Number of Neurons", "arrows": "to", "title": "Subtopic of Multi-layer Fully-Connected Neural Networks", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Multi-layer Fully-Connected Neural Networks", "to": "Total Number of Parameters", "arrows": "to", "title": "Subtopic of Multi-layer Fully-Connected Neural Networks", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Multi-layer Fully-Connected Neural Networks", "to": "Notational Consistency", "arrows": "to", "title": "Subtopic of Multi-layer Fully-Connected Neural Networks", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Overview", "to": "Feature Engineering", "arrows": "to", "title": "Subtopic of Machine Learning Overview", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Deep Learning", "to": "Neural Networks Parameters", "arrows": "to", "title": "Subtopic of Deep Learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Deep Learning", "to": "Learned Features", "arrows": "to", "title": "Subtopic of Deep Learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Activation Functions", "to": "Sigmoid Function", "arrows": "to", "title": "Subtopic of Activation Functions", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Activation Functions", "to": "Tanh Function", "arrows": "to", "title": "Subtopic of Activation Functions", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Activation Functions", "to": "Leaky ReLU", "arrows": "to", "title": "Subtopic of Activation Functions", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Activation Functions", "to": "GELU Function", "arrows": "to", "title": "Subtopic of Activation Functions", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Activation Functions", "to": "Softplus Function", "arrows": "to", "title": "Subtopic of Activation Functions", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Activation Functions", "to": "Identity Function", "arrows": "to", "title": "Subtopic of Activation Functions", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Deep Learning Representations", "to": "House Price Prediction Example", "arrows": "to", "title": "Subtopic of Deep Learning Representations", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Deep Learning Representations", "to": "Feature Maps and Representation Transferability", "arrows": "to", "title": "Subtopic of Deep Learning Representations", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Deep Learning Representations", "to": "Complex Features in Neural Networks", "arrows": "to", "title": "Subtopic of Deep Learning Representations", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Modules in Modern Neural Networks", "to": "Matrix Multiplication as a Building Block", "arrows": "to", "title": "Subtopic of Modules in Modern Neural Networks", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Modules in Modern Neural Networks", "to": "MLP Composition of Modules", "arrows": "to", "title": "Subtopic of Modules in Modern Neural Networks", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Layer Normalization", "to": "LN-S(z)", "arrows": "to", "title": "Subtopic of Layer Normalization", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Layer Normalization", "to": "Affine Transformation", "arrows": "to", "title": "Subtopic of Layer Normalization", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Layer Normalization", "to": "Scaling-Invariant Property", "arrows": "to", "title": "Subtopic of Layer Normalization", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Models", "to": "MLP (Multi-Layer Perceptron)", "arrows": "to", "title": "Subtopic of Machine Learning Models", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MLP (Multi-Layer Perceptron)", "to": "Matrix Multiplication Module", "arrows": "to", "title": "Subtopic of MLP (Multi-Layer Perceptron)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MLP (Multi-Layer Perceptron)", "to": "Nonlinear Activation Module", "arrows": "to", "title": "Subtopic of MLP (Multi-Layer Perceptron)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Models", "to": "ResNet (Residual Network)", "arrows": "to", "title": "Subtopic of Machine Learning Models", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "ResNet (Residual Network)", "to": "Residual Block", "arrows": "to", "title": "Subtopic of ResNet (Residual Network)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "ResNet (Residual Network)", "to": "Simplified ResNet Architecture", "arrows": "to", "title": "Subtopic of ResNet (Residual Network)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Architectures", "to": "ResNet Architecture", "arrows": "to", "title": "Subtopic of Machine Learning Architectures", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "ResNet Architecture", "to": "Convolutional Layers", "arrows": "to", "title": "Subtopic of ResNet Architecture", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "ResNet Architecture", "to": "Batch Normalization Variants", "arrows": "to", "title": "Subtopic of ResNet Architecture", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Architectures", "to": "Transformer Architecture", "arrows": "to", "title": "Subtopic of Machine Learning Architectures", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Batch Normalization Variants", "to": "Layer Normalization (LN)", "arrows": "to", "title": "Subtopic of Batch Normalization Variants", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Layer Normalization (LN)", "to": "LN-S Module", "arrows": "to", "title": "Subtopic of Layer Normalization (LN)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Layer Normalization (LN)", "to": "Affine Transformation in LN", "arrows": "to", "title": "Subtopic of Layer Normalization (LN)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Convolutional Layers", "to": "Parameter Sharing", "arrows": "to", "title": "Subtopic of Convolutional Layers", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Convolutional Layers", "to": "Efficiency Comparison", "arrows": "to", "title": "Subtopic of Convolutional Layers", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Convolutional Layers", "to": "Channel Concept", "arrows": "to", "title": "Subtopic of Convolutional Layers", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning", "to": "Convolutional Neural Networks (CNN)", "arrows": "to", "title": "Subtopic of Machine Learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Convolutional Neural Networks (CNN)", "to": "1-D Convolution Layer", "arrows": "to", "title": "Subtopic of Convolutional Neural Networks (CNN)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "1-D Convolution Layer", "to": "Filter Vector", "arrows": "to", "title": "Subtopic of 1-D Convolution Layer", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "1-D Convolution Layer", "to": "Bias Scalar", "arrows": "to", "title": "Subtopic of 1-D Convolution Layer", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "1-D Convolution Layer", "to": "Matrix Multiplication with Shared Parameters", "arrows": "to", "title": "Subtopic of 1-D Convolution Layer", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "Conv1D-S Module", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "Total Parameters Conv1D", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "2-D Convolution (Conv2D-S)", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "Total Parameters Conv2D", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Layer Normalization (LN)", "to": "Scale-Invariant Property", "arrows": "to", "title": "Subtopic of Layer Normalization (LN)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "Other Normalization Layers", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Other Normalization Layers", "to": "Batch Normalization", "arrows": "to", "title": "Subtopic of Other Normalization Layers", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Other Normalization Layers", "to": "Group Normalization", "arrows": "to", "title": "Subtopic of Other Normalization Layers", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "Convolutional Neural Networks (CNNs)", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Backpropagation", "to": "Differentiable Circuit", "arrows": "to", "title": "Subtopic of Backpropagation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Backpropagation", "to": "Gradient Computation", "arrows": "to", "title": "Subtopic of Backpropagation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Fundamentals", "to": "Backward Function in Machine Learning", "arrows": "to", "title": "Subtopic of Machine Learning Fundamentals", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Backward Function in Machine Learning", "to": "Jacobian Matrix", "arrows": "to", "title": "Subtopic of Backward Function in Machine Learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Backward Function in Machine Learning", "to": "Chain Rule Application", "arrows": "to", "title": "Subtopic of Backward Function in Machine Learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Fundamentals", "to": "Partial Derivatives in ML", "arrows": "to", "title": "Subtopic of Machine Learning Fundamentals", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Fundamentals", "to": "Chain Rule for Auto-Differentiation", "arrows": "to", "title": "Subtopic of Machine Learning Fundamentals", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Partial Derivatives in ML", "to": "Scalar Functions and Vectors", "arrows": "to", "title": "Subtopic of Partial Derivatives in ML", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Partial Derivatives in ML", "to": "Multi-Variate Function Challenges", "arrows": "to", "title": "Subtopic of Partial Derivatives in ML", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Backpropagation", "to": "Chain Rule", "arrows": "to", "title": "Subtopic of Backpropagation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Backpropagation", "to": "Loss Function Composition", "arrows": "to", "title": "Subtopic of Backpropagation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Backpropagation", "to": "Auto-Differentiation", "arrows": "to", "title": "Subtopic of Backpropagation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "Deep Learning Packages", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Fundamentals", "to": "Backpropagation Algorithm", "arrows": "to", "title": "Subtopic of Machine Learning Fundamentals", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Backpropagation Algorithm", "to": "Efficiency of Backward Functions", "arrows": "to", "title": "Subtopic of Backpropagation Algorithm", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Neural Networks", "to": "Backward Functions", "arrows": "to", "title": "Subtopic of Neural Networks", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Backward Functions", "to": "Matrix Multiplication Module (MM)", "arrows": "to", "title": "Subtopic of Backward Functions", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Backward Functions", "to": "Activations", "arrows": "to", "title": "Subtopic of Backward Functions", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Backward Function Overview", "to": "Matrix Multiplication Backward Function", "arrows": "to", "title": "Subtopic of Backward Function Overview", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Matrix Multiplication Backward Function", "to": "Vectorized Notation", "arrows": "to", "title": "Subtopic of Matrix Multiplication Backward Function", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Matrix Multiplication Backward Function", "to": "Efficiency Considerations", "arrows": "to", "title": "Subtopic of Matrix Multiplication Backward Function", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Backward Function Overview", "to": "Activation Functions Backward Function", "arrows": "to", "title": "Subtopic of Backward Function Overview", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "Binary Classification Problem", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Binary Classification Problem", "to": "MLP Model", "arrows": "to", "title": "Subtopic of Binary Classification Problem", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MLP Model", "to": "Modules and Parameters", "arrows": "to", "title": "Subtopic of MLP Model", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Loss Function", "to": "Intermediate Variables", "arrows": "to", "title": "Subtopic of Loss Function", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Backpropagation", "to": "Forward Pass", "arrows": "to", "title": "Subtopic of Backpropagation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Backpropagation", "to": "Backward Pass", "arrows": "to", "title": "Subtopic of Backpropagation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Backpropagation", "to": "Intermediate Values Storage", "arrows": "to", "title": "Subtopic of Backpropagation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Parallelism in Training Examples", "to": "Basic Idea", "arrows": "to", "title": "Subtopic of Parallelism in Training Examples", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Fundamentals", "to": "Backward Function for Loss Functions", "arrows": "to", "title": "Subtopic of Machine Learning Fundamentals", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Backward Function for Loss Functions", "to": "Squared Loss (MSE)", "arrows": "to", "title": "Subtopic of Backward Function for Loss Functions", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Backward Function for Loss Functions", "to": "Logistic Loss", "arrows": "to", "title": "Subtopic of Backward Function for Loss Functions", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Loss Functions", "to": "Cross-Entropy Loss Function", "arrows": "to", "title": "Subtopic of Machine Learning Loss Functions", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Back-propagation for MLPs", "to": "Forward Pass in MLP", "arrows": "to", "title": "Subtopic of Back-propagation for MLPs", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Matrix Notation in Machine Learning", "to": "Training Examples Representation", "arrows": "to", "title": "Subtopic of Matrix Notation in Machine Learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Training Examples Representation", "to": "First-Layer Activations", "arrows": "to", "title": "Subtopic of Training Examples Representation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Vectorization", "to": "Broadcasting", "arrows": "to", "title": "Subtopic of Vectorization", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Matrix Notation in Machine Learning", "to": "Generalization to Multiple Layers", "arrows": "to", "title": "Subtopic of Matrix Notation in Machine Learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Models", "to": "Matricization Approach", "arrows": "to", "title": "Subtopic of Machine Learning Models", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Matricization Approach", "to": "Implementation Subtlety", "arrows": "to", "title": "Subtopic of Matricization Approach", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Implementation Subtlety", "to": "Data Matrix Representation", "arrows": "to", "title": "Subtopic of Implementation Subtlety", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Generalization and Regularization", "to": "Training Loss Function", "arrows": "to", "title": "Subtopic of Generalization and Regularization", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Loss Functions", "to": "Training Loss", "arrows": "to", "title": "Subtopic of Loss Functions", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Loss Functions", "to": "Test Error", "arrows": "to", "title": "Subtopic of Loss Functions", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Training Loss", "to": "Mean Squared Error (MSE)", "arrows": "to", "title": "Subtopic of Training Loss", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Loss Functions", "to": "Empirical Distribution", "arrows": "to", "title": "Subtopic of Loss Functions", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Loss Functions", "to": "Population Distribution", "arrows": "to", "title": "Subtopic of Loss Functions", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Basics", "to": "Training vs Test Distributions", "arrows": "to", "title": "Subtopic of Machine Learning Basics", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Training vs Test Distributions", "to": "Generalization Gap", "arrows": "to", "title": "Subtopic of Training vs Test Distributions", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Bias-Variance Tradeoff", "to": "Double Descent Phenomenon", "arrows": "to", "title": "Subtopic of Bias-Variance Tradeoff", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Bias-Variance Tradeoff", "to": "Training and Test Datasets", "arrows": "to", "title": "Subtopic of Bias-Variance Tradeoff", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Bias-Variance Tradeoff", "to": "Linear Regression Models", "arrows": "to", "title": "Subtopic of Bias-Variance Tradeoff", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Training and Test Datasets", "to": "Quadratic Function Example", "arrows": "to", "title": "Subtopic of Training and Test Datasets", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "Polynomial Fitting", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Polynomial Fitting", "to": "Variance", "arrows": "to", "title": "Subtopic of Polynomial Fitting", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Models", "to": "Linear Model Limitations", "arrows": "to", "title": "Subtopic of Machine Learning Models", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Linear Model Limitations", "to": "Bias Definition", "arrows": "to", "title": "Subtopic of Linear Model Limitations", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Models", "to": "5th Degree Polynomial Models", "arrows": "to", "title": "Subtopic of Machine Learning Models", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "5th Degree Polynomial Models", "to": "Generalization Failure", "arrows": "to", "title": "Subtopic of 5th Degree Polynomial Models", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Bias-Variance Tradeoff", "to": "Model Complexity", "arrows": "to", "title": "Subtopic of Bias-Variance Tradeoff", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Bias-Variance Tradeoff", "to": "Test Error Decomposition", "arrows": "to", "title": "Subtopic of Bias-Variance Tradeoff", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Bias-Variance Tradeoff", "to": "Bias Term", "arrows": "to", "title": "Subtopic of Bias-Variance Tradeoff", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Bias-Variance Tradeoff", "to": "Variance Term", "arrows": "to", "title": "Subtopic of Bias-Variance Tradeoff", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Double Descent Phenomenon", "to": "Model-wise Double Descent", "arrows": "to", "title": "Subtopic of Double Descent Phenomenon", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Fundamentals", "to": "Model Evaluation", "arrows": "to", "title": "Subtopic of Machine Learning Fundamentals", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Mean Squared Error (MSE)", "to": "Average Model (h_avg)", "arrows": "to", "title": "Subtopic of Mean Squared Error (MSE)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Mean Squared Error (MSE)", "to": "Unavoidable Noise (\u03c3^2)", "arrows": "to", "title": "Subtopic of Mean Squared Error (MSE)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Regression Problems", "to": "Training Dataset", "arrows": "to", "title": "Subtopic of Regression Problems", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Regression Problems", "to": "Test Example", "arrows": "to", "title": "Subtopic of Regression Problems", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Bias-Variance Tradeoff", "to": "Expected Test Error", "arrows": "to", "title": "Subtopic of Bias-Variance Tradeoff", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Mean Squared Error (MSE)", "to": "Claim 8.1.1", "arrows": "to", "title": "Subtopic of Mean Squared Error (MSE)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "Model Complexity and Test Errors", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Double Descent Phenomenon", "to": "Overparameterized Models", "arrows": "to", "title": "Subtopic of Double Descent Phenomenon", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Double Descent Phenomenon", "to": "Sample-wise Double Descent", "arrows": "to", "title": "Subtopic of Double Descent Phenomenon", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "Historical Context and Recent Discoveries", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Sample-wise Double Descent", "to": "Optimal Algorithms", "arrows": "to", "title": "Subtopic of Sample-wise Double Descent", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Double Descent Phenomenon", "to": "Regularization Tuning", "arrows": "to", "title": "Subtopic of Double Descent Phenomenon", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Model-wise Double Descent", "to": "Implicit Regularization", "arrows": "to", "title": "Subtopic of Model-wise Double Descent", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Theory", "to": "Learning Guarantees", "arrows": "to", "title": "Subtopic of Machine Learning Theory", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Learning Guarantees", "to": "Union Bound Lemma", "arrows": "to", "title": "Subtopic of Learning Guarantees", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Learning Guarantees", "to": "Hoeffding Inequality (Chernoff Bound)", "arrows": "to", "title": "Subtopic of Learning Guarantees", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "Sample Complexity Bounds", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Sample Complexity Bounds", "to": "Model Selection Methods", "arrows": "to", "title": "Subtopic of Sample Complexity Bounds", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Sample Complexity Bounds", "to": "Generalization Error", "arrows": "to", "title": "Subtopic of Sample Complexity Bounds", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "Learning Theory", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "Gradient Descent Optimizer", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Gradient Descent Optimizer", "to": "Minimum Norm Solution", "arrows": "to", "title": "Subtopic of Gradient Descent Optimizer", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Double Descent Phenomenon", "to": "Model Complexity Measures", "arrows": "to", "title": "Subtopic of Double Descent Phenomenon", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Model Complexity Measures", "to": "Number of Parameters", "arrows": "to", "title": "Subtopic of Model Complexity Measures", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Model Complexity Measures", "to": "Norm of Learned Model", "arrows": "to", "title": "Subtopic of Model Complexity Measures", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Double Descent Phenomenon", "to": "Regularization Strength", "arrows": "to", "title": "Subtopic of Double Descent Phenomenon", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "Hypothesis Function", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "Training Error", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "Empirical Risk Minimization (ERM)", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "PAC Assumptions", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Basics", "to": "Hypothesis Class", "arrows": "to", "title": "Subtopic of Machine Learning Basics", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Hypothesis Class", "to": "Finite Hypothesis Classes", "arrows": "to", "title": "Subtopic of Hypothesis Class", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Uniform Convergence", "to": "Training Error vs Generalization Error", "arrows": "to", "title": "Subtopic of Uniform Convergence", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Uniform Convergence", "to": "Union Bound Application", "arrows": "to", "title": "Subtopic of Uniform Convergence", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Training Error vs Generalization Error", "to": "Probability of Error", "arrows": "to", "title": "Subtopic of Training Error vs Generalization Error", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Training Error vs Generalization Error", "to": "Sample Size Determination", "arrows": "to", "title": "Subtopic of Training Error vs Generalization Error", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "Empirical Risk Minimization", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "Generalization Error Guarantees", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Generalization Error Guarantees", "to": "Bernoulli Random Variable Z", "arrows": "to", "title": "Subtopic of Generalization Error Guarantees", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Bernoulli Random Variable Z", "to": "Training Set Sampling", "arrows": "to", "title": "Subtopic of Bernoulli Random Variable Z", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Generalization Error Guarantees", "to": "Hoeffding Inequality", "arrows": "to", "title": "Subtopic of Generalization Error Guarantees", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Theory", "to": "Sample Complexity", "arrows": "to", "title": "Subtopic of Machine Learning Theory", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Theory", "to": "Hypotheses Space (H)", "arrows": "to", "title": "Subtopic of Machine Learning Theory", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Theory", "to": "Optimal Hypothesis (h*)", "arrows": "to", "title": "Subtopic of Machine Learning Theory", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Hypothesis Class Switching", "to": "Bias Decrease", "arrows": "to", "title": "Subtopic of Hypothesis Class Switching", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Hypothesis Class Switching", "to": "Variance Increase", "arrows": "to", "title": "Subtopic of Hypothesis Class Switching", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Sample Complexity Bound", "to": "Corollary Proof", "arrows": "to", "title": "Subtopic of Sample Complexity Bound", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Infinite Hypothesis Classes", "to": "Bit Representation", "arrows": "to", "title": "Subtopic of Infinite Hypothesis Classes", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Sample Complexity", "to": "Floating Point Representation", "arrows": "to", "title": "Subtopic of Sample Complexity", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Sample Complexity", "to": "Hypothesis Class Size", "arrows": "to", "title": "Subtopic of Sample Complexity", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Empirical Risk Minimization (ERM)", "to": "Non-ERM Algorithms", "arrows": "to", "title": "Subtopic of Empirical Risk Minimization (ERM)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "Hypothesis Space", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Hypothesis Space", "to": "Parameterization of Hypotheses", "arrows": "to", "title": "Subtopic of Hypothesis Space", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "VC Dimension", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "VC Dimension", "to": "Shattering Sets", "arrows": "to", "title": "Subtopic of VC Dimension", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Regularization in Deep Learning", "to": "Explicit Regularization Techniques", "arrows": "to", "title": "Subtopic of Regularization in Deep Learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Regularization in Deep Learning", "to": "Implicit Regularization Effect", "arrows": "to", "title": "Subtopic of Regularization in Deep Learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Regularization in Machine Learning", "to": "Sparsity Regularization", "arrows": "to", "title": "Subtopic of Regularization in Machine Learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Sparsity Regularization", "to": "L1 Norm (LASSO)", "arrows": "to", "title": "Subtopic of Sparsity Regularization", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Regularization in Machine Learning", "to": "L2 Norm Regularization", "arrows": "to", "title": "Subtopic of Regularization in Machine Learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Regularization in Machine Learning", "to": "Deep Learning Regularization Techniques", "arrows": "to", "title": "Subtopic of Regularization in Machine Learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Optimizers and Generalization", "to": "Global Minima Variability", "arrows": "to", "title": "Subtopic of Optimizers and Generalization", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Model Selection via Cross Validation", "to": "Cross Validation Techniques", "arrows": "to", "title": "Subtopic of Model Selection via Cross Validation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Regularization", "to": "Regularized Loss", "arrows": "to", "title": "Subtopic of Regularization", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Regularization", "to": "Regularizer", "arrows": "to", "title": "Subtopic of Regularization", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Regularized Loss", "to": "\u03bb (Lambda)", "arrows": "to", "title": "Subtopic of Regularized Loss", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Regularizer", "to": "\u039b_2 Regularization", "arrows": "to", "title": "Subtopic of Regularizer", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "\u039b_2 Regularization", "to": "Weight Decay", "arrows": "to", "title": "Subtopic of \u039b_2 Regularization", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Regularization", "to": "Inductive Bias", "arrows": "to", "title": "Subtopic of Regularization", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Regularization", "to": "Training Loss/Cost Function", "arrows": "to", "title": "Subtopic of Regularization", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Regularization", "to": "Regularizer Term", "arrows": "to", "title": "Subtopic of Regularization", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Regularization", "to": "Regularization Parameter (\u03bb)", "arrows": "to", "title": "Subtopic of Regularization", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Model Selection", "to": "Cross Validation", "arrows": "to", "title": "Subtopic of Model Selection", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Cross Validation", "to": "Polynomial Regression Models", "arrows": "to", "title": "Subtopic of Cross Validation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Cross Validation", "to": "Regularization Parameters", "arrows": "to", "title": "Subtopic of Cross Validation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Model Selection", "to": "Validation Set Size", "arrows": "to", "title": "Subtopic of Model Selection", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Model Selection", "to": "Hold Out Cross Validation", "arrows": "to", "title": "Subtopic of Model Selection", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Model Selection", "to": "k-fold Cross Validation", "arrows": "to", "title": "Subtopic of Model Selection", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Hold Out Cross Validation", "to": "Retraining on Full Dataset", "arrows": "to", "title": "Subtopic of Hold Out Cross Validation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Cross Validation", "to": "Leave-One-Out Cross Validation", "arrows": "to", "title": "Subtopic of Cross Validation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Techniques", "to": "Data Scarcity", "arrows": "to", "title": "Subtopic of Machine Learning Techniques", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Cross Validation", "to": "Leave-One-Out CV", "arrows": "to", "title": "Subtopic of Cross Validation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Bayesian Statistics", "to": "MLE", "arrows": "to", "title": "Subtopic of Bayesian Statistics", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Bayesian Statistics", "to": "Prior Distribution", "arrows": "to", "title": "Subtopic of Bayesian Statistics", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Cross Validation", "to": "Hold-out Cross Validation", "arrows": "to", "title": "Subtopic of Cross Validation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Algorithms", "to": "Training Set S", "arrows": "to", "title": "Subtopic of Machine Learning Algorithms", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Empirical Risk Minimization", "to": "Hypotheses Training", "arrows": "to", "title": "Subtopic of Empirical Risk Minimization", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Empirical Risk Minimization", "to": "Training Error Selection", "arrows": "to", "title": "Subtopic of Empirical Risk Minimization", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Hold-out Cross Validation", "to": "Validation Set S_cv", "arrows": "to", "title": "Subtopic of Hold-out Cross Validation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Hold-out Cross Validation", "to": "Model Selection Based on Validation Error", "arrows": "to", "title": "Subtopic of Hold-out Cross Validation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Bayesian Machine Learning", "to": "Predictive Distribution", "arrows": "to", "title": "Subtopic of Bayesian Machine Learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Bayesian Machine Learning", "to": "Fully Bayesian Prediction", "arrows": "to", "title": "Subtopic of Bayesian Machine Learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Bayesian Machine Learning", "to": "Computational Challenges", "arrows": "to", "title": "Subtopic of Bayesian Machine Learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "k-means Algorithm", "to": "Initialization", "arrows": "to", "title": "Subtopic of k-means Algorithm", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "k-means Algorithm", "to": "Convergence", "arrows": "to", "title": "Subtopic of k-means Algorithm", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "k-means Algorithm", "to": "Distortion Function", "arrows": "to", "title": "Subtopic of k-means Algorithm", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "k-means Algorithm", "to": "Coordinate Descent on J", "arrows": "to", "title": "Subtopic of k-means Algorithm", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "k-means Algorithm", "to": "Distortion Function J", "arrows": "to", "title": "Subtopic of k-means Algorithm", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "k-means Algorithm", "to": "Convergence Properties", "arrows": "to", "title": "Subtopic of k-means Algorithm", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "EM Algorithms", "to": "EM for Mixture of Gaussians", "arrows": "to", "title": "Subtopic of EM Algorithms", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Bayesian Inference", "to": "Posterior Approximation", "arrows": "to", "title": "Subtopic of Bayesian Inference", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Posterior Approximation", "to": "MAP Estimation", "arrows": "to", "title": "Subtopic of Posterior Approximation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "MAP Estimation", "to": "MLE vs MAP", "arrows": "to", "title": "Subtopic of MAP Estimation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Posterior Approximation", "to": "Prior Selection", "arrows": "to", "title": "Subtopic of Posterior Approximation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Unsupervised Learning", "to": "Clustering", "arrows": "to", "title": "Subtopic of Unsupervised Learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Clustering", "to": "K-Means Algorithm", "arrows": "to", "title": "Subtopic of Clustering", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Unsupervised Learning", "to": "Mixture of Gaussians Model", "arrows": "to", "title": "Subtopic of Unsupervised Learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Mixture of Gaussians Model", "to": "Latent Variables", "arrows": "to", "title": "Subtopic of Mixture of Gaussians Model", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Mixture of Gaussians Model", "to": "Joint Distribution", "arrows": "to", "title": "Subtopic of Mixture of Gaussians Model", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Model Parameters", "to": "Likelihood Estimation", "arrows": "to", "title": "Subtopic of Model Parameters", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Models", "to": "EM Algorithm", "arrows": "to", "title": "Subtopic of Machine Learning Models", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "EM Algorithm", "to": "E-step", "arrows": "to", "title": "Subtopic of EM Algorithm", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "EM Algorithm", "to": "M-step", "arrows": "to", "title": "Subtopic of EM Algorithm", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "E-step", "to": "Gaussian Mixture Model", "arrows": "to", "title": "Subtopic of E-step", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "EM Algorithm", "to": "Soft Assignments", "arrows": "to", "title": "Subtopic of EM Algorithm", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "EM Algorithm", "to": "K-means Clustering", "arrows": "to", "title": "Subtopic of EM Algorithm", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "Expectation-Maximization Algorithm", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Expectation-Maximization Algorithm", "to": "Convergence Guarantees", "arrows": "to", "title": "Subtopic of Expectation-Maximization Algorithm", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Convex Functions", "to": "Strict Convexity", "arrows": "to", "title": "Subtopic of Convex Functions", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Jensen's Inequality", "to": "Theorem Statement", "arrows": "to", "title": "Subtopic of Jensen's Inequality", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Jensen's Inequality", "to": "Concave Functions", "arrows": "to", "title": "Subtopic of Jensen's Inequality", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "EM Algorithm", "to": "Latent Variable Models", "arrows": "to", "title": "Subtopic of EM Algorithm", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "EM Algorithm", "to": "Log-Likelihood Maximization", "arrows": "to", "title": "Subtopic of EM Algorithm", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning_Concepts", "to": "EM_Algorithm", "arrows": "to", "title": "Subtopic of Machine_Learning_Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "EM_Algorithm", "to": "Likelihood_Estimation", "arrows": "to", "title": "Subtopic of EM_Algorithm", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Likelihood_Estimation", "to": "Non_Convex_Optimization", "arrows": "to", "title": "Subtopic of Likelihood_Estimation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "EM_Algorithm", "to": "E_Step", "arrows": "to", "title": "Subtopic of EM_Algorithm", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "EM_Algorithm", "to": "M_Step", "arrows": "to", "title": "Subtopic of EM_Algorithm", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Likelihood_Estimation", "to": "Latent_Variables", "arrows": "to", "title": "Subtopic of Likelihood_Estimation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "EM_Algorithm", "to": "Single_Example_Optimization", "arrows": "to", "title": "Subtopic of EM_Algorithm", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "Evidence Lower Bound (ELBO)", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Jensen's Inequality", "to": "Lower Bound Derivation", "arrows": "to", "title": "Subtopic of Jensen's Inequality", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "Optimizing Q Distribution", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "EM Algorithm", "to": "Log-Likelihood Optimization", "arrows": "to", "title": "Subtopic of EM Algorithm", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Log-Likelihood Optimization", "to": "Single Example Case", "arrows": "to", "title": "Subtopic of Log-Likelihood Optimization", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Log-Likelihood Optimization", "to": "Multiple Examples Case", "arrows": "to", "title": "Subtopic of Log-Likelihood Optimization", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Expectation-Maximization Algorithm", "to": "E-step Calculation", "arrows": "to", "title": "Subtopic of Expectation-Maximization Algorithm", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Expectation-Maximization Algorithm", "to": "M-step Maximization", "arrows": "to", "title": "Subtopic of Expectation-Maximization Algorithm", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "M-step Maximization", "to": "Parameter Updates", "arrows": "to", "title": "Subtopic of M-step Maximization", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Parameter Updates", "to": "\u03b8 Update Rule", "arrows": "to", "title": "Subtopic of Parameter Updates", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "ELBO Interpretation", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "ELBO Interpretation", "to": "Alternative ELBO Formulations", "arrows": "to", "title": "Subtopic of ELBO Interpretation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Alternative ELBO Formulations", "to": "KL Divergence in ELBO", "arrows": "to", "title": "Subtopic of Alternative ELBO Formulations", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "Mixture of Gaussians", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Mixture of Gaussians", "to": "EM Algorithm Steps", "arrows": "to", "title": "Subtopic of Mixture of Gaussians", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "EM Algorithm", "to": "Convergence Proof", "arrows": "to", "title": "Subtopic of EM Algorithm", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "Expectation-Maximization (EM) Algorithm", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Expectation-Maximization (EM) Algorithm", "to": "M-step Update Rule", "arrows": "to", "title": "Subtopic of Expectation-Maximization (EM) Algorithm", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "M-step Update Rule", "to": "Lagrangian Method", "arrows": "to", "title": "Subtopic of M-step Update Rule", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "Variational Inference", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Variational Inference", "to": "Variational Auto-Encoder (VAE)", "arrows": "to", "title": "Subtopic of Variational Inference", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Variational Inference", "to": "ELBO", "arrows": "to", "title": "Subtopic of Variational Inference", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Variational Inference", "to": "Mean Field Assumption", "arrows": "to", "title": "Subtopic of Variational Inference", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Mean Field Assumption", "to": "Discrete Latent Variables", "arrows": "to", "title": "Subtopic of Mean Field Assumption", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Variational Inference", "to": "Continuous Latent Variables", "arrows": "to", "title": "Subtopic of Variational Inference", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Techniques", "to": "Gaussian Mixture Models", "arrows": "to", "title": "Subtopic of Machine Learning Techniques", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Optimizing Continuous Latent Variables", "to": "Succinct Representation of Distribution Qi", "arrows": "to", "title": "Subtopic of Optimizing Continuous Latent Variables", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Succinct Representation of Distribution Qi", "to": "Mean and Variance Functions", "arrows": "to", "title": "Subtopic of Succinct Representation of Distribution Qi", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Optimizing Continuous Latent Variables", "to": "Encoder-Decoder Framework", "arrows": "to", "title": "Subtopic of Optimizing Continuous Latent Variables", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Optimizing Continuous Latent Variables", "to": "Efficient Evaluation of ELBO", "arrows": "to", "title": "Subtopic of Optimizing Continuous Latent Variables", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "ELBO Optimization", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "ELBO Optimization", "to": "Gradient Ascent in ELBO", "arrows": "to", "title": "Subtopic of ELBO Optimization", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Efficient Evaluation of ELBO", "to": "Gaussian Distributions", "arrows": "to", "title": "Subtopic of Efficient Evaluation of ELBO", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "Expectation Maximization (EM)", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Gradient Computation", "to": "Reparameterization Trick", "arrows": "to", "title": "Subtopic of Gradient Computation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Data Normalization", "to": "Mean Removal", "arrows": "to", "title": "Subtopic of Data Normalization", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Data Normalization", "to": "Variance Scaling", "arrows": "to", "title": "Subtopic of Data Normalization", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Major Axis of Variation", "to": "Projection and Variance Maximization", "arrows": "to", "title": "Subtopic of Major Axis of Variation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Reparameterization Trick", "to": "Gradient Estimation", "arrows": "to", "title": "Subtopic of Reparameterization Trick", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Principal Components Analysis (PCA)", "to": "Data Subspace Identification", "arrows": "to", "title": "Subtopic of Principal Components Analysis (PCA)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Data Redundancy Detection", "to": "PCA Algorithm Introduction", "arrows": "to", "title": "Subtopic of Data Redundancy Detection", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "PCA Algorithm Introduction", "to": "Normalization Process", "arrows": "to", "title": "Subtopic of PCA Algorithm Introduction", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Data Redundancy Detection", "to": "Car Example", "arrows": "to", "title": "Subtopic of Data Redundancy Detection", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Data Redundancy Detection", "to": "Pilot Survey Example", "arrows": "to", "title": "Subtopic of Data Redundancy Detection", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Normalization Process", "to": "Normalization Formula", "arrows": "to", "title": "Subtopic of Normalization Process", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Principal Component Analysis (PCA)", "to": "Projection of Data Points", "arrows": "to", "title": "Subtopic of Principal Component Analysis (PCA)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Projection of Data Points", "to": "Variance Maximization", "arrows": "to", "title": "Subtopic of Projection of Data Points", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Variance Maximization", "to": "Empirical Covariance Matrix", "arrows": "to", "title": "Subtopic of Variance Maximization", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Variance Maximization", "to": "Principal Eigenvector", "arrows": "to", "title": "Subtopic of Variance Maximization", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Variance Maximization", "to": "k-Dimensional Subspace", "arrows": "to", "title": "Subtopic of Variance Maximization", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "PCA", "to": "Eigenvectors of Sigma", "arrows": "to", "title": "Subtopic of PCA", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "PCA", "to": "Dimensionality Reduction", "arrows": "to", "title": "Subtopic of PCA", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "PCA", "to": "Principal Components", "arrows": "to", "title": "Subtopic of PCA", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "PCA", "to": "Approximation Error Minimization", "arrows": "to", "title": "Subtopic of PCA", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "PCA", "to": "Applications", "arrows": "to", "title": "Subtopic of PCA", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Independent Component Analysis (ICA)", "to": "Cocktail Party Problem", "arrows": "to", "title": "Subtopic of Independent Component Analysis (ICA)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Independent Component Analysis (ICA)", "to": "Mixing Matrix (A)", "arrows": "to", "title": "Subtopic of Independent Component Analysis (ICA)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Independent Component Analysis (ICA)", "to": "Unmixing Matrix (W)", "arrows": "to", "title": "Subtopic of Independent Component Analysis (ICA)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Principal Component Analysis (PCA)", "to": "Data Visualization", "arrows": "to", "title": "Subtopic of Principal Component Analysis (PCA)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Principal Component Analysis (PCA)", "to": "Dimension Reduction", "arrows": "to", "title": "Subtopic of Principal Component Analysis (PCA)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Principal Component Analysis (PCA)", "to": "Noise Reduction", "arrows": "to", "title": "Subtopic of Principal Component Analysis (PCA)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Noise Reduction", "to": "Eigenfaces Method", "arrows": "to", "title": "Subtopic of Noise Reduction", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "ICA Ambiguities", "to": "Permutation Matrix", "arrows": "to", "title": "Subtopic of ICA Ambiguities", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "ICA Ambiguities", "to": "Scaling Ambiguity", "arrows": "to", "title": "Subtopic of ICA Ambiguities", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "ICA Ambiguities", "to": "Sign Change Ambiguity", "arrows": "to", "title": "Subtopic of ICA Ambiguities", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Density Transformation", "to": "1D Example", "arrows": "to", "title": "Subtopic of Density Transformation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Density Transformation", "to": "General Case", "arrows": "to", "title": "Subtopic of Density Transformation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "ICA Ambiguities", "to": "Scaling Factor", "arrows": "to", "title": "Subtopic of ICA Ambiguities", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "ICA Ambiguities", "to": "Sign Changes Irrelevance", "arrows": "to", "title": "Subtopic of ICA Ambiguities", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "ICA Ambiguities", "to": "Non-Gaussian Sources", "arrows": "to", "title": "Subtopic of ICA Ambiguities", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "ICA Ambiguities", "to": "Gaussian Data Example", "arrows": "to", "title": "Subtopic of ICA Ambiguities", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Gaussian Data Example", "to": "Mixing Matrix A", "arrows": "to", "title": "Subtopic of Gaussian Data Example", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Gaussian Data Example", "to": "Rotation Matrix R", "arrows": "to", "title": "Subtopic of Gaussian Data Example", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Gaussian Data Example", "to": "Mixed Data x'", "arrows": "to", "title": "Subtopic of Gaussian Data Example", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Independent Component Analysis (ICA)", "to": "Mixing Matrix", "arrows": "to", "title": "Subtopic of Independent Component Analysis (ICA)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Independent Component Analysis (ICA)", "to": "Gaussian Data Limitation", "arrows": "to", "title": "Subtopic of Independent Component Analysis (ICA)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Gaussian Data Limitation", "to": "Rotationally Symmetric Distributions", "arrows": "to", "title": "Subtopic of Gaussian Data Limitation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Densities and Linear Transformations", "to": "Linear Transformation Impact", "arrows": "to", "title": "Subtopic of Densities and Linear Transformations", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Maximum Likelihood Estimation", "to": "Joint Distribution of Sources", "arrows": "to", "title": "Subtopic of Maximum Likelihood Estimation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Joint Distribution of Sources", "to": "Density on x=As=W^-1s", "arrows": "to", "title": "Subtopic of Joint Distribution of Sources", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Density on x=As=W^-1s", "to": "Cumulative Distribution Function (CDF)", "arrows": "to", "title": "Subtopic of Density on x=As=W^-1s", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Cumulative Distribution Function (CDF)", "to": "Sigmoid Function as Default Density", "arrows": "to", "title": "Subtopic of Cumulative Distribution Function (CDF)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "Data Preprocessing Assumptions", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "Logistic Function Properties", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "Log Likelihood Function", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "Gradient Ascent Rule", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "Training Example Independence Assumption", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "Stochastic Gradient Ascent", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Self-supervised Learning", "to": "Foundation Models", "arrows": "to", "title": "Subtopic of Self-supervised Learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Foundation Models", "to": "Pretraining Phase", "arrows": "to", "title": "Subtopic of Foundation Models", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Foundation Models", "to": "Adaptation Phase", "arrows": "to", "title": "Subtopic of Foundation Models", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Overview", "to": "Transfer Learning", "arrows": "to", "title": "Subtopic of Machine Learning Overview", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Pretraining Phase", "to": "Unlabeled Dataset", "arrows": "to", "title": "Subtopic of Pretraining Phase", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Adaptation Phase", "to": "Labeled Task Dataset", "arrows": "to", "title": "Subtopic of Adaptation Phase", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Transfer Learning", "to": "Pretrained Model", "arrows": "to", "title": "Subtopic of Transfer Learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Pretraining Phase", "to": "Self-Supervised Loss", "arrows": "to", "title": "Subtopic of Pretraining Phase", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Adaptation Methods", "to": "Labeled Dataset", "arrows": "to", "title": "Subtopic of Machine Learning Adaptation Methods", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Adaptation Methods", "to": "Zero-Shot Learning", "arrows": "to", "title": "Subtopic of Machine Learning Adaptation Methods", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Adaptation Methods", "to": "Few-Shhot Learning", "arrows": "to", "title": "Subtopic of Machine Learning Adaptation Methods", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Adaptation Methods", "to": "Adaptation Algorithm", "arrows": "to", "title": "Subtopic of Machine Learning Adaptation Methods", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Adaptation Algorithm", "to": "Linear Probe Approach", "arrows": "to", "title": "Subtopic of Adaptation Algorithm", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Adaptation Algorithm", "to": "Finetuning Algorithm", "arrows": "to", "title": "Subtopic of Adaptation Algorithm", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Adaptation Methods", "to": "Language Problem Methods", "arrows": "to", "title": "Subtopic of Machine Learning Adaptation Methods", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Self-Supervised Learning", "to": "Representation Function", "arrows": "to", "title": "Subtopic of Self-Supervised Learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Self-Supervised Learning", "to": "Supervised Contrastive Algorithms", "arrows": "to", "title": "Subtopic of Self-Supervised Learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Self-Supervised Learning", "to": "Data Augmentation", "arrows": "to", "title": "Subtopic of Self-Supervised Learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Data Augmentation", "to": "Positive Pair", "arrows": "to", "title": "Subtopic of Data Augmentation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Data Augmentation", "to": "Negative Pair", "arrows": "to", "title": "Subtopic of Data Augmentation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Adaptation Methods", "to": "Finetuning Pretrained Models", "arrows": "to", "title": "Subtopic of Machine Learning Adaptation Methods", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Finetuning Pretrained Models", "to": "Prediction Model Structure", "arrows": "to", "title": "Subtopic of Finetuning Pretrained Models", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Finetuning Pretrained Models", "to": "Optimization Goal", "arrows": "to", "title": "Subtopic of Finetuning Pretrained Models", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Pretraining Methods in CV", "to": "Supervised Pretraining", "arrows": "to", "title": "Subtopic of Pretraining Methods in CV", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Pretraining Methods in CV", "to": "Contrastive Learning", "arrows": "to", "title": "Subtopic of Pretraining Methods in CV", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "Loss Function Analysis", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Pretrained Large Language Models", "to": "Language Model Probability Distribution", "arrows": "to", "title": "Subtopic of Pretrained Large Language Models", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Contrastive Learning", "to": "SIMCLR Algorithm", "arrows": "to", "title": "Subtopic of Contrastive Learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "SIMCLR Algorithm", "to": "Augmentation Techniques", "arrows": "to", "title": "Subtopic of SIMCLR Algorithm", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Models", "to": "Conditional Probability Modeling", "arrows": "to", "title": "Subtopic of Machine Learning Models", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Conditional Probability Modeling", "to": "Parameterized Model", "arrows": "to", "title": "Subtopic of Conditional Probability Modeling", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Parameterized Model", "to": "Embeddings and Representations", "arrows": "to", "title": "Subtopic of Parameterized Model", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Models", "to": "Transformer Model", "arrows": "to", "title": "Subtopic of Machine Learning Models", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Transformer Model", "to": "Input-Output Interface", "arrows": "to", "title": "Subtopic of Transformer Model", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Transformer Model", "to": "Training Process", "arrows": "to", "title": "Subtopic of Transformer Model", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Adaptation Techniques", "to": "Zero-shot Learning", "arrows": "to", "title": "Subtopic of Machine Learning Adaptation Techniques", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Adaptation Techniques", "to": "In-context Learning", "arrows": "to", "title": "Subtopic of Machine Learning Adaptation Techniques", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Zero-shot Learning", "to": "Language Model Utilization", "arrows": "to", "title": "Subtopic of Zero-shot Learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "In-context Learning", "to": "Prompt Construction", "arrows": "to", "title": "Subtopic of In-context Learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Reinforcement Learning", "to": "Sequential Decision Making", "arrows": "to", "title": "Subtopic of Reinforcement Learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Reinforcement Learning", "to": "Reward Function", "arrows": "to", "title": "Subtopic of Reinforcement Learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Techniques", "to": "Language Models", "arrows": "to", "title": "Subtopic of Machine Learning Techniques", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Language Models", "to": "Conditional Probability in Language Models", "arrows": "to", "title": "Subtopic of Language Models", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Conditional Probability in Language Models", "to": "Temperature Parameter", "arrows": "to", "title": "Subtopic of Conditional Probability in Language Models", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Techniques", "to": "Adaptation Methods", "arrows": "to", "title": "Subtopic of Machine Learning Techniques", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Adaptation Methods", "to": "Finetuning", "arrows": "to", "title": "Subtopic of Adaptation Methods", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Policy Execution", "to": "Value Function", "arrows": "to", "title": "Subtopic of Policy Execution", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Value Function", "to": "Bellman Equations", "arrows": "to", "title": "Subtopic of Value Function", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Bellman Equations", "to": "Immediate Reward", "arrows": "to", "title": "Subtopic of Bellman Equations", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Value Function", "to": "Optimal Value Function", "arrows": "to", "title": "Subtopic of Value Function", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Value Function", "to": "Bellman's Equation", "arrows": "to", "title": "Subtopic of Value Function", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Policy", "to": "Optimal Policy", "arrows": "to", "title": "Subtopic of Policy", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Reinforcement Learning", "to": "Markov Decision Processes (MDP)", "arrows": "to", "title": "Subtopic of Reinforcement Learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Markov Decision Processes (MDP)", "to": "States", "arrows": "to", "title": "Subtopic of Markov Decision Processes (MDP)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Markov Decision Processes (MDP)", "to": "Actions", "arrows": "to", "title": "Subtopic of Markov Decision Processes (MDP)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Markov Decision Processes (MDP)", "to": "State Transition Probabilities", "arrows": "to", "title": "Subtopic of Markov Decision Processes (MDP)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Markov Decision Processes (MDP)", "to": "Discount Factor", "arrows": "to", "title": "Subtopic of Markov Decision Processes (MDP)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Reinforcement Learning Overview", "to": "Markov Decision Process (MDP)", "arrows": "to", "title": "Subtopic of Reinforcement Learning Overview", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Markov Decision Process (MDP)", "to": "State Transition", "arrows": "to", "title": "Subtopic of Markov Decision Process (MDP)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Reinforcement Learning Overview", "to": "Total Payoff Calculation", "arrows": "to", "title": "Subtopic of Reinforcement Learning Overview", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Total Payoff Calculation", "to": "Discount Factor (\u03b3)", "arrows": "to", "title": "Subtopic of Total Payoff Calculation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Reinforcement Learning Overview", "to": "Policy Definition", "arrows": "to", "title": "Subtopic of Reinforcement Learning Overview", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Algorithms", "to": "Markov Decision Processes (MDPs)", "arrows": "to", "title": "Subtopic of Machine Learning Algorithms", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Markov Decision Processes (MDPs)", "to": "Value Iteration", "arrows": "to", "title": "Subtopic of Markov Decision Processes (MDPs)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Markov Decision Processes (MDPs)", "to": "Policy Iteration", "arrows": "to", "title": "Subtopic of Markov Decision Processes (MDPs)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Markov Decision Processes (MDPs)", "to": "Learning Model for MDPs", "arrows": "to", "title": "Subtopic of Markov Decision Processes (MDPs)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Learning Model for MDPs", "to": "Inverted Pendulum Problem", "arrows": "to", "title": "Subtopic of Learning Model for MDPs", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Learning Model for MDPs", "to": "State Transition Probabilities Estimation", "arrows": "to", "title": "Subtopic of Learning Model for MDPs", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Overview", "to": "Optimal Policy in MDPs", "arrows": "to", "title": "Subtopic of Machine Learning Overview", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Overview", "to": "Value Iteration and Policy Iteration", "arrows": "to", "title": "Subtopic of Machine Learning Overview", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Value Iteration and Policy Iteration", "to": "Finite-State MDPs", "arrows": "to", "title": "Subtopic of Value Iteration and Policy Iteration", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Value Iteration and Policy Iteration", "to": "Value Iteration Algorithm", "arrows": "to", "title": "Subtopic of Value Iteration and Policy Iteration", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Value Iteration Algorithm", "to": "Synchronous Updates", "arrows": "to", "title": "Subtopic of Value Iteration Algorithm", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Value Iteration Algorithm", "to": "Asynchronous Updates", "arrows": "to", "title": "Subtopic of Value Iteration Algorithm", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Value Iteration", "to": "Convergence of Value Functions", "arrows": "to", "title": "Subtopic of Value Iteration", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Markov Decision Processes (MDP)", "to": "Optimal Policy Determination", "arrows": "to", "title": "Subtopic of Markov Decision Processes (MDP)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Policy Iteration", "to": "Bellman's Equations", "arrows": "to", "title": "Subtopic of Policy Iteration", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Policy Iteration", "to": "Greedy Policy with Respect to V", "arrows": "to", "title": "Subtopic of Policy Iteration", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Markov Decision Processes (MDP)", "to": "Comparison Between Algorithms", "arrows": "to", "title": "Subtopic of Markov Decision Processes (MDP)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "State Transition Probabilities", "to": "Estimating State Transitions", "arrows": "to", "title": "Subtopic of State Transition Probabilities", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Markov Decision Process (MDP)", "to": "Expected Immediate Reward", "arrows": "to", "title": "Subtopic of Markov Decision Process (MDP)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Markov Decision Process (MDP)", "to": "Learning in MDPs with Unknown Transitions", "arrows": "to", "title": "Subtopic of Markov Decision Process (MDP)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Algorithms", "to": "Continuous State MDPs", "arrows": "to", "title": "Subtopic of Machine Learning Algorithms", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Algorithms", "to": "Finite State MDPs", "arrows": "to", "title": "Subtopic of Machine Learning Algorithms", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Model Creation Methods", "to": "Physics Simulation", "arrows": "to", "title": "Subtopic of Model Creation Methods", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Physics Simulation", "to": "Open Dynamics Engine", "arrows": "to", "title": "Subtopic of Physics Simulation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Model Creation Methods", "to": "Learning from Data", "arrows": "to", "title": "Subtopic of Model Creation Methods", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Discretization in MDPs", "to": "Supervised Learning Problem", "arrows": "to", "title": "Subtopic of Discretization in MDPs", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Discretization in MDPs", "to": "Piecewise Constant Representation", "arrows": "to", "title": "Subtopic of Discretization in MDPs", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Discretization in MDPs", "to": "Curse of Dimensionality", "arrows": "to", "title": "Subtopic of Discretization in MDPs", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "State Representation", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Value Function Approximation", "to": "Model or Simulator", "arrows": "to", "title": "Subtopic of Value Function Approximation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Models", "to": "Linear Model Prediction", "arrows": "to", "title": "Subtopic of Machine Learning Models", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Linear Model Prediction", "to": "Learning Algorithm", "arrows": "to", "title": "Subtopic of Linear Model Prediction", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Models", "to": "Deterministic vs Stochastic Models", "arrows": "to", "title": "Subtopic of Machine Learning Models", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Models", "to": "Non-linear Functions", "arrows": "to", "title": "Subtopic of Machine Learning Models", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Overview", "to": "Fitted Value Iteration", "arrows": "to", "title": "Subtopic of Machine Learning Overview", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Supervised Learning", "to": "Regression Algorithms", "arrows": "to", "title": "Subtopic of Supervised Learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Models", "to": "Non-linear Feature Mappings", "arrows": "to", "title": "Subtopic of Machine Learning Models", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Fitted Value Iteration", "to": "Discrete Action Space", "arrows": "to", "title": "Subtopic of Fitted Value Iteration", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Fitted Value Iteration", "to": "Supervised Learning Algorithm", "arrows": "to", "title": "Subtopic of Fitted Value Iteration", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Fitted Value Iteration", "to": "State Sampling", "arrows": "to", "title": "Subtopic of Fitted Value Iteration", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Fitted Value Iteration", "to": "Action Evaluation", "arrows": "to", "title": "Subtopic of Fitted Value Iteration", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Value Iteration", "to": "Expectation Approximation", "arrows": "to", "title": "Subtopic of Value Iteration", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Expectation Approximation", "to": "Deterministic Simulators", "arrows": "to", "title": "Subtopic of Expectation Approximation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Expectation Approximation", "to": "Gaussian Noise Model", "arrows": "to", "title": "Subtopic of Expectation Approximation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Policy Iteration", "to": "Bellman Updates", "arrows": "to", "title": "Subtopic of Policy Iteration", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Policy Iteration", "to": "VE Procedure", "arrows": "to", "title": "Subtopic of Policy Iteration", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "VE Procedure", "to": "k Parameter", "arrows": "to", "title": "Subtopic of VE Procedure", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "VE Procedure", "to": "Initialization Option 1", "arrows": "to", "title": "Subtopic of VE Procedure", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "VE Procedure", "to": "Initialization Option 2", "arrows": "to", "title": "Subtopic of VE Procedure", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "VE Procedure", "to": "Update Rule (15.12)", "arrows": "to", "title": "Subtopic of VE Procedure", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Value Iteration", "to": "Policy Update Rule (15.13)", "arrows": "to", "title": "Subtopic of Value Iteration", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Chapter 15 Overview", "to": "Optimal Bellman Equation", "arrows": "to", "title": "Subtopic of Chapter 15 Overview", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Chapter 15 Overview", "to": "Policy Iteration Speedup", "arrows": "to", "title": "Subtopic of Chapter 15 Overview", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Chapter 15 Overview", "to": "Value Iteration Preference", "arrows": "to", "title": "Subtopic of Chapter 15 Overview", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Finite-horizon MDPs", "to": "Optimal Value Function Recovery", "arrows": "to", "title": "Subtopic of Finite-horizon MDPs", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Markov Decision Processes (MDP)", "to": "Finite Horizon MDPs", "arrows": "to", "title": "Subtopic of Markov Decision Processes (MDP)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Finite Horizon MDPs", "to": "Time-Dependent Policies", "arrows": "to", "title": "Subtopic of Finite Horizon MDPs", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Time-Dependent Policies", "to": "Non-Stationary Optimal Policy", "arrows": "to", "title": "Subtopic of Time-Dependent Policies", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Finite Horizon MDPs", "to": "Dynamic Environment Models", "arrows": "to", "title": "Subtopic of Finite Horizon MDPs", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Markov Decision Processes (MDP)", "to": "Expectation Calculation", "arrows": "to", "title": "Subtopic of Markov Decision Processes (MDP)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Markov Decision Processes (MDP)", "to": "Rewards Dependency on States and Actions", "arrows": "to", "title": "Subtopic of Markov Decision Processes (MDP)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Markov Decision Processes (MDP)", "to": "Infinite Horizon MDPs", "arrows": "to", "title": "Subtopic of Markov Decision Processes (MDP)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Infinite Horizon MDPs", "to": "Discount Factor \u03b3", "arrows": "to", "title": "Subtopic of Infinite Horizon MDPs", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "Value Function in RL", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Value Function in RL", "to": "Dynamic Programming", "arrows": "to", "title": "Subtopic of Value Function in RL", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "Bellman Update", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Value Iteration", "to": "Geometric Convergence", "arrows": "to", "title": "Subtopic of Value Iteration", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Linear Quadratic Regulation (LQR)", "to": "Continuous Setting", "arrows": "to", "title": "Subtopic of Linear Quadratic Regulation (LQR)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Linear Quadratic Regulation (LQR)", "to": "Linear Transitions", "arrows": "to", "title": "Subtopic of Linear Quadratic Regulation (LQR)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Linear Quadratic Regulation (LQR)", "to": "Quadratic Rewards", "arrows": "to", "title": "Subtopic of Linear Quadratic Regulation (LQR)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Optimal Value Function", "to": "Quadratic Assumption", "arrows": "to", "title": "Subtopic of Optimal Value Function", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Optimal Value Function", "to": "Dynamics of Model", "arrows": "to", "title": "Subtopic of Optimal Value Function", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Optimal Policy", "to": "Linear Optimal Action", "arrows": "to", "title": "Subtopic of Optimal Policy", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "LQR Model Assumptions", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "LQR Algorithm Steps", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "LQR Algorithm Steps", "to": "Step 1: Estimate Matrices", "arrows": "to", "title": "Subtopic of LQR Algorithm Steps", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "LQR Algorithm Steps", "to": "Step 2: Derive Optimal Policy", "arrows": "to", "title": "Subtopic of LQR Algorithm Steps", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "Dynamic Programming Application", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "Linear Quadratic Regulator (LQR)", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Linear Quadratic Regulator (LQR)", "to": "Discrete Ricatti Equations", "arrows": "to", "title": "Subtopic of Linear Quadratic Regulator (LQR)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "Inverted Pendulum Example", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "Linearization of Dynamics", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Linearization of Dynamics", "to": "Taylor Expansion", "arrows": "to", "title": "Subtopic of Linearization of Dynamics", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "Optimization in RL", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "LQG Framework", "to": "Partial Observability", "arrows": "to", "title": "Subtopic of LQG Framework", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Differential Dynamic Programming (DDP)", "to": "Nominal Trajectory Generation", "arrows": "to", "title": "Subtopic of Differential Dynamic Programming (DDP)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Linearization of Dynamics", "to": "Rewriting Dynamics", "arrows": "to", "title": "Subtopic of Linearization of Dynamics", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Differential Dynamic Programming (DDP)", "to": "Reward Function Approximation", "arrows": "to", "title": "Subtopic of Differential Dynamic Programming (DDP)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "Partially Observable MDPs (POMDP)", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Partially Observable MDPs (POMDP)", "to": "Observation Layer", "arrows": "to", "title": "Subtopic of Partially Observable MDPs (POMDP)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Partially Observable MDPs (POMDP)", "to": "Belief State", "arrows": "to", "title": "Subtopic of Partially Observable MDPs (POMDP)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "LQR Extension", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Belief State", "to": "Kalman Filter", "arrows": "to", "title": "Subtopic of Belief State", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Algorithms", "to": "LQR Updates", "arrows": "to", "title": "Subtopic of Machine Learning Algorithms", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Policy Gradient (REINFORCE)", "to": "Randomized Policy", "arrows": "to", "title": "Subtopic of Policy Gradient (REINFORCE)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Policy Gradient (REINFORCE)", "to": "Expected Total Payoff", "arrows": "to", "title": "Subtopic of Policy Gradient (REINFORCE)", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Kalman Filter", "to": "Predict Step", "arrows": "to", "title": "Subtopic of Kalman Filter", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Kalman Filter", "to": "Update Step", "arrows": "to", "title": "Subtopic of Kalman Filter", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Kalman Filter", "to": "Belief States Update", "arrows": "to", "title": "Subtopic of Kalman Filter", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Update Step", "to": "Kalman Gain", "arrows": "to", "title": "Subtopic of Update Step", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Kalman Filter", "to": "Step 1", "arrows": "to", "title": "Subtopic of Kalman Filter", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Kalman Filter", "to": "Step 2", "arrows": "to", "title": "Subtopic of Kalman Filter", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Kalman Filter", "to": "Step 3", "arrows": "to", "title": "Subtopic of Kalman Filter", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Concepts", "to": "Policy Gradients", "arrows": "to", "title": "Subtopic of Machine Learning Concepts", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Policy Gradients", "to": "Expectation Estimation", "arrows": "to", "title": "Subtopic of Policy Gradients", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Expectation Estimation", "to": "Sample-Based Estimation", "arrows": "to", "title": "Subtopic of Expectation Estimation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Gradient Calculation", "to": "Log Probability Calculation", "arrows": "to", "title": "Subtopic of Gradient Calculation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Policy Gradients", "to": "Reward Function Estimation", "arrows": "to", "title": "Subtopic of Policy Gradients", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Reward Function Estimation", "to": "Expectation Maximization", "arrows": "to", "title": "Subtopic of Reward Function Estimation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Gradient Ascent", "to": "Reparametrization Technique", "arrows": "to", "title": "Subtopic of Gradient Ascent", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Reward Function Estimation", "to": "REINFORCE Algorithm", "arrows": "to", "title": "Subtopic of Reward Function Estimation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Policy Gradient Theorem", "to": "Log Probability Derivative", "arrows": "to", "title": "Subtopic of Policy Gradient Theorem", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Policy Gradient Theorem", "to": "Vanilla REINFORCE Algorithm", "arrows": "to", "title": "Subtopic of Policy Gradient Theorem", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Log Probability Derivative", "to": "Trajectory Probability Change", "arrows": "to", "title": "Subtopic of Log Probability Derivative", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Vanilla REINFORCE Algorithm", "to": "Empirical Trajectories Estimation", "arrows": "to", "title": "Subtopic of Vanilla REINFORCE Algorithm", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Policy Gradients", "to": "Trajectory Probability", "arrows": "to", "title": "Subtopic of Policy Gradients", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Policy Gradients", "to": "Expectation Equations", "arrows": "to", "title": "Subtopic of Policy Gradients", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Expectation Equations", "to": "Simplification of Formula (17.8)", "arrows": "to", "title": "Subtopic of Expectation Equations", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Reinforcement Learning", "to": "Policy Gradient Methods", "arrows": "to", "title": "Subtopic of Reinforcement Learning", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Policy Gradient Methods", "to": "Law of Total Expectation", "arrows": "to", "title": "Subtopic of Policy Gradient Methods", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Law of Total Expectation", "to": "Estimator Simplification", "arrows": "to", "title": "Subtopic of Law of Total Expectation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Policy Gradient Methods", "to": "Baseline Estimation", "arrows": "to", "title": "Subtopic of Policy Gradient Methods", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Policy Gradient Methods", "to": "Trajectory Collection", "arrows": "to", "title": "Subtopic of Policy Gradient Methods", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Baseline Estimation", "to": "Gradient Estimator Update", "arrows": "to", "title": "Subtopic of Baseline Estimation", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning_Theory", "to": "Double_Descent_Phenomenon", "arrows": "to", "title": "Subtopic of Machine_Learning_Theory", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine_Learning_Theory", "to": "Statistical_Mechanics_of_Learning", "arrows": "to", "title": "Subtopic of Machine_Learning_Theory", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Literature", "to": "Bias-Variance Trade-off Reconciliation", "arrows": "to", "title": "Subtopic of Machine Learning Literature", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Literature", "to": "Double Descent for Weak Features", "arrows": "to", "title": "Subtopic of Machine Learning Literature", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Literature", "to": "Variational Inference Review", "arrows": "to", "title": "Subtopic of Machine Learning Literature", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Literature", "to": "Foundation Models Opportunities and Risks", "arrows": "to", "title": "Subtopic of Machine Learning Literature", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Literature", "to": "Few-Shot Learning Capabilities", "arrows": "to", "title": "Subtopic of Machine Learning Literature", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Literature", "to": "Contrastive Learning Framework", "arrows": "to", "title": "Subtopic of Machine Learning Literature", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Literature", "to": "BERT Pre-training Methodology", "arrows": "to", "title": "Subtopic of Machine Learning Literature", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Literature", "to": "Implicit Bias Study", "arrows": "to", "title": "Subtopic of Machine Learning Literature", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Literature", "to": "High-Dimensional Statistical Analysis", "arrows": "to", "title": "Subtopic of Machine Learning Literature", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Papers", "to": "Implicit Bias in Machine Learning", "arrows": "to", "title": "Subtopic of Machine Learning Papers", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Papers", "to": "Deep Residual Learning", "arrows": "to", "title": "Subtopic of Machine Learning Papers", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Papers", "to": "Theoretical Guarantees for Deep Reinforcement Learning", "arrows": "to", "title": "Subtopic of Machine Learning Papers", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}, {"from": "Machine Learning Papers", "to": "Generalization Error Analysis", "arrows": "to", "title": "Subtopic of Machine Learning Papers", "color": {"color": "#333333", "highlight": "#000000"}, "width": 2, "dashes": false, "is_hierarchical": true}];
        const otherEdgesData = [{"id": "other_Auto-Differentiation_Deep Learning Packages_depends_on", "from": "Auto-Differentiation", "to": "Deep Learning Packages", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Fitted Value Iteration_Continuous State Space_depends_on", "from": "Fitted Value Iteration", "to": "Continuous State Space", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Mean Field Assumption_Variational Inference_subtopic", "from": "Mean Field Assumption", "to": "Variational Inference", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Dual Formulation of SVM_KKT Conditions_related_to", "from": "Dual Formulation of SVM", "to": "KKT Conditions", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Independent Components Analysis (ICA)_Cocktail Party Problem_subtopic", "from": "Independent Components Analysis (ICA)", "to": "Cocktail Party Problem", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Softplus Function_Activation Functions_subtopic", "from": "Softplus Function", "to": "Activation Functions", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Decision Boundary_Functional Margins_related_to", "from": "Decision Boundary", "to": "Functional Margins", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Transformer Model_Autoregressive Text Decoding_related_to", "from": "Transformer Model", "to": "Autoregressive Text Decoding", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Double Descent for Weak Features_Machine Learning Literature_subtopic", "from": "Double Descent for Weak Features", "to": "Machine Learning Literature", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Mean and Variance Functions_Succinct Representation of Distribution Qi_subtopic", "from": "Mean and Variance Functions", "to": "Succinct Representation of Distribution Qi", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Bernoulli Distribution_Exponential Family Distributions_related_to", "from": "Bernoulli Distribution", "to": "Exponential Family Distributions", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Regularization_Bias-Variance Tradeoff_related_to", "from": "Regularization", "to": "Bias-Variance Tradeoff", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Initialization_k-means Algorithm_subtopic", "from": "Initialization", "to": "k-means Algorithm", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Two-Layer Neural Network_ReLU Function_depends_on", "from": "Two-Layer Neural Network", "to": "ReLU Function", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Overview_Unsupervised Learning_contains", "from": "Machine Learning Overview", "to": "Unsupervised Learning", "arrows": "to", "title": "contains", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Algorithms_Generative Learning Algorithms_subtopic", "from": "Machine Learning Algorithms", "to": "Generative Learning Algorithms", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Tanh Function_Activation Functions_subtopic", "from": "Tanh Function", "to": "Activation Functions", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_EM Algorithm_Machine Learning Models_related_to", "from": "EM Algorithm", "to": "Machine Learning Models", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Kernel Functions_Feature Mapping_subtopic", "from": "Kernel Functions", "to": "Feature Mapping", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Concepts_Feature Mapping_subtopic", "from": "Machine Learning Concepts", "to": "Feature Mapping", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Fully-Connected Neural Networks_Two-Layer Neural Network_example_of", "from": "Fully-Connected Neural Networks", "to": "Two-Layer Neural Network", "arrows": "to", "title": "example_of", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Polynomial Fitting_Overfitting_depends_on", "from": "Polynomial Fitting", "to": "Overfitting", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Techniques_EM Algorithms_related_to", "from": "Machine Learning Techniques", "to": "EM Algorithms", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Concepts_Optimization Constraints_depends_on", "from": "Machine Learning Concepts", "to": "Optimization Constraints", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Model Evaluation_Mean Squared Error (MSE)_subtopic", "from": "Model Evaluation", "to": "Mean Squared Error (MSE)", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Concepts_Generalization Error_related_to", "from": "Machine Learning Concepts", "to": "Generalization Error", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Language Model Probability Distribution_Pretrained Large Language Models_subtopic", "from": "Language Model Probability Distribution", "to": "Pretrained Large Language Models", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_SMO Algorithm_Lagrange Multipliers_uses", "from": "SMO Algorithm", "to": "Lagrange Multipliers", "arrows": "to", "title": "uses", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Geometric Margin_Decision Boundary_depends_on", "from": "Geometric Margin", "to": "Decision Boundary", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Algorithms_Policy Gradient (REINFORCE)_subtopic", "from": "Machine Learning Algorithms", "to": "Policy Gradient (REINFORCE)", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Finetuning Algorithm_Adaptation Algorithm_subtopic", "from": "Finetuning Algorithm", "to": "Adaptation Algorithm", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Concepts_Support Vector Machines (SVM)_related_to", "from": "Machine Learning Concepts", "to": "Support Vector Machines (SVM)", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Model Parameters_Log Likelihood Function_related_to", "from": "Model Parameters", "to": "Log Likelihood Function", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Concepts_Backpropagation_depends_on", "from": "Machine Learning Concepts", "to": "Backpropagation", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Log Likelihood_Maximum Likelihood Estimation_subtopic", "from": "Log Likelihood", "to": "Maximum Likelihood Estimation", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Model Complexity and Test Errors_Double Descent Phenomenon_depends_on", "from": "Model Complexity and Test Errors", "to": "Double Descent Phenomenon", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Algorithms_Stochastic Gradient Descent (SGD)_contains", "from": "Machine Learning Algorithms", "to": "Stochastic Gradient Descent (SGD)", "arrows": "to", "title": "contains", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Logistic Regression_Decision Boundaries_related_to", "from": "Logistic Regression", "to": "Decision Boundaries", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Models_Locally Weighted Linear Regression_has_subtopic", "from": "Machine Learning Models", "to": "Locally Weighted Linear Regression", "arrows": "to", "title": "has_subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Concepts_Markov Decision Processes (MDP)_contains", "from": "Machine Learning Concepts", "to": "Markov Decision Processes (MDP)", "arrows": "to", "title": "contains", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Covariance Matrix_Multivariate Normal Distribution_depends_on", "from": "Covariance Matrix", "to": "Multivariate Normal Distribution", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Labeled Dataset_Machine Learning Adaptation Methods_related_to", "from": "Labeled Dataset", "to": "Machine Learning Adaptation Methods", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Zero-Shot Learning_Machine Learning Adaptation Methods_subtopic", "from": "Zero-Shot Learning", "to": "Machine Learning Adaptation Methods", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Model Parameters_Likelihood Function_related_to", "from": "Model Parameters", "to": "Likelihood Function", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Baseline Estimation_Value Function Approximation_depends_on", "from": "Baseline Estimation", "to": "Value Function Approximation", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Maximum Likelihood Estimation (MLE)_Likelihood Function_uses", "from": "Maximum Likelihood Estimation (MLE)", "to": "Likelihood Function", "arrows": "to", "title": "uses", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Models_Logistic Regression_has_subtopic", "from": "Machine Learning Models", "to": "Logistic Regression", "arrows": "to", "title": "has_subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Adaptation Methods_In-context Learning_subtopic", "from": "Adaptation Methods", "to": "In-context Learning", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Eigenvectors of Sigma_Dimensionality Reduction_depends_on", "from": "Eigenvectors of Sigma", "to": "Dimensionality Reduction", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Few-Shhot Learning_Machine Learning Adaptation Methods_subtopic", "from": "Few-Shhot Learning", "to": "Machine Learning Adaptation Methods", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Optimal Value Function_Bellman's Equation_related_to", "from": "Optimal Value Function", "to": "Bellman's Equation", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Fitted Value Iteration_Feature Mapping_related_to", "from": "Fitted Value Iteration", "to": "Feature Mapping", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Sample Complexity Bounds_Machine Learning Concepts_subtopic", "from": "Sample Complexity Bounds", "to": "Machine Learning Concepts", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Markov Decision Process (MDP)_State Transition Probabilities_depends_on", "from": "Markov Decision Process (MDP)", "to": "State Transition Probabilities", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Maximum Likelihood Estimation_Machine Learning Basics_subtopic", "from": "Maximum Likelihood Estimation", "to": "Machine Learning Basics", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Sigmoid Function_Activation Functions_subtopic", "from": "Sigmoid Function", "to": "Activation Functions", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Normal Equations_Least Squares Revisited_subtopic", "from": "Normal Equations", "to": "Least Squares Revisited", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Generalization Error_Sample Complexity Bounds_subtopic", "from": "Generalization Error", "to": "Sample Complexity Bounds", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Geometric Margins_Machine Learning Concepts_subtopic", "from": "Geometric Margins", "to": "Machine Learning Concepts", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Gradient Calculation_Matrix Derivatives_subtopic", "from": "Gradient Calculation", "to": "Matrix Derivatives", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Transformer Model_Conditional Probability_depends_on", "from": "Transformer Model", "to": "Conditional Probability", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Overfitting_Machine Learning Basics_subtopic", "from": "Overfitting", "to": "Machine Learning Basics", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Concepts_Housing Price Prediction_related_to", "from": "Machine Learning Concepts", "to": "Housing Price Prediction", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Value Iteration and Policy Iteration_Discretization in MDPs_subtopic", "from": "Value Iteration and Policy Iteration", "to": "Discretization in MDPs", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Concepts_Double Descent Phenomenon_subtopic", "from": "Machine Learning Concepts", "to": "Double Descent Phenomenon", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Discriminative Learning Algorithms_Logistic Regression_related_to", "from": "Discriminative Learning Algorithms", "to": "Logistic Regression", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_BERT Pre-training Methodology_Machine Learning Literature_subtopic", "from": "BERT Pre-training Methodology", "to": "Machine Learning Literature", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Convergence Criteria_KKT Conditions_related_to", "from": "Convergence Criteria", "to": "KKT Conditions", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Generalization Gap_Training vs Test Distributions_related_to", "from": "Generalization Gap", "to": "Training vs Test Distributions", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Naive Bayes Algorithm_Discretization_subtopic", "from": "Naive Bayes Algorithm", "to": "Discretization", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_ELBO Optimization_Efficient Evaluation of ELBO_subtopic", "from": "ELBO Optimization", "to": "Efficient Evaluation of ELBO", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Regularization in Machine Learning_Kernel Methods_related_to", "from": "Regularization in Machine Learning", "to": "Kernel Methods", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Adaptation Phase_Transfer Learning_subtopic", "from": "Adaptation Phase", "to": "Transfer Learning", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Step 2: Derive Optimal Policy_LQR Algorithm Steps_subtopic", "from": "Step 2: Derive Optimal Policy", "to": "LQR Algorithm Steps", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Probability Distributions_Gaussian Distribution_subtopic", "from": "Probability Distributions", "to": "Gaussian Distribution", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Probability Distributions_Log Likelihood_depends_on", "from": "Probability Distributions", "to": "Log Likelihood", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Models_Neural Networks_contains", "from": "Machine Learning Models", "to": "Neural Networks", "arrows": "to", "title": "contains", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Conditional Distribution Modeling_Bernoulli Distribution_subtopic", "from": "Conditional Distribution Modeling", "to": "Bernoulli Distribution", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Generalized Linear Model (GLM)_Exponential Family Distributions_related_to", "from": "Generalized Linear Model (GLM)", "to": "Exponential Family Distributions", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Neural Networks_Classification Problem_depends_on", "from": "Neural Networks", "to": "Classification Problem", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Generalization Error_Uniform Convergence_depends_on", "from": "Generalization Error", "to": "Uniform Convergence", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Discretization_Continuous State MDPs_subtopic", "from": "Discretization", "to": "Continuous State MDPs", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Cross Validation_k-fold Cross Validation_subtopic", "from": "Cross Validation", "to": "k-fold Cross Validation", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Training Error_Empirical Risk Minimization (ERM)_related_to", "from": "Training Error", "to": "Empirical Risk Minimization (ERM)", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Concepts_Feature Selection_subtopic", "from": "Machine Learning Concepts", "to": "Feature Selection", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Value Function in RL_Optimal Value Function_subtopic", "from": "Value Function in RL", "to": "Optimal Value Function", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Concepts_Naive Bayes Classifier_related_to", "from": "Machine Learning Concepts", "to": "Naive Bayes Classifier", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Concepts_Jensen's Inequality_subtopic", "from": "Machine Learning Concepts", "to": "Jensen's Inequality", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Posterior Distribution_Predictive Distribution_related_to", "from": "Posterior Distribution", "to": "Predictive Distribution", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_LQG Framework_Machine Learning Concepts_depends_on", "from": "LQG Framework", "to": "Machine Learning Concepts", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Model Complexity_Bias-Variance Tradeoff_related_to", "from": "Model Complexity", "to": "Bias-Variance Tradeoff", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Fully-Connected Neural Networks_ReLU Activation Function_uses", "from": "Fully-Connected Neural Networks", "to": "ReLU Activation Function", "arrows": "to", "title": "uses", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Piecewise Constant Representation_Discretization in MDPs_subtopic", "from": "Piecewise Constant Representation", "to": "Discretization in MDPs", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Supervised Learning_Deep Learning_subtopic", "from": "Supervised Learning", "to": "Deep Learning", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Linear Regression_Cost Function_subtopic", "from": "Linear Regression", "to": "Cost Function", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Hessian Matrix_Optimization in RL_related_to", "from": "Hessian Matrix", "to": "Optimization in RL", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Markov Decision Processes (MDP)_Policy Iteration_subtopic", "from": "Markov Decision Processes (MDP)", "to": "Policy Iteration", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Unit Vector w/||w||_Vector w_subtopic", "from": "Unit Vector w/||w||", "to": "Vector w", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Backward Function for Loss Functions_Cross-Entropy Loss_subtopic", "from": "Backward Function for Loss Functions", "to": "Cross-Entropy Loss", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Concepts_Locally Weighted Linear Regression (LWLR)_subtopic", "from": "Machine Learning Concepts", "to": "Locally Weighted Linear Regression (LWLR)", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Logistic Regression Derivation_Perceptron Algorithm_related_to", "from": "Logistic Regression Derivation", "to": "Perceptron Algorithm", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Unlabeled Dataset_Pretraining Phase_related_to", "from": "Unlabeled Dataset", "to": "Pretraining Phase", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Supervised Learning_Linear Regression_subtopic", "from": "Supervised Learning", "to": "Linear Regression", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Contrastive Learning_Negative Pair_related_to", "from": "Contrastive Learning", "to": "Negative Pair", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Reinforcement learning_Continuous state MDPs_has_subtopic", "from": "Reinforcement learning", "to": "Continuous state MDPs", "arrows": "to", "title": "has_subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Pilot Survey Example_Data Redundancy Detection_related_to", "from": "Pilot Survey Example", "to": "Data Redundancy Detection", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Sigmoid Function_Tanh Function_related_to", "from": "Sigmoid Function", "to": "Tanh Function", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Natural Parameter for Bernoulli_Bernoulli Distribution_subtopic", "from": "Natural Parameter for Bernoulli", "to": "Bernoulli Distribution", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Classification_Binary Classification_subtopic", "from": "Classification", "to": "Binary Classification", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Multivariate Normal Distributions for Classes_Gaussian Discriminant Analysis (GDA)_subtopic", "from": "Multivariate Normal Distributions for Classes", "to": "Gaussian Discriminant Analysis (GDA)", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Cross-Entropy Loss_Probabilistic Model_related_to", "from": "Cross-Entropy Loss", "to": "Probabilistic Model", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Negative Log-Likelihood_Probabilistic Model_depends_on", "from": "Negative Log-Likelihood", "to": "Probabilistic Model", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Expectation-Maximization (EM) Algorithm_Reparameterization Trick_related_to", "from": "Expectation-Maximization (EM) Algorithm", "to": "Reparameterization Trick", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Kernel Functions_Sufficient Conditions for Valid Kernels_subtopic", "from": "Kernel Functions", "to": "Sufficient Conditions for Valid Kernels", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Logistic Regression_Margins_related_to", "from": "Logistic Regression", "to": "Margins", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Value Iteration_Continuous State MDPs_depends_on", "from": "Value Iteration", "to": "Continuous State MDPs", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Concepts_Value Iteration_contains", "from": "Machine Learning Concepts", "to": "Value Iteration", "arrows": "to", "title": "contains", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Vector w_Decision Boundary_related_to", "from": "Vector w", "to": "Decision Boundary", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Classification_Multi-class Classification_subtopic", "from": "Classification", "to": "Multi-class Classification", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Reinforcement learning_Learning a model for an MDP_subtopic_of", "from": "Reinforcement learning", "to": "Learning a model for an MDP", "arrows": "to", "title": "subtopic_of", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Synchronous Updates_Value Iteration Algorithm_subtopic", "from": "Synchronous Updates", "to": "Value Iteration Algorithm", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Naive Bayes Spam Filter_Parameter Estimation_depends_on", "from": "Naive Bayes Spam Filter", "to": "Parameter Estimation", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Dynamic Programming Application_Machine Learning Concepts_subtopic", "from": "Dynamic Programming Application", "to": "Machine Learning Concepts", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Multinomial Random Variable_Maximum Likelihood Estimates_depends_on", "from": "Multinomial Random Variable", "to": "Maximum Likelihood Estimates", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning_Contrastive Learning_related_to", "from": "Machine Learning", "to": "Contrastive Learning", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Perceptron Algorithm_Multi-class Classification_depends_on", "from": "Perceptron Algorithm", "to": "Multi-class Classification", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Generalized Linear Models (GLMs)_Ordinary Least Squares_subtopic", "from": "Generalized Linear Models (GLMs)", "to": "Ordinary Least Squares", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Stochastic Gradient Descent (SGD)_Gradient Calculation_subtopic", "from": "Stochastic Gradient Descent (SGD)", "to": "Gradient Calculation", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Convergence Proof_Jensen's Inequality_depends_on", "from": "Convergence Proof", "to": "Jensen's Inequality", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Regression Problems_Bias-Variance Tradeoff_subtopic", "from": "Regression Problems", "to": "Bias-Variance Tradeoff", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Objective Function Dual_Value Dual Problem_subtopic", "from": "Objective Function Dual", "to": "Value Dual Problem", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Linear Regression_Locally Weighted Linear Regression_subtopic", "from": "Linear Regression", "to": "Locally Weighted Linear Regression", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Coordinate Ascent Algorithm_Sequential Minimal Optimization (SMO) Algorithm_related_to", "from": "Coordinate Ascent Algorithm", "to": "Sequential Minimal Optimization (SMO) Algorithm", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Sample Complexity_Generalization Error_depends_on", "from": "Sample Complexity", "to": "Generalization Error", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Gaussian Discriminant Analysis (GDA)_Machine Learning Models_subtopic", "from": "Gaussian Discriminant Analysis (GDA)", "to": "Machine Learning Models", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Concepts_k-means Algorithm_has_subtopic", "from": "Machine Learning Concepts", "to": "k-means Algorithm", "arrows": "to", "title": "has_subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Concepts_EM Algorithm_subtopic", "from": "Machine Learning Concepts", "to": "EM Algorithm", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Beta Update Equation_Inner Product Computation_depends_on", "from": "Beta Update Equation", "to": "Inner Product Computation", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Techniques_Model Selection_contains", "from": "Machine Learning Techniques", "to": "Model Selection", "arrows": "to", "title": "contains", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Curse of Dimensionality_Discretization in MDPs_subtopic", "from": "Curse of Dimensionality", "to": "Discretization in MDPs", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Implicit Regularization Effect_Regularization in Deep Learning_subtopic", "from": "Implicit Regularization Effect", "to": "Regularization in Deep Learning", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Logistic Regression_Conditional Distribution_subtopic", "from": "Logistic Regression", "to": "Conditional Distribution", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Loss Function_Negative Log-Likelihood_depends_on", "from": "Loss Function", "to": "Negative Log-Likelihood", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Lagrangian Function_Dual Problem_related_to", "from": "Lagrangian Function", "to": "Dual Problem", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Models_Neural Networks_depends_on", "from": "Machine Learning Models", "to": "Neural Networks", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Bias-Variance Tradeoff_Machine Learning Basics_depends_on", "from": "Bias-Variance Tradeoff", "to": "Machine Learning Basics", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Weight Matrices and Biases_Total Number of Neurons_related_to", "from": "Weight Matrices and Biases", "to": "Total Number of Neurons", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Contrastive Learning Framework_Machine Learning Literature_subtopic", "from": "Contrastive Learning Framework", "to": "Machine Learning Literature", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Concave Functions_Jensen's Inequality_related_to", "from": "Concave Functions", "to": "Jensen's Inequality", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Concepts_LMS with Features_subtopic", "from": "Machine Learning Concepts", "to": "LMS with Features", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Variational Inference Review_Machine Learning Literature_subtopic", "from": "Variational Inference Review", "to": "Machine Learning Literature", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Data Scarcity_k-fold Cross Validation_related_to", "from": "Data Scarcity", "to": "k-fold Cross Validation", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Functional Margin_Geometric Margin_related_to", "from": "Functional Margin", "to": "Geometric Margin", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Dynamics of Model_Optimal Value Function_subtopic", "from": "Dynamics of Model", "to": "Optimal Value Function", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Overview_Supervised Learning_contains", "from": "Machine Learning Overview", "to": "Supervised Learning", "arrows": "to", "title": "contains", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Training Example Independence Assumption_Machine Learning Concepts_depends_on", "from": "Training Example Independence Assumption", "to": "Machine Learning Concepts", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Maximum Likelihood Estimation_Laplace Smoothing_depends_on", "from": "Maximum Likelihood Estimation", "to": "Laplace Smoothing", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Algorithms_Newton's Method_subtopic", "from": "Machine Learning Algorithms", "to": "Newton's Method", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Prediction Equation_Intercept Term Calculation_depends_on", "from": "Prediction Equation", "to": "Intercept Term Calculation", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Latent Variable Models_EM Algorithm_subtopic", "from": "Latent Variable Models", "to": "EM Algorithm", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Algorithms_Neural Networks Overview_related_to", "from": "Machine Learning Algorithms", "to": "Neural Networks Overview", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Linear Regression_Gradient Descent_related_to", "from": "Linear Regression", "to": "Gradient Descent", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Generalized Linear Models (GLMs)_Logistic Regression_subtopic", "from": "Generalized Linear Models (GLMs)", "to": "Logistic Regression", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Backward Functions_Loss Functions_subtopic", "from": "Backward Functions", "to": "Loss Functions", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Neural Networks Parameters_Deep Learning_subtopic", "from": "Neural Networks Parameters", "to": "Deep Learning", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Contour Plots_Multivariate Normal Distribution_related_to", "from": "Contour Plots", "to": "Multivariate Normal Distribution", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Models_Gaussian Discriminant Analysis (GDA)_subtopic", "from": "Machine Learning Models", "to": "Gaussian Discriminant Analysis (GDA)", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Bias-Variance Tradeoff_Sample Complexity Bounds_subtopic", "from": "Bias-Variance Tradeoff", "to": "Sample Complexity Bounds", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Gradient Descent Methods_Batch Gradient Descent_contains", "from": "Gradient Descent Methods", "to": "Batch Gradient Descent", "arrows": "to", "title": "contains", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Optimizers and Generalization_Implicit Regularization_contains", "from": "Optimizers and Generalization", "to": "Implicit Regularization", "arrows": "to", "title": "contains", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Conditional Probability_Softmax Function_subtopic", "from": "Conditional Probability", "to": "Softmax Function", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Policy Definition_Value Function_depends_on", "from": "Policy Definition", "to": "Value Function", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Likelihood Function_Probability Distribution_subtopic", "from": "Likelihood Function", "to": "Probability Distribution", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Continuous State Space_Markov Decision Process (MDP)_related_to", "from": "Continuous State Space", "to": "Markov Decision Process (MDP)", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Log Likelihood Function_Gradient Ascent Rule_subtopic", "from": "Log Likelihood Function", "to": "Gradient Ascent Rule", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Supervised Learning Algorithm_Linear Regression_subtopic", "from": "Supervised Learning Algorithm", "to": "Linear Regression", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_LQR Algorithm Steps_Machine Learning Concepts_subtopic", "from": "LQR Algorithm Steps", "to": "Machine Learning Concepts", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Functional Margins_Machine Learning Concepts_subtopic", "from": "Functional Margins", "to": "Machine Learning Concepts", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Immediate Reward_Bellman Equations_related_to", "from": "Immediate Reward", "to": "Bellman Equations", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Normal Equations Method_Matrix Derivatives_contains", "from": "Normal Equations Method", "to": "Matrix Derivatives", "arrows": "to", "title": "contains", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Learning Guarantees_Binary Classification_related_to", "from": "Learning Guarantees", "to": "Binary Classification", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Jensen's Inequality_Evidence Lower Bound (ELBO)_depends_on", "from": "Jensen's Inequality", "to": "Evidence Lower Bound (ELBO)", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Generalization Error_PAC Assumptions_depends_on", "from": "Generalization Error", "to": "PAC Assumptions", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Self-Supervised Loss_Pretraining Phase_subtopic", "from": "Self-Supervised Loss", "to": "Pretraining Phase", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Adaptation Methods_Pretraining Methods in CV_subtopic", "from": "Machine Learning Adaptation Methods", "to": "Pretraining Methods in CV", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Concepts_Policy Gradient Methods_has_subtopic", "from": "Machine Learning Concepts", "to": "Policy Gradient Methods", "arrows": "to", "title": "has_subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Distortion Function_k-means Algorithm_subtopic", "from": "Distortion Function", "to": "k-means Algorithm", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Dual Problem Formulation_KKT Conditions_depends_on", "from": "Dual Problem Formulation", "to": "KKT Conditions", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Concepts_Jensen's Inequality_related_to", "from": "Machine Learning Concepts", "to": "Jensen's Inequality", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Optimal Intercept b_Lagrangian Optimization_related_to", "from": "Optimal Intercept b", "to": "Lagrangian Optimization", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Algorithms_Batch Gradient Descent_depends_on", "from": "Machine Learning Algorithms", "to": "Batch Gradient Descent", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Algorithms_Linear Regression_contains", "from": "Machine Learning Algorithms", "to": "Linear Regression", "arrows": "to", "title": "contains", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Probability Distribution_Design Matrix X_depends_on", "from": "Probability Distribution", "to": "Design Matrix X", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Bernoulli Distribution_Logistic Function_subtopic", "from": "Bernoulli Distribution", "to": "Logistic Function", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Convex Functions_Jensen's Inequality_depends_on", "from": "Convex Functions", "to": "Jensen's Inequality", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Independence Assumption_Likelihood Function_related_to", "from": "Independence Assumption", "to": "Likelihood Function", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Non-separable Case_Optimization Problem_depends_on", "from": "Non-separable Case", "to": "Optimization Problem", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Basics_Empirical Risk Minimization (ERM)_has_subtopic", "from": "Machine Learning Basics", "to": "Empirical Risk Minimization (ERM)", "arrows": "to", "title": "has_subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Models_Logistic Regression_subtopic", "from": "Machine Learning Models", "to": "Logistic Regression", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Deep Learning_Machine Learning Overview_subtopic", "from": "Deep Learning", "to": "Machine Learning Overview", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Concepts_Value Function Approximation_related_to", "from": "Machine Learning Concepts", "to": "Value Function Approximation", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Training Set_Maximum Likelihood Estimation_depends_on", "from": "Training Set", "to": "Maximum Likelihood Estimation", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Convolutional Neural Networks (CNNs)_1-D Convolution Layer_subtopic", "from": "Convolutional Neural Networks (CNNs)", "to": "1-D Convolution Layer", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_System Dynamics_Step 1_depends_on", "from": "System Dynamics", "to": "Step 1", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Double Descent Phenomenon_Bias-Variance Tradeoff_related_to", "from": "Double Descent Phenomenon", "to": "Bias-Variance Tradeoff", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Concepts_Principal Components Analysis (PCA)_related_to", "from": "Machine Learning Concepts", "to": "Principal Components Analysis (PCA)", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Binary Classification Problem_Loss Function_depends_on", "from": "Binary Classification Problem", "to": "Loss Function", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Support Vector Machine (SVM)_Optimization Problem_depends_on", "from": "Support Vector Machine (SVM)", "to": "Optimization Problem", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Feature Vector Selection_Generative Models_depends_on", "from": "Feature Vector Selection", "to": "Generative Models", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Models_Maximum Likelihood Estimation (MLE)_has_subtopic", "from": "Machine Learning Models", "to": "Maximum Likelihood Estimation (MLE)", "arrows": "to", "title": "has_subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Concepts_Markov Decision Processes (MDP)_related_to", "from": "Machine Learning Concepts", "to": "Markov Decision Processes (MDP)", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Posterior Distribution_EM Algorithm_depends_on", "from": "Posterior Distribution", "to": "EM Algorithm", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Feature Engineering_Machine Learning Overview_subtopic", "from": "Feature Engineering", "to": "Machine Learning Overview", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Generalization Error_Training Error_related_to", "from": "Generalization Error", "to": "Training Error", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Transfer Learning_Machine Learning Overview_subtopic", "from": "Transfer Learning", "to": "Machine Learning Overview", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Kernel Matrix Properties_Examples of Kernels in Practice_related_to", "from": "Kernel Matrix Properties", "to": "Examples of Kernels in Practice", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Softplus Function_ReLU Function_related_to", "from": "Softplus Function", "to": "ReLU Function", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Log-Likelihood Maximization_EM Algorithm_depends_on", "from": "Log-Likelihood Maximization", "to": "EM Algorithm", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Fitted Value Iteration_Value Function Approximation_has_subtopic", "from": "Fitted Value Iteration", "to": "Value Function Approximation", "arrows": "to", "title": "has_subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Linear Regression_Gradient Descent_depends_on", "from": "Linear Regression", "to": "Gradient Descent", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Concepts_Properties of Kernels_related_to", "from": "Machine Learning Concepts", "to": "Properties of Kernels", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Linear Quadratic Regulator (LQR)_Optimization in RL_subtopic", "from": "Linear Quadratic Regulator (LQR)", "to": "Optimization in RL", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Foundation Models Opportunities and Risks_Machine Learning Literature_subtopic", "from": "Foundation Models Opportunities and Risks", "to": "Machine Learning Literature", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Few-Shot Learning Capabilities_Machine Learning Literature_subtopic", "from": "Few-Shot Learning Capabilities", "to": "Machine Learning Literature", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Theory_Optimization Problems_depends_on", "from": "Machine Learning Theory", "to": "Optimization Problems", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Dual Form of Problem_Kernel Trick_related_to", "from": "Dual Form of Problem", "to": "Kernel Trick", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Techniques_Principal Component Analysis (PCA)_contains", "from": "Machine Learning Techniques", "to": "Principal Component Analysis (PCA)", "arrows": "to", "title": "contains", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Contrastive Learning_Positive Pair_related_to", "from": "Contrastive Learning", "to": "Positive Pair", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Naive Bayes Classifier_Laplace Smoothing_subtopic", "from": "Naive Bayes Classifier", "to": "Laplace Smoothing", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Bias-Variance Tradeoff_Hypothesis Class_subtopic", "from": "Bias-Variance Tradeoff", "to": "Hypothesis Class", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Hypotheses Space (H)_Uniform Convergence_related_to", "from": "Hypotheses Space (H)", "to": "Uniform Convergence", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Concepts_Underfitting_related_to", "from": "Machine Learning Concepts", "to": "Underfitting", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Modern Neural Networks_Backpropagation_has_subtopic", "from": "Modern Neural Networks", "to": "Backpropagation", "arrows": "to", "title": "has_subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Exponential Family Distributions_Bernoulli Distribution_subtopic", "from": "Exponential Family Distributions", "to": "Bernoulli Distribution", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Optimization Methods_Newton's Method_subtopic", "from": "Optimization Methods", "to": "Newton's Method", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Coordinate Descent on J_k-means Algorithm_subtopic", "from": "Coordinate Descent on J", "to": "k-means Algorithm", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_LQR Algorithm_Step 3_depends_on", "from": "LQR Algorithm", "to": "Step 3", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Gaussian Discriminant Analysis (GDA)_Multivariate Normal Distribution_subtopic", "from": "Gaussian Discriminant Analysis (GDA)", "to": "Multivariate Normal Distribution", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_ICA Overview_Maximum Likelihood Estimation_depends_on", "from": "ICA Overview", "to": "Maximum Likelihood Estimation", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning_Reinforcement Learning_has_subtopic", "from": "Machine Learning", "to": "Reinforcement Learning", "arrows": "to", "title": "has_subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Logits_Softmax Function_depends_on", "from": "Logits", "to": "Softmax Function", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Chapter 16 Introduction_Finite-horizon MDPs_depends_on", "from": "Chapter 16 Introduction", "to": "Finite-horizon MDPs", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Loss Function Analysis_Machine Learning Concepts_subtopic", "from": "Loss Function Analysis", "to": "Machine Learning Concepts", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Uniform Convergence_Generalization Error_subtopic", "from": "Uniform Convergence", "to": "Generalization Error", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Gradient Descent Methods_Stochastic Gradient Descent_contains", "from": "Gradient Descent Methods", "to": "Stochastic Gradient Descent", "arrows": "to", "title": "contains", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Markov Decision Processes (MDP)_Value Iteration_subtopic", "from": "Markov Decision Processes (MDP)", "to": "Value Iteration", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Cocktail Party Problem_Mixing Matrix (A)_related_to", "from": "Cocktail Party Problem", "to": "Mixing Matrix (A)", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Probabilistic Interpretation_Regression Problem_related_to", "from": "Probabilistic Interpretation", "to": "Regression Problem", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Concepts_EM Algorithms_has_subtopic", "from": "Machine Learning Concepts", "to": "EM Algorithms", "arrows": "to", "title": "has_subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Logistic Regression_Logit_depends_on", "from": "Logistic Regression", "to": "Logit", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Linear Quadratic Regulator (LQR)_Optimal Policy_subtopic", "from": "Linear Quadratic Regulator (LQR)", "to": "Optimal Policy", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Probability Vector_Softmax Function_related_to", "from": "Probability Vector", "to": "Softmax Function", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Basics_Regularization_subtopic", "from": "Machine Learning Basics", "to": "Regularization", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Empirical Distribution_Training Loss_related_to", "from": "Empirical Distribution", "to": "Training Loss", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Concepts_Bias-Variance Tradeoff_subtopic", "from": "Machine Learning Concepts", "to": "Bias-Variance Tradeoff", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Concepts_Kernel Functions_contains", "from": "Machine Learning Concepts", "to": "Kernel Functions", "arrows": "to", "title": "contains", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Softmax Function_Probabilistic Model_subtopic", "from": "Softmax Function", "to": "Probabilistic Model", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Models_Binary Classification_has_subtopic", "from": "Machine Learning Models", "to": "Binary Classification", "arrows": "to", "title": "has_subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Concepts_Bias-Variance Tradeoff_depends_on", "from": "Machine Learning Concepts", "to": "Bias-Variance Tradeoff", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Double Descent Phenomenon_Bias-Variance Tradeoff_subtopic", "from": "Double Descent Phenomenon", "to": "Bias-Variance Tradeoff", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Support Vector Machines (SVM)_Optimization Problem_depends_on", "from": "Support Vector Machines (SVM)", "to": "Optimization Problem", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Classification and Logistic Regression_Multi-class Classification_subtopic", "from": "Classification and Logistic Regression", "to": "Multi-class Classification", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Supervised Learning Problem_Discretization in MDPs_subtopic", "from": "Supervised Learning Problem", "to": "Discretization in MDPs", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Regularization_Model Complexity_subtopic", "from": "Regularization", "to": "Model Complexity", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Discretization_Curse of Dimensionality_depends_on", "from": "Discretization", "to": "Curse of Dimensionality", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Concepts_Differential Dynamic Programming (DDP)_subtopic", "from": "Machine Learning Concepts", "to": "Differential Dynamic Programming (DDP)", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Optimization Problems_Primal Problem_subtopic", "from": "Optimization Problems", "to": "Primal Problem", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Policy Gradients_Reward Function_depends_on", "from": "Policy Gradients", "to": "Reward Function", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Gaussian Distribution_Kalman Filter_related_to", "from": "Gaussian Distribution", "to": "Kalman Filter", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Algorithms_Markov Decision Processes (MDP)_related_to", "from": "Machine Learning Algorithms", "to": "Markov Decision Processes (MDP)", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Value Function_Policy Execution_depends_on", "from": "Value Function", "to": "Policy Execution", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Concepts_Regression Problems_contains", "from": "Machine Learning Concepts", "to": "Regression Problems", "arrows": "to", "title": "contains", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Concepts_Backpropagation_contains", "from": "Machine Learning Concepts", "to": "Backpropagation", "arrows": "to", "title": "contains", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Distance to Decision Boundary_Decision Boundary_subtopic", "from": "Distance to Decision Boundary", "to": "Decision Boundary", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Classification Model_Probabilistic Assumptions_depends_on", "from": "Classification Model", "to": "Probabilistic Assumptions", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Continuous Latent Variables_Variational Inference_subtopic", "from": "Continuous Latent Variables", "to": "Variational Inference", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Jensen's Inequality_Convex Functions_subtopic", "from": "Jensen's Inequality", "to": "Convex Functions", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Design Matrix_Least Squares Revisited_subtopic", "from": "Design Matrix", "to": "Least Squares Revisited", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Gradient Descent_Cost Function J(\u03b8)_related_to", "from": "Gradient Descent", "to": "Cost Function J(\u03b8)", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Normal Equations_Matrix Derivatives_subtopic", "from": "Normal Equations", "to": "Matrix Derivatives", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Optimization Techniques_Coordinate Ascent_contains", "from": "Machine Learning Optimization Techniques", "to": "Coordinate Ascent", "arrows": "to", "title": "contains", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Lagrangian Function_Primal Problem_related_to", "from": "Lagrangian Function", "to": "Primal Problem", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Naive Bayes Algorithm_Conditional Probability_depends_on", "from": "Naive Bayes Algorithm", "to": "Conditional Probability", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Linear Quadratic Regulator (LQR)_Parameter Estimation_depends_on", "from": "Linear Quadratic Regulator (LQR)", "to": "Parameter Estimation", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Differential Dynamic Programming (DDP)_Linearization of Dynamics_subtopic", "from": "Differential Dynamic Programming (DDP)", "to": "Linearization of Dynamics", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Adaptation Methods_Zero-shot Learning_subtopic", "from": "Adaptation Methods", "to": "Zero-shot Learning", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Overview_Bayesian Inference_contains", "from": "Machine Learning Overview", "to": "Bayesian Inference", "arrows": "to", "title": "contains", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Techniques_Differential Dynamic Programming (DDP)_contains", "from": "Machine Learning Techniques", "to": "Differential Dynamic Programming (DDP)", "arrows": "to", "title": "contains", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Concepts_Loss Function_related_to", "from": "Machine Learning Concepts", "to": "Loss Function", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Test Error Decomposition_Bias-Variance Tradeoff_subtopic", "from": "Test Error Decomposition", "to": "Bias-Variance Tradeoff", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Decision Boundary_Geometric Margins_related_to", "from": "Decision Boundary", "to": "Geometric Margins", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Quadratic Assumption_Dynamics of Model_depends_on", "from": "Quadratic Assumption", "to": "Dynamics of Model", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Reward Function_Finite-State MDPs_related_to", "from": "Reward Function", "to": "Finite-State MDPs", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Models_Generative Learning Algorithms_contains", "from": "Machine Learning Models", "to": "Generative Learning Algorithms", "arrows": "to", "title": "contains", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Algorithms_Logistic Regression_related_to", "from": "Machine Learning Algorithms", "to": "Logistic Regression", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_GELU Function_ReLU Function_variant_of", "from": "GELU Function", "to": "ReLU Function", "arrows": "to", "title": "variant_of", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Gradient Computation_Backpropagation_related_to", "from": "Gradient Computation", "to": "Backpropagation", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Finite-State MDPs_Value Iteration and Policy Iteration_subtopic", "from": "Finite-State MDPs", "to": "Value Iteration and Policy Iteration", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Underfitting_Bias-Variance Tradeoff_depends_on", "from": "Underfitting", "to": "Bias-Variance Tradeoff", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_M-step_ELBO (Evidence Lower Bound)_related_to", "from": "M-step", "to": "ELBO (Evidence Lower Bound)", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_ReLU Function_Activation Functions_subtopic", "from": "ReLU Function", "to": "Activation Functions", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Optimal Hypothesis (h*)_Generalization Error_depends_on", "from": "Optimal Hypothesis (h*)", "to": "Generalization Error", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Concepts_Layer Normalization (LN)_subtopic", "from": "Machine Learning Concepts", "to": "Layer Normalization (LN)", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Overview_Deep Learning Introduction_related_to", "from": "Machine Learning Overview", "to": "Deep Learning Introduction", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_ELBO_Variational Inference_depends_on", "from": "ELBO", "to": "Variational Inference", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Pretraining Phase_Transfer Learning_subtopic", "from": "Pretraining Phase", "to": "Transfer Learning", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Encoder-Decoder Framework_Optimizing Continuous Latent Variables_related_to", "from": "Encoder-Decoder Framework", "to": "Optimizing Continuous Latent Variables", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Efficient Evaluation of ELBO_Optimizing Continuous Latent Variables_depends_on", "from": "Efficient Evaluation of ELBO", "to": "Optimizing Continuous Latent Variables", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Learned Features_Deep Learning_subtopic", "from": "Learned Features", "to": "Deep Learning", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Generative Learning Algorithms_Class Priors_subtopic", "from": "Generative Learning Algorithms", "to": "Class Priors", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Basics_Linear Regression_depends_on", "from": "Machine Learning Basics", "to": "Linear Regression", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Modern Neural Networks_Modules in Modern Neural Networks_has_subtopic", "from": "Modern Neural Networks", "to": "Modules in Modern Neural Networks", "arrows": "to", "title": "has_subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Models_Logistic Regression_related_to", "from": "Machine Learning Models", "to": "Logistic Regression", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_EM Algorithm_Evidence Lower Bound (ELBO)_subtopic", "from": "EM Algorithm", "to": "Evidence Lower Bound (ELBO)", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Optimal Value Function_Value Iteration Algorithm_subtopic", "from": "Optimal Value Function", "to": "Value Iteration Algorithm", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Optimization Problem_Lagrangian Function_related_to", "from": "Optimization Problem", "to": "Lagrangian Function", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Kernels in Machine Learning_Feature Maps_depends_on", "from": "Kernels in Machine Learning", "to": "Feature Maps", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Generative Learning Algorithms_Naive Bayes Classifier_subtopic", "from": "Generative Learning Algorithms", "to": "Naive Bayes Classifier", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Model Selection_Bias-Variance Tradeoff_related_to", "from": "Model Selection", "to": "Bias-Variance Tradeoff", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Exponential Family Distributions_Poisson Distribution_subtopic", "from": "Exponential Family Distributions", "to": "Poisson Distribution", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Training Set_Machine Learning Concepts_subtopic", "from": "Training Set", "to": "Machine Learning Concepts", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Probability Calculation_Multinomial Event Model_subtopic", "from": "Probability Calculation", "to": "Multinomial Event Model", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Partial Observability_LQG Framework_subtopic", "from": "Partial Observability", "to": "LQG Framework", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Loss Functions_Logistic Loss Function_subtopic", "from": "Machine Learning Loss Functions", "to": "Logistic Loss Function", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Expected Test Error_Mean Squared Error (MSE)_subtopic", "from": "Expected Test Error", "to": "Mean Squared Error (MSE)", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Generalized Linear Models (GLM)_Exponential Family Distributions_related_to", "from": "Generalized Linear Models (GLM)", "to": "Exponential Family Distributions", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Reward Function_Reinforcement Learning_subtopic", "from": "Reward Function", "to": "Reinforcement Learning", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Value Iteration Algorithm_Value Iteration and Policy Iteration_subtopic", "from": "Value Iteration Algorithm", "to": "Value Iteration and Policy Iteration", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_E-step_ELBO (Evidence Lower Bound)_related_to", "from": "E-step", "to": "ELBO (Evidence Lower Bound)", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Data Preprocessing Assumptions_Logistic Function Properties_depends_on", "from": "Data Preprocessing Assumptions", "to": "Logistic Function Properties", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Optimization Problem_Functional Margin_related_to", "from": "Optimization Problem", "to": "Functional Margin", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Generalized Linear Models (GLM)_Logistic Regression_contains", "from": "Generalized Linear Models (GLM)", "to": "Logistic Regression", "arrows": "to", "title": "contains", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Sufficient Statistic for Bernoulli_Bernoulli Distribution_subtopic", "from": "Sufficient Statistic for Bernoulli", "to": "Bernoulli Distribution", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Test Error_Population Distribution_related_to", "from": "Test Error", "to": "Population Distribution", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Leaky ReLU_ReLU Function_variant_of", "from": "Leaky ReLU", "to": "ReLU Function", "arrows": "to", "title": "variant_of", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Foundation Models_Machine Learning Overview_subtopic", "from": "Foundation Models", "to": "Machine Learning Overview", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Pretrained Model_Transfer Learning_depends_on", "from": "Pretrained Model", "to": "Transfer Learning", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Basics_Classification Problem_depends_on", "from": "Machine Learning Basics", "to": "Classification Problem", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Bernoulli Distribution_Gaussian Discriminant Analysis (GDA)_subtopic", "from": "Bernoulli Distribution", "to": "Gaussian Discriminant Analysis (GDA)", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Explicit Regularization Techniques_Regularization in Deep Learning_subtopic", "from": "Explicit Regularization Techniques", "to": "Regularization in Deep Learning", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Log-Likelihood Function_Maximum Likelihood Estimation (MLE)_subtopic", "from": "Log-Likelihood Function", "to": "Maximum Likelihood Estimation (MLE)", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Bellman Equations_Value Function_subtopic", "from": "Bellman Equations", "to": "Value Function", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Algorithms_Cross Validation_subtopic", "from": "Machine Learning Algorithms", "to": "Cross Validation", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Vapnik's Theorem_Uniform Convergence_depends_on", "from": "Vapnik's Theorem", "to": "Uniform Convergence", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_VC Dimension_Vapnik's Theorem_related_to", "from": "VC Dimension", "to": "Vapnik's Theorem", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Exponential Family Distributions_Bernoulli Distribution_related_to", "from": "Exponential Family Distributions", "to": "Bernoulli Distribution", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Cocktail Party Problem_Unmixing Matrix (W)_depends_on", "from": "Cocktail Party Problem", "to": "Unmixing Matrix (W)", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Uniform Convergence_Hypothesis Class_related_to", "from": "Uniform Convergence", "to": "Hypothesis Class", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Likelihood Function_Maximum Likelihood Estimation_related_to", "from": "Likelihood Function", "to": "Maximum Likelihood Estimation", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Asynchronous Updates_Value Iteration Algorithm_subtopic", "from": "Asynchronous Updates", "to": "Value Iteration Algorithm", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Recovering w from Alpha_Lagrangian Optimization_related_to", "from": "Recovering w from Alpha", "to": "Lagrangian Optimization", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Dual Form of Problem_Lagrangian Function_depends_on", "from": "Dual Form of Problem", "to": "Lagrangian Function", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Concepts_Independent Component Analysis (ICA)_contains", "from": "Machine Learning Concepts", "to": "Independent Component Analysis (ICA)", "arrows": "to", "title": "contains", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Backward Function for Loss Functions_Efficiency Considerations_subtopic", "from": "Backward Function for Loss Functions", "to": "Efficiency Considerations", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Concepts_Backpropagation_subtopic", "from": "Machine Learning Concepts", "to": "Backpropagation", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_E-step_EM Algorithm_subtopic", "from": "E-step", "to": "EM Algorithm", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Algorithms_Gradient Descent Update Rule_depends_on", "from": "Machine Learning Algorithms", "to": "Gradient Descent Update Rule", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Constructing GLMs_Ordinary Least Squares_subtopic", "from": "Constructing GLMs", "to": "Ordinary Least Squares", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Linearization of Dynamics_Inverted Pendulum Example_related_to", "from": "Linearization of Dynamics", "to": "Inverted Pendulum Example", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Jensen's Inequality_Optimizing Q Distribution_subtopic", "from": "Jensen's Inequality", "to": "Optimizing Q Distribution", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Optimization Problem_Geometric Margin_related_to", "from": "Optimization Problem", "to": "Geometric Margin", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Density Transformation_ICA Algorithm_related_to", "from": "Density Transformation", "to": "ICA Algorithm", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Predict Step_Gaussian Distribution_has_subtopic", "from": "Predict Step", "to": "Gaussian Distribution", "arrows": "to", "title": "has_subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Data Scarcity_Leave-One-Out Cross Validation_related_to", "from": "Data Scarcity", "to": "Leave-One-Out Cross Validation", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Probability Estimation_Laplace Smoothing_subtopic", "from": "Probability Estimation", "to": "Laplace Smoothing", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Algorithms_Empirical Risk Minimization_depends_on", "from": "Machine Learning Algorithms", "to": "Empirical Risk Minimization", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Step 1: Estimate Matrices_LQR Algorithm Steps_subtopic", "from": "Step 1: Estimate Matrices", "to": "LQR Algorithm Steps", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Mini-batch SGD_Deep Learning Model Training Steps_related_to", "from": "Mini-batch SGD", "to": "Deep Learning Model Training Steps", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Dual Formulation of SVM_Lagrange Multipliers_depends_on", "from": "Dual Formulation of SVM", "to": "Lagrange Multipliers", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Activation Functions_ReLU Function_subtopic", "from": "Activation Functions", "to": "ReLU Function", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Reinforcement learning_Connections between Policy and Value Iteration (Optional)_has_subtopic", "from": "Reinforcement learning", "to": "Connections between Policy and Value Iteration (Optional)", "arrows": "to", "title": "has_subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Optimization Problems_Dual Problem_subtopic", "from": "Optimization Problems", "to": "Dual Problem", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Theory_Empirical Risk Minimization (ERM)_subtopic", "from": "Machine Learning Theory", "to": "Empirical Risk Minimization (ERM)", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Vector y_Least Squares Revisited_subtopic", "from": "Vector y", "to": "Least Squares Revisited", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Naive Bayes Algorithm_Training Set_depends_on", "from": "Naive Bayes Algorithm", "to": "Training Set", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_LQR Model Assumptions_Machine Learning Concepts_subtopic", "from": "LQR Model Assumptions", "to": "Machine Learning Concepts", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Algorithms_Stochastic Gradient Descent_related_to", "from": "Machine Learning Algorithms", "to": "Stochastic Gradient Descent", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Model Selection Methods_Sample Complexity Bounds_subtopic", "from": "Model Selection Methods", "to": "Sample Complexity Bounds", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Language Problem Methods_Machine Learning Adaptation Methods_related_to", "from": "Language Problem Methods", "to": "Machine Learning Adaptation Methods", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Labeled Task Dataset_Adaptation Phase_related_to", "from": "Labeled Task Dataset", "to": "Adaptation Phase", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Regularization_Overfitting_depends_on", "from": "Regularization", "to": "Overfitting", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Value Iteration and Policy Iteration_Machine Learning Overview_subtopic", "from": "Value Iteration and Policy Iteration", "to": "Machine Learning Overview", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Weight Matrices and Biases_Total Number of Parameters_related_to", "from": "Weight Matrices and Biases", "to": "Total Number of Parameters", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_M-step_EM Algorithm_subtopic", "from": "M-step", "to": "EM Algorithm", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Overview_Value Function Approximation_related_to", "from": "Machine Learning Overview", "to": "Value Function Approximation", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Optimal Value Function_Dynamic Programming_depends_on", "from": "Optimal Value Function", "to": "Dynamic Programming", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Concepts_Linear Regression_contains", "from": "Machine Learning Concepts", "to": "Linear Regression", "arrows": "to", "title": "contains", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Policy Gradients_Gradient Ascent_subtopic", "from": "Policy Gradients", "to": "Gradient Ascent", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_SIMCLR Algorithm_Loss Function_depends_on", "from": "SIMCLR Algorithm", "to": "Loss Function", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Leaky ReLU_Activation Functions_subtopic", "from": "Leaky ReLU", "to": "Activation Functions", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Concepts_Probability Distributions_related_to", "from": "Machine Learning Concepts", "to": "Probability Distributions", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Feature Maps_Deep Learning_subtopic", "from": "Feature Maps", "to": "Deep Learning", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Logistic Regression_Probability Calculation_subtopic", "from": "Logistic Regression", "to": "Probability Calculation", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Bias-Variance Trade-off Reconciliation_Machine Learning Literature_subtopic", "from": "Bias-Variance Trade-off Reconciliation", "to": "Machine Learning Literature", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Multi-class Classification_Softmax Function_depends_on", "from": "Multi-class Classification", "to": "Softmax Function", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Finite Horizon MDPs_Optimal Policy_related_to", "from": "Finite Horizon MDPs", "to": "Optimal Policy", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Log Partition Function for Bernoulli_Bernoulli Distribution_subtopic", "from": "Log Partition Function for Bernoulli", "to": "Bernoulli Distribution", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Fully Bayesian Prediction_Computational Challenges_depends_on", "from": "Fully Bayesian Prediction", "to": "Computational Challenges", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_GELU Function_Activation Functions_subtopic", "from": "GELU Function", "to": "Activation Functions", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Expectation Maximization (EM)_Evidence Lower Bound (ELBO)_subtopic", "from": "Expectation Maximization (EM)", "to": "Evidence Lower Bound (ELBO)", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Double Descent Phenomenon_Machine Learning Concepts_subtopic", "from": "Double Descent Phenomenon", "to": "Machine Learning Concepts", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Training Set_Posterior Distribution_depends_on", "from": "Training Set", "to": "Posterior Distribution", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Chain Rule_Gradient Computation_subtopic", "from": "Chain Rule", "to": "Gradient Computation", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Learning Theory_Machine Learning Concepts_subtopic", "from": "Learning Theory", "to": "Machine Learning Concepts", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Naive Bayes Classifier_Maximum Likelihood Estimation_subtopic", "from": "Naive Bayes Classifier", "to": "Maximum Likelihood Estimation", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Overfitting_Bias-Variance Tradeoff_depends_on", "from": "Overfitting", "to": "Bias-Variance Tradeoff", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_5th Degree Polynomial Models_Overfitting_subtopic", "from": "5th Degree Polynomial Models", "to": "Overfitting", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Concepts_Functional Margin_subtopic", "from": "Machine Learning Concepts", "to": "Functional Margin", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Models_Convolutional Layers_has_subtopic", "from": "Machine Learning Models", "to": "Convolutional Layers", "arrows": "to", "title": "has_subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Classification and Logistic Regression_Logistic Regression_subtopic", "from": "Classification and Logistic Regression", "to": "Logistic Regression", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Sequential Decision Making_Reinforcement Learning_subtopic", "from": "Sequential Decision Making", "to": "Reinforcement Learning", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Log Likelihood_Lower Bound Derivation_depends_on", "from": "Log Likelihood", "to": "Lower Bound Derivation", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Spam/Non-Spam Classification_Word Generation Process_depends_on", "from": "Spam/Non-Spam Classification", "to": "Word Generation Process", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Objective Function Primal_Value Primal Problem_subtopic", "from": "Objective Function Primal", "to": "Value Primal Problem", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Supervised Learning_Support Vector Machines_subtopic", "from": "Supervised Learning", "to": "Support Vector Machines", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_State Representation_Discretization_subtopic", "from": "State Representation", "to": "Discretization", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Training vs Test Distributions_Machine Learning Basics_subtopic", "from": "Training vs Test Distributions", "to": "Machine Learning Basics", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Finite State MDPs_Machine Learning Algorithms_related_to", "from": "Finite State MDPs", "to": "Machine Learning Algorithms", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_High-Dimensional Statistical Analysis_Machine Learning Literature_subtopic", "from": "High-Dimensional Statistical Analysis", "to": "Machine Learning Literature", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Training Set_Hypothesis Function_depends_on", "from": "Training Set", "to": "Hypothesis Function", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Adaptation Algorithm_Machine Learning Adaptation Methods_subtopic", "from": "Adaptation Algorithm", "to": "Machine Learning Adaptation Methods", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Maximum Likelihood Estimates_Laplace Smoothing_subtopic", "from": "Maximum Likelihood Estimates", "to": "Laplace Smoothing", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Markov Decision Process (MDP)_Policy Iteration_depends_on", "from": "Markov Decision Process (MDP)", "to": "Policy Iteration", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Linear Probe Approach_Adaptation Algorithm_subtopic", "from": "Linear Probe Approach", "to": "Adaptation Algorithm", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Concepts_Kernel Trick_subtopic", "from": "Machine Learning Concepts", "to": "Kernel Trick", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Probability Estimation_Naive Bayes Classifier_related_to", "from": "Probability Estimation", "to": "Naive Bayes Classifier", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Gaussian Discriminant Analysis (GDA)_Decision Boundaries_related_to", "from": "Gaussian Discriminant Analysis (GDA)", "to": "Decision Boundaries", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Succinct Representation of Distribution Qi_Optimizing Continuous Latent Variables_subtopic", "from": "Succinct Representation of Distribution Qi", "to": "Optimizing Continuous Latent Variables", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Feature Mapping_Kernels as Similarity Metrics_related_to", "from": "Feature Mapping", "to": "Kernels as Similarity Metrics", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Convergence_k-means Algorithm_subtopic", "from": "Convergence", "to": "k-means Algorithm", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Markov Decision Processes (MDP)_Reward Function_includes", "from": "Markov Decision Processes (MDP)", "to": "Reward Function", "arrows": "to", "title": "includes", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Stochastic Gradient Descent (SGD)_Deep Learning Model Training Steps_related_to", "from": "Stochastic Gradient Descent (SGD)", "to": "Deep Learning Model Training Steps", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Binary Classification_Logistic Function_depends_on", "from": "Binary Classification", "to": "Logistic Function", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Techniques_Cross Validation_depends_on", "from": "Machine Learning Techniques", "to": "Cross Validation", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Concepts_Kernel Methods_related_to", "from": "Machine Learning Concepts", "to": "Kernel Methods", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Backpropagation Algorithm_Chain Rule Application_subtopic", "from": "Backpropagation Algorithm", "to": "Chain Rule Application", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Neural Networks_Regression Problem_depends_on", "from": "Neural Networks", "to": "Regression Problem", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Gaussian Distribution Assumption_Mean and Variance Functions_subtopic", "from": "Gaussian Distribution Assumption", "to": "Mean and Variance Functions", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Optimization in RL_Machine Learning Concepts_depends_on", "from": "Optimization in RL", "to": "Machine Learning Concepts", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Concepts_Overfitting_related_to", "from": "Machine Learning Concepts", "to": "Overfitting", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Techniques_Independent Components Analysis (ICA)_related_to", "from": "Machine Learning Techniques", "to": "Independent Components Analysis (ICA)", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Optimization Problem_Linear Regression_depends_on", "from": "Optimization Problem", "to": "Linear Regression", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Concepts_Geometric Margin_subtopic", "from": "Machine Learning Concepts", "to": "Geometric Margin", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Hypothesis Function_Training Error_subtopic", "from": "Hypothesis Function", "to": "Training Error", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Optimal Parameters Alpha_Dual Problem Formulation_subtopic", "from": "Optimal Parameters Alpha", "to": "Dual Problem Formulation", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Algorithms_Gradient Ascent_subtopic", "from": "Machine Learning Algorithms", "to": "Gradient Ascent", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Mixture of Gaussians Model_Model Parameters_subtopic", "from": "Mixture of Gaussians Model", "to": "Model Parameters", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Logistic Regression_Gradient Calculation_depends_on", "from": "Logistic Regression", "to": "Gradient Calculation", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Car Example_Data Redundancy Detection_related_to", "from": "Car Example", "to": "Data Redundancy Detection", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Matrix Notation in Machine Learning_Vectorization_subtopic", "from": "Matrix Notation in Machine Learning", "to": "Vectorization", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Underfitting_Machine Learning Basics_subtopic", "from": "Underfitting", "to": "Machine Learning Basics", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Discrete Latent Variables_Mean Field Assumption_related_to", "from": "Discrete Latent Variables", "to": "Mean Field Assumption", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Chapter 9 Regularization and Model Selection_Regularization_subtopic", "from": "Chapter 9 Regularization and Model Selection", "to": "Regularization", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Discriminative Learning Algorithms_Perceptron Algorithm_related_to", "from": "Discriminative Learning Algorithms", "to": "Perceptron Algorithm", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Linear Regression_LMS Algorithm_subtopic", "from": "Linear Regression", "to": "LMS Algorithm", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Overview_Support Vector Machines (SVM)_related_to", "from": "Machine Learning Overview", "to": "Support Vector Machines (SVM)", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Multi-class Classification_Softmax Function_uses", "from": "Multi-class Classification", "to": "Softmax Function", "arrows": "to", "title": "uses", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Identity Function_Activation Functions_subtopic", "from": "Identity Function", "to": "Activation Functions", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Locally Weighted Linear Regression_Non-Parametric Algorithms_is_a", "from": "Locally Weighted Linear Regression", "to": "Non-Parametric Algorithms", "arrows": "to", "title": "is_a", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Supervised Learning_Kernel Methods_subtopic", "from": "Supervised Learning", "to": "Kernel Methods", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Identity Function_Activation Functions_depends_on", "from": "Identity Function", "to": "Activation Functions", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_State Transition Probabilities_Finite-State MDPs_related_to", "from": "State Transition Probabilities", "to": "Finite-State MDPs", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Principal Eigenvector_Lagrange Multipliers_depends_on", "from": "Principal Eigenvector", "to": "Lagrange Multipliers", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Back-propagation for MLPs_Gradient Computation_depends_on", "from": "Back-propagation for MLPs", "to": "Gradient Computation", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Markov Decision Process (MDP)_Value Iteration_depends_on", "from": "Markov Decision Process (MDP)", "to": "Value Iteration", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Parameter Estimation_Machine Learning Models_depends_on", "from": "Parameter Estimation", "to": "Machine Learning Models", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Estimator Simplification_Expectation Calculation_subtopic", "from": "Estimator Simplification", "to": "Expectation Calculation", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Backpropagation Algorithm_Gradient Computation_subtopic", "from": "Backpropagation Algorithm", "to": "Gradient Computation", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Optimization Problem_Optimal Margin Classifier_subtopic", "from": "Optimization Problem", "to": "Optimal Margin Classifier", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Linear Model Limitations_Underfitting_related_to", "from": "Linear Model Limitations", "to": "Underfitting", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Loss Function_Gradient Computation_depends_on", "from": "Loss Function", "to": "Gradient Computation", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Concepts_Backpropagation_related_to", "from": "Machine Learning Concepts", "to": "Backpropagation", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Support Vector Machines (SVMs)_SMO Algorithm_derives_from", "from": "Support Vector Machines (SVMs)", "to": "SMO Algorithm", "arrows": "to", "title": "derives_from", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Evidence Lower Bound (ELBO)_Gradient Computation_related_to", "from": "Evidence Lower Bound (ELBO)", "to": "Gradient Computation", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Implicit Bias Study_Machine Learning Literature_subtopic", "from": "Implicit Bias Study", "to": "Machine Learning Literature", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Machine Learning Algorithms_Gradient Descent_related_to", "from": "Machine Learning Algorithms", "to": "Gradient Descent", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Model Evaluation_Bias-Variance Tradeoff_related_to", "from": "Model Evaluation", "to": "Bias-Variance Tradeoff", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Value Function Approximation_Fitted Value Iteration_subtopic", "from": "Value Function Approximation", "to": "Fitted Value Iteration", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Principal Components_Dimensionality Reduction_related_to", "from": "Principal Components", "to": "Dimensionality Reduction", "arrows": "to", "title": "related_to", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Optimal Policy in MDPs_Machine Learning Overview_depends_on", "from": "Optimal Policy in MDPs", "to": "Machine Learning Overview", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Other Activation Functions_Multi-layer Fully-Connected Neural Networks_depends_on", "from": "Other Activation Functions", "to": "Multi-layer Fully-Connected Neural Networks", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Support Vector Machines_Regularization_depends_on", "from": "Support Vector Machines", "to": "Regularization", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Training Process_Cross-Entropy Loss_depends_on", "from": "Training Process", "to": "Cross-Entropy Loss", "arrows": "to", "title": "depends_on", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}, {"id": "other_Parameter Estimation_Maximum Likelihood Estimation (MLE)_subtopic", "from": "Parameter Estimation", "to": "Maximum Likelihood Estimation (MLE)", "arrows": "to", "title": "subtopic", "color": {"color": "#cccccc", "highlight": "#aaaaaa"}, "width": 1, "dashes": true, "is_hierarchical": false}]; // All non-hierarchical edges
        const detailsDivId = "nodeDetails";
        // ------------------------------------------

        const nodes = new vis.DataSet(nodesData);
        const hierarchicalEdges = new vis.DataSet(hierarchicalEdgesData);
        // Create an empty DataSet for dynamic edges initially
        const dynamicOtherEdges = new vis.DataSet([]);

        const container = document.getElementById('mynetwork');
        const detailsDiv = document.getElementById(detailsDivId);
        const loadingOverlay = document.getElementById('loadingOverlay');

        // Combine static hierarchical edges and dynamic other edges into a single DataSet for the network
        // We start with only hierarchical edges visible. Other edges are added dynamically on selection.
        const allEdges = new vis.DataSet(hierarchicalEdges.get());


        const data = {
            nodes: nodes,
            edges: allEdges // Use combined dataset (starts with hierarchical only)
        };

        const options = {
            layout: {
                hierarchical: {
                    enabled: true,
                    levelSeparation: 150,
                    nodeSpacing: 100,
                    treeSpacing: 200,
                    direction: 'UD', // Up-Down layout
                    sortMethod: 'directed' // Try 'hubsize' if UD doesn't look right
                }
            },
            physics: {
                enabled: true, // Enable physics initially for hierarchical layout
                hierarchicalRepulsion: { // Physics specifically for hierarchical
                    centralGravity: 0.0,
                    springLength: 100,
                    springConstant: 0.01,
                    nodeDistance: 120,
                    damping: 0.09
                },
                minVelocity: 0.75,
                solver: 'hierarchicalRepulsion' // Use the specific solver
            },
            interaction: {
                 hover: true, // Show tooltips on hover
                 tooltipDelay: 300
            },
            nodes: {
                borderWidth: 1,
                borderWidthSelected: 2
            },
            edges: {
                smooth: { // Smoother curves for hierarchical
                    type: "cubicBezier",
                    forceDirection: "vertical",
                    roundness: 0.4
                }
            }
        };

        const network = new vis.Network(container, data, options);
        let currentlyShownOtherEdges = []; // Keep track of dynamically added edges

        // --- Event Listeners ---

        // Hide loading overlay and optionally disable physics after stabilization
        network.on("stabilizationIterationsDone", function () {
            loadingOverlay.style.display = 'none'; // Hide loading overlay
            // Optional: Disable physics for smoother interaction after layout
            // network.setOptions({ physics: false });
            // console.log("Physics disabled after stabilization.");
        });

        // Show details and related edges on node selection
        network.on("selectNode", function (params) {
            if (params.nodes.length > 0) {
                const selectedNodeId = params.nodes[0];
                const nodeData = nodes.get(selectedNodeId); // Get full node data

                // Display description
                if (nodeData && detailsDiv) {
                    detailsDiv.innerHTML = `
                        <h3>${nodeData.label} Details</h3>
                        <p><b>Type:</b> ${nodeData.node_type || 'unknown'}</p>
                        <p><b>Description:</b><br>${nodeData.description || 'N/A'}</p>
                    `;
                } else {
                     detailsDiv.innerHTML = "<h3>Node Details</h3><p>Details not found.</p>";
                }

                // Clear previously shown 'other' edges from the dynamic DataSet
                if (currentlyShownOtherEdges.length > 0) {
                    // console.log("Removing other edges:", currentlyShownOtherEdges);
                    dynamicOtherEdges.remove(currentlyShownOtherEdges); // Remove from dynamic set
                    allEdges.remove(currentlyShownOtherEdges); // Also remove from the main network edges set
                    currentlyShownOtherEdges = [];
                }

                // Find relevant 'other' edges from the pre-processed list
                const relevantEdges = otherEdgesData.filter(edge => edge.from === selectedNodeId || edge.to === selectedNodeId);
                if (relevantEdges.length > 0) {
                    // console.log("Adding other edges:", relevantEdges);
                    dynamicOtherEdges.add(relevantEdges); // Add to dynamic set (for tracking)
                    allEdges.add(relevantEdges); // Add to the main network edges set to make them visible
                    currentlyShownOtherEdges = relevantEdges.map(edge => edge.id); // Store IDs to remove later
                    detailsDiv.innerHTML += `<p><b>Other Relationships Shown (${relevantEdges.length})</b></p>`;
                }

            }
        });

        // Clear details and temporary edges when deselecting
         network.on("deselectNode", function (params) {
             detailsDiv.innerHTML = "<h3>Node Details</h3><p>Select a node to see details.</p>";
             // Clear previously shown 'other' edges
             if (currentlyShownOtherEdges.length > 0) {
                //  console.log("Removing other edges on deselect:", currentlyShownOtherEdges);
                 dynamicOtherEdges.remove(currentlyShownOtherEdges); // Remove from dynamic set
                 allEdges.remove(currentlyShownOtherEdges); // Also remove from the main network edges set
                 currentlyShownOtherEdges = [];
             }
         });

         // Optional: Also clear on background click
         network.on("click", function (params) {
             if (params.nodes.length === 0 && params.edges.length === 0) { // Clicked on background
                 detailsDiv.innerHTML = "<h3>Node Details</h3><p>Select a node to see details.</p>";
                 if (currentlyShownOtherEdges.length > 0) {
                    //  console.log("Removing other edges on background click:", currentlyShownOtherEdges);
                     dynamicOtherEdges.remove(currentlyShownOtherEdges); // Remove from dynamic set
                     allEdges.remove(currentlyShownOtherEdges); // Also remove from the main network edges set
                     currentlyShownOtherEdges = [];
                 }
             }
         });

    </script>
</body>
</html>
